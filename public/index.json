[{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cost_journey/1_configure_services/","title":"Configure Services","tags":[],"description":"","content":"Create S3 Bucket Create a single S3 bucket that will contain the journey files for all workloads in that account.\n  Log into the console via SSO, go to the S3 service page\n  Click Create bucket\n  Enter a Bucket name starting with cost (we have used cost-wa-reports, you will need to use a unique bucket name) and click Create bucket:   Upload the following object into the bucket. Code/cost_journey.csv   You can edit this CSV file to customize your journey for your organization. The definitions used within this file are at the end of this lab in the tear down step.\n You have now setup the S3 bucket which contains your organizations journey configuration, all the journeys for the workloads.\n Create the Lambda Function   Go to the Lambda Console\n  Click Create function\n  Select Author from scratch\n  Enter a function name of Cost_W-A_Journey\n  Select a runtime of Python 3.6, this is a specifically required version\n  Under Permissions:\n   Execution role: Create a new role from AWS policy templates Role name: extract-wa-reports_role   Click Create function:   Select the lambda_function.py and paste the following code:\n  Lambda function code   import boto3 import json import os import logging import urllib.parse logger = logging.getLogger() logger.setLevel(logging.INFO) s3_bucket = os.environ['S3_BUCKET'] s3_key = os.environ['S3_KEY'] Image_XSize = os.environ['Image_XSize'] Image_YSize = os.environ['Image_YSize'] ################# # Boto3 Clients # ################# wa_client = boto3.client('wellarchitected') s3_client = boto3.client('s3') ############## # Parameters # ############## # The maximum number of results the API can return in a list workloads call. list_workloads_max_results_maximum = 50 # The maximum number of results the API can return in a list answers call. list_answers_max_results_maximum = 50 # The maximum number of results the API can return in a list milestones call. list_milestone_max_results_maximum = 50 def get_all_workloads(): # Get a list of all workloads list_workloads_result = wa_client.list_workloads(MaxResults=list_workloads_max_results_maximum) logger.info(f'Found {len(list_workloads_result)} Well-Archtected workloads.') workloads_all = list_workloads_result['WorkloadSummaries'] while 'NextToken' in list_workloads_result: next_token = list_workloads_result['NextToken'] list_workloads_result = wa_client.list_workloads( MaxResults=list_workloads_max_results_maximum, NextToken=next_token ) workloads_all += list_workloads_result['WorkloadSummaries'] return (workloads_all) def get_milestones(workload_id): # Get latest milestone review date milestones = wa_client.list_milestones( WorkloadId=workload_id, MaxResults=list_milestone_max_results_maximum )['MilestoneSummaries'] # If workload has milestone get them. logger.info(f'Workload {workload_id} has {len(milestones)} milestones.') if milestones: for milestone in milestones: milestone['RecordedAt'] = milestone['RecordedAt'].isoformat() return milestones def get_lens(workload_id): # Which lenses have been activated for this workload lens_reviews_result = wa_client.list_lens_reviews( WorkloadId=workload_id )['LensReviewSummaries'] # An array to hold the reviews we specifically want, in this case its those that used the 'wellarchitected' lens lens_reviews = [] # Go through each lens review \u0026amp; look for W-A lenses of the right version for lens_review in lens_reviews_result: alias = lens_review['LensAlias'] version = lens_review['LensVersion'] # Add W-A lenses from the right version only to the array if (\u0026quot;wellarchitected\u0026quot; in alias) and (\u0026quot;2020-07-02\u0026quot; in version): lens_reviews.append(lens_review) # return the selected lens reviews logger.info(f'Workload {workload_id} has used {len(lens_reviews)} lens') return lens_reviews def get_lens_answers(workload_id, lens_reviews): # Loop through each activated lens list_answers_result = [] for lens in lens_reviews: lens_name = lens['LensName'] logger.info(f'Looking at {lens_name} answers for Workload {workload_id}') # Get All answers for the lens list_answers_reponse = wa_client.list_answers( WorkloadId=workload_id, LensAlias=lens['LensAlias'], MaxResults=list_answers_max_results_maximum ) # An array to hold the answers that were 'selected' in the review cost_answers = [] # Flatten the answer result to include LensAlias and Milestone Number for answer_result in list_answers_reponse['AnswerSummaries']: pillarid = answer_result['PillarId'] # If its a cost answer/Best practice that was selected, then store it if \u0026quot;costOptimization\u0026quot; in pillarid: # Get the list of selected answers/best practices answers = answer_result['SelectedChoices'] # Go through each selected answer for answer in answers: # Remove answers with '_no', as they are the \u0026quot;none of these\u0026quot; answers if \u0026quot;_no\u0026quot; not in answer: # Add all selected answers to the array cost_answers.append(answer) # Return all selected cost answers/best practices return cost_answers def get_journey(): # Get the cost journey information from S3 response = s3_client.get_object( Bucket=s3_bucket, Key='cost_journey.csv' ) data = [] # Array of dicts/maps, that have all the best practices \u0026amp; their journey attributes from the journey file best_practices = [] # Get the file contents \u0026amp; split it line by line, remove trailing formatting characters data = response['Body'].read().decode('utf-8') lines = data.split('\\r\\n') # Go through each line in the journey file for line in lines: # Only get lines with \u0026quot;cost\u0026quot;, which ignores the header line if \u0026quot;cost\u0026quot; in line: # Break the line up by ',', as its a CSV line_attributes = line.split(',') # Put each element into a dict/map best_practice_attributes = {\u0026quot;id\u0026quot;: line_attributes[0], \u0026quot;name\u0026quot;: line_attributes[1], \u0026quot;risk\u0026quot;: line_attributes[2], \u0026quot;phase\u0026quot;: int(line_attributes[3]), \u0026quot;order\u0026quot;: int(line_attributes[4]), \u0026quot;effort\u0026quot;: int(line_attributes[5]), \u0026quot;duration\u0026quot;: int(line_attributes[6]), \u0026quot;frequency\u0026quot;: line_attributes[7]} # Add the map into an array best_practices.append(best_practice_attributes) # Find out how big each phase is, so we can scale the image accordingly ph1_sum = 0 ph2_sum = 0 ph3_sum = 0 ph4_sum = 0 # The size of each phase is the effort of all best practices + the duration between them # Go through each best practice for best_practice in best_practices: # If its a cost BP, ignore the header or any other non-cost BPs if \u0026quot;cost\u0026quot; in best_practice['id']: # Get the phase of the best prac incase they are not ordered in the file phase = best_practice['phase'] # Add the effort \u0026amp; duration to each phases total if phase == 1: ph1_sum = ph1_sum + best_practice['effort'] + best_practice['duration'] elif phase == 2: ph2_sum = ph2_sum + best_practice['effort'] + best_practice['duration'] elif phase == 3: ph3_sum = ph3_sum + best_practice['effort'] + best_practice['duration'] elif phase == 4: ph4_sum = ph4_sum + best_practice['effort'] + best_practice['duration'] # Put the scale factor of each phase into an array # The phases are staggered, ph1: x=0, ph2 = x=150, ph3 = x=300, ph4 = x=450 # The scale is (total image size - stagger) / phase_size. So that each phase could be scaled to fit the Image_XSize specified phase_scales = [(int(Image_XSize) - 0) / int(ph1_sum), (int(Image_XSize) - 150) / int(ph2_sum), (int(Image_XSize) - 300) / int(ph3_sum), (int(Image_XSize) - 450) / int(ph4_sum)] # Scale all phases by the smallest overall factor - which is the largest phase x_scale_factor = min(phase_scales) # Return the image scale factor \u0026amp; the answers/best practices from the journey file return x_scale_factor, best_practices def draw_journey(scale_factor, best_practices, report_answers): # Current cursor positions, come in \u0026amp; down a little from the top left current_xpos = 10 current_ypos = 10*scale_factor + 10 # Create html file headers \u0026amp; part of the body wa_journey = \u0026quot;\u0026lt;HTML xmlns=\\\u0026quot;http://www.w3.org/1999/xhtml\\\u0026quot;\u0026gt;\\n\u0026lt;HEAD\u0026gt;\\n\u0026lt;meta charset=\\\u0026quot;utf-8\\\u0026quot;\u0026gt;\u0026lt;/meta\u0026gt;\\n\\t\u0026lt;TITLE\u0026gt;Journey\u0026lt;/TITLE\u0026gt;\\n\u0026lt;/HEAD\u0026gt;\\n\\n\u0026quot; wa_journey = wa_journey + \u0026quot;\u0026lt;BODY\u0026gt;\\n\\t\u0026lt;p\u0026gt;Here is the journey\u0026lt;/p\u0026gt;\\n\\t\\t\u0026lt;svg xmlns=\\\u0026quot;http://www.w3.org/2000/svg\\\u0026quot; width=\\\u0026quot;1080\\\u0026quot; height=\\\u0026quot;1920\\\u0026quot;\u0026gt;\\n\u0026quot; wa_journey = wa_journey + \u0026quot;\\t\\t\u0026lt;svg width=\\\u0026quot;\u0026quot; + str(Image_XSize) + \u0026quot;\\\u0026quot; height=\\\u0026quot;\u0026quot; + str(Image_YSize) + \u0026quot;\\\u0026quot;\u0026gt;\\n\u0026quot; # loop counter \u0026amp; tracking of the current phase so we can drop down on the next phase x = 1 current_phase = 1 # Go through each best practice while x \u0026lt;= len(best_practices): # Get the best practices in order current_bestprac = dict(list(filter(lambda best_practice: best_practice['order'] == x, best_practices))[0]) # If its a new phase move down \u0026amp; across in the image, otherwise hold steady if current_bestprac['phase'] == current_phase: current_ypos = current_ypos else: # Move across by 150 from the prevoius phase to stagger the start current_xpos = current_phase * 150 # Move down by 10 (maximum effort/diameter) x scale factor, + a buffer of 10px current_ypos = current_ypos + 10*scale_factor + 10 # We're on a new phase current_phase = current_phase + 1 # Move to the center of the next circle, which is adding the duration between and half effort (radius) from previous point current_xpos = current_xpos + (current_bestprac['duration'])/2*scale_factor + (current_bestprac['effort'])/2*scale_factor # Check if it was a selected best practice/answer, if so colour it green if current_bestprac['id'] in report_answers: # print Green wa_journey = wa_journey + \u0026quot;\u0026lt;circle cx=\\\u0026quot;\u0026quot; + str(current_xpos) + \u0026quot;\\\u0026quot; cy=\\\u0026quot;\u0026quot; + str(current_ypos) + \u0026quot;\\\u0026quot; r=\\\u0026quot;\u0026quot; + str(current_bestprac[\u0026quot;effort\u0026quot;]/2 * scale_factor) + \u0026quot;\\\u0026quot; stroke=\\\u0026quot;black\\\u0026quot; stroke-width=\\\u0026quot;\u0026quot; + str(current_bestprac['frequency']) + \u0026quot;\\\u0026quot; fill=\\\u0026quot;green\\\u0026quot;\u0026gt;\\n\u0026quot; wa_journey = wa_journey + \u0026quot;\u0026lt;title\u0026gt;\u0026quot; + current_bestprac['name'] + \u0026quot;\u0026lt;/title\u0026gt;\\n\u0026quot; wa_journey = wa_journey + \u0026quot;\u0026lt;/circle\u0026gt;\\n\u0026quot; # It wasnt selected so print red for high risk or blue otherwise else: # print red if high risk if current_bestprac['risk'] == \u0026quot;High\u0026quot;: wa_journey = wa_journey + \u0026quot;\u0026lt;circle cx=\\\u0026quot;\u0026quot; + str(current_xpos) + \u0026quot;\\\u0026quot; cy=\\\u0026quot;\u0026quot; + str(current_ypos) + \u0026quot;\\\u0026quot; r=\\\u0026quot;\u0026quot; + str(current_bestprac[\u0026quot;effort\u0026quot;]/2 * scale_factor) + \u0026quot;\\\u0026quot; stroke=\\\u0026quot;black\\\u0026quot; stroke-width=\\\u0026quot;\u0026quot; + str(current_bestprac['frequency']) + \u0026quot;\\\u0026quot; fill=\\\u0026quot;red\\\u0026quot;\u0026gt;\\n\u0026quot; wa_journey = wa_journey + \u0026quot;\u0026lt;title\u0026gt;\u0026quot; + current_bestprac['name'] + \u0026quot;\u0026lt;/title\u0026gt;\\n\u0026quot; wa_journey = wa_journey + \u0026quot;\u0026lt;/circle\u0026gt;\\n\u0026quot; # print blue otherwise else: wa_journey = wa_journey + \u0026quot;\u0026lt;circle cx=\\\u0026quot;\u0026quot; + str(current_xpos) + \u0026quot;\\\u0026quot; cy=\\\u0026quot;\u0026quot; + str(current_ypos) + \u0026quot;\\\u0026quot; r=\\\u0026quot;\u0026quot; + str(current_bestprac[\u0026quot;effort\u0026quot;]/2 * scale_factor) + \u0026quot;\\\u0026quot; stroke=\\\u0026quot;black\\\u0026quot; stroke-width=\\\u0026quot;\u0026quot; + str(current_bestprac['frequency']) + \u0026quot;\\\u0026quot; fill=\\\u0026quot;blue\\\u0026quot;\u0026gt;\\n\u0026quot; wa_journey = wa_journey + \u0026quot;\u0026lt;title\u0026gt;\u0026quot; + current_bestprac['name'] + \u0026quot;\u0026lt;/title\u0026gt;n\u0026quot; wa_journey = wa_journey + \u0026quot;\u0026lt;/circle\u0026gt;\\n\u0026quot; # Move to the end of the current circle, add half the effort current_xpos = current_xpos + (current_bestprac['effort'])/2*scale_factor # Onto the next best practice x = x + 1 # Add the trailing HTML wa_journey = wa_journey + \u0026quot;\u0026lt;script type=\\\u0026quot;text/javascript\\\u0026quot;\u0026gt;\u0026lt;![CDATA[\\n\\t(function() {\\n\\t\\tvar tooltip = document.getElementById('tooltip');\\n\\t\\t})();\\n\\t]\u0026gt;\\n\\n\\t\\tvar triggers = document.getElementsByClassName('tooltip-trigger');\\n\\tfor (var i = 0; i \u0026lt; triggers.length; i++) {\\n\\ttriggers[i].addEventListener('mousemove', showTooltip);\\n\\t\\ttriggers[i].addEventListener('mouseout', hideTooltip);\\n\\t}\\n\\n\\tfunction showTooltip(evt) {\\n\\t\\ttooltip.setAttributeNS(null, \\\u0026quot;visibility\\\u0026quot;, \\\u0026quot;visible\\\u0026quot;);\\n\\t}\\n\\tfunction hideTooltip() {\\n\\t\\ttooltip.setAttributeNS(null, \\\u0026quot;visibility\\\u0026quot;, \\\u0026quot;hidden\\\u0026quot;);\\n\\t}\u0026quot; wa_journey = wa_journey + \u0026quot;\u0026lt;/SVG\u0026gt;\\n\u0026lt;/BODY\u0026gt;\\n\u0026lt;/HTML\u0026gt;\\n\u0026quot; # Return the html text of the journey return wa_journey def lambda_handler(event, context): workloads_all = get_all_workloads() # Generate workload JSON file logger.info(f'Generate JSON object for each workload.') # Text to build the HTML index file of all workloads generated_workloads = \u0026quot;\u0026lt;HTML\u0026gt;\\n\u0026lt;HEAD\u0026gt;\\n\u0026lt;/HEAD\u0026gt;\\n\u0026lt;BODY\u0026gt;\\n\u0026quot; for workload in workloads_all: # Get workload info from WAR Tool API, workload_id = workload['WorkloadId'] workload_name = workload['WorkloadName'] milestones = get_milestones(workload_id) lens_reviews = get_lens(workload_id) if len(lens_reviews) \u0026gt; 0: list_answers_result = get_lens_answers(workload_id, lens_reviews) # Build JSON of workload data workload_report_data = {} # Get the answers from the W-A report workload_report_data['report_answers'] = list_answers_result # Get the scale factor \u0026amp; best practices from the journey file scale_factor, best_practices = get_journey() # Create the Journey image \u0026amp; HTML file te journey_image = draw_journey(scale_factor, best_practices, workload_report_data['report_answers']) # Write to S3 journey_file_name = workload_name + '_' + workload_id + '.html' html_link = \u0026quot;https://\u0026quot; + s3_bucket + '.s3.amazonaws.com/' + s3_key + '/' + urllib.parse.quote_plus(journey_file_name) s3_client.put_object( Body=journey_image, Bucket=s3_bucket, Key=f'{s3_key}/{journey_file_name}' ) # Add the next workload file to the index file generated_workloads = generated_workloads + \u0026quot;\u0026lt;A href=\\\u0026quot;\u0026quot; + html_link + \u0026quot;\\\u0026quot;\u0026gt;\u0026quot; + workload_name + \u0026quot;\u0026lt;/A\u0026gt;\u0026lt;BR\u0026gt;\\n\u0026quot; # Add the closing HTML in the index file generated_workloads = generated_workloads + \u0026quot;\u0026lt;/HTML\u0026gt;\u0026quot; # Write the index file to the S3 bucket file_name = \u0026quot;W-A Workload Journeys.html\u0026quot; s3_client.put_object( Body=generated_workloads, Bucket=s3_bucket, Key=f'{file_name}' )      Above the code click Configuration, select Environment variables   Click Edit and add the following variables, then click Save\n   Image_XSize - 1440 Image_YSize - 900 S3_BUCKET - (the name of your bucket created previously) S3_KEY - WorkloadReports   Select General configuration, click Edit, change the timeout to 1min, click Save  You have now created the lambda function, however we need to add permissions before its run\n Create the IAM Role Modify the IAM role that is used by the Lambda function, to allow access to your S3 bucket and your Well-Architected reviews.\n  Go to the IAM Console\n  Go to Roles and select the extract-wa-reports_role role\n  Add an inline policy\n  Modify the policy below replacing \u0026lt;S3_BUCKET_NAME\u0026gt;, and paste it into the json:\n { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Action\u0026quot;: [ \u0026quot;s3:GetObject\u0026quot;, \u0026quot;s3:PutObject\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:s3:::\u0026lt;S3_BUCKET_NAME\u0026gt;*\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot; } ] }    Click Review policy, enter a name of WAReportAccess click Create policy\n  Click Attach policies, and attach the WellArchitectedConsoleReadOnlyAccess\n  You have now added the required permissions and all configuration is complete.\n  function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/2_create_journey\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_testing_backup_and_restore_of_data/1_prerequisite/","title":"Deploy the Infrastructure","tags":[],"description":"","content":"For many organizations, the data that they possess is one of the most valuable assets they have. Backing up data frequently is of vital importance for the long lasting success of any organization. However, a backup of data is only valuable if data can be recovered/restored from the backup. In the cloud, backing up data and testing the restore is easier compared to on-premises datacenters. Automating this process with appropriate notification systems will ensure that an organization\u0026rsquo;s data is backed up frequently, the backups are tested to ensure expected recovery, and appropriate people are notified in case of failures.\nYou will use AWS CloudFormation to provision some resources needed for this lab. As part of this lab, the CloudFormation stack that you provision will create an EC2 Instance, an SNS Topic, and a Lambda Function. You can view the CloudFormation template here for a complete list of all resources that are provisioned. This lab will only work in us-east-1.\n1.1 Log into the AWS console If you are attending an in-person workshop and were provided with an AWS account by the instructor:\n  Click here for instructions to access your assigned AWS account:     Go to https://dashboard.eventengine.run/login\n  Enter the 12 character hashcode you were provided and click \u0026ldquo;Proceed\u0026rdquo;   [optional] assign a name to your account (this is referred to as \u0026ldquo;Team name\u0026rdquo;)\n click \u0026ldquo;Set Team Name\u0026rdquo; Enter a name and click \u0026ldquo;Set Team Name\u0026rdquo;    Click \u0026ldquo;AWS Console\u0026rdquo;\n  AWS credentials   IMPORTANT Copy the provided credentials and save them. You wil need these to complete the workshop   Copy the whole code block corresponding to the system you are using.\n  Access the AWS console  Click \u0026ldquo;Open Console\u0026rdquo;. The AWS Console will open.     If you are using your own AWS account:   Click here for instructions to use your own AWS account:    Sign in to the AWS Management Console as an IAM user who has PowerUserAccess or AdministratorAccess permissions, to ensure successful execution of this lab.   \n1.2 Deploy the infrastructure using AWS CloudFormation Click here to deploy the stack.\nUnder PARAMETERS:\n Select an AvailabilityZone to launch the resources in. For LatestAmiId leave the default value. This will automatically retrieve the latest AMI ID for Amazon Linux 2. Specify an email address that you have access to for NotificationEmail.  Check the box I acknowledge that AWS CloudFormation might create IAM resources.\nClick CREATE / CREATE STACK.\nMANUAL STEPS\n Use your administrator account to access the CloudFormation console - https://console.aws.amazon.com/cloudformation/. Click on CREATE STACK. Under PREREQUISITE - PREPARE TEMPLATE, select the option TEMPLATE IS READY. Under SPECIFY TEMPLATE, select the option AMAZON S3 URL, enter the link - https://aws-well-architected-labs-virginia.s3.amazonaws.com/Reliability/200_Testing_Backup_and_Restore_of_Data/backup-lab.yaml and click NEXT. Enter a STACK NAME such as WA-Backup-Lab. Select an AvailabilityZone to launch the resources in. For LatestAmiId leave the default value. This will automatically retrieve the latest AMI ID for Amazon Linux 2. For NotificationEmail, specify an email address that you have access to. Leave default values for the rest of the fields and click NEXT. No changes are needed on the CONFIGURE STACK OPTIONS page, click NEXT. Review the details of the stack, scroll down to CAPABILITIES, and check the box next to I acknowledge that AWS CloudFormation might create IAM resources. Click CREATE STACK.   Note: Once stack creation starts, monitor the email address you entered. You should receive an email from SNS with the subject AWS Notification - Subscription Confirmation. Click on the link Confirm subscription to confirm the subscription of your email to the SNS Topic.\n The stack takes about 2 minutes to create all the resources. Periodically refresh the page until you see that the STACK STATUS is in CREATE_COMPLETE. Once the stack is in CREATE_COMPLETE, visit the OUTPUTS section for the stack and note down the KEY and VALUE for each of the outputs. This information will be used later in the lab.\nYou can view the simple application running on the instance by visiting the URL specified in the outputs. If you get an error that says \u0026ldquo;Connection refused\u0026rdquo;, wait a couple of minutes and try again.\n function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/2_configure_backup_plan\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_pricing_model_analysis/1_pricing_sources/","title":"Create Pricing Data Sources","tags":[],"description":"","content":"Create S3 Bucket and Folders Create a single S3 bucket that contains two folders - od_pricedata and sp_pricedata, these will contain the on-demand pricing data and the Savings Plans pricing data.\n  Log into the console via SSO, go to the S3 service page:   Click Create bucket:\n  Enter a Bucket name starting with cost (we have used cost-sptool-pricingfiles, you will need to use a unique bucket name) and click Create bucket:   Click on the (bucket name):   Click Create folder:   Enter a folder name of od_pricedata, click Save:   Click Create folder:   Enter a folder name of sp_pricedata, click Save:   You have now setup the S3 bucket with the two folders that will contain the OnDemand and Savings Plans pricing data. Create IAM Role and Policies   Go to the IAM Console\n  Select Policies and Create Policy   Edit the following policy and replace (S3 pricing bucket) with your bucket name, on the JSON tab, enter the following policy, click Review policy:\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;S3SPTool\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;s3:PutObject\u0026quot;, \u0026quot;s3:DeleteObjectVersion\u0026quot;, \u0026quot;s3:DeleteObject\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:s3:::(S3 pricing bucket)/*\u0026quot; } ] }    Policy Name S3PricingLambda, Description Access to S3 for Lambda SPTool function, click Create policy:   Select Roles, click Create role:   Select Lambda, click Next: Permissions:   Select the S3PricingLambda policy, click Next: Tags:   Click Next: Review\n  Role name SPToolS3Lambda, click Create role:   Setup On-Demand Pricing Lambda Function Create the On-Demand Lambda function to get the pricing information, and extract the required parts from it.\n  Go to the Lambda service page:   Click Create function:   Enter the following details:\n Select: Author from scratch Function name: Cost_SPTool_ODPricing_Download Runtime: Python (Latest) Execution Role: Use an existing role Role name: SPToolS3Lambda    Click Create function   Copy and paste the following code into the Function code section:\n# Lambda Function Code - SPTool_OD_pricing_Download # Function to download OnDemand pricing, get out the required lines \u0026amp; upload it to S3 as a zipped file # It will find 'OnDemand' and 'Compute Instance', and write to a file # Written by natbesh@amazon.com # Please reachout to costoptimization@amazon.com if there's any comments or suggestions import boto3 import gzip import urllib3 def lambda_handler(event, context): # Create the connection http = urllib3.PoolManager() try: # Get the EC2 OnDemand pricing file, its huge \u0026gt;1GB r = http.request('GET', 'https://pricing.us-east-1.amazonaws.com/offers/v1.0/aws/AmazonEC2/current/index.csv') # Put the response data into a variable \u0026amp; split it into an array line by line plaintext_content = r.data plaintext_lines = plaintext_content.splitlines() # Varaible to hold the OnDemand pricing data pricing_output = \u0026quot;\u0026quot; # Go through each of the pricing lines to find the ones we need for line in plaintext_lines: # If the line contains 'OnDemand' or 'Compute Instance' then add it to the output string if ((str(line).find('OnDemand') != -1) and (str(line).find('RunInstances') != -1)): pricing_output += str(line.decode(\u0026quot;utf-8\u0026quot;)).replace('\u0026quot;', '') pricing_output += \u0026quot;\\n\u0026quot; # Add the output to a local temporary file \u0026amp; zip it with gzip.open('/tmp/od_pricedata.txt.gz', 'wb') as f: f.write(pricing_output.encode()) # Upload the zipped file to S3 s3 = boto3.resource('s3') # Specify the local file, the bucket, and the folder and object name - you MUST have a folder and object name s3.meta.client.upload_file('/tmp/od_pricedata.txt.gz', 'bucket_name', 'od_pricedata/od_pricedata.txt.gz') # Die if you cant get the pricing file except Exception as e: print(e) raise e # Return happy return { 'statusCode': 200 }    Edit the pasted code, replacing bucket_name with the name of your bucket:   Click Deploy above the code\n  Scroll down and edit Basic settings:\n Memory: 4096MB Timeout: 2min Click save     Scroll to the top and click Test   Enter an Event name of Test, click Create:   Click Test:   The function will run, it will take a minute or two given the size of the pricing files and processing required, then return success. Click Details and verify there is headroom in the configured resources and duration to allow any increases in pricing file size over time:   One of the most common reasons for this lab failing is the pricing files growing in size \u0026amp; Lambda times out. We have configured a balance between cost/performance with 4Gb memory, if the lab fails at some point - ensure the lambda has enough memory and passes this test.\n Go to your S3 bucket and into the od_pricedata folder and you should see a gz file of non-zero size is in it:   Setup Savings Plan Pricing Lambda Function Create the Savings Plan Lambda function to get the pricing information, and extract the required parts from it.\n  Go to the Lambda service page:   Click Create function:   Enter the following details:\n Select: Author from scratch Function name: Cost_SPTool_SPPricing_Download Runtime: Python (Latest) Execution Role: Use an existing role Role name: SPToolS3Lambda    Click Create function   Copy and paste the following code into the Function code section:\n# Lambda Function Code - SPTool_SP_pricing_Download # Function to download SavingsPlans pricing, get out the required lines \u0026amp; upload it to S3 as a zipped file # It will get each regions pricing file in CSV, find 'Usage' and '1yr', and write to a file # Written by natbesh@amazon.com # Please reachout to costoptimization@amazon.com if there's any comments or suggestions import boto3 import gzip import urllib3 import json def lambda_handler(event, context): # Create the connection http = urllib3.PoolManager() try: # Get the SavingsPlans pricing index file, so you can get all the region files, which have the pricing in them r = http.request('GET', 'https://pricing.us-east-1.amazonaws.com/savingsPlan/v1.0/aws/AWSComputeSavingsPlan/current/region_index.json') # Load the json file into a variable, and parse it sp_regions = r.data sp_regions_json = (json.loads(sp_regions)) # Variable to hold all of the pricing data, its large at over 150MB sp_pricing_data = \u0026quot;\u0026quot; # Cycle through each regions pricing file, to get the data we need for region in sp_regions_json['regions']: # Get the CSV URL url = \u0026quot;https://pricing.us-east-1.amazonaws.com\u0026quot; + region['versionUrl'] url = url.replace('.json', '.csv') # Create a connection \u0026amp; get the regions pricing data CSV file http = urllib3.PoolManager() r = http.request('GET', url) spregion_content = r.data # Split the lines into an array spregion_lines = spregion_content.splitlines() # Go through each of the pricing lines for line in spregion_lines: # If the line has 'Usage' then grab it for pricing data, exclude all others if (str(line).find('Usage') != -1): sp_pricing_data += str(line.decode(\u0026quot;utf-8\u0026quot;)) sp_pricing_data += \u0026quot;\\n\u0026quot; # Compress the text into a local temporary file with gzip.open('/tmp/sp_pricedata.txt.gz', 'wb') as f: f.write(sp_pricing_data.encode()) # Upload the file to S3 s3 = boto3.resource('s3') # Specify the local file, the bucket, and the folder and object name - you MUST have a folder and object name s3.meta.client.upload_file('/tmp/sp_pricedata.txt.gz', 'bucket_name', 'sp_pricedata/sp_pricedata.txt.gz') # Die if you cant get the file except Exception as e: print(e) raise e # Return happy return { 'statusCode': 200 }    Edit the pasted code, replacing bucket_name with the name of your bucket:   Click Deploy above the code\n  Edit Basic settings below:\n Memory: 2048MB Timeout: 2min Click save     Scroll to the top and click Test   Enter an Event name of Test, click Create:   Click Test:   The function will run, it will take a minute or two given the size of the pricing files and processing required, then return success. Click Details and verify there is headroom in the configured resources and duration to allow any increases in pricing file size over time:   One of the most common reasons for this lab failing is the pricing files growing in size \u0026amp; Lambda times out. We have configured a balance between cost/performance with 4Gb memory, if the lab fails at some point - ensure the lambda has enough memory and passes this test.\n Go to your S3 bucket and into the sp_pricedata folder and you should see a gz file of non-zero size is in it:   CloudWatch Events Setup We will setup a CloudWatch Event to periodically run the Lambda functions, this will update the pricing and include any newly released instances.\n  Go to the CloudWatch service page:   Click on Events, then click Rules:   Click Create rule   For the Event Source select Schedule and set the required period, we have selected 5 days, click Add target:   Add the SPTool_ODPricing_Download Lambda function, and click Add target:   Add the SPTool_SPPricing_Download Lambda function, and click Configure details:   Add the name SPTool-pricing, optionally add a description and click Create rule:   You have now successfully configured a CloudWatch event, it will run the two Lambda functions and update the pricing information every 5 days.\n Prepare the Pricing Data Source We will prepare a pricing data source which we will use to join with the CUR. In this example we will take 1 year No Upfront Savings Plans rates and join them to On-Demand pricing. You can modify this part to select 3 year or Partial or All-Upfront rates.\n  Go to the Glue Service page:   Click Crawlers from the left menu:   Click Add crawler:   Enter a crawler name of OD_Pricing and click Next:   Ensure Data stores is the source type, click Next:   Click the folder icon to list the S3 folders in your account:   Expand the bucket which contains your pricing folders, and select the folder name od_pricedata, click Select:   Click Next:   Click Next:   Create an IAM role with a name of SPToolPricing, click Next:   Leave the frequency as Run on demand, and click Next:   Click on Add database:   Enter a database name of pricing, and click Create:   Click Next:   Click Finish:   Select the crawler OD_Pricing and click Run crawler:   Once its run, you should see tables created:   Repeat Steps 3 through to 17 with the following details:\n Crawler name: SP_Pricing Include path: s3://(pricing bucket)/sp_pricedata (replace pricing bucket) IAM Role: Choose an existing IAM role and AWSGlueServiceRole-SPToolPricing Database: pricing    Open the IAM Console in a new tab, click Policies:   Click on the AWSGlueServiceRole-SPToolPricing role:   Type in SPTool and click on the policy name AWSGlueServiceRole-SPTool:   Click Edit policy:   Click JSON:   Edit the Resource line by removing the od_pricedata folder to leave the bucket:   Click Review policy:   Click Save changes:   Go back to the Glue console, select the SP_Pricing crawler, click Run crawler:   Click on Databases:   Click on Pricing:   Click Tables in pricing:   Click od_pricedata:   Click Edit schema:   Click double next to col9:   Select string and click Update:   Click Save:   You have successfully setup the pricing data source. We have a database of on demand and Savings Plans rates.\n  function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/2_usage_source\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_using_awscli_to_manage_wa_reviews/","title":"Level 200: Using AWSCLI to Manage WA Reviews","tags":[],"description":"","content":"  Your browser doesn't support video, or if you're on GitHub head to https://wellarchitectedlabs.com to watch the video.   Authors  Eric Pullen, Performance Efficiency Lead Well-Architected  Introduction The purpose if this lab is to walk you through using the AWS Command Line Interface (AWS CLI) to access the features of the AWS Well-Architected Tool. You will create a workload, review an Operational Excellence question, save the workload, create a milestone, and examine and download the Well-Architected Review report.\nThe knowledge you acquire will help you learn how to programmatically access content in the Well-Architected tool in alignment with the AWS Well-Architected Framework. Goals:  Learn where resources about the questions and best practices are located. Learn how to use milestones to track your progress again high and medium risks over time. Learn how to generate a report or view the results of the review in the Well-Architected Tool.  Prerequisites:  An AWS Account that you are able to use for testing, that is not used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to use Well-Architected Tool (WellArchitectedConsoleFullAccess managed policy).  Costs:  There are no costs for this lab AWS Pricing   Time to complete  The lab should take approximately 30 minutes to complete  Steps:  Configure Environment   Create a Well-Architected Workload   Performing a review   Saving a milestone   Viewing and downloading the report   Optional - Programmatic access via API   Teardown    function prevStep(){ window.open(\"\", \"_self\") } function nextStep(){ window.open(\".\\/1_configure_env\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool  Start Lab  "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_using_awscli_to_manage_wa_reviews/1_configure_env/","title":"Configure Environment","tags":[],"description":"","content":"Install AWS CLI v2 The AWS Command Line Interface (AWS CLI) is a unified tool that provides a consistent interface for interacting with all parts of AWS.\nLinux This includes:\n AWS CloudShell All native Linux installs Windows Subsystem for Linux (WSL)  Verify existing version:\n Run the following command aws --version   If the version number is less than 2.1.12 or you get \u0026ldquo;command not found\u0026rdquo;  curl \u0026quot;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026quot; -o \u0026quot;awscliv2.zip\u0026quot; unzip awscliv2.zip sudo ./aws/install --update  After typing the commands, you should see the following in your console: For additional troubleshooting, see the detailed installation instructions here MacOS See the detailed MacOS installation instructions here Windows See the detailed Windows installation instructions here  function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/2_create_workload\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_fault_isolation_with_shuffle_sharding/1_deploy_workload/","title":"Deploy the workload","tags":[],"description":"","content":"Traditionally most workloads are designed to withstand infrastructure failure by deploying workload components across multiple Availability Zones/Regions, implementing self-healing capabilities such as AutoScaling, etc. While such techniques are effective in ensuring uptime of workload resources, they do not address issues introduced at the workload application level (i.e. a software bug). Leveraging bulkhead architectures and shuffle sharding techniques will provide additional reliability to workloads by limiting the blast radius of failures so that only a subset of users are impacted by such failures.\nYou will use AWS CloudFormation to provision the resources needed for this lab. The CloudFormation stack that you provision will create an Application Load Balancer, Target Groups, and EC2 instances in a new VPC.\n1.1 Log into the AWS console If you are attending an in-person workshop and were provided with an AWS account by the instructor:\n  Click here for instructions to access your assigned AWS account:     Go to https://dashboard.eventengine.run/login\n  Enter the 12 character hashcode you were provided and click \u0026ldquo;Proceed\u0026rdquo;   [optional] assign a name to your account (this is referred to as \u0026ldquo;Team name\u0026rdquo;)\n click \u0026ldquo;Set Team Name\u0026rdquo; Enter a name and click \u0026ldquo;Set Team Name\u0026rdquo;    Click \u0026ldquo;AWS Console\u0026rdquo;\n  AWS credentials   IMPORTANT Copy the provided credentials and save them. You wil need these to complete the workshop   Copy the whole code block corresponding to the system you are using.\n  Access the AWS console  Click \u0026ldquo;Open Console\u0026rdquo;. The AWS Console will open.     If you are using your own AWS account:   Click here for instructions to use your own AWS account:    Sign in to the AWS Management Console as an IAM user who has PowerUserAccess or AdministratorAccess permissions, to ensure successful execution of this lab.   \n1.2 Deploy the workload using AWS CloudFormation   Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and click Create Stack \u0026gt; With new resources (standard)\n  For Prepare template select Template is ready\n For Template source select Amazon S3 URL In the text box under Amazon S3 URL specify https://aws-well-architected-labs-virginia.s3.amazonaws.com/Reliability/300_Fault_Isolation_with_Shuffle_Sharding/regular.yaml    Click Next\n  For Stack name use Shuffle-sharding-lab\n  No changes are required for Parameters. Click Next\n  For Configure stack options click Next\n  On the Review page:\n Scroll to the end of the page and select I acknowledge that AWS CloudFormation might create IAM resources with custom names. This ensures CloudFormation has permission to create resources related to IAM. Additional information can be found here .  Note: The template creates an IAM role and Instance Profile for EC2. These are the minimum permissions necessary for the instances to be managed by AWS Systems Manager. These permissions can be reviewed in the CloudFormation template under the \u0026ldquo;Resources\u0026rdquo; section - InstanceRole.\n Click Create stack    This will take you to the CloudFormation stack status page, showing the stack creation in progress.\n Click on the Events tab Scroll through the listing. It shows (in reverse order) the activities performed by CloudFormation, such as starting to create a resource and then completing the resource creation. Any errors encountered during the creation of the stack will be listed in this tab.  The stack takes about 5 mins to create all the resources. Periodically refresh the page until you see that the Stack Status is in CREATE_COMPLETE. The stack creates the following resources:\n A new VPC, subnets, Internet Gateway, Route tables to host the workload in 4 EC2 instances that host the application An Application Load Balancer, Listener and rules, and Target Groups to route traffic IAM resources (roles, policies) that allow the EC2 instances to be managed by AWS Systems Manager An SSM Document that will be run on the instances  Once the stack is in CREATE_COMPLETE, visit the Outputs section for the stack and note down the Key and Value for each of the outputs. This information will be used in the lab.\n1.3 Test the application Now that the application has been deployed, it is time to test it to understand how it works. The sample application used in this lab is a simple web application that returns a message with the Worker that responded to the request. Customers pass in a query string with the request to identify themselves. The query string used here is name.\n  Copy the URL provided in the Outputs section of the CloudFormation stack created in the previous string.\n  Append the query string /?name=Alpha to the URL and paste it into a web browser. The full string should look similar to this - http://shuffle-alb-1p2xbmzo541rr-1602891463.us-east-1.elb.amazonaws.com/?name=Alpha. Refresh the web browser a few times to see that responses are being returned from different EC2 instances on the back-end\n The list of EC2 instances in your workload can be viewed in the AWS Console here     Update the value for the query string to one of the other customers, the possible values are - Alpha, Bravo, Charlie, Delta, Echo, and Foxtrot\n http://shuffle-alb-1p2xbmzo541rr-1602891463.us-east-1.elb.amazonaws.com/?name=Alpha http://shuffle-alb-1p2xbmzo541rr-1602891463.us-east-1.elb.amazonaws.com/?name=Bravo http://shuffle-alb-1p2xbmzo541rr-1602891463.us-east-1.elb.amazonaws.com/?name=Charlie http://shuffle-alb-1p2xbmzo541rr-1602891463.us-east-1.elb.amazonaws.com/?name=Delta http://shuffle-alb-1p2xbmzo541rr-1602891463.us-east-1.elb.amazonaws.com/?name=Echo http://shuffle-alb-1p2xbmzo541rr-1602891463.us-east-1.elb.amazonaws.com/?name=Foxtrot  Note: If you see a response that says \u0026ldquo;This site can\u0026rsquo;t be reached\u0026rdquo;, please make sure you are using the URL obtained from the outputs section of the CloudFormation stack and not the sample URL provided in this lab guide.\n  Refresh the web browser multiple times to verify that all customers are able to receive responses from all EC2 instances (workers) in the back-end\n   function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/2_impact_of_failures\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_monitoring_linux_ec2_cloudwatch/1_deploy_vpc/","title":"Deploying the infrastructure","tags":["Linux","Amazon Linux","EC2","CloudWatch","CloudWatch Dashboard"],"description":"","content":" The CloudFormation template that accompanies this lab requires the ability to create IAM Roles and IAM Instance Profiles. If the account you are using does not have these capabilities, you will not be able to complete this lab.\n Deploy VPC This step will create the VPC and all components using the example CloudFormation template.\n Download the latest version of the CloudFormation template here: vpc-alb-app-db.yaml  Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Click Create Stack, then With new resources (standard).  Click Upload a template file and then click Choose file.  Choose the CloudFormation template you downloaded in step 1, return to the CloudFormation console page and click Next. Enter the following details:  Stack name: The name of this stack. For this lab, use PerfLab-VPC and match the case. Parameters: Parameters may be left as defaults, you can find out more in the description for each.    At the bottom of the page click Next. In this lab, we use tags, which are key-value pairs, that can help you identify your stacks. Enter Owner in the left column which is the key, and your email address in the right column which is the value. We will not use additional permissions or advanced options so click Next. For more information, see Setting AWS CloudFormation Stack Options . Review the information for the stack. When you\u0026rsquo;re satisfied with the configuration, at the bottom of the page check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack.   After a few minutes the final stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE. You can click the refresh button to check on the current status. You have now created the VPC stack (well actually CloudFormation did it for you).\n  When the stack status is CREATE_COMPLETE, you can continue to the next step.\n   function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/2_deploy_instance\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_monitoring_windows_ec2_cloudwatch/1_deploy_vpc/","title":"Deploying the infrastructure","tags":["Windows Server","Windows","EC2","CloudWatch","CloudWatch Dashboard"],"description":"","content":" The CloudFormation template that accompanies this lab requires the ability to create IAM Roles and IAM Instance Profiles. If the account you are using does not have these capabilities, you will not be able to complete this lab.\n Deploy VPC This step will create the VPC and all components using the example CloudFormation template.\n Download the latest version of the CloudFormation template here: vpc-alb-app-db.yaml  Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Click Create Stack, then With new resources (standard).  Click Upload a template file and then click Choose file.  Choose the CloudFormation template you downloaded in step 1, return to the CloudFormation console page and click Next. Enter the following details:  Stack name: The name of this stack. For this lab, use PerfLab-VPC and match the case. Parameters: Parameters may be left as defaults, you can find out more in the description for each.    At the bottom of the page click Next. In this lab, we use tags, which are key-value pairs, that can help you identify your stacks. Enter Owner in the left column which is the key, and your email address in the right column which is the value. We will not use additional permissions or advanced options so click Next. For more information, see Setting AWS CloudFormation Stack Options . Review the information for the stack. When you\u0026rsquo;re satisfied with the configuration, at the bottom of the page check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack.   After a few minutes the final stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE. You can click the refresh button to check on the current status. You have now created the VPC stack (well actually CloudFormation did it for you).\n  When the stack status is CREATE_COMPLETE, you can continue to the next step.\n   function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/2_deploy_instance\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_autonomous_monitoring_of_cryptographic_activity_with_kms/","title":"Level 300: Autonomous Monitoring Of Cryptographic Activity With KMS","tags":[],"description":"","content":"Authors  Tim Robinson, Well-Architected Geo Solutions Architect. Stephen Salim, Well-Architected Geo Solutions Architect.  Introduction The ability to provide traceability and automatically react to security events occurring in your workload is vital to maintaining a high security posture within your application. Through the use monitoring key activity metrics, architects are able to detect potentially malicious behaviour at an early stage and respond according to the event. By combining this early detection approach with appropriate alerting, we can create an autonomous feedback loop which ensures that cloud administrators are adequately informed before a serious event takes place.\nIn AWS you can use AWS CloudTrail to capture all API based activity within an AWS account. However, simply capturing these activities is not be sufficient without the ability to contextualize events and create the mechanism to automatically react in an appropriate manner. The integration of Amazon CloudWatch in combination with CloudTrail and our other services, allows customers to produce adequate alerting and visibility to important system events triggered by a key activity metric.\nOne such example of a key activity metric would be Key Management Service (KMS) activity. KMS is integral to most secure architecture designs and responsible for autonomous encryption and decryption activity. Whilst we would expect regular activity which is triggered as a byproduct from user interaction with an architecture, significantly high activity could be an early warning signal that the architecture could be subject to data exfiltration by an interested third party or competitor.\nIn this lab we will walk you through an example scenario of monitoring our KMS service for encryption and decryption activity. We will autonomously detect abormal activity beyond a predefined threshold and respond accordingly, using the following services:\n AWS CloudTrail - Used for capturing API events within the environment. Amazon CloudWatch Log Groups - Used to log our CloudTrail API events. Amazon CloudWatch Metric Filter to create apply filter so we can measure the only the events that matters for us. Amazon CloudWatch Alarms to allow our system to react against pre created events Amazon Simple Notification Service to allow us to send email notification when an event occurs.  Our lab is divided into several sections as follows:\n Deploy the lab base infrastructure. Configure the ECS repository and deploy the application stack. Configure the workload logging and alarm. Testing the workload functionality.  We have included CloudFormation templates for the first few steps to get your started, and also provide optional templates for the rest of the lab so you can choose between creating the monitoring resources via cloudformation or manually through the console.\nNote: For simplicity, we have used Sydney \u0026lsquo;ap-southeast-2\u0026rsquo; as the default region for this lab. Please ensure all lab interaction is completed from this region.\n Goals  Analyze CloudTrail and target specific API events. Integrate CloudTrail events with A CloudWatch Log Group to record the event. Apply appropriate metric filters and alarms to trigger an event. Integrate SNS to respond appropriately to the event.  Prerequisites  An AWS account that you are able to use for testing, that is not used for production or other purposes. If you want to follow the command line instructions, you should have the AWS command line installed and configured. Please follow this guide  In this Lab we will be creating a local docker image, please ensure you have docker installed in your machine, and you are running Docker version 18.09.9 or above.  NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\n Steps:  Deploy The Lab Base Infrastructure   Configure ECS Respository and Deploy The Application Stack   Configure CloudTrail    Configure The Workload Logging and Alarm   Testing the Workload Functionality   Teardown   "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_organization_data_cur_connection/1_create_static_resources_source/","title":"Create Static Resources","tags":[],"description":"","content":"Create Amazon S3 Bucket and Folders Well create an S3 bucket to store the organizations data to be combined with your cost and usage report. This will hold your organisation data so we can connect it to Athena.\n Login via SSO to your Cost Optimization account, go into the S3 console:  Click Create bucket and create a bucket. You will need to use a unique bucket name with cost at the start, (we have used cost-aws-lab-organisation-bucket). Make a note of this as we will be using it later.  Create IAM Role and Policies Well create an IAM role and policy for the AWS Lambda function to access the organizations data \u0026amp; write it to S3. This role will be used to get the list of accounts in the Organization and the meta data attached to them such as name and email. This is then placed in our S3 bucket.\n  Go to the IAM Console\n  Select Policies and Create policy\n   On the JSON tab the following policy and replace (bucket name) with your bucket name from before and replace (account id) with your Management Account id which manages your Organization. Enter the following policy, click Review policy:\n{ \u0026quot;Version\u0026quot;:\u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;:[ { \u0026quot;Sid\u0026quot;:\u0026quot;S3Org\u0026quot;, \u0026quot;Effect\u0026quot;:\u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;:[ \u0026quot;s3:PutObject\u0026quot;, \u0026quot;s3:DeleteObjectVersion\u0026quot;, \u0026quot;s3:DeleteObject\u0026quot; ], \u0026quot;Resource\u0026quot;:\u0026quot;arn:aws:s3:::(bucket name)/*\u0026quot; }, { \u0026quot;Sid\u0026quot;:\u0026quot;OrgData\u0026quot;, \u0026quot;Effect\u0026quot;:\u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;:[ \u0026quot;organizations:ListAccounts\u0026quot;, \u0026quot;organizations:ListCreateAccountStatus\u0026quot;, \u0026quot;organizations:DescribeOrganization\u0026quot;, \u0026quot;organizations:ListTagsForResource\u0026quot; ], \u0026quot;Resource\u0026quot;:\u0026quot;*\u0026quot; }, { \u0026quot;Sid\u0026quot;:\u0026quot;Logs\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;logs:CreateLogGroup\u0026quot;, \u0026quot;logs:CreateLogStream\u0026quot;, \u0026quot;logs:PutLogEvents\u0026quot;, \u0026quot;logs:DescribeLogStreams\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:logs:*:*:*\u0026quot; }, { \u0026quot;Sid\u0026quot;: \u0026quot;assume\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:iam::(account id):role/OrganizationLambdaAccessRole\u0026quot; } ] }    Fill in the following\n   Policy Name LambdaOrgPolicy Description Access to S3 for Lambda function to collect Orginization data Click Create policy  Select Roles, click Create role  Select Lambda, click Next: Permissions:  Type lambda into the search and select the LambdaOrgPolicy policy, click Next: Tags   Click Next: Review\n  Role name LambdaOrgRole, click Create role:\n  Create IAM Role and Policies in Management account As we need to pull the data from the Management account we need to allow our role to do this.\n  Log into your Management account\n  Go to the IAM Console\n  Select Policies and Create policy. Copy steps 2 - 4 from above to create the below policy called ListOrganizations.\n  { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;OrgData\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;organizations:ListAccounts\u0026quot;, \u0026quot;organizations:ListCreateAccountStatus\u0026quot;, \u0026quot;organizations:DescribeOrganization\u0026quot;, \u0026quot;organizations:ListTagsForResource\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] }  Select Roles, click Create role  Choose Another AWS account, and enter your sub account id which is where we started the lab, Next: Permissions   Search for Organizations and select the ListOrganizations policy you just made. Click Next: Tags then click Next: Review\n  Role name OrganizationLambdaAccessRole, click Create role:\n   Search for your new role in the roles page and click on the role name. Click on Trusted relationships tab then Edit trusted relationship   On the JSON tab the replace the current json with the following policy and replace (sub account id) with your sub account id from before, click Update Trust policy:\n { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;AWS\u0026quot;: \u0026quot;arn:aws:iam::(sub account id):role/LambdaOrgRole\u0026quot; }, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot; } ] }    Now you have completed this section you have setup the resources that will enable you to collect your Organizations data. We will use these resources in the next section when creating our Lambda function. Please return to the sub account you created your S3 bucket in.\n  function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/2_create_automation_resources_source\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/100_labs/100_cloudfront_with_s3_bucket_origin/1_create_s3/","title":"Create S3 bucket","tags":[],"description":"","content":"Create an Amazon S3 bucket to host static content using the Amazon S3 console. For more information about Amazon S3, see Introduction to Amazon S3 .\n Open the Amazon S3 console at https://console.aws.amazon.com/s3/ . From the console dashboard, choose Create bucket.  Enter a Bucket name for your bucket, type a unique DNS-compliant name for your new bucket. Follow these naming guidelines:   The name must be unique across all existing bucket names in Amazon S3. The name must not contain uppercase characters. The name must start with a lowercase letter or number. The name must be between 3 and 63 characters long.  Choose an AWS Region where you want the bucket to reside. Choose a Region close to you to minimize latency and costs, or to address regulatory requirements. Note that for this example we will accept the default settings and this bucket is secure by default. Consider enabling additional security options such as logging and encryption, the S3 documentation has additional information such as Protecting Data in Amazon S3 . Accept default value for Block all public access as CloudFront will serve the content for you from S3. Enable bucket versioning , to keep multiple versions of an object so you can recover an object if you unintentionally modify or delete it.  Click Create bucket.  "},{"uri":"https://wellarchitectedlabs.com/security/100_labs/","title":"100 Level Foundational Labs","tags":[],"description":"","content":"List of labs available  AWS Account Setup and Root User   Creating your first Identity and Access Management User, Group, Role   CloudFront with S3 Bucket Origin   Enable Security Hub   Create a Data Bunker Account   "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/","title":"200 Level Intermediate Labs","tags":[],"description":"","content":"List of labs available  Automated Deployment of Detective Controls   Automated Deployment of EC2 Web Application   Automated Deployment of IAM Groups and Roles   Level 200: Automated Deployment of VPC   Level 200: Automated Deployment of Web Application Firewall   Level 200: Automated IAM User Cleanup   Level 200: Basic EC2 Web Application Firewall Protection   Level 200: AWS Certificate Manager Request Public Certificate   Level 200: CloudFront for Web Application   Level 200: CloudFront with WAF Protection   Level 200: Remote Configuration, Installation, and Viewing of CloudWatch logs   "},{"uri":"https://wellarchitectedlabs.com/security/100_labs/100_aws_account_and_root_user/1_intro/","title":"Account Settings &amp; Root User Security","tags":[],"description":"","content":"When you first create an Amazon Web Services (AWS) account, you begin with a single sign-in identity that has complete access to all AWS services and resources in the account. This identity is called the AWS account root user and is accessed by signing in with the email address and password that you used to create the account.\nIt is strongly recommended that you only use the root user by exception. Instead, adhere to the best practice of using the root user only to setup identity federation using AWS Single Sign-On or an identity provider configured in IAM. To view the tasks that require root login you need to sign in as the root user, see AWS Tasks That Require Root User .\nIf you dont have an existing organizational structure with AWS Organizations , AWS Control Tower is the easiest way to get started. For more information see Security Foundations and Identity and Access Management in the AWS Well-Architected security whitepaper.\n 1.1 Generate and Review the AWS Account Credential Report Its good to get an idea of what you have configured already in your AWS account especially if you have had it for a while. You should audit your security configuration in the following situations:\n On a periodic basis. You should perform the steps described here at regular intervals as a best practice for security. If there are changes in your organization, such as people leaving. If you have stopped using one or more individual AWS services. This is important for removing permissions that users in your account no longer need. If you\u0026rsquo;ve added or removed software in your accounts, such as applications on Amazon EC2 instances, AWS OpsWorks stacks, AWS CloudFormation templates, etc. If you ever suspect that an unauthorized person might have accessed your account.  As you review your account\u0026rsquo;s security configuration, follow these guidelines:\n Be thorough. Look at all aspects of your security configuration, including those you might not use regularly. Don\u0026rsquo;t assume. If you are unfamiliar with some aspect of your security configuration (for example, the reasoning behind a particular policy or the existence of a role), investigate the business need until you are satisfied. Keep things simple. To make auditing (and management) easier, use IAM groups, consistent naming schemes, and straightforward policies.  More information can be found at https://docs.aws.amazon.com/general/latest/gr/aws-security-audit-guide.html \nYou can use the AWS Management Console to download a credential report as a comma-separated values (CSV) file. Please note that credential report can take 4 hours to reflect changes. To download a credential report using the AWS Management Console:\n Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ . In the navigation pane, click Credential report. Click Download Report.  Further information about the report can be found at https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_getting-report.html \n1.2 Enable a Virtual MFA Device for Your AWS Account Root User You can use IAM in the AWS Management Console to configure and enable a virtual MFA device for your root user. To manage MFA devices for the AWS account, you must be signed in to AWS using your root user credentials. You cannot manage MFA devices for the root user using other credentials.\nIf your MFA device is lost, stolen, or not working, you can still sign in using alternative factors of authentication. To do this, you must verify your identity using the email and phone that are registered with your account. This means that if you can\u0026rsquo;t sign in with your MFA device, you can sign in by verifying your identity using the email and phone that are registered with your account. Before you enable MFA for your root user, review your account settings and contact information to make sure that you have access to the email and phone number. To learn about signing in using alternative factors of authentication, see What If an MFA Device Is Lost or Stops Working ?. To disable this feature, contact AWS Support .\n  Use your AWS account email address and password to sign in as the AWS account root user to the IAM console at https://console.aws.amazon.com/iam/   Do one of the following:\n  Option 1: Click Dashboard, and under Security Status, expand Activate MFA on your root user.\n  Option 2: On the right side of the navigation bar, click your account name, and click Security Credentials. If necessary, click Continue to Security Credentials. Then expand the Multi-Factor Authentication (MFA) section on the page.\n    Click Manage MFA or Activate MFA, depending on which option you chose in the preceding step.\n  In the wizard, click A virtual MFA device and then click Next Step.\n  Confirm that a virtual MFA app is installed on the device, and then click Next Step. IAM generates and displays configuration information for the virtual MFA device, including a QR code graphic. The graphic is a representation of the secret configuration key that is available for manual entry on devices that do not support QR codes.\n  With the Manage MFA Device wizard still open, open the virtual MFA app on the device.\n  If the virtual MFA software supports multiple accounts (multiple virtual MFA devices), then click the option to create a new account (a new virtual device).\n  The easiest way to configure the app is to use the app to scan the QR code. If you cannot scan the code, you can type the configuration information manually.\n To use the QR code to configure the virtual MFA device, follow the app instructions for scanning the code. For example, you might need to tap the camera icon or tap a command like Scan account barcode, and then use the device\u0026rsquo;s camera to scan the QR code. If you cannot scan the code, type the configuration information manually by typing the Secret Configuration Key value into the app. For example, to do this in the AWS Virtual MFA app, click Manually add account, and then type the secret configuration key and click Create.  Important\nMake a secure backup of the QR code or secret configuration key, or make sure that you enable multiple virtual MFA devices for your account. A virtual MFA device might become unavailable, for example, if you lose the smartphone where the virtual MFA device is hosted). If that happens, you will not be able to sign in to your account and you will have to contact customer service to remove MFA protection for the account.\nNote\nThe QR code and secret configuration key generated by IAM are tied to your AWS account and cannot be used with a different account. They can, however, be reused to configure a new MFA device for your account in case you lose access to the original MFA device.\nThe device starts generating six-digit numbers.\n  In the Manage MFA Device wizard, in the Authentication Code 1 box, type the six-digit number that\u0026rsquo;s currently displayed by the MFA device. Wait up to 30 seconds for the device to generate a new number, and then type the new six-digit number into the Authentication Code 2 box.\nImportant\nSubmit your request immediately after generating the codes. If you generate the codes and then wait too long to submit the request, the MFA device successfully associates with the user but the MFA device is out of sync. This happens because time-based one-time passwords (TOTP) expire after a short period of time. If this happens, you can resync the device.\n  Click Next Step, and then click Finish.\n  The device is ready for use with AWS. For information about using MFA with the AWS Management Console, see Using MFA Devices With Your IAM Sign-in Page .\n1.3 Configure Account Security Challenge Questions Configure account security challenge questions because they are used to verify that you own an AWS account.\n Use your AWS account email address and password to sign in as the AWS account root user and open the AWS account settings page at https://console.aws.amazon.com/billing/home?#/account/ . Navigate to security challenge questions configuration section.  Select three challenge questions and enter answers for each. Securely store the questions and answers as you would passwords or other credentials. Click update.  1.4 Configure Account Alternate Contacts Alternate contacts enable AWS to contact another person about issues with the account, even if you are unavailable.\n Use your AWS account email address and password to sign in as the AWS account root user and open the AWS account settings page at https://console.aws.amazon.com/billing/home?#/account/ . Navigate to alternate contacts configuration section.  Enter contact details for billing, operations and security. Click update.  1.5 Remove Your AWS Account Root User Access Keys You use an access key (an access key ID and secret access key) to make programmatic requests to AWS. However, do not use your AWS account root user access key. The access key for your AWS account gives full access to all your resources for all AWS services, including your billing information. You cannot restrict the permissions associated with your AWS account access key.\n Check in the credential report; if you don\u0026rsquo;t already have an access key for your AWS account, don\u0026rsquo;t create one unless you absolutely need to. Instead, use your account email address and password to sign in to the AWS Management Console and create an IAM user for yourself that has administrative privileges. This will be explained in a later section. If you do have an access key for your AWS account, delete it unless you have a specific requirement. To delete or rotate your AWS account access keys, go to the Security Credentials page in the AWS Management Console and sign in with your account\u0026rsquo;s email address and password. You can manage your access keys in the Access keys section.   Never share your AWS account password or access keys with anyone.  1.6 Periodically Change the AWS Account Root User Password You must be signed in as the AWS account root user in order to change the password. To learn how to reset a forgotten root user password, see Resetting Your Lost or Forgotten Passwords or Access Keys .\nTo change the password for the root user:\n  Use your AWS account email address and password to sign in to the AWS Management Console as the root user.\nNote\nIf you previously signed in to the console with IAM user credentials, your browser might remember this preference and open your account-specific sign-in page. You cannot use the IAM user sign-in page to sign in with your AWS account root user credentials. If you see the IAM user sign-in page, click Sign-in using root account credentials near the bottom of the page to return to the main sign-in page. From there, you can type your AWS account email address and password.\n  In the upper right corner of the console, click your account name or number and then click My Account.\n  On the right side of the page, next to the Account Settings section, click Edit.\n  On the Password line choose Click here to change your password.\n   Choose a strong password. Although you can set an account password policy for IAM users, that policy does not apply to your AWS account root user.\nAWS requires that your password meet these conditions:\n have a minimum of 8 characters and a maximum of 128 characters include a minimum of three of the following mix of character types: uppercase, lowercase, numbers, and ! @ # $ % ^ \u0026amp; * () \u0026lt;\u0026gt; [] {} | _ + - = symbols not be identical to your AWS account name or email address  Note\nAWS is rolling out improvements to the sign-in process. One of those improvements is to enforce a more secure password policy for your account. If your account has been upgraded, you are required to meet the password policy above. If your account has not yet been upgraded, then AWS does not enforce this policy, but highly recommends that you follow its guidelines for a more secure password.\nTo protect your password, it\u0026rsquo;s important to follow these best practices:\n Change your password periodically and keep your password private, since anyone who knows your password can access your account. Use a different password on AWS than you use on other sites. Avoid passwords that are easy to guess. These include passwords such as secret, password, amazon, or 123456. They also include things like a dictionary word, your name, email address, or other personal information that can easily be obtained.    1.7 Configure a Strong Password Policy for Your Users You can set a password policy on your AWS account to specify complexity requirements and mandatory rotation periods for your IAM users' passwords. The IAM password policy does not apply to the AWS root account password.\nTo create or change a password policy:\n Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/. In the navigation pane, click Account Settings. In the Password Policy section, select the options you want to apply to your password policy. Click Apply Password Policy.  "},{"uri":"https://wellarchitectedlabs.com/security/100_labs/100_enable_security_hub/1_intro/","title":"Enable AWS Security Hub via AWS Console","tags":[],"description":"","content":"Table of Contents  Getting Started   1. Getting Started The AWS console provides a graphical user interface to search and work with the AWS services. We will use the AWS console to enable AWS Security Hub.\n1.1 AWS Security Hub Once you have logged into your AWS account you can use the search facility to locate Security Hub. All you need to do is type in Security Hub in the search field. Once Security Hub shows up you can click on Security Hub to go to the Security Hub service. Alternatively you can go directly to the AWS Security Hub Console. AWS Security Hub Console 1.2 Enable AWS Security Hub In the AWS Security Hub service console you can click on the Enable Security Hub orange button to enable AWS Security Hub in your account. AWS Security Hub requires services permissions to run within your account. You can review the service role permissions in the following screen. Remember to click Enable AWS Security Hub 1.3 Explore AWS Security Hub With AWS Security Hub now enabled in your account, you can explore the security insights AWS Security Hub offers. "},{"uri":"https://wellarchitectedlabs.com/security/100_labs/100_aws_account_and_root_user/","title":"AWS Account Setup and Root User","tags":[],"description":"","content":"Last Updated: September 2020\nAuthor: Ben Potter, Security Lead, Well-Architected\nIntroduction This hands-on lab will guide you through the introductory steps to configure contacts for your AWS account and secure the root user. This process is not required for accounts you manage with AWS Organizations or AWS Control Tower . For more information see Security Foundations in the AWS Well-Architected security whitepaper . For accounts used for business purposes it is recommended to start with AWS Control Tower. For personal or invidual accounts you can follow the steps in this unofficial video from the author of this lab.\n  Prerequisites  An AWS account that you are able to use for testing. Root user login.  Costs  There are no costs for this lab AWS Pricing   Steps  Account Settings \u0026amp; Root User Security   Tear down   References \u0026amp; Useful Resources  AWS Tasks That Require Root User  Credential Report  AWS Identity and Access Management User Guide  IAM Best Practices and Use Cases  Resetting Your Lost or Forgotten Passwords or Access Keys  Using MFA Devices With Your IAM Sign-in Page  What If an MFA Device Is Lost or Stops Working   "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/100_labs/100_dependency_monitoring/1_deploy_infrastructure/","title":"Deploy the Infrastructure","tags":[],"description":"","content":"Many workloads depend on external resources or services for data or additional capabilities such as 3rd party data providers or service providers, DNS providers, etc. Functionality or outcomes of the workload may be at risk when dependent resources or services become degraded or unreachable.\nMonitoring these dependencies will enable quick action to ensure business continuity is not affected. Setting up alerting and notifications will ensure that appropriate team members are aware of issues and can take action to address the situation.\nThis lab provides examples of how to implement Well-Architected Operational Excellence best practices such as Implement dependency telemetry, Alert when workload outcomes are at risk, and Enable push notifications.\nIn this lab there is an external service (3rd party data provider) that provides data which will be consumed by the workload. This has been emulated in this lab by using an EC2 instance which acts as the 3rd party data provider, and it writes data to an S3 bucket at 50 second intervals. Amazon S3 notification feature enables you to receive notifications when certain events happen in your bucket.\nFor this use-case the notification has been configured on the S3 bucket to invoke a lambda function after every write to the bucket, using the S3 PutObject API. The objective of this lab is to create awareness when an external service is experiencing downtime or is otherwise impaired. For this example the assumption is that the 3rd party data provider is experiencing downtime when data is no longer being written to the S3 bucket.\n1.1 Log into the AWS console If you are attending an in-person workshop and were provided with an AWS account by the instructor:\n  Click here for instructions to access your assigned AWS account:     Go to https://dashboard.eventengine.run/login\n  Enter the 12 character hashcode you were provided and click \u0026ldquo;Proceed\u0026rdquo;   [optional] assign a name to your account (this is referred to as \u0026ldquo;Team name\u0026rdquo;)\n click \u0026ldquo;Set Team Name\u0026rdquo; Enter a name and click \u0026ldquo;Set Team Name\u0026rdquo;    Click \u0026ldquo;AWS Console\u0026rdquo;\n  AWS credentials   IMPORTANT Copy the provided credentials and save them. You wil need these to complete the workshop   Copy the whole code block corresponding to the system you are using.\n  Access the AWS console  Click \u0026ldquo;Open Console\u0026rdquo;. The AWS Console will open.     If you are using your own AWS account:   Click here for instructions to use your own AWS account:    Sign in to the AWS Management Console as an IAM user who has PowerUserAccess or AdministratorAccess permissions, to ensure successful execution of this lab.   \n1.2 Deploy the infrastructure using AWS CloudFormation You will use AWS CloudFormation to provision resources that will emulate the workload described in the use-case. AWS CloudFormation provides you a common language to model and provision AWS and third party application resources by applying Infrastructure as Code in your cloud environment.\n  Download the dependency_monitoring.yaml CloudFormation template (right-click on the link and select \u0026ldquo;Save Link As\u0026hellip;\u0026quot;)\n  Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and click Create Stack \u0026gt; With new resources (standard)\n  Leave Prepare template setting as-is since you already have a template ready (dependency_monitoring.yaml)\n For Template source select Upload a template file Click Choose file and supply the CloudFormation template you downloaded: dependency_monitoring.yaml    Click Next\n  For Stack name use Dependency-Monitoring-Lab\n  Parameters\n BucketName - enter a name for the S3 bucket that will be created as part of the lab. Amazon S3 bucket names are globally unique, and the namespace is shared by all AWS accounts, so make sure you name the bucket as uniquely as possible. For example - wa-lab-\u0026lt;your last name\u0026gt;-\u0026lt;date\u0026gt;\u0026lt;time\u0026gt;. LatestAmiId - leave the default value here. This will ensure that CloudFormation will retrieve the latest Amazon Linux AMI for the region you are launching the stack in. NotificationEmail - specify an email address that you have access to. This is the email address that notifications related to the dependent service will be sent to. Click Next    For Configure stack options click Next\n  On the Review page:\n Scroll to the end of the page and select I acknowledge that AWS CloudFormation might create IAM resources with custom names. This ensures CloudFormation has permission to create resources related to IAM. Additional information can be found here .  Note: The template creates 2 roles for Lambda as well as a role and instance profile for an EC2 instance. They are the minimum permissions necessary to read and write from an S3 bucket created as part of this lab and create an OpsItem in OpsCenter. These permissions can be reviewed in the CloudFormation template under \u0026ldquo;Resources\u0026rdquo; section - DataReadLambdaRole, OpsItemLambdaRole, and InstanceRole.\n Click Create stack    This will take you to the CloudFormation stack status page, showing the stack creation in progress.\n Click on the Events tab Scroll through the listing. It shows (in reverse order) the activities performed by CloudFormation, such as starting to create a resource and then completing the resource creation. Any errors encountered during the creation of the stack will be listed in this tab.    Once stack creation starts, monitor the email address you entered. You should receive an email from SNS with the subject AWS Notification - Subscription Confirmation. Click on the link Confirm subscription to confirm the subscription of your email to the SNS Topic. This will allow SNS to send email notifications to the email address specified.\n  The stack takes about 3 mins to create all the resources. Periodically refresh the page until you see that the Stack Status is in CREATE_COMPLETE. The stack creates the following resources:\n A new VPC, subnets, Internet Gateway, Route tables to host the workload in An EC2 instance that acts as the 3rd party data provider An S3 bucket and Lambda function that act as the workload IAM resources (roles, policies) that allow different services to access each other An SNS Topic for notifications  Once the stack is in CREATE_COMPLETE, visit the Outputs section for the stack and note down the Key and Value for each of the outputs. This information will be used later in the lab.\n function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/2_understand_metrics\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_licensing/1_pricing_sources/","title":"Create Pricing Data Source","tags":[],"description":"","content":" If you have RHEL usage in your CUR you can skip this step, this step sets up a provided Cost and Usage report with RHEL usage for analysis.\n Create the pricing data source We will create a data source with approximately 24 hours of usage. This is a sample data source which contains multiple workloads, which is representative of running small web server applications.\nCreate the pricing data   Log into the console via SSO.\n  Go to the S3 service dashboard\n  Create a bucket, with a name starting with cost-\n  Go into the bucket and create 2 folders, before and after:   Go into the before folder and upload the following file: Code/BeforeCUR.gz   Go into the after folder and upload the following file: Code/AfterCUR.gz   You now have your sample usage files ready to be setup.\n Setup Athena   Go into the Athena service dashboard\n  Create the costmaster database if it does not exist, copy and paste the following command:\n create database if not exists costmaster;    Create the before table, modify the location line at the bottom, in the query below by replacing (bucketname) with the name of your bucket, and paste the following query into Athena:\n    Create before table - Athena query    CREATE EXTERNAL TABLE if not exists `costmaster.before`( `identity_line_item_id` string, `identity_time_interval` string, `bill_invoice_id` string, `bill_billing_entity` string, `bill_bill_type` string, `bill_payer_account_id` bigint, `bill_billing_period_start_date` string, `bill_billing_period_end_date` string, `line_item_usage_account_id` bigint, `line_item_line_item_type` string, `line_item_usage_start_date` string, `line_item_usage_end_date` string, `line_item_product_code` string, `line_item_usage_type` string, `line_item_operation` string, `line_item_availability_zone` string, `line_item_resource_id` string, `line_item_usage_amount` double, `line_item_normalization_factor` double, `line_item_normalized_usage_amount` double, `line_item_currency_code` string, `line_item_unblended_rate` double, `line_item_unblended_cost` double, `line_item_blended_rate` double, `line_item_blended_cost` string, `line_item_line_item_description` string, `line_item_tax_type` string, `line_item_legal_entity` string, `product_product_name` string, `product_availability` string, `product_capacitystatus` string, `product_clock_speed` string, `product_current_generation` string, `product_database_engine` string, `product_dedicated_ebs_throughput` string, `product_deployment_option` string, `product_description` string, `product_durability` string, `product_ecu` string, `product_edition` string, `product_engine_code` string, `product_enhanced_networking_supported` string, `product_event_type` string, `product_free_query_types` string, `product_from_location` string, `product_from_location_type` string, `product_group` string, `product_group_description` string, `product_instance_family` string, `product_instance_type` string, `product_instance_type_family` string, `product_license_model` string, `product_location` string, `product_location_type` string, `product_max_iops_burst_performance` string, `product_max_iopsvolume` bigint, `product_max_throughputvolume` string, `product_max_volume_size` string, `product_memory` string, `product_message_delivery_frequency` string, `product_message_delivery_order` string, `product_min_volume_size` string, `product_network_performance` string, `product_normalization_size_factor` double, `product_operating_system` string, `product_operation` string, `product_physical_processor` string, `product_pre_installed_sw` string, `product_processor_architecture` string, `product_processor_features` string, `product_product_family` string, `product_queue_type` string, `product_region` string, `product_servicecode` string, `product_servicename` string, `product_sku` string, `product_storage` string, `product_storage_class` string, `product_storage_media` string, `product_subscription_type` string, `product_tenancy` string, `product_to_location` string, `product_to_location_type` string, `product_transfer_type` string, `product_usagetype` string, `product_vcpu` bigint, `product_version` string, `product_volume_type` string, `pricing_lease_contract_length` string, `pricing_offering_class` string, `pricing_purchase_option` string, `pricing_rate_id` bigint, `pricing_public_on_demand_cost` string, `pricing_public_on_demand_rate` double, `pricing_term` string, `pricing_unit` string, `reservation_amortized_upfront_cost_for_usage` double, `reservation_amortized_upfront_fee_for_billing_period` double, `reservation_effective_cost` double, `reservation_end_time` string, `reservation_modification_status` string, `reservation_normalized_units_per_reservation` string, `reservation_number_of_reservations` string, `reservation_recurring_fee_for_usage` double, `reservation_reservation_a_r_n` string, `reservation_start_time` string, `reservation_subscription_id` bigint, `reservation_total_reserved_normalized_units` string, `reservation_total_reserved_units` string, `reservation_units_per_reservation` string, `reservation_unused_amortized_upfront_fee_for_billing_period` double, `reservation_unused_normalized_unit_quantity` double, `reservation_unused_quantity` double, `reservation_unused_recurring_fee` double, `reservation_upfront_value` double, `resource_tags_aws_autoscaling_group_name` string, `resource_tags_aws_created_by` string, `resource_tags_aws_ec2spot_fleet_request_id` string, `resource_tags_user_cost_center` string, `resource_tags_user_department` string, `resource_tags_user_environment` string, `resource_tags_user_name` string, `resource_tags_user_workload` string, `resource_tags_user_workload_type` string, `product_category` string, `resource_tags_user_tag21_nov` string, `resource_tags_user_application` string, `resource_tags_user_tier` string, `product_content_type` string, `product_granularity` string, `product_origin` string, `product_recipient` string, `product_volume_api_name` string, `savings_plan_total_commitment_to_date` double, `savings_plan_savings_plan_a_r_n` string, `savings_plan_savings_plan_rate` double, `savings_plan_used_commitment` double, `savings_plan_savings_plan_effective_cost` double, `savings_plan_amortized_upfront_commitment_for_billing_period` double, `savings_plan_recurring_commitment_for_billing_period` double, `product_instance` string, `product_provisioned` string, `product_request_description` string, `product_request_type` string, `product_fee_code` string, `product_fee_description` string, `year` bigint, `month` bigint) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\u0001' STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION 's3://(bucketname)/before/' TBLPROPERTIES ( \u0026quot;skip.header.line.count\u0026quot;=\u0026quot;1\u0026quot;)    Create the after table, modify the location line at the bottom, in the query below by replacing (bucketname) with the name of your bucket, and paste the following query into Athena:    Create after table - Athena query    CREATE EXTERNAL TABLE if not exists `costmaster.after`( `identity_line_item_id` string, `identity_time_interval` string, `bill_invoice_id` string, `bill_billing_entity` string, `bill_bill_type` string, `bill_payer_account_id` bigint, `bill_billing_period_start_date` string, `bill_billing_period_end_date` string, `line_item_usage_account_id` bigint, `line_item_line_item_type` string, `line_item_usage_start_date` string, `line_item_usage_end_date` string, `line_item_product_code` string, `line_item_usage_type` string, `line_item_operation` string, `line_item_availability_zone` string, `line_item_resource_id` string, `line_item_usage_amount` double, `line_item_normalization_factor` double, `line_item_normalized_usage_amount` double, `line_item_currency_code` string, `line_item_unblended_rate` double, `line_item_unblended_cost` double, `line_item_blended_rate` double, `line_item_blended_cost` string, `line_item_line_item_description` string, `line_item_tax_type` string, `line_item_legal_entity` string, `product_product_name` string, `product_availability` string, `product_capacitystatus` string, `product_clock_speed` string, `product_current_generation` string, `product_database_engine` string, `product_dedicated_ebs_throughput` string, `product_deployment_option` string, `product_description` string, `product_durability` string, `product_ecu` string, `product_edition` string, `product_engine_code` string, `product_enhanced_networking_supported` string, `product_event_type` string, `product_free_query_types` string, `product_from_location` string, `product_from_location_type` string, `product_group` string, `product_group_description` string, `product_instance_family` string, `product_instance_type` string, `product_instance_type_family` string, `product_license_model` string, `product_location` string, `product_location_type` string, `product_max_iops_burst_performance` string, `product_max_iopsvolume` bigint, `product_max_throughputvolume` string, `product_max_volume_size` string, `product_memory` string, `product_message_delivery_frequency` string, `product_message_delivery_order` string, `product_min_volume_size` string, `product_network_performance` string, `product_normalization_size_factor` double, `product_operating_system` string, `product_operation` string, `product_physical_processor` string, `product_pre_installed_sw` string, `product_processor_architecture` string, `product_processor_features` string, `product_product_family` string, `product_queue_type` string, `product_region` string, `product_servicecode` string, `product_servicename` string, `product_sku` string, `product_storage` string, `product_storage_class` string, `product_storage_media` string, `product_subscription_type` string, `product_tenancy` string, `product_to_location` string, `product_to_location_type` string, `product_transfer_type` string, `product_usagetype` string, `product_vcpu` bigint, `product_version` string, `product_volume_type` string, `pricing_lease_contract_length` string, `pricing_offering_class` string, `pricing_purchase_option` string, `pricing_rate_id` bigint, `pricing_public_on_demand_cost` string, `pricing_public_on_demand_rate` double, `pricing_term` string, `pricing_unit` string, `reservation_amortized_upfront_cost_for_usage` double, `reservation_amortized_upfront_fee_for_billing_period` double, `reservation_effective_cost` double, `reservation_end_time` string, `reservation_modification_status` string, `reservation_normalized_units_per_reservation` string, `reservation_number_of_reservations` string, `reservation_recurring_fee_for_usage` double, `reservation_reservation_a_r_n` string, `reservation_start_time` string, `reservation_subscription_id` bigint, `reservation_total_reserved_normalized_units` string, `reservation_total_reserved_units` string, `reservation_units_per_reservation` string, `reservation_unused_amortized_upfront_fee_for_billing_period` double, `reservation_unused_normalized_unit_quantity` double, `reservation_unused_quantity` double, `reservation_unused_recurring_fee` double, `reservation_upfront_value` double, `resource_tags_aws_autoscaling_group_name` string, `resource_tags_aws_created_by` string, `resource_tags_aws_ec2spot_fleet_request_id` string, `resource_tags_user_cost_center` string, `resource_tags_user_department` string, `resource_tags_user_environment` string, `resource_tags_user_name` string, `resource_tags_user_workload` string, `resource_tags_user_workload_type` string, `product_category` string, `resource_tags_user_tag21_nov` string, `resource_tags_user_application` string, `resource_tags_user_tier` string, `product_content_type` string, `product_granularity` string, `product_origin` string, `product_recipient` string, `product_volume_api_name` string, `savings_plan_total_commitment_to_date` double, `savings_plan_savings_plan_a_r_n` string, `savings_plan_savings_plan_rate` double, `savings_plan_used_commitment` double, `savings_plan_savings_plan_effective_cost` double, `savings_plan_amortized_upfront_commitment_for_billing_period` double, `savings_plan_recurring_commitment_for_billing_period` double, `product_instance` string, `product_provisioned` string, `product_request_description` string, `product_request_type` string, `product_fee_code` string, `product_fee_description` string, `year` bigint, `month` bigint) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\u0001' STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION 's3://(bucketname)/after/' TBLPROPERTIES ( \u0026quot;skip.header.line.count\u0026quot;=\u0026quot;1\u0026quot;)    Test and Verify   Confirm the before table is readable, copy and paste the following query into Athena and ensure it returns lines:\n SELECT * FROM \u0026quot;costmaster\u0026quot;.\u0026quot;before\u0026quot; limit 10;    Confirm the after table is readable, copy and paste the following query into Athena and ensure it returns lines:\n SELECT * FROM \u0026quot;costmaster\u0026quot;.\u0026quot;after\u0026quot; limit 10;    You have successfully setup the cost and usage data source. We have a database of licensed and unlicensed usage to analyze and verify.\n  function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/2_analyze_understand\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_cur_queries/queries/analytics/","title":"Analytics","tags":[],"description":"","content":"These are queries for AWS Services under the Analytics product family .\nUse the clipboard in the top right of the text boxes below to copy all of the text to be pasted.\n You may need to change variables used as placeholders in your query. ${table_Name} is a common variable which needs to be replaced. Example: cur_db.cur_table\n Table of Contents   AWS Glue   Query Description This query will provide daily unblended and usage information per linked account for AWS Glue. The output will include detailed information about the resource id (Glue Crawler) and API operation. The cost will be summed and in descending order.\nPricing Please refer to the Glue pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d') AS day_line_item_usage_start_date, line_item_operation, SPLIT_PART(line_item_resource_id, 'crawler/', 2) AS split_line_item_resource_id, SUM(CAST(line_item_usage_amount AS double)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS decimal(16, 8))) AS sum_line_item_unblended_cost FROM ${table_name} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND product_product_name = ('AWS Glue') AND line_item_line_item_type NOT IN ('Tax','Credit','Refund','EdpDiscount','Fee','RIFee') GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d'), line_item_operation, line_item_resource_id ORDER BY day_line_item_usage_start_date, sum_line_item_unblended_cost DESC;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         Amazon Kinesis   Query Description This query will provide daily unblended and usage information per linked account for each Kinesis product (Amazon Kinesis, Amazon Kinesis Firehose, and Amazon Kinesis Analytics). The output will include detailed information about the resource id (Stream, Delivery Stream, etc\u0026hellip;) and API operation. The cost will be summed and in descending order.\nPricing Please refer to the Kinesis pricing pages:\nAmazon Kinesis Data Streams Pricing Amazon Kinesis Data Firehose Pricing Amazon Kinesis Data Analytics Pricing Amazon Kinesis Video Streams pricing Sample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d') AS day_line_item_usage_start_date, SPLIT_PART(line_item_resource_id,':',6) as split_line_item_resource_id, product_product_name, SUM(CAST(line_item_usage_amount AS double)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS decimal(16, 8))) AS sum_line_item_unblended_cost FROM ${table_Name} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND product_product_name IN ('Amazon Kinesis','Amazon Kinesis Firehose','Amazon Kinesis Analytics','Amazon Kinesis Video') AND line_item_line_item_type NOT IN ('Tax','Credit','Refund','EdpDiscount','Fee','RIFee') GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d'), line_item_resource_id, product_product_name ORDER BY day_line_item_usage_start_date, sum_line_item_unblended_cost DESC;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         Amazon Elasticsearch   Query Description This query will provide daily unblended and amortized cost as well as usage information per linked account for Amazon Elasticsearch. The output will include detailed information about the resource id (ES Domain), usage type, and API operation. The usage amount and cost will be summed and the cost will be in descending order. This query includes RI and SP true up which will show any upfront fees to the account that purchased the pricing model.\nPricing Please refer to the Elasticsearch pricing page . Please refer to this blog for Cost Optimization techniques .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d') AS day_line_item_usage_start_date, SPLIT_PART(line_item_resource_id,':',6) as split_line_item_resource_id, product_product_family, product_instance_family, product_instance_type, pricing_term, product_storage_media, product_transfer_type, sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END) \u0026quot;usage_quantity\u0026quot;, sum (\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;, sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0 WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0 WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;reservation_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (\u0026quot;reservation_unused_amortized_upfront_fee_for_billing_period\u0026quot; + \u0026quot;reservation_unused_recurring_fee\u0026quot;) WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN 0 ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) \u0026quot;amortized_cost\u0026quot;, sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (-\u0026quot;savings_plan_amortized_upfront_commitment_for_billing_period\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (-\u0026quot;reservation_amortized_upfront_fee_for_billing_period\u0026quot;) ELSE 0 END) \u0026quot;ri_sp_trueup\u0026quot;, sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN \u0026quot;line_item_unblended_cost\u0026quot; WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN \u0026quot;line_item_unblended_cost\u0026quot;ELSE 0 END) \u0026quot;ri_sp_upfront_fees\u0026quot; FROM ${table_name} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND product_product_name = 'Amazon Elasticsearch Service' AND line_item_line_item_type NOT IN ('Tax','Credit','Refund','EdpDiscount') GROUP BY 1,2,3,4,5,6,7,8,9,10 ORDER BY day_line_item_usage_start_date, product_product_family, unblended_cost DESC;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         Amazon EMR   Query Description This query will provide daily unblended cost and usage information per linked account for Amazon EMR. The cost will be summed and the cost will be in descending order.\nPricing Please refer to the EMR pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d') AS day_line_item_usage_start_date, SPLIT_PART(line_item_usage_type ,':',2) AS split_line_item_usage_type, SUM(CAST(line_item_usage_amount AS double)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS decimal(16,8))) AS sum_line_item_unblended_cost FROM ${table_name} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND product_product_name = 'Amazon Elastic MapReduce' AND line_item_line_item_type NOT IN ('Tax','Credit','Refund','EdpDiscount','Fee','RIFee') GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d'), line_item_usage_type, line_item_line_item_type ORDER BY day_line_item_usage_start_date, sum_line_item_usage_amount, sum_line_item_unblended_cost, split_line_item_usage_type;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         Amazon QuickSight   Query Description This query will provide monthly unblended and usage information per linked account for Amazon QuickSight. The output will include detailed information about the usage type and its usage amount. The cost will be summed and in descending order.\nPricing Please refer to the Amazon QuickSight pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,'%Y-%m') AS month_line_item_usage_start_date, CASE WHEN LOWER(line_item_usage_type) LIKE 'qs-user-enterprise%' THEN 'Users - Enterprise' WHEN LOWER(line_item_usage_type) LIKE 'qs-user-standard%' THEN 'Users - Standard' WHEN LOWER(line_item_usage_type) LIKE 'qs-reader-usage%' THEN 'Reader Usage' WHEN LOWER(line_item_usage_type) LIKE '%spice' THEN 'SPICE' ELSE line_item_usage_type END as purchase_type_line_item_usage_type, SUM(CAST(line_item_usage_amount AS double)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS decimal(16,8))) AS sum_line_item_unblended_cost FROM ${table_name} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND product_product_name = 'Amazon QuickSight' AND line_item_line_item_type NOT IN ('Tax','Credit','Refund','EdpDiscount','Fee','RIFee') GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,'%Y-%m'), CASE WHEN LOWER(line_item_usage_type) LIKE 'qs-user-enterprise%' THEN 'Users - Enterprise' WHEN LOWER(line_item_usage_type) LIKE 'qs-user-standard%' THEN 'Users - Standard' WHEN LOWER(line_item_usage_type) LIKE 'qs-reader-usage%' THEN 'Reader Usage' WHEN LOWER(line_item_usage_type) LIKE '%spice' THEN 'SPICE' ELSE line_item_usage_type END ORDER BY month_line_item_usage_start_date, sum_line_item_unblended_cost DESC;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback       CUR queries are provided as is. We recommend validating your data by comparing it against your monthly bill and Cost Explorer prior to making any financial decisions. If you wish to provide feedback on these queries, there is an error, or you want to make a suggestion, please email: curquery@amazon.com "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_enterprise_dashboards/1_create_cost_intelligence/","title":"Create Cost Intelligence Dashboard","tags":[],"description":"","content":"Authors  Alee Whitman, Commercial Architect (AWS OPTICS)  Contributors  Arun Santhosh, Specialist SA (Amazon QuickSight) Kareem Syed-Mohammed, Senior Product Manager - Technical (Amazon QuickSight)  FAQ The FAQ for this dashboard is here. Request Template Access Ensure you have requested access to the Cost Intelligence template here. Optional: Advanced Setup using a CloudFormation Template This section is optional and automates the creation of the Cost Intelligence Dashboard using a CloudFormation template. The CloudFormation template allows you to complete the lab in less than half the time as the standard setup. You will require permissions to modify CloudFormation templates, create an IAM role, create an S3 Bucket, and create an Athena Database. If you do not have the required permissions skip over this section to continue using the standard setup.\n  Click here to continue with the CloudFormation Advanced Setup   Create the Cost Intelligence Dashboard using a CloudFormation Template   Login via SSO in your Cost Optimization account\n  Click the Launch CloudFormation button below to open the pre-populated stack template in your CloudFormation console and select Next\n Launch CloudFormation Template      Enter a Stack name for your template such as Cost-Intelligence-Dashboard-QuickSight   Review 1stReadMe parameter to confirm prerequisites before specifying the other parameters   Validate your Athena primary workgroup has an output location\n Open a new tab or window and navigate to the Athena console Select Workgroup: primary  Click the bubble next to primary and then select view detail  Confirm your Query result location is configured with an S3 bucket path.  If configured, continue to step 6. If not configured, continue to setting up by clicking Edit workgroup    Add the S3 bucket path you have selected for your Query result location and click save     Update your BucketFolderPath with the S3 path where your year partitions of CUR data are stored To validate the correct path for your year partitions of the CUR data follow the tasks below:\n Open a new tab or window and navigate to the S3 console Select the S3 Bucket your CUR is located in  Navigate your folders until you find the folder with the year partitions of the CUR  Tip: Your yearly partitions folder is located in the folder with your .yml file, monthly folders, and status report   Add the identified BucketFolderPath to the CloudFormation parameter making sure to not add trailing / (eg - BucketName/FolderName/\u0026hellip;/FolderName)  Tip: copy and paste the S3 URI then remove the leading \u0026lsquo;s3://\u0026rsquo; and the ending \u0026lsquo;/'      Update your QuickSightUser with your QuickSight username To validate your QuickSight complete the tasks below:\n Open a new tab or window and navigate to the QuickSight console Click on the profile icon in the top right side of the navigation bar, then select Manage QuickSight  Locate your username in the manage users section     Select Next at the bottom of Specify stack details and then select Next again on the Configure stack options page\n  Review the configuration, click I acknowledge that AWS CloudFormation might create IAM resources, and click Create stack.   You will see the stack will start in CREATE_IN_PROGRESS   Once complete, the stack will show CREATE_COMPLETE   Navigate to Dashboards page in your QuickSight console, click on your Dashboard name   Skip to the bottom of the page to step 8 of the Create the Dashboard section to finish setting up your dashboard\n    To create the dashboard using the standard setup move to the Create Athena Views section\nCreate Athena Views The data source for the dashboard will be an Athena view of your existing Cost and Usage Report (CUR). The default dashboard assumes you have both Savings Plans and Reserved Instances, if not you will need to create the alternate views.\n  Login via SSO in your Cost Optimization account, go into the Athena console:\n  Modify and run the following queries to confirm if you have Savings Plans, and Reserved Instances in your usage. If no lines are returned, you have no Savings Plans or Reserved Instances. Replace (database).(tablename) and run the following:\nSavings Plans:\n select * from (database).(tablename) where savings_plan_savings_plan_a_r_n not like '' limit 10  Reserved Instances:\n select * from (database).(tablename) where reservation_reservation_a_r_n not like '' limit 10    Create the Summary view by modifying the following code, and executing it in Athena:\n View1 - Summary     Create the EC2_Running_Cost view by modifying the following code, and executing it in Athena:\n View2 - EC2_Running_Cost     Create the Compute savings plan eligible spend view by modifying the following code, and executing it in Athena:\n View3 - compute savings plan eligible spend     Create the s3 view by modifying the following code, and executing it in Athena:\n View4 - s3     Create the RI SP Mapping view by modifying the following code, and executing it in Athena:\n View5 - RI SP Mapping     The Athena Views are updated to reflect any additions in the cost and usage report. If you created your dashboard prior to October 26, 2020 you will want to update to the latest views.\n Create QuickSight Data Sets We will now create the data sets in QuickSight from the Athena views.\n  Go to the QuickSight service homepage\n  Click Manage data:   Click New dataset   Click Athena   Enter a data source name of Cost_Dashboard and click Create data source:   Select the costmaster database, and the summary_view table, click Edit/Preview data:   Select SPICE to change your Query mode:   Hover over payer_account_id to get the drop down arrow and click on it the then hover over Change data type then select # Int:   Repeat step 8 for linked_account_id to change it to Int\n  Select Save:   Select the summary_view Data Set:   Click Schedule refresh:   Click Create:   Enter a schedule, it needs to be refreshed daily, and click Create:   Click Cancel to exit:   Click the x in the top corner:   Repeat steps 3-16, creating data sets with the remaining Athena views. The data source name will be Cost_Dashboard, and select the following views as the table:\n   s3_view ec2_running_cost compute_savings_plan_eligible_spend   Select summary_view Data Set:   Select Edit data set\n  Select Add Data:   Select your ri_sp_mapping view and click Select:   Select the two circles to open the Join configuration then select Left to change your join type:   Select Add a new join clause two times so you have 3 join clauses:   Create following 3 join clauses then click Apply:\n ri_sp_arn = ri_sp_arn_mapping payer_account_id = payer_account_id_mapping billing_period = billing_period_mapping     Change the payer_account_id_mapping field to Int:   Select Save   You now have your data set setup ready to create a visualization.\n Create the Dashboard We will now use the CLI to create the dashboard from the Cost Intelligence Dashboard template, then create an Analysis you can customize and modify in the next step.\n  Go to this page to request access to the template. Enter your AWS AccountID and click Submit: Template Access   Edit the following command, replacing AccountID and region, then using the CLI list the QuickSight datasets and copy the Name and Arn for the 4 datasets: s3_view, ec2_running_cost, compute_savings_plan_eligible_spend, summary_view:\n aws quicksight list-data-sets --aws-account-id (AccountID) --region (region)     Get your users Arn by editing the following command, replacing AccountID and region, then using the CLI run the command:\n aws quicksight list-users --aws-account-id (AccountID) --namespace default --region (region)     Create a local file create-dashboard.json with the text below, replace the values (Account ID) on line 2, (User ARN) one line 7, and each Dataset (ARN) on lines 25, 30, 35, 40:\n { \u0026quot;AwsAccountId\u0026quot;: \u0026quot;(Account ID)\u0026quot;, \u0026quot;DashboardId\u0026quot;: \u0026quot;cost_intelligence_dashboard\u0026quot;, \u0026quot;Name\u0026quot;: \u0026quot;Cost Intelligence Dashboard\u0026quot;, \u0026quot;Permissions\u0026quot;: [ { \u0026quot;Principal\u0026quot;: \u0026quot;(User ARN)\u0026quot;, \u0026quot;Actions\u0026quot;: [ \u0026quot;quicksight:DescribeDashboard\u0026quot;, \u0026quot;quicksight:ListDashboardVersions\u0026quot;, \u0026quot;quicksight:UpdateDashboardPermissions\u0026quot;, \u0026quot;quicksight:QueryDashboard\u0026quot;, \u0026quot;quicksight:UpdateDashboard\u0026quot;, \u0026quot;quicksight:DeleteDashboard\u0026quot;, \u0026quot;quicksight:DescribeDashboardPermissions\u0026quot;, \u0026quot;quicksight:UpdateDashboardPublishedVersion\u0026quot; ] } ], \u0026quot;SourceEntity\u0026quot;: { \u0026quot;SourceTemplate\u0026quot;: { \u0026quot;DataSetReferences\u0026quot;: [ { \u0026quot;DataSetPlaceholder\u0026quot;: \u0026quot;summary_view\u0026quot;, \u0026quot;DataSetArn\u0026quot;: \u0026quot;(Summary Dataset ARN)\u0026quot; }, { \u0026quot;DataSetPlaceholder\u0026quot;: \u0026quot;ec2_running_cost\u0026quot;, \u0026quot;DataSetArn\u0026quot;: \u0026quot;(ec2_running_cost Dataset ARN)\u0026quot; }, { \u0026quot;DataSetPlaceholder\u0026quot;: \u0026quot;compute_savings_plan_eligible_spend\u0026quot;, \u0026quot;DataSetArn\u0026quot;: \u0026quot;(compute_savings_plan_eligible_spend Dataset ARN)\u0026quot; }, { \u0026quot;DataSetPlaceholder\u0026quot;: \u0026quot;s3_view\u0026quot;, \u0026quot;DataSetArn\u0026quot;: \u0026quot;(s3_view Dataset ARN)\u0026quot; } ], \u0026quot;Arn\u0026quot;: \u0026quot;arn:aws:quicksight:us-east-1:869004330191:template/optics_cost_analysis\u0026quot; } }, \u0026quot;VersionDescription\u0026quot;: \u0026quot;1\u0026quot; }    To create the dashboard from the template, edit then run the following command, replacing (region) and you should receive a 202 response:\n aws quicksight create-dashboard --cli-input-json file://create-dashboard.json --region (region)     After a few minutes the dashboard will become available in QuickSight under All dashboard, click on the Dashboard name:     Click here - if you do not see your dashboard   Edit and run the following command:\n aws quicksight describe-dashboard --aws-account-id (YOUR ACCOUNT ID) --dashboard-id cost_intelligence_dashboard --region (region)  Correct the listed errors and run the delete-dashboard command followed by the original create-dashboard command:\n aws quicksight delete-dashboard --aws-account-id (YOUR ACCOUNT ID) --dashboard-id cost_intelligence_dashboard --region (region)      Click Share, click Share dashboard:,   Click Manage dashboard access:   Add the required users, or share with all users, ensure you check Save as for each user, then click the x to close the window:   Click Save as:   Enter an Analysis name and click Create:   Perform steps 11 and 12 above to create additional analyses for other teams, this will allow each team to have their own customizable analysis.\n You will now have an analysis created from the template that you can edit and modify:   You have successfully created the analysis from a template. For a detailed description of the dashboard read the FAQ   function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/2_modify_cost_intelligence\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_clock_source_performance/1_deploy/","title":"Deploying the infrastructure","tags":[],"description":"","content":" The CloudFormation template that accompanies this lab requires the ability to create IAM Roles and IAM Instance Profiles. If the account you are using does not have these capabilities, you will not be able to complete this lab.\n   Download the time_test.yaml CloudFormation template to your machine.\n  This lab assumes you will be deploying to the default VPC within your AWS account. If you wish to deploy to a different VPC, just select the subnet that corresponds to your VPC. If you have modified the default VPC or are using a VPC you have created, ensure that the subnet you are deploying the EC2 instances into can communicate with the internet and with AWS Systems Manager. One method for this is to Create a VPC endpoint for SSM    Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and click Create Stack \u0026gt; With new resources   Leave Prepare template setting as-is\n For Template source select Upload a template file Click Choose file and supply the CloudFormation template you downloaded: time_test.yaml     Click Next\n  For Stack name use TimeTest\n  Parameters\n  Look over the Parameters and their default values.\n  Stack Name  Whatever you want to call the stack for this test\n  EC2InstanceSubnetId  The subnet you wish to deploy the 2 EC2 instances into for testing.\n  KVMNodeInstanceType  What size KVM Node (which implies it is a Nitro instance) to use for the test\n  KeyName  SSH keyname to use for the test (in case you want to ssh into the box to run additional tests)\n  LatestAmiId  This will auto-populate with the latest version of the Amazon Linux AMI\n  XenNodeInstanceType  Which non-nitro Xen based node to use for the test\n  Click Next\n    For Configure stack options we recommend configuring tags, which are key-value pairs, that can help you identify your stacks and the resources they create. For example, enter Owner in the left column which is the key, and your email address in the right column which is the value. We will not use additional permissions or advanced options so click Next. For more information, see Setting AWS CloudFormation Stack Options .\n  For Review\n Review the contents of the page At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Create stack     This will take you to the CloudFormation stack status page, showing the stack creation in progress.\n Click on the Events tab Scroll through the listing. It shows the activities performed by CloudFormation (newest events at top), such as starting to create a resource and then completing the resource creation. Any errors encountered during the creation of the stack will be listed in this tab.     When it shows status CREATE_COMPLETE, then you are finished with this step.\n   function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/2_testing_before\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/reliability/100_labs/100_deploy_cloudformation/","title":"Level 100: Deploy a Reliable Multi-tier Infrastructure using CloudFormation","tags":["implement_change"],"description":"Learn to improve reliability of a service by using automation to deploy a reliable cloud infrastructure","content":"Author  Seth Eliot, Principal Reliability Solutions Architect, AWS Well-Architected  Introduction This hands-on lab will guide you through the steps to improve reliability of a service by using automation to deploy a reliable cloud infrastructure. When this lab is completed, you will have deployed two CloudFormation templates. The first will deploy an Amazon Virtual Private Cloud (VPC). The second will deploy into your VPC, a reliable 3-tier infrastructure using Amazon EC2 distributed across three Availability Zones. You will then review the features of the deployed infrastructure and learn how they contribute to reliability.\nAWS Well-Architected offers two different CloudFormation labs illustrating Reliability best practices. Choose which lab you prefer (or do both):\n This lab is a 100 lab where you will do deployment-only using an AWS CloudFormation template. You will deploy a multi-tier reliable architecture. If you would prefer a more advanced lab where you create and modify CloudFormation, please see the 200 level lab Deploy and Update CloudFormation . Because the 200 lab includes modification and update as part of the exercise, it uses a simplified, single-tier, non-reliable architecture.  The skills you learn will help you build resilient workloads in alignment with the AWS Well-Architected Framework best practices for reliability:\nThe architecture of the infrastructure you will deploy is represented by this diagram: Goals By the end of this lab, you will be able to:\n Automate infrastructure deployment for a workload Understand how the deployed workload infrastructure contributes to reliability of the workload  Prerequisites If you are running this at an AWS sponsored workshop then you may be provided with an AWS Account to use, in which case the following pre-requisites will be satisfied by the provided AWS account. If you are running this using your own AWS Account, then please note the following prerequisites:\n An AWS Account that you are able to use for testing. This account MUST NOT be used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to create IAM Roles, EC2 instances, S3 buckets, DynamoDb tables, VPCs, Subnets, and Internet Gateways   function prevStep(){ window.open(\"\", \"_self\") } function nextStep(){ window.open(\".\\/1_deploy_vpc\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool  Start Lab  Steps:  Deploy VPC using CloudFormation   Deploy Web Application and Infrastructure using CloudFormation   Explore the Web Application   Explore the CloudFormation Template   Tear down this lab   Costs NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\n  This lab will cost approximately $5.50 per day when deployed The majority of this cost is the charge for NatGateway-Hours Please follow the directions for Tear Down to avoid unwanted costs after you have concluded this lab  "},{"uri":"https://wellarchitectedlabs.com/reliability/100_labs/","title":"100 Labs","tags":[],"description":"","content":"List of labs available  Level 100: Deploy a Reliable Multi-tier Infrastructure using CloudFormation  Learn to improve reliability of a service by using automation to deploy a reliable cloud infrastructure\n "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_automated_deployment_of_iam_groups_and_roles/1_cfn_create_iam_groups_policies/","title":"AWS CloudFormation to Create Groups, Policies and Roles with MFA Enforced","tags":[],"description":"","content":"Using AWS CloudFormation we are going to deploy a set of groups, roles, and managed policies that will help with your security \u0026ldquo;baseline\u0026rdquo; of your AWS account.\n1.1 Create AWS CloudFormation Stack  Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Click Create stack.  Enter the following Amazon S3 URL: https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/baseline-iam.yaml and click Next.  Enter the following details:   Stack name: The name of this stack. For this lab, use baseline-iam. AllowRegion: A single region to restrict access, enter your preferred region. BaselineExportName: The CloudFormation export name prefix used with the resource name for the resources created, for example, Baseline-PrivilegedAdminRole. BaselineNamePrefix: The prefix for roles, groups, and policies created by this stack. IdentityManagementAccount: (optional) AccountId that contains centralized IAM users and is trusted to assume all roles, or blank for no cross-account trust. Note that the trusted account needs to be appropriately secured. OrganizationsRootAccount: (optional) AccountId that is trusted to assume Organizations role, or blank for no cross-account trust. Note that the trusted account needs to be appropriately secured. ToolingManagementAccount: AccountId that is trusted to assume the ReadOnly and StackSet roles, or blank for no cross-account trust. Note that the trusted account needs to be appropriately secured.  At the bottom of the page click Next. In this lab, we won\u0026rsquo;t add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you\u0026rsquo;re satisfied with the configuration, check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack.  After a few minutes the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE. You have now set up a number of managed polices, groups, and roles that you can test to improve your AWS security!  "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_automated_deployment_of_web_application_firewall/1_config_waf/","title":"Configure AWS WAF","tags":[],"description":"","content":"Using AWS CloudFormation , we are going to deploy a basic example AWS WAF configuration for use with CloudFront.\n Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create stack.  Enter the following Amazon S3 URL: https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/waf-global.yaml and click Next.  Enter the following details:   Stack name: The name of this stack. For this lab, use waf. WAFName: Enter the base name to be used for resource and export names for this stack. For this lab, you can use Lab1. WAFCloudWatchPrefix: Enter the name of the CloudWatch prefix to use for each rule using alphanumeric characters only. For this lab, you can use Lab1. The remainder of the parameters can be left as defaults.  At the bottom of the page click Next. In this lab, we won\u0026rsquo;t add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you\u0026rsquo;re satisfied with the configuration, click Create stack. After a few minutes the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE. You have now set up a basic AWS WAF configuration ready for CloudFront to use!  "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_cloudfront_for_web_application/1_config_cloudfront/","title":"Configure CloudFront - EC2 or Load Balancer","tags":[],"description":"","content":"Using the AWS Management Console, we will create a CloudFront distribution, and link it to the AWS WAF ACL we previously created.\n Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. From the console dashboard, choose Create Distribution.  Click Get Started in the Web section.  Specify the following settings for the distribution:   In Origin Domain Name enter the DNS or domain name from your elastic load balancer or EC2 instance.   Click Create Distribution. For more information on the other configuration options, see Values That You Specify When You Create or Update a Web Distribution in the CloudFront documentation.  After CloudFront creates your distribution, the value of the Status column for your distribution will change from In Progress to Deployed.  When your distribution is deployed, confirm that you can access your content using your new CloudFront URL or CNAME. Copy the Domain Name into a web browser to test.  For more information, see Testing a Web Distribution in the CloudFront documentation. 7. You have now configured Amazon CloudFront with basic settings.\nFor more information on configuring CloudFront, see Viewing and Updating CloudFront Distributions in the CloudFront documentation.\n"},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_1_aws_account_setup/1_iam_access/","title":"Configure IAM access","tags":[],"description":"","content":"NOTE: You will need to sign into the management account with root account credentials to perform this action.\nTo allow access to your billing information without using the root credentials you need to enable IAM access. This allows other users (non-root) to access billing information in the management account. This approach provides individual sign-in information for each user, and you can grant each user only the permissions they need to work with your account. For example, you can grant your financial teams access to the billing information only, and ensure they dont have access to resources in the account.\n  Log in to your management account as the root user, Click on the account name in the top right, and click on My Account from the menu:   Scroll down to IAM User and Role Access to Billing Information, and click Edit:   Select Activate IAM Access and click on Update:   Confirm that IAM user/role access to billing information is activated:   You will now be able to provide access to non-root users to billing information via IAM policies.\nNOTE: Logout as the root user before continuing.\n function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/2_account_structure\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_100_quick_steps_to_security_success/1_control_tower/","title":"Control Tower","tags":[],"description":"","content":"Leverage AWS ControlTower to create a set of Core AWS accounts and setup additional accounts for shared services such as build tools and individual environments for your workload. If you currently only have one account, create a new AWS account for your Control Tower management account and invite your existing account to join as a legacy AWS account. You can then migrate your workload to new accounts over time.\nControl Tower applies a number of Service Control Policies to all accounts in your AWS Organization. This will prevent modification of AWS CloudTrail trails and AWS Config rule sets in addition to a number of actions on resources matching the pattern \u0026lsquo;*aws-controltower* or \u0026lsquo;*AWSControlTower*\u0026rsquo;. If you are enabling Control Tower in an existing account you can use an AWS Config conformance pack to evaluate how your accounts may be affected by some AWS Control Tower guardrails. See AWS Control Tower Detective Guardrails as an AWS Config Conformance Pack .\nWalk through   Understand best practices for your AWS environment and plan your landing zone . If you are building your own landing zone you should mirror the landing zone structure . This structure has a root account, specific accounts for logging and auditing, and allows for you to create an account per workload environment. If you are currently operating in a single account it is best practice to sign up for a new management account to enable Control Tower in and invite the existing account to join as a legacy account. This will allow you to continue to use your existing account as is but still apply baseline security controls and logging to it. If you are currently leveraging AWS Organizations it is best practice to sign up for a new management account if your current management account is used for purposes other than enabling Organizations and sharing identity. The only resources in your management account are those for enabling Control Tower, other guard rails and identity.\n  (If required) Sign up for a new management account   Enable Control Tower on the management account for your organization\n If you have an existing organization refer to the documentation on applying Control Tower to existing organizations  If you are not leveraging Control Tower, create an AWS Organization in the root account    Invite any existing AWS accounts by enrolling an existing account in Control Tower . If you are not using Control Tower then invite an existing account to join your organization   For each additional AWS account required use the account factory to create a new account. Consider applying best practices as a baseline such as lock away your AWS account root user access keys and using multi-factor authentication . If you are not leveraging Control Tower then\n Create a new account in organizations . Make note of the organizations account access role. Create a new IAM role in the root account that has permission to assume that role to access the new AWS account *. Setup a logging account, secure Amazon S3 bucket and turn on your AWS Organization CloudTrail     "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_5_cost_visualization/1_create_dataset/","title":"Create a data set","tags":[],"description":"","content":"We will create a data set so that QuickSight can access our Athena data set, and visualize our CUR data.\n  Log on to the console via SSO, go to the QuickSight service, Enter your email address and click Continue:   Click Manage data in the top right:   Click New data set:   Click Athena:   Enter a Data source name, and click Create data source:   Select the costmaster database, and then the CUR table you created in Athena, and click Select:   Select Directly query your data, and click Visualize:   You have now configured a dataset to be able to create visualizations.\n  function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/2_create_visualizations\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_2_cost_and_usage_governance/1_create_test_group/","title":"Create a group of users for testing","tags":[],"description":"","content":"This lab requires you to develop a restrictive IAM policy, then apply the policies to a group of users, then login as a user in that group and verify the policy. We will create this test group.\n  Go to the IAM service page:   Click on Groups, click Create New Group:   Set the group name to CostTest and click Next Step:   Click Next Step:   Click Create Group:   Click Users:   Click Add user:   Configure the user as follows:\n Username: TestUser1 Access type: AWS Management Console access Console password: Autogenerated password Un-select Require password reset Click Next: Permissions     Select the CostTest group, and click Next: Tags:   Click Next: Review:   Review the details and click Create user:   Record the logon link, the User and the Password for later use, click Close:    function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/2_ec2_restrict_region\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_2_cost_and_usage_governance/1_budget_forecast/","title":"Create and implement an AWS Budget for monthly forecasted cost ","tags":[],"description":"","content":"Budgets allow you to manage cost and usage by providing notifications when cost or usage are outside of configured amounts. They cannot be used to restrict actions, only notify on usage after it has occurred.\nBudgets and notifications are updated when your billing data is updated, which is at least once per day.\nNOTE: You may not receive an alarm for a forecasted budget if your account is new. Forecasting requires existing usage within the account.\nCreate a monthly cost budget for your account We will create a monthly cost budget which will notify if the forecasted amount exceeds the budget.\n  Log into the console via SSO, go to the Billing console:   Select Budgets from the left menu:   Click on Create a budget:   Ensure Cost Budget is selected, and click Set your budget \u0026gt;:   Create a cost budget, enter the following details:\n Name: CostBudget1 Period: Monthly Budget effective dates: Recurring Budget Start Month: (select current month) Budget amount: Fixed Budgeted amount: $1 (enter an amount a lot LESS than last months cost), Other fields: leave as defaults:     Scroll down and click Configure alerts \u0026gt;:   Select:\n Send alert based on: Forecasted Costs Alert threshold: 100% of budgeted amount Email contacts: (your email address) Click on Confirm budget \u0026gt;:     Review the configuration, and click Create:   You should see the current forecast will exceed the budget (it should be red, you may need to refresh your browser):   10: You will receive an email similar to this within a few minutes: You have created a forecasted budget, when your forecasted costs for the entire account are predicted to exceed the forecast, you will receive a notification.\n  function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/2_budget_ec2actual\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_iam_permission_boundaries_delegating_role_creation/1_create_policies/","title":"Create IAM policies","tags":[],"description":"","content":"1.1 Create policy for permission boundary This policy will be used for the permission boundary when the developer role creates their own user role with their delegated permissions. In this lab using AWS IAM we are only going to allow the us-east-1 (North Virginia) and us-west-1 (North California) regions, optionally you can change these to your favourite regions and add / remove as many as you need. The only service actions we are going to allow in these regions are AWS EC2 and AWS Lambda, note that these services require additional supporting actions if you were to re-use this policy after this lab, depending on your requirements.\n Sign in to the AWS Management Console as an IAM user with MFA enabled that can assume roles in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . If you need to enable MFA follow the IAM User Guide . You will need to log out and back in again with MFA so your session has MFA active. In the navigation pane, click Policies and then click Create policy.  On the Create policy page click the JSON tab.  Replace the example start of the policy that is already in the editor with the policy below.  { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;EC2RestrictRegion\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;ec2:*\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;Condition\u0026quot;: { \u0026quot;StringEquals\u0026quot;: { \u0026quot;aws:RequestedRegion\u0026quot;: [ \u0026quot;us-east-1\u0026quot;, \u0026quot;us-west-1\u0026quot; ] } } }, { \u0026quot;Sid\u0026quot;: \u0026quot;LambdaRestrictRegion\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;lambda:*\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;Condition\u0026quot;: { \u0026quot;StringEquals\u0026quot;: { \u0026quot;aws:RequestedRegion\u0026quot;: [ \u0026quot;us-east-1\u0026quot;, \u0026quot;us-west-1\u0026quot; ] } } } ] } Click Review policy. Enter the name of restrict-region-boundary and any description to help you identify the policy, verify the summary and then click Create policy.  1.2 Create developer IAM restricted policy This policy will be attached to the developer role, and will allow the developer to create policies and roles with a name prefix of app1, and only if the permission boundary restrict-region-boundary is attached. You will need to change the account id placeholders of 123456789012 to your account number in 5 places. You can find your account id by navigating to https://console.aws.amazon.com/billing/home?#/account in the console. Naming prefixes are useful when you have different teams or in this case different applications running in the same AWS account. They can be used to keep your resources looking tidy, and also in IAM policy as the resource as we are doing here.\n Create a managed policy using the JSON policy below and name of createrole-restrict-region-boundary.  { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;CreatePolicy\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;iam:CreatePolicy\u0026quot;, \u0026quot;iam:CreatePolicyVersion\u0026quot;, \u0026quot;iam:DeletePolicyVersion\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:iam::123456789012:policy/app1*\u0026quot; }, { \u0026quot;Sid\u0026quot;: \u0026quot;CreateRole\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;iam:CreateRole\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:iam::123456789012:role/app1*\u0026quot;, \u0026quot;Condition\u0026quot;: { \u0026quot;StringEquals\u0026quot;: { \u0026quot;iam:PermissionsBoundary\u0026quot;: \u0026quot;arn:aws:iam::123456789012:policy/restrict-region-boundary\u0026quot; } } }, { \u0026quot;Sid\u0026quot;: \u0026quot;AttachDetachRolePolicy\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;iam:DetachRolePolicy\u0026quot;, \u0026quot;iam:AttachRolePolicy\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:iam::123456789012:role/app1*\u0026quot;, \u0026quot;Condition\u0026quot;: { \u0026quot;ArnEquals\u0026quot;: { \u0026quot;iam:PolicyARN\u0026quot;: [ \u0026quot;arn:aws:iam::123456789012:policy/*\u0026quot;, \u0026quot;arn:aws:iam::aws:policy/*\u0026quot; ] } } } ] } 1.3 Create developer IAM console access policy This policy allows list and read type IAM service actions so you can see what you have created using the console. Note that it is not a requirement if you simply wanted to create the role and policy, or if you were using the Command Line Interface (CLI) or CloudFormation.\n Create a managed policy using the JSON policy below and name of iam-restricted-list-read.  { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;Get\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;iam:ListPolicies\u0026quot;, \u0026quot;iam:GetRole\u0026quot;, \u0026quot;iam:GetPolicyVersion\u0026quot;, \u0026quot;iam:ListRoleTags\u0026quot;, \u0026quot;iam:GetPolicy\u0026quot;, \u0026quot;iam:ListPolicyVersions\u0026quot;, \u0026quot;iam:ListAttachedRolePolicies\u0026quot;, \u0026quot;iam:ListRoles\u0026quot;, \u0026quot;iam:ListRolePolicies\u0026quot;, \u0026quot;iam:GetRolePolicy\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] } "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_iam_tag_based_access_control_for_ec2/1_create_policies/","title":"Create IAM policies","tags":[],"description":"","content":"The policies are split into five different functions for demonstration purposes, you may like to modify and combine them to use after this lab to your exact requirements. In addition to enforcing tags, a region restriction only allow regions us-east-1 (North Virginia) and us-west-1 (North California).\n1.1 Create policy named ec2-list-read This policy allows read only permissions with a region condition. The only service actions we are going to allow are EC2, note that you typically require additional supporting actions such as Elastic Load Balancing if you were to re-use this policy after this lab, depending on your requirements.\n Sign in to the AWS Management Console as an IAM user with MFA enabled that can assume roles in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . If you need to enable MFA follow the IAM User Guide . You will need to log out and back in again with MFA so your session has MFA active. In the navigation pane, click Policies and then click Create policy.  On the Create policy page click the JSON tab.  Replace the example start of the policy that is already in the editor with the policy below.  { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;ec2listread\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;ec2:Describe*\u0026quot;, \u0026quot;ec2:Get*\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;Condition\u0026quot;: { \u0026quot;StringEquals\u0026quot;: { \u0026quot;aws:RequestedRegion\u0026quot;: [ \u0026quot;us-east-1\u0026quot;, \u0026quot;us-west-1\u0026quot; ] } } } ] } Click Review policy. Enter the name of ec2-list-read and any description to help you identify the policy, verify the summary and then click Create policy.  1.2 Create policy named ec2-create-tags This policy allows the creation of tags for EC2, with a condition of the action being RunInstances , which is launching an instance.\n Create a managed policy using the JSON policy below and name of ec2-create-tags.  { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;ec2createtags\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;ec2:CreateTags\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;Condition\u0026quot;: { \u0026quot;StringEquals\u0026quot;: { \u0026quot;ec2:CreateAction\u0026quot;: \u0026quot;RunInstances\u0026quot; } } } ] } 1.3 Create policy named ec2-create-tags-existing This policy allows creation (and overwriting) of EC2 tags only if the resources are already tagged Team / Alpha.\n Create a managed policy using the JSON policy below and name of ec2-create-tags-existing.  { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;ec2createtagsexisting\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;ec2:CreateTags\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;Condition\u0026quot;: { \u0026quot;StringEquals\u0026quot;: { \u0026quot;ec2:ResourceTag/Team\u0026quot;: \u0026quot;Alpha\u0026quot; }, \u0026quot;ForAllValues:StringEquals\u0026quot;: { \u0026quot;aws:TagKeys\u0026quot;: [ \u0026quot;Team\u0026quot;, \u0026quot;Name\u0026quot; ] }, \u0026quot;StringEqualsIfExists\u0026quot;: { \u0026quot;aws:RequestTag/Team\u0026quot;: \u0026quot;Alpha\u0026quot; } } } ] } 1.4 Create policy named ec2-run-instances This first section of this policy allows instances to be launched, only if the conditions of region and specific tag keys are matched. The second section allows other resources to be created at instance launch time with region condition.\n Create a managed policy using the JSON policy below and name of ec2-run-instances.  { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;ec2runinstances\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;ec2:RunInstances\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:ec2:*:*:instance/*\u0026quot;, \u0026quot;Condition\u0026quot;: { \u0026quot;StringEquals\u0026quot;: { \u0026quot;aws:RequestedRegion\u0026quot;: [ \u0026quot;us-east-1\u0026quot;, \u0026quot;us-west-1\u0026quot; ], \u0026quot;aws:RequestTag/Team\u0026quot;: \u0026quot;Alpha\u0026quot; }, \u0026quot;ForAllValues:StringEquals\u0026quot;: { \u0026quot;aws:TagKeys\u0026quot;: [ \u0026quot;Name\u0026quot;, \u0026quot;Team\u0026quot; ] } } }, { \u0026quot;Sid\u0026quot;: \u0026quot;ec2runinstancesother\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;ec2:RunInstances\u0026quot;, \u0026quot;Resource\u0026quot;: [ \u0026quot;arn:aws:ec2:*:*:subnet/*\u0026quot;, \u0026quot;arn:aws:ec2:*:*:key-pair/*\u0026quot;, \u0026quot;arn:aws:ec2:*::snapshot/*\u0026quot;, \u0026quot;arn:aws:ec2:*:*:launch-template/*\u0026quot;, \u0026quot;arn:aws:ec2:*:*:volume/*\u0026quot;, \u0026quot;arn:aws:ec2:*:*:security-group/*\u0026quot;, \u0026quot;arn:aws:ec2:*:*:placement-group/*\u0026quot;, \u0026quot;arn:aws:ec2:*:*:network-interface/*\u0026quot;, \u0026quot;arn:aws:ec2:*::image/*\u0026quot; ], \u0026quot;Condition\u0026quot;: { \u0026quot;StringEquals\u0026quot;: { \u0026quot;aws:RequestedRegion\u0026quot;: [ \u0026quot;us-east-1\u0026quot;, \u0026quot;us-west-1\u0026quot; ] } } } ] } 1.5 Create policy named ec2-manage-instances This policy allows reboot, terminate, start and stop of instances, with a condition of the key Team is Alpha and region.\n Create a managed policy using the JSON policy below and name of ec2-manage-instances.  { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;ec2manageinstances\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;ec2:RebootInstances\u0026quot;, \u0026quot;ec2:TerminateInstances\u0026quot;, \u0026quot;ec2:StartInstances\u0026quot;, \u0026quot;ec2:StopInstances\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;Condition\u0026quot;: { \u0026quot;StringEquals\u0026quot;: { \u0026quot;ec2:ResourceTag/Team\u0026quot;: \u0026quot;Alpha\u0026quot;, \u0026quot;aws:RequestedRegion\u0026quot;: [ \u0026quot;us-east-1\u0026quot;, \u0026quot;us-west-1\u0026quot; ] } } } ] } "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_lambda_cross_account_iam_role_assumption/1_create_role_acct_2/","title":"Create role for Lambda in account 2","tags":[],"description":"","content":"  Sign in to the AWS Management Console as an IAM user or role in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ .\n  Click Roles on the left, then create role.\n  Click Another AWS account, enter the account id for account 1 (the origin), then click Next: Permissions.\n  Do not select any managed policies, click Next: Tags.\n  Click Next: Review.\n  Enter LambdaS3ListBuckets for the Role name then click Create role.\n  From the list of roles click the name of LambdaS3ListBuckets.\n  Copy the Role ARN and store for use later in this lab.\n  Click Add inline policy, then click JSON tab.\n  Replace the sample json with the following, then click Review Policy.\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;S3ListAllMyBuckets\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;s3:ListAllMyBuckets\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] }    Name this policy LambdaS3ListBucketsPolicy, then click Create policy.\n  "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_automated_deployment_of_detective_controls/1_create_stack/","title":"Create Stack","tags":[],"description":"","content":"Creating this CloudFormation stack will configure CloudTrail including a new trail, an S3 bucket, and a CloudWatch Logs group for CloudTrail logs. You can optionally configure AWS Config and Amazon GuardDuty by setting the CloudFormation parameter for each.\n  Download the latest version of the CloudFormation template here: cloudtrail-config-guardduty.yaml   Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and click Create Stack \u0026gt; With new resources   Leave Prepare template setting as-is\n For Template source select Upload a template file Click Choose file and supply the CloudFormation template you downloaded: cloudtrail-config-guardduty.yaml     Click Next\n  For Stack name use DetectiveControls\n  Parameters\n  Look over the Parameters and their default values.\n  Under General section only enable the service if you have not configured already. CloudTrail is enabled by default, if you have enabled already this will create another trail and S3 bucket.\n  CloudTrailBucketName: The name of the new S3 bucket to create for CloudTrail to send logs to.\n  IMPORTANT: Bucket names need to be unique across all AWS buckets, and only contain lowercase letters, numbers, and hyphens.\n  ConfigBucketName: The name of the new S3 bucket to create for Config to save config snapshots to.\n  GuardDutyEmailAddress: The email address you own that will receive the alerts, you must have access to this address for testing.\n  Click Next\n    For Configure stack options we recommend configuring tags, which are key-value pairs, that can help you identify your stacks and the resources they create. For example, enter Owner in the left column which is the key, and your email address in the right column which is the value. We will not use additional permissions or advanced options so click Next. For more information, see Setting AWS CloudFormation Stack Options .\n  For Review\n Review the contents of the page At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Create stack     This will take you to the CloudFormation stack status page, showing the stack creation in progress.\n Click on the Events tab Scroll through the listing. It shows the activities performed by CloudFormation (newest events at top), such as starting to create a resource and then completing the resource creation. Any errors encountered during the creation of the stack will be listed in this tab.     When it shows status CREATE_COMPLETE, then you are finished with this step.\n  You have now set up detective controls to log to your buckets and retain events, giving you the ability to search history and later enable pro-active monitoring of your AWS account!\nYou should receive an email to confirm the SNS email subscription, you must confirm this. Note as the email is directly from GuardDuty via SNS is will be JSON format.\n "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_automated_cur_updates_and_ingestion/1_cf_stack/","title":"Create the CloudFormation Stack","tags":[],"description":"","content":"This step is used when there is a single CUR being delivered, and have it automatically update Athena/Glue when there are new versions and new months data.\nWe will follow the steps here: https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/setting-up-athena.html#use-athena-cf to implement the CloudFormation template, which will automatically update existing CURs, and include new CURs when they are delivered.\nNOTE: IAM roles will be created, these are used to:\n  Add event notification to existing S3 buckets\n  Create s3 buckets and upload objects\n  Create and run a Glue crawler\n  Create and update a Glue database and tables\n  Please review the CloudFormation template with your security team.\n  We will build the following solution:   Log into the console via SSO. Go to the S3 dashboard, go to the bucket and folders which contain your CUR file. Open the CloudFormation(CF) file and save it locally:   Here is a sample of the CF file:   Go to the CloudFormation dashboard and create a stack:   Load the template and click Next:   Specify the details for the stack and click Next:   Review the configuration, click I acknowledge that AWS CloudFormation might create IAM resources, and click Create stack:   You will see the stack will start in CREATE_IN_PROGRESS:   Once complete, the stack will show CREATE_COMPLETE:   Click on Resources to view the resources that it will create:   Go to the AWS Glue dashboard:   Click on Databases and click the database starting with athenacurcfn:   View the table within that database and its properties:   You will see that the table is populated, the recordCount should be greater than 0. You can now go to Athena and load the partitions and view the cost and usage reports.\n function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/2_multiple_curs\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_workload_efficiency/1_data_sources/","title":"Create the Data Sources","tags":[],"description":"","content":"We first need to create data sources containing the application logs, and the cost and usage reports. In this lab we provide sample files, it is recommended you use these files initially, then use your own files after you are familiar with the requirements and process.\nWe place both logs into S3, crawl them with Glue and then use Athena to confirm a database is created that we can use.\nCopy files into S3 We will create a bucket and folders in S3, then copy the sample application log files, and cost and usage reports into the folders.\nNOTE Please read the steps carefully, as the naming is critical and any mistakes will require you to rebuild the lab or make significant and repetitive changes.\n  Log into the AWS console via SSO:   Make sure you run everything in a single region\n  Go to the S3 console, Create a new S3 Bucket, it can have any name, but make it start with costefficiencylab to make it identifiable.\n  Create a folder in the new bucket with a name: applogfiles_workshop. NOTE: You MUST name the folder applogfiles_workshop   Upload the application log file to the folder: Step1_access_log.gz     Click here - if using your own log files   If you will be using your own application log files, systems manager can be used to run commands across your environment and copy files from multiple servers to S3.\nDepending on your operating system, you can execute CLI on your application servers to copy the application log files to your S3 bucket. The following Linux sample will copy all access logs from the httpd log directory to the s3 bucket created above using the hostname to separate each servers logs:\n HOSTNAME=$(hostname) aws s3 cp --recursive /var/log/httpd/ s3://applogfiles-workshop/$HOSTNAME --exclude \u0026quot;*\u0026quot; --include \u0026quot;access_log*\u0026quot;      Create a folder named costusagefiles_workshop, inside the same bucket.\nNOTE: You MUST name the folder costusagefiles_workshop, this will make pasting the code faster.\n  Copy the sample file to your bucket into the costusagefiles_workshop folder:\n Step1CUR.gz     Create applicaton log file data source with Glue We will create a database with the uploaded application logs, with AWS Glue. For the application log files, we show you how to write a custom classifier, so you can handle any log file format from any application.\nFor our sample application logs, we have supplied Apache web server log files. The in-bulit AWS Glue classifier COMBINEDAPACHELOG will recognize these files, for example, it will read the timestamp as a single string. We will customize the interpreter to break this up into a date column, timestamp column, and timezone column. This will demonstrate how to write a customer classifier. The reference for classifiers is here: https://docs.aws.amazon.com/glue/latest/dg/custom-classifier.html A sample log file line is:\n 10.0.1.80 - - [26/Nov/2019:00:00:07 +0000] \u0026quot;GET /health.html HTTP/1.1\u0026quot; 200 55 \u0026quot;-\u0026quot; \u0026quot;ELB-HealthChecker/2.0\u0026quot;  The original columns are:\n- Client IP - Ident - Auth - HTTP Timestamp* - Request - Response - Bytes - Referrer - Agent  Using the custom classifier, we will make it build the following columns instead:\n- Client IP - Ident - Auth - Date* - Time* - Timezone* - Request - Response - Bytes - Referrer - Agent    Go to the Glue console and click Classifiers:   Click Add classifier and create it with the following details:\n  Classifier name: WebLogs\n  Classifier type: Grok\n  Classification: Logs\n  Grok pattern:\n%{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{DATE:logdate}\\:%{TIME:logtime} %{INT:tz}\\] \u0026quot;(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\u0026quot; %{NUMBER:response} (?:%{Bytes:bytes=%{NUMBER}|-}) %{QS:referrer} %{QS:agent}    Custom patterns:\nDATE %{MONTHDAY}/%{MONTH}/%{YEAR}      Click Create A classifier tells Glue how to interpret the log file lines, and how to create columns. Each column is contained within %{}, and has the pattern, the separator \u0026lsquo;:', and the column name.\nBy using the custom classifier, we have separated the column timestamp into 3 columns of logdate, logtime and tz. You can compare the custom classifier we wrote with the COMBINEDAPACHELOG classifier:\n Custom - %{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{DATE:logdate}\\:%{TIME:logtime} %{INT:tz}\\] \u0026quot;(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\u0026quot; %{NUMBER:response} (?:%{Bytes:bytes=%{NUMBER}|-}) %{QS:referrer} %{QS:agent} Builtin - %{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\] \u0026quot;(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\u0026quot; %{NUMBER:response} (?:%{Bytes:bytes=%{NUMBER}|-}) %{QS:referrer} %{QS:agent}    Next we will create a crawler to read the log files, and build a database. Click on Crawlers and click Add crawler:   Configure the crawler:\n Crawler name will be ApplicationLogs Expand Tags, description.. next to our Weblogs classifier, cilck Add Click Next:     Crawler source type is Data stores, click Next:   Click the folder icon and expand your bucket created above, select the radio button next to the applogfiles_workshop. Do NOT select the actual file or bucket, select the folder. Click Select.   Click Next\n  Select No to not add another data store, click Next\n  Create an IAM role named AWSGlueServiceRole-CostWebLogs and click Next:   Frequency will be run on demand, click Next\n  Click Add database, you MUST name it webserverlogs, click Create.\n  Click Next:   Click Finish\n  Select and Run crawler, this will create a single database and table with our log files. We need to wait until the crawler has finished, this will take 1-2 minutes. Click refresh to check if it has finished.\n  We will confirm the database has built correctly. Click Databases on the left, and click on the database webserverlogs, you may need to click refresh:   Click Tables in webserverlogs, and click the table applogfiles_workshop\n  You can see the table is created, the Name, the Location, and the recordCount has a large number of records in it (the number may be different to the image below):   Scroll down and you can see the columns, and that they are all string. This will be a small hurdle for non-string columns like bytes if you want to perform a mathematical function on it. We will work around this with Athena in our example.\n    Click here - if using your own log files   If using your own application files, you may wish to adjust the field types here. This will typically be anything numerical that you would do mathematical operations on, like a sum or average.\n    Go to the Athena service console, and select the webserverlogs database\n  Click the three dots next to the table applogfiles_workshop, and click Preview table:   View the results which will show 10 lines of your log. Note how there are separate columns logdate, logtime and tz that we created. The default classifier would have had a single column of text for the timestamp.   Create cost and usage data source with Glue To measure efficiency we need to know the cost of the workload, so we will use the Cost and Usage Report. We will follow the process above to create\n    READ ONLY - If using your own CUR files   If you are using your own Cost and Usage Reports, you will need to have them already configured and delivered as per this lab . The rest of this section is not required, as the Cost and Usage data will be correctly setup.\n    To use the files from this lab, follow the steps below:\n  Go into the Glue console, click Crawlers, and click Add crawler\n  Use the crawler name CostUsage and click Next\n  Select Data stores as the crawler source type, click Next\n  Click the folder icon, Select the S3 folder created above costefficiency and select the costusagefiles-workshop folder, make sure you dont select the bucket or file.\n  Click Select, then click Next\n  Select No do not another data store, click Next\n  Create an IAM role named AWSGlueServiceRole-Costusage, click Next\n  Set the frequency to run on demand, click Next\n  Cilck Add database, it MUST be named CostUsage, and click Create\n  click Next\n  Review and click Finish\n  Run the crawler CostUsage, then use Athena to check the database costusage was created and has records in the table costusagefiles_workshop, as per the Application logs database setup above.\n  You now have both the application and cost data sources setup, ready to create an efficiency metric.\n  function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/2_efficiency_data\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/100_labs/100_create_a_data_bunker/1_instructions/","title":"Creating data bunker account in console","tags":[],"description":"","content":"1. Create a logging account from the organizations management account Best practice is to have a separate logging account for your data bunker. This account should only be accessible by folks in your security group with a read only role. How you create this account will depend on your organization\u0026rsquo;s policies, the instructions below are guidance on how to do this. If you do not currently have a landing zone setup see the quest Quick Steps to Security Success for a more in-depth discussion.\n Login to the management account of your AWS Organization If you do not have an account within your organization to store security logs. Navigate to AWS Organizations and select Create Account. Include a cross account access role and note it\u0026rsquo;s name (default is OrganizationAccountAccessRole) - we will modify this later to remove unnecessary access (Optional) If your role does not have permission to assume any role you will also have to add an IAM policy. The AWS administrator policy has this by default, otherwise follow the steps in the AWS Organizations Documentation to grant permissions to access the role Consider applying best practices as a baseline such as lock away your AWS account root user access keys and using multi-factor authentication  Navigate to Settings and take a note of your Organization ID  2. Create the bucket for CloudTrail logs  Switch roles into the logging account for your organization Navigate to S3 Press Create Bucket Enter a Bucket name for your bucket, type a unique DNS-compliant name for your new bucket. Follow these naming guidelines:   The name must be unique across all existing bucket names in Amazon S3. The name must not contain uppercase characters. The name must start with a lowercase letter or number. The name must be between 3 and 63 characters long.   Choose an AWS Region where you want the bucket to reside. Choose a Region close to you to minimize latency and costs, or to address regulatory requirements. Note that for this example we will accept the default settings and this bucket is secure by default. Consider enabling additional security options such as logging and encryption, the S3 documentation has additional information such as Protecting Data in Amazon S3 . Accept default value for Block all public access. Enable bucket versioning , to keep multiple versions of an object so you can recover an object if you unintentionally modify or delete it. Click Create bucket. Press the bucket we just create and navigate to the Properties tab (Strongly recommended unless tearing down immediately) Under Object Lock, enable compliance mode and set a retention period. The length of the retention period will depend on your organizational requirements. If you are enabling this just for baseline security start with 31 days to keep one month of logs. Note: You will be unable to delete files within this window or the bucket if objects still exist in it Under the Permissions tab, replace the Bucket Policy with the following, replacing [bucket] and [organization id]. Press Save  { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AWSCloudTrailAclCheck20150319\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;cloudtrail.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::[bucket]\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;AWSCloudTrailWrite20150319\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;cloudtrail.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:PutObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::[bucket]/AWSLogs/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;s3:x-amz-acl\u0026#34;: \u0026#34;bucket-owner-full-control\u0026#34; } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;AWSCloudTrailWrite20150319\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;cloudtrail.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:PutObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::[bucket]/AWSLogs/[organization id]/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;s3:x-amz-acl\u0026#34;: \u0026#34;bucket-owner-full-control\u0026#34; } } } ] } (Optional) Next we will add a life cycle policy to clean up old logs. Navigate to Management (Optional) Add a life cycle rule named Delete old logs, press Next (Optional) Add a transition rule for both the current and previous versions to move to Glacier after 32 days. Press Next (Optional) Select the current and previous versions and set them to delete after 365 days  3. Ensure cross account access is read-only These instructions outline how to modify the cross account access created in step 1 is read-only. As with step 1, this will depend on how your organization\u0026rsquo;s policies. The key is that our security team are not able to modify data in our data bunker. Human access should only be in a break-glass emergency situation.\nNote: Following these steps will prevent OrganizationAccountAccessRole from making further changes to this account. Ensure other services such as Amazon Guard Duty and AWS Security Hub are configured before proceeding. If further changes are needed you will have to reset the root credentials for the security account.\n Navigate to IAM and select Roles Select the organizations account access role for your organization: Note: the default is OrganizationAccountAccessRole Press Attach Policy and attach the AWS managed ReadOnlyAccess Policy Navigate back to the OrganizationAccountAccessRole and press the X to remove the AdministratorAccess policy  4. Turn on CloudTrail from the management account  Switch back to the management account Navigate to CloudTrail Select Trails from the menu on the left Press Create Trail Enter a name for the trail such as OrganizationTrail Select Yes next to Apply trail to my organization Under Storage location, select No for Create new S3 bucket and enter the bucket name of the bucket created in step 2  Verification  Switch back to the Security account Navigate to the S3 bucket previously created (Optional) You can start to explore the logs using CloudTrail    License Licensed under the Apache 2.0 and MITnoAttr License.\nCopyright 2019-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\nLicensed under the Apache License, Version 2.0 (the \u0026ldquo;License\u0026rdquo;). You may not use this file except in compliance with the License. A copy of the License is located at\nhttps://aws.amazon.com/apache2.0/ or in the \u0026ldquo;license\u0026rdquo; file accompanying this file. This file is distributed on an \u0026ldquo;AS IS\u0026rdquo; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n"},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_deploy_and_update_cloudformation/1_deploy_infra/","title":"Deploy Infrastructure using a CloudFormation Stack","tags":[],"description":"","content":"This lab illustrates best practices for reliability as described in the AWS Well-Architected Reliability pillar.\nHow do you implement change?\n Best practice: Deploy changes with automation: Deployments and patching are automated to eliminate negative impact. Design principle: Manage change in automation: Changes to your infrastructure should be made using automation. The changes that need to be managed include changes to the automation, which then can be tracked and reviewed.  When this lab is completed, you will have deployed and edited a CloudFormation template. Using this template you will deploy a VPC, an S3 bucket and an EC2 instance running a simple web server.\n1.1 Log into the AWS console If you are attending an in-person workshop and were provided with an AWS account by the instructor:\n  Click here for instructions to access your assigned AWS account:     Go to https://dashboard.eventengine.run/login\n  Enter the 12 character hashcode you were provided and click \u0026ldquo;Proceed\u0026rdquo;   [optional] assign a name to your account (this is referred to as \u0026ldquo;Team name\u0026rdquo;)\n click \u0026ldquo;Set Team Name\u0026rdquo; Enter a name and click \u0026ldquo;Set Team Name\u0026rdquo;    Click \u0026ldquo;AWS Console\u0026rdquo;\n  AWS credentials   IMPORTANT Copy the provided credentials and save them. You wil need these to complete the workshop   Copy the whole code block corresponding to the system you are using.\n  Access the AWS console  Click \u0026ldquo;Open Console\u0026rdquo;. The AWS Console will open.     If you are using your own AWS account:   Click here for instructions to use your own AWS account:    Sign in to the AWS Management Console as an IAM user who has PowerUserAccess or AdministratorAccess permissions, to ensure successful execution of this lab.   \n1.2 The CloudFormation template You will begin by deploying a CloudFormation stack that creates a simple VPC as shown in this diagram:\n Download the simple_stack.yaml CloudFormation template Open this file in a Text Editor  Preferably use an editor that is YAML aware like vi/vim, VS Code, or Notepad++ Do NOT use a Word Processor    The template is written in a format called YAML , which is commonly used for configuration files. The format of the file is important, especially indents and hyphens. CloudFormation templates can also be written in JSON.\nLook through the file. You will notice several sections:\n  The Parameters section is used to prompt for inputs that can be used elsewhere in the template. The template is asking for several inputs, but also provides default values for each one. Look through these and start to reason about what each one is.\n  The Conditions section is where you can setup if/then-like control of what happens during template deployment. It defines the circumstances under which entities are created or configured.\n  The Resources section is the \u0026ldquo;heart\u0026rdquo; of the template. It is where you define the infrastructure to be deployed. Look at the first resource defined.\n It is the VPC (Amazon Virtual Private Cloud) It has a logical ID which in this case is SimpleVPC. This logical ID is how we refer to the VPC resource within the CloudFormation template. It has a Type which tells CloudFormation which type of resource to create And it has Properties that define the values used to create the VPC    The Outputs section is used to display selective information about resources in the stack.\n  The Metadata section here is used to group and order how the CloudFormation parameters are displayed when you deploy the template using the AWS Console\n     CloudFormation tip     When editing CloudFormation templates written in YAML, be extra cautious that you maintain the correct number of spaces for each indentation   Indents are always in increments of two spaces    You will now use this template to launch a CloudFormation stack that will deploy AWS resources in your AWS account.\n1.3 Deploying an AWS CloudFormation stack to create a simple VPC   Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and click Create Stack \u0026gt; With new resources   Leave Prepare template setting as-is\n For Template source select Upload a template file Click Choose file and supply the CloudFormation template you downloaded: simple_stack.yaml     Click Next\n  For Stack name use CloudFormationLab\n  Parameters\n  Look over the Parameters and their default values.\n  Click Next\n    For Configure stack options we recommend configuring tags, which are key-value pairs, that can help you identify your stacks and the resources they create. For example, enter Owner in the left column which is the key, and your email address in the right column which is the value. We will not use additional permissions or advanced options so click Next. For more information, see Setting AWS CloudFormation Stack Options .\n  For Review\n Review the contents of the page At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Create stack     This will take you to the CloudFormation stack status page, showing the stack creation in progress.\n Click on the Events tab Scroll through the listing. It shows the activities performed by CloudFormation (newest events at top), such as starting to create a resource and then completing the resource creation. Any errors encountered during the creation of the stack will be listed in this tab.     When it shows status CREATE_COMPLETE, then you are finished with this step.\n  Deployment will take approximately 30 seconds to deploy.\n   function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/2_understand_deploy\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/200_labs/200_automating_operations_with_playbooks_and_runbooks/1_deploy_the_lab_network_infrastructure/","title":"Deploy Lab Network Infrastructure","tags":[],"description":"","content":"In our first sections we will build a network infrastructure for our Application using Virtual Public Cloud (VPC) , along with it\u0026rsquo;s public and private subnets across two Availability Zones , and other basic network building blocks for accessing the internet such as Internet Gateway and NAT gateway To simplify the deployment we have created a CloudFormation Template that can be deployed in your AWS account that will deploy the VPC and Application resources shown in diagram below.\nPlease follow below steps to continue.\nIn this initial section we will be deploying the VPC network architecture.\n1.1. Deploy base VPC infrastructure. To deploy the VPC infrastructure you can either deploy the CloudFormation template directly from the command line or via the console.\nYou can get the template here.   Click here for CloudFormation command-line deployment steps   Command Line Deployment: To deploy from the command line, ensure that you have installed and configured AWS CLI with the appropriate credentials.\n1.1.1. Execute Command aws cloudformation create-stack --stack-name waopslab-base-vpc \\ --template-body file://base-vpc.yml Note: Please adjust your command-line if you are using profiles within your aws command line as required.\n1.1.2. Confirm that the stack has installed correctly. You can do this by running the describe-stacks command as follows:\naws cloudformation desribe-stacks --stack-name waopslab-base-vpc Locate the StackStatus and confirm it is set to CREATE_COMPLETE as shown here:\n1.1.3. Take note of this stack output as we will need it for later sections of the lab.\n    Click here for CloudFormation console deployment steps   Console: If you decide to deploy the stack from the console, ensure that you follow below requirements \u0026amp; step:\n Please follow this guide for information on how to deploy the cloudformation template. Use waopslab-base-vpc as the Stack Name, as this is referenced by other stacks later in the lab.  1.2. Note Cloudformation Template Outputs When the CloudFormation template deployment is completed, note the outputs produced by the newly created stack as these will be required at later points in the lab.\nYou can do this by clicking on the stack name you just created, and select the Outputs Tab as shown in diagram below.\nYou can now proceed to Section 2 of the lab where we will build out the actual application stack.\n   function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/2_configure_ecs_repository_and_deploy_application_stack\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step      END OF SECTION 1\n "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_remote_configuration_installation_and_viewing_cloudwatch_logs/1_deploy_cfn_stack/","title":"Deploy the CloudFormation Stack","tags":[],"description":"","content":"This portion of the lab shows you how to deploy an EC2 instance using a CloudFormation template. The CloudFormation template will deploy the following:\n Lab VPC: The VPC used in this lab. This VPC contains a single public subnet, each with its own route table. An Internet Gateway and is used to route traffic to the public subnet. EC2 Instance: This is the EC2 instance that hosts the simple Apache web application. You will also be configuring the CloudWatch Logs Agent to work on this instance. IAM Role: An IAM role that allows the instance to send logs and metrics to CloudWatch and allows SSM actions to be performed on the instance. S3 Bucket: The S3 bucket to store log files generated in this lab.   Download the CloudFormation template provided in this lab . OPTIONAL: Look through the CloudFormation template and comments to see the resources deployed. More information on templates can be found here . Go to CloudFormation console , click Create Stack, and select With new resources (standard). In the Specify Template menu, choose Upload a template file, then Choose file, and select the security-lab-stack.yaml template you downloaded.  In the Specify Stack Details menu:  Enter a stack name, such as security-cw-lab. Note the name down, as you will need to re-visit this stack for Outputs later on. Enter your name in the DisplayedName field, this will be the name that appears on your sample website! Enter an S3 bucket name in the BucketName field. Amazon S3 bucket names are globally unique, and the namespace is shared by all AWS accounts, so make sure your bucket is names as uniquely as possible. For example: wa-lab-\u0026lt;your-account-id\u0026gt;-\u0026lt;date\u0026gt;. Do not modify the LatestAmiId field. This uses a public parameter stored in Systems Manager Parameter Store that will automatically use the latest Amazon Machine Image (AMI) for the EC2 instance.     No changes are needed on the Configure Stack Options page. Click through Next.\n  On the Review page, check the box in the Capabilities section to allow the creation of an IAM role. This selection gives the CloudFormation template permission to create IAM roles - in particular, the role used to allow the EC2 instance to interact with SSM and CloudWatch. Click Create Stack. You will be taken back to the CloudFormation console, where your stack will be launched.\n  Once the stack shows CREATE COMPLETE, click on the Outputs tab and click on the WebsiteURL, you will be brought to your sample web server.  Recap: In this portion of the lab, you deployed a CloudFormation stack to create the base resources needed for this lab.\n function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/2_install_cw_agent\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_bidirectional_replication_for_s3/1_deploy_infra/","title":"Deploy the Infrastructure","tags":[],"description":"","content":"You will create two Amazon S3 buckets in two different AWS regions. The Ohio region (also known as us-east-2) will be referred to throughout this lab as the east S3 bucket, and Oregon (also known as us-west-2) will be referred to as the west S3 bucket.\n1.1 Log into the AWS console If you are attending an in-person workshop and were provided with an AWS account by the instructor:\n  Click here for instructions to access your assigned AWS account:     Go to https://dashboard.eventengine.run/login\n  Enter the 12 character hashcode you were provided and click \u0026ldquo;Proceed\u0026rdquo;   [optional] assign a name to your account (this is referred to as \u0026ldquo;Team name\u0026rdquo;)\n click \u0026ldquo;Set Team Name\u0026rdquo; Enter a name and click \u0026ldquo;Set Team Name\u0026rdquo;    Click \u0026ldquo;AWS Console\u0026rdquo;\n  AWS credentials   IMPORTANT Copy the provided credentials and save them. You wil need these to complete the workshop   Copy the whole code block corresponding to the system you are using.\n  Access the AWS console  Click \u0026ldquo;Open Console\u0026rdquo;. The AWS Console will open.     If you are using your own AWS account:   Click here for instructions to use your own AWS account:    Sign in to the AWS Management Console as an IAM user who has PowerUserAccess or AdministratorAccess permissions, to ensure successful execution of this lab.   \n1.2 Deploy the infrastructure in two AWS Regions using an AWS CloudFormation template You will deploy the infrastructure for two Amazon S3 buckets. Since these will be in two different regions, you will need to create an AWS CloudFormation stack in each region. You will use the same CloudFormation template for both regions.\n Download the s3_bucket.yaml CloudFormation template  1.2.1 Deploy east S3 bucket  It is recommended that you deploy the east s3 bucket in the Ohio region. This region is also known as us-east-2.  Use the drop-down to select this region  If you choose to use a different region, you will need to ensure future steps are consistent with your region choice.   On the AWS Console go to the CloudFormation console  Select Stacks Create a CloudFormation stack (with new resources) using the CloudFormation Template file and the Upload a template file option. For Stack name use S3-CRR-lab-east Under Parameters enter a NamingPrefix  This will be used to name your S3 buckets Must be string consisting of lowercase letters, numbers, periods (.), and dashes (-) between five and 40 characters This will be part of your Amazon S3 bucket name, which must be unique across all of S3. Record this value in an accessible place \u0026ndash; you will need it again later in the lab.   Click Next until the last page At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Create stack You can go ahead and create the west bucket before this CloudFormation stack completes  Troubleshooting: If your CloudFormation stack deployment fails with the error \u0026lt;bucket name\u0026gt; already exists\n You did not pick a unique enough NamingPrefix Delete the failed stack Start over and choose a more unique NamingPrefix Amazon S3 bucket names share a global name space across all of AWS (including all AWS regions)  1.2.2 Deploy west S3 bucket   It is recommended that you deploy the west s3 bucket in the Oregon region for this lab. This region is also known as us-west-2.\n Use the drop-down to select this region  If you choose to use a different region, you will need to ensure future steps are consistent with your region choice.    On the AWS Console go to the CloudFormation console   Select Stacks\n  Create a CloudFormation stack (with new resources) using the same CloudFormation Template file as before, and the Upload a template file option.\n  For Stack name use S3-CRR-lab-west\n  Under Parameters enter a NamingPrefix\n You must use the same value as you did previously    Click Next until the last page\n  At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names\n  Click Create stack\n  1.2.3 Get bucket information  Go back to the Ohio AWS Region and wait for the CloudFormation stack you created there to complete Click on the Outputs tab and record the Value of the S3 bucket name in an accessible location as east bucket Go to the the Oregon AWS Region and do the same thing, copying that S3 bucket name down as west bucket Go to the Amazon S3 console and verify that both buckets were created.  Although S3 buckets are specific to an AWS region, the Amazon S3 console shows all buckets from all AWS Regions The two S3 buckets you will work with begin with \u0026lt;your_naming_prefix\u0026gt;-crrlab Note the regions for the two S3 buckets your created There are also two new logging buckets \u0026ndash; you will not need to do any actions with these.   Click on either the east region or west region bucket, and note the following  This bucket is empty - We will be adding objects to the bucket soon Click on Properties and note what properties are Enabled      Click here to learn why are these properties enabled     Versioning is Enabled: For S3 Replication, both source and destination buckets MUST have versioning enabled\n  Default encryption is Enabled: In our exercise we are demonstrating replication of encrypted objects. It is a best practice to encrypt your data at rest.\n  Object-level logging is Enabled: This logging will be used later in the lab. It is used to better understand replication operations AWS takes on your behalf.\n     function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/2_configure_replication\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_testing_for_resiliency_of_ec2/1_prerequisite/","title":"Deploy the Infrastructure and Application","tags":[],"description":"","content":"The first step of this lab is to deploy the static web application stack. If you have already run the following two labs (and have not torn down the resources) then you have already deployed the necessary infrastructure. Proceed to next step Configure Execution Environment\n Security: Level 200: Automated Deployment of VPC  Security: Level 200: Automated Deployment of EC2 Web Application   If you have not already deployed the necessary infrastructure, then follow these steps:\n You will first deploy an Amazon Virtual Private Cloud (VPC) You will then deploy a Static WebApp hosted on Amazon EC2 instances For each of these deployments:  If you are comfortable deploying a CloudFormation stack, then use the Express Steps If you require detailed guidance in how to deploy a CloudFormation stack, then use the Guided Steps    1.1 Log into the AWS console If you are attending an in-person workshop and were provided with an AWS account by the instructor:\n  Click here for instructions to access your assigned AWS account:     Go to https://dashboard.eventengine.run/login\n  Enter the 12 character hashcode you were provided and click \u0026ldquo;Proceed\u0026rdquo;   [optional] assign a name to your account (this is referred to as \u0026ldquo;Team name\u0026rdquo;)\n click \u0026ldquo;Set Team Name\u0026rdquo; Enter a name and click \u0026ldquo;Set Team Name\u0026rdquo;    Click \u0026ldquo;AWS Console\u0026rdquo;\n  AWS credentials   IMPORTANT Copy the provided credentials and save them. You wil need these to complete the workshop   Copy the whole code block corresponding to the system you are using.\n  Access the AWS console  Click \u0026ldquo;Open Console\u0026rdquo;. The AWS Console will open.     If you are using your own AWS account:   Click here for instructions to use your own AWS account:    Sign in to the AWS Management Console as an IAM user who has PowerUserAccess or AdministratorAccess permissions, to ensure successful execution of this lab.   \n1.2 Deploy the VPC infrastructure Choose either the Express Steps or Guided Steps\n  In the AWS Console, choose the AWS region you wish to use - if possible we recommend using us-east-2 (Ohio)  Express Steps (Deploy the VPC infrastructure)  Download the vpc-alb-app-db.yaml CloudFormation template Deploy the CloudFormation template  Name the stack WebApp1-VPC (case sensitive) Leave all CloudFormation Parameters at their default values   When the stack status is CREATE_COMPLETE, you can continue to the next step  Guided Steps (Deploy the VPC infrastructure)   Click here for detailed instructions to deploy the VPC:   This step will create the VPC and all components using the example CloudFormation template.\n Download the latest version of the CloudFormation template here: vpc-alb-app-db.yaml  Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Click Create Stack, then With new resources (standard).  Click Upload a template file and then click Choose file.  Choose the CloudFormation template you downloaded in step 1, return to the CloudFormation console page and click Next. Enter the following details:  Stack name: The name of this stack. For this lab, use WebApp1-VPC and match the case. Parameters: Parameters may be left as defaults, you can find out more in the description for each.    At the bottom of the page click Next. In this lab, we use tags, which are key-value pairs, that can help you identify your stacks. Enter Owner in the left column which is the key, and your email address in the right column which is the value. We will not use additional permissions or advanced options so click Next. For more information, see Setting AWS CloudFormation Stack Options . Review the information for the stack. When you\u0026rsquo;re satisfied with the configuration, at the bottom of the page check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack.   After a few minutes the final stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE. You can click the refresh button to check on the current status. You have now created the VPC stack (well actually CloudFormation did it for you).\n  When the stack status is CREATE_COMPLETE, you can continue to the next step\n    1.3 Deploy the EC2s and Static WebApp infrastructure Choose either the Express Steps or Guided Steps\n Express Steps (Deploy the EC2s and Static WebApp infrastructure)  Download the staticwebapp.yaml CloudFormation template Choose the same AWS region as you did for the VPC (if you used our recommendation, this is us-east-2 (Ohio)) Deploy the CloudFormation template  Name the stack WebApp1-Static (case sensitive) Leave all CloudFormation Parameters at their default values   When the stack status is CREATE_COMPLETE, you can continue to the next step  Guided Steps (Deploy the EC2s and Static WebApp infrastructure)   Click here for detailed instructions to deploy the WebApp:     Download the latest version of the CloudFormation template here: staticwebapp.yaml   Choose the same AWS region as you did for the VPC (if you used our recommendation, this is us-east-2 (Ohio))\n  Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and click Create Stack \u0026gt; With new resources   Leave Prepare template setting as-is\n For Template source select Upload a template file Click Choose file and supply the CloudFormation template you downloaded: staticwebapp.yaml     Click Next\n  For Stack name use WebApp1-Static\n  Parameters\n  Look over the Parameters and their default values.\n  Click Next\n    For Configure stack options we recommend configuring tags, which are key-value pairs, that can help you identify your stacks and the resources they create. For example, enter Owner in the left column which is the key, and your email address in the right column which is the value. We will not use additional permissions or advanced options so click Next. For more information, see Setting AWS CloudFormation Stack Options .\n  For Review\n Review the contents of the page At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Create stack     This will take you to the CloudFormation stack status page, showing the stack creation in progress.\n Click on the Events tab Scroll through the listing. It shows the activities performed by CloudFormation (newest events at top), such as starting to create a resource and then completing the resource creation. Any errors encountered during the creation of the stack will be listed in this tab.     When it shows status CREATE_COMPLETE, then you are finished with this step.\n    Website URL  Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation.  Wait until WebApp1-Static stack status is CREATE_COMPLETE before proceeding. This should take about four minutes Click on the WebApp1-Static stack Click on the Outputs tab For the Key WebsiteURL copy the value. This is the URL of your test web service Save this URL - you will need it later     function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/2_configure_env\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_health_checks_and_dependencies/1_deploy_app/","title":"Deploy the Infrastructure and Application","tags":[],"description":"","content":"You will create a multi-tier architecture using AWS and run a simple service on it. The service is a web server running on Amazon EC2 fronted by an Elastic Load Balancer reverse-proxy, with a dependency on Amazon DynamoDB.\nNote: The concepts covered by this lab apply whether your service dependency is an AWS resource like Amazon DynamoDB, or an external service called via API. The DynamoDB dependency here acts as a mock for an external service called RecommendationService. The getRecommendation API on this service is a dependency for the web service used in this lab. getRecommendation is actually a get_item call to a DynamoDB table.\n1.1 Log into the AWS console If you are attending an in-person workshop and were provided with an AWS account by the instructor:\n  Click here for instructions to access your assigned AWS account:     Go to https://dashboard.eventengine.run/login\n  Enter the 12 character hashcode you were provided and click \u0026ldquo;Proceed\u0026rdquo;   [optional] assign a name to your account (this is referred to as \u0026ldquo;Team name\u0026rdquo;)\n click \u0026ldquo;Set Team Name\u0026rdquo; Enter a name and click \u0026ldquo;Set Team Name\u0026rdquo;    Click \u0026ldquo;AWS Console\u0026rdquo;\n  AWS credentials   IMPORTANT Copy the provided credentials and save them. You wil need these to complete the workshop   Copy the whole code block corresponding to the system you are using.\n  Access the AWS console  Click \u0026ldquo;Open Console\u0026rdquo;. The AWS Console will open.     If you are using your own AWS account:   Click here for instructions to use your own AWS account:    Sign in to the AWS Management Console as an IAM user who has PowerUserAccess or AdministratorAccess permissions, to ensure successful execution of this lab.   \n1.2 Deploy the application using an AWS CloudFormation template You will deploy the service infrastructure including simple service code and some sample data.\n It is recommended that you use the Ohio region. This region is also known as us-east-2, which you will see referenced throughout this lab.  If you choose to use a different region, you will need to ensure future steps are consistent with your region choice.    1.2.1 Deploy the VPC infrastructure  If you are comfortable deploying a CloudFormation stack, then use the Express Steps If you require detailed guidance in how to deploy a CloudFormation stack, then use the Guided Steps  Choose either the Express Steps or Guided Steps\n Express Steps (Deploy the VPC infrastructure)  Download the vpc-alb-app-db.yaml CloudFormation template Make sure you are in AWS region: us-east-2 (Ohio) Deploy the CloudFormation template  Name the stack WebApp1-VPC (case sensitive) Leave all CloudFormation Parameters at their default values   When the stack status is CREATE_COMPLETE, you can continue to the next step  Guided Steps (Deploy the VPC infrastructure)   Click here for detailed instructions to deploy the VPC:   This step will create the VPC and all components using the example CloudFormation template.\n Download the latest version of the CloudFormation template here: vpc-alb-app-db.yaml  Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Click Create Stack, then With new resources (standard).  Click Upload a template file and then click Choose file.  Choose the CloudFormation template you downloaded in step 1, return to the CloudFormation console page and click Next. Enter the following details:  Stack name: The name of this stack. For this lab, use WebApp1-VPC and match the case. Parameters: Parameters may be left as defaults, you can find out more in the description for each.    At the bottom of the page click Next. In this lab, we use tags, which are key-value pairs, that can help you identify your stacks. Enter Owner in the left column which is the key, and your email address in the right column which is the value. We will not use additional permissions or advanced options so click Next. For more information, see Setting AWS CloudFormation Stack Options . Review the information for the stack. When you\u0026rsquo;re satisfied with the configuration, at the bottom of the page check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack.   After a few minutes the final stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE. You can click the refresh button to check on the current status. You have now created the VPC stack (well actually CloudFormation did it for you).\n  When the stack status is CREATE_COMPLETE, you can continue to the next step\n    1.2.2 Deploy the web app infrastructure and service Wait until the VPC CloudFormation stack status is CREATE_COMPLETE, then continue. This will take about four minutes.\nChoose either the Express Steps or Guided Steps\n Express Steps (Deploy the EC2s and Static WebApp infrastructure)  Download the staticwebapp.yaml CloudFormation template Make sure you are in AWS region: us-east-2 (Ohio) Deploy the CloudFormation template  Name the stack HealthCheckLab (case sensitive) Leave all CloudFormation Parameters at their default values   When the stack status is CREATE_COMPLETE, you can continue to the next step  Guided Steps (Deploy the EC2s and Static WebApp infrastructure)   Click here for detailed instructions to deploy the WebApp:     Download the latest version of the CloudFormation template here: staticwebapp.yaml   Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and click Create Stack \u0026gt; With new resources   Leave Prepare template setting as-is\n For Template source select Upload a template file Click Choose file and supply the CloudFormation template you downloaded: staticwebapp.yaml     Click Next\n  For Stack name use HealthCheckLab\n  Parameters\n  Look over the Parameters and their default values.\n  Click Next\n    For Configure stack options we recommend configuring tags, which are key-value pairs, that can help you identify your stacks and the resources they create. For example, enter Owner in the left column which is the key, and your email address in the right column which is the value. We will not use additional permissions or advanced options so click Next. For more information, see Setting AWS CloudFormation Stack Options .\n  For Review\n Review the contents of the page At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Create stack     This will take you to the CloudFormation stack status page, showing the stack creation in progress.\n Click on the Events tab Scroll through the listing. It shows the activities performed by CloudFormation (newest events at top), such as starting to create a resource and then completing the resource creation. Any errors encountered during the creation of the stack will be listed in this tab.     When it shows status CREATE_COMPLETE, then you are finished with this step.\n    1.3 View the website for web service   Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation.\n Wait until HealthCheckLab stack status is CREATE_COMPLETE before proceeding. This should take about four minutes Click on the HealthCheckLab stack Click on the Outputs tab For the Key WebsiteURL copy the value. This is the URL of your test web service  Hint: it will start with http://healt-alb and end in \u0026lt;aws region\u0026gt;.elb.amazonaws.com      Click the URL and it will bring up the website:\n  The website simulates a recommendation engine making personalized suggestions for classic television shows. You should note the following features:\n Area A shows the personalized recommendation  It shows first name of the user and the show that was recommended The workshop simulation is simple. On every request it chooses a user at random, and shows a recommendation statically mapped to that user. The user names, television show names, and this mapping are in a DynamoDB table, which is simulating the RecommendationService   Area B shows metadata which is useful to you during the lab  The instance_id and availability_zone enable you to see which EC2 server and Availability Zone were used for each request There is one EC2 instance deployed per Availability Zone Refresh the website several times, note that the EC2 instance and Availability Zone change from among the three available This is Elastic Load Balancing (ELB) distributing these stateless requests among the available EC2 server instances across Availability Zones       Well-Architected for Reliability: Best practices     Use highly available network connectivity for your workload public endpoints: Elastic Load Balancing provides load balancing across Availability Zones, performs Layer 4 (TCP) or Layer 7 (http/https) routing, integrates with AWS WAF, and integrates with AWS Auto Scaling to help create a self-healing infrastructure and absorb increases in traffic while releasing resources when traffic decreases.   Implement loosely coupled dependencies: Dependencies such as queuing systems, streaming systems, workflows, and load balancers are loosely coupled. Loose coupling helps isolate behavior of a component from other components that depend on it, increasing resiliency and agility.   Deploy the workload to multiple locations: Distribute workload data and resources across multiple Availability Zones or, where necessary, across AWS Regions. These locations can be as diverse as required.       function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/2_handle_dependency\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/1_deploy_infra/","title":"Deploy the Infrastructure and Application","tags":[],"description":"","content":"You will create a multi-tier architecture using AWS and run a simple service on it. The service is a web server running on Amazon EC2 fronted by an Elastic Load Balancer reverse-proxy, with a data store on Amazon Relational Database Service (RDS).\n1.1 Log into the AWS console If you are attending an in-person workshop and were provided with an AWS account by the instructor:\n  Click here for instructions to access your assigned AWS account:     Go to https://dashboard.eventengine.run/login\n  Enter the 12 character hashcode you were provided and click \u0026ldquo;Proceed\u0026rdquo;   [optional] assign a name to your account (this is referred to as \u0026ldquo;Team name\u0026rdquo;)\n click \u0026ldquo;Set Team Name\u0026rdquo; Enter a name and click \u0026ldquo;Set Team Name\u0026rdquo;    Click \u0026ldquo;AWS Console\u0026rdquo;\n  AWS credentials   IMPORTANT Copy the provided credentials and save them. You wil need these to complete the workshop   Copy the whole code block corresponding to the system you are using.\n  Access the AWS console  Click \u0026ldquo;Open Console\u0026rdquo;. The AWS Console will open.     If you are using your own AWS account:   Click here for instructions to use your own AWS account:    Sign in to the AWS Management Console as an IAM user who has PowerUserAccess or AdministratorAccess permissions, to ensure successful execution of this lab. You will need the AWS credentials, AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY, of this IAM user for later use in this lab.  If you do not have this IAM user\u0026rsquo;s credentials or you wish to create a new IAM user with needed permissions, follow the instructions here to create them      \n1.2 Checking for existing service-linked roles If you are attending an in-person workshop and were provided with an AWS account by the instructor: Skip this step and go directly to step Create the \u0026ldquo;deployment machine\u0026rdquo; .\nIf you are using your own AWS account: Follow these steps , and then return here and resume with the following instructions.\n1.3 Create the \u0026ldquo;deployment machine\u0026rdquo; Here you will build a state machine using AWS Step Functions and AWS Lambda that orchestrates the deployment of the multi-tier infrastructure. This is not the service infrastructure itself, but meta-infrastructure we use to build the actual infrastructure.\nLearn more: After the lab see this blog post on how AWS Step Functions and AWS CodePipelines can work together to deploy your infrastructure\n  Decide which deployment option you will use for this lab. It can be run as single region or multi region (two region) deployment.\n single region is faster to get up and running multi region enables you to test some additional aspects of cross-regional resilience. Decide on one of these options, then in later steps choose the appropriate instructions for the option you have chosen. If you are attending an in-person workshop, your instructor will specify which to use.    Get the CloudFormation template: Download the appropriate file (You can right-click then choose download; or you can right click and copy the link to use with wget)\n single region: download CloudFormation template here  multi region: download CloudFormation template here     Ensure you have selected the Ohio region. This region is also known as us-east-2, which you will see referenced throughout this lab.   Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and click Create Stack:   Leave \u0026ldquo;Prepare template\u0026rdquo; setting as-is\n 1 - For \u0026ldquo;Template source\u0026rdquo; select \u0026ldquo;Upload a template file\u0026rdquo; 2 - Specify the CloudFormation template you downloaded     Click the Next button. For \u0026ldquo;Stack name\u0026rdquo; enter:\n DeployResiliencyWorkshop    On the same screen, for \u0026ldquo;Parameters\u0026rdquo; enter the appropriate values:\n If you are attending an in-person workshop and were provided with an AWS account by the instructor: Leave all the parameters at their default values If you are using your own AWS account: Set the first three parameters using these instructions and leave all other parameters at their default values. You optionally may review the default values of this CloudFormation template here     Click the Next button.\n On the \u0026ldquo;Configure stack options\u0026rdquo; page, click Next again On the \u0026ldquo;Review DeployResiliencyWorkshop\u0026rdquo; page, scroll to the bottom and tick the checkbox I acknowledge that AWS CloudFormation might create IAM resources. Click the Create stack button.     This will take you to the CloudFormation stack status page, showing the stack creation in progress.\nThis will take approximately a minute to deploy. When it shows status CREATE_COMPLETE, then you are finished with this step.\n  1.4 Deploy infrastructure and run the service   Go to the AWS Step Function console at https://console.aws.amazon.com/states\n  On the Step Functions dashboard, you will see State Machines and you will have a new one named DeploymentMachine-random characters. Click on that state machine. This will bring up an execution console. Click on the Start execution button.   On the \u0026ldquo;New execution\u0026rdquo; dialog, for \u0026ldquo;Enter an execution name\u0026rdquo; delete the auto-generated name and replace it with: BuildResiliency\n  Then for \u0026ldquo;Input\u0026rdquo; enter JSON that will be used to supply parameter values to the Lambdas in the workflow.\n  single region uses the following values:\n{ \u0026quot;log_level\u0026quot;: \u0026quot;DEBUG\u0026quot;, \u0026quot;region_name\u0026quot;: \u0026quot;us-east-2\u0026quot;, \u0026quot;cfn_region\u0026quot;: \u0026quot;us-east-2\u0026quot;, \u0026quot;cfn_bucket\u0026quot;: \u0026quot;aws-well-architected-labs-ohio\u0026quot;, \u0026quot;folder\u0026quot;: \u0026quot;Reliability/\u0026quot;, \u0026quot;workshop\u0026quot;: \u0026quot;300-ResiliencyofEC2RDSandS3\u0026quot;, \u0026quot;boot_bucket\u0026quot;: \u0026quot;aws-well-architected-labs-ohio\u0026quot;, \u0026quot;boot_prefix\u0026quot;: \u0026quot;Reliability/\u0026quot;, \u0026quot;websiteimage\u0026quot; : \u0026quot;https://aws-well-architected-labs-ohio.s3.us-east-2.amazonaws.com/images/Cirque_of_the_Towers.jpg\u0026quot; }    multi region uses the values here   Note: for websiteimage you can supply an alternate link to a public-read-only image in an S3 bucket you control. This will allow you to run S3 resiliency tests as part of the lab\n  Then click the Start Execution button.\n    The \u0026ldquo;deployment machine\u0026rdquo; is now deploying the infrastructure and service you will use for resiliency testing.\n   Time until you can start\u0026hellip; Single region Multi region     EC2 failure injection test 15-20 min 15-20 min   RDS and AZ failure injection tests 20-25 min 40-45 min   Multi-region failure injection tests NA 50-55 min   Total deployment time 20-25 min 50-55 min      You can watch the state machine as it executes by clicking the icon to expand the visual workflow to the full screen.\n  You can also watch the CloudFormation stacks as they are created and transition from CREATE_IN_PROGRESS to CREATE_COMPLETE.   Note: If you are in a workshop, the instructor will share background and technical information while your service is deployed.\n  You can start the first test (EC2 failure injection testing) when the web tier has been deployed in the Ohio region. Look for the WaitForWebApp step (for single region) or WaitForWebApp1 step (for multi region) to have completed successfully. This will look something like this on the visual workflow.\n Above screen shot is for single region. for multi region see this diagram instead     1.5 View website for test web service   Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation.\n click on the WebServersforResiliencyTesting stack click on the \u0026ldquo;Outputs\u0026rdquo; tab For the Key WebSiteURL copy the value. This is the URL of your test web service.     Click the value and it will bring up the website:\n  (image will vary depending on what you supplied for websiteimage)\n function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/2_configure_env\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_autonomous_monitoring_of_cryptographic_activity_with_kms/1_deploy_the_lab_base_infrastructure/","title":"Deploy The Lab Base Infrastructure","tags":[],"description":"","content":"In this section, we will build out a Virtual Public Cloud (VPC) , together with public and private subnets across two Availability Zones , Internet Gateway and NAT gateway along with the necessary routes from both public and private subnets.\nThis VPC will become the baseline network architecture within which the application will run. When we successfully complete our initial template deployment, our deployed workload should reflect the following diagram:\nTo deploy the infrastructure template follow the appropriate steps:\n1.1. Get the Cloudformation Template. To deploy the first CloudFormation template, you can either deploy directly from the command line or via the console.\nYou can get the template here.   Click here for CloudFormation command-line deployment steps   Command Line Deployment: To deploy from the command line, ensure that you have installed and configured AWS CLI with the appropriate credentials.\n1.1.1. Execute Command aws cloudformation create-stack --stack-name pattern3-base \\ --template-body file://pattern1-base.yml \\ --region ap-southeast-2 Note: Please adjust your command-line if you are using profiles within your aws command line as required.\n1.1.2. Confirm that the stack has installed correctly. You can do this by running the describe-stacks command as follows:\naws cloudformation desribe-stacks --stack-name pattern3-base Locate the StackStatus and confirm it is set to CREATE_COMPLETE as shown here:\n1.1.3. Take note of this stack output as we will need it for later sections of the lab.\n    Click here for CloudFormation console deployment steps   Console: If you decide to deploy the stack from the console, ensure that you follow below requirements \u0026amp; step:\n Please follow this guide for information on how to deploy the cloudformation template. Use pattern1-base as the Stack Name, as this is referenced by other stacks later in the lab.  1.2. Note Cloudformation Template Outputs When the CloudFormation template deployment is completed, note the outputs produced by the newly created stack as these will be required at later points in the lab.\nYou can do this by clicking on the stack name you just created, and select the Outputs Tab as shown in diagram below.\nYou can now proceed to Section 2 of the lab where we will build out the application stack.\n   END OF SECTION 1\n "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_autonomous_patching_with_ec2_image_builder_and_systems_manager/1_deploy_the_lab_base_infrastructure/","title":"Deploy The Lab Base Infrastructure","tags":[],"description":"","content":"In this section, we will build out a Virtual Public Cloud (VPC) , together with public and private subnets across two Availability Zones , Internet Gateway and NAT gateway along with the necessary routes from both public and private subnets.\nThis VPC will become the baseline network architecture within which the application will run. When we successfully complete our initial stage template deployment, our deployed workload should reflect the following diagram:\nTo deploy the template for the base infrastructure build follow the approptiate steps:\n1.1. Get the Cloudformation Template. To deploy the first CloudFormation template, you can either deploy directly from the command line or via the console.\nYou can get the template here.   Click here for CloudFormation command-line deployment steps   Command Line Deployment: To deploy from the command line, ensure that you have installed and configured AWS CLI with the appropriate credentials.\n1.1.1. Execute Command aws cloudformation create-stack --stack-name pattern3-base \\ --template-body file://pattern3-base.yml \\ --region ap-southeast-2 Note: Please adjust your command-line if you are using profiles within your aws command line as required.\n1.1.2. Confirm that the stack has installed correctly. You can do this by running the describe-stacks command as follows:\naws cloudformation desribe-stacks --stack-name pattern3-base Locate the StackStatus and confirm it is set to CREATE_COMPLETE as shown here:\n1.1.3. Take note of this stack output as we will need it for later sections of the lab.\n    Click here for CloudFormation console deployment steps   Console: If you need detailed instructions on how to deploy CloudFormation stacks from within the console, please follow this guide.  Use pattern3-base as the Stack Name, as this is referenced by other stacks later in the lab.  1.2. Note Cloudformation Template Outputs When the CloudFormation template deployment is completed, note the outputs produced by the newly created stack as these will be required at later points in the lab.\nYou can do this by clicking on the stack name you just created, and select the Outputs Tab as shown in diagram below.\nYou can now proceed to Section 2 of the lab where we will build out the application stack.\n   END OF SECTION 1\n "},{"uri":"https://wellarchitectedlabs.com/reliability/100_labs/100_deploy_cloudformation/1_deploy_vpc/","title":"Deploy VPC using CloudFormation","tags":[],"description":"","content":"1.1 Log into the AWS console If you are attending an in-person workshop and were provided with an AWS account by the instructor:\n  Click here for instructions to access your assigned AWS account:     Go to https://dashboard.eventengine.run/login\n  Enter the 12 character hashcode you were provided and click \u0026ldquo;Proceed\u0026rdquo;   [optional] assign a name to your account (this is referred to as \u0026ldquo;Team name\u0026rdquo;)\n click \u0026ldquo;Set Team Name\u0026rdquo; Enter a name and click \u0026ldquo;Set Team Name\u0026rdquo;    Click \u0026ldquo;AWS Console\u0026rdquo;\n  AWS credentials   IMPORTANT Copy the provided credentials and save them. You wil need these to complete the workshop   Copy the whole code block corresponding to the system you are using.\n  Access the AWS console  Click \u0026ldquo;Open Console\u0026rdquo;. The AWS Console will open.     If you are using your own AWS account:   Click here for instructions to use your own AWS account:    Sign in to the AWS Management Console as an IAM user who has PowerUserAccess or AdministratorAccess permissions, to ensure successful execution of this lab.   \n1.2 Configure your AWS Region  Select the Ohio region. This region is also known as us-east-2, which you will see referenced throughout this lab.  AWS offers you the ability to deploy to over 20 regions located across the globe Each region is fully isolated from the others to isolate any issues and achieve high availability, Each region is comprised of multiple Availability Zones, which are fully isolated partitions of our infrastructure (more on this later)    1.3 Deploy the VPC infrastructure This step will create the VPC and all components using the example CloudFormation template.\n Download the latest version of the CloudFormation template here: vpc-alb-app-db.yaml  Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Click Create Stack, then With new resources (standard).  Click Upload a template file and then click Choose file.  Choose the CloudFormation template you downloaded in step 1, return to the CloudFormation console page and click Next. Enter the following details:  Stack name: The name of this stack. For this lab, use WebApp1-VPC and match the case. Parameters: Parameters may be left as defaults, you can find out more in the description for each.    At the bottom of the page click Next. In this lab, we use tags, which are key-value pairs, that can help you identify your stacks. Enter Owner in the left column which is the key, and your email address in the right column which is the value. We will not use additional permissions or advanced options so click Next. For more information, see Setting AWS CloudFormation Stack Options . Review the information for the stack. When you\u0026rsquo;re satisfied with the configuration, at the bottom of the page check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack.   After a few minutes the final stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE. You can click the refresh button to check on the current status. You have now created the VPC stack (well actually CloudFormation did it for you).\n  When the stack status is CREATE_COMPLETE, you can continue to the next step.\n   function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/2_deploy_webapp\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_automated_iam_user_cleanup/1_lambda_iam_cleanup/","title":"Deploying IAM Lambda Cleanup with AWS SAM","tags":[],"description":"","content":"  Download these two templates (or by cloning this repository):\n cloudformation-iam-user-cleanup.yaml  lambda-iam-user-cleanup.py     Create an Amazon S3 bucket if you don\u0026rsquo;t already have one, it needs to be in the same AWS region being deployed into.\n  Now that you have the S3 bucket created and the files downloaded to your machine. You can start to create your deployment package on the command line with AWS SAM. Make sure you are working in the folder where where you have downloaded the files to.\nRun the following command to prepare your deployment package:\naws cloudformation package --template-file cloudformation-iam-user-cleanup.yaml --output-template-file output-template.yaml --s3-bucket \u0026lt;bucket\u0026gt;\n  Once you have finished preparing the package you can deploy the CloudFormation with AWS SAM:\nNOTE: The template file to use here is the output file from the previous command:\naws cloudformation deploy --template-file output-template.yaml --stack-name IAM-User-Cleanup --capabilities CAPABILITY_IAM --parameter-overrides NotificationEmail=\u0026lt;replace_with_your_email_address\u0026gt;\n  Once you have completed the deployment of your AWS Lambda function, test the function by going to the AWS Lambda function in your AWS account and create a dummy event by selecting test.\nIf your test runs successfully you should receive an email from:\nAWS Notifications no-reply@sns.amazonaws.com\nwith the subject line of: IAM user cleanup from \u0026lt;account_ID\u0026gt;\nand the body of the email will have a status report from the findings. E.g. IAM Users and AWS Access Keys which require a cleanup\nIAM user cleanup successfully ran.\nUser John Doe has not logged in since 2018-04-19 08:36:18+00:00 and needs cleanup\nUser John Doe has not used access key AKIAIOSFODNN7EXAMPLE in since 2018-04-22 21:32: 00+00:00 and needs cleanup\nUser John Doe has not used access key AKIAIOSFODNN7EXAMPLE in since 2018-04-22 20:08:00+00:00 and needs cleanup\nPrincipal 012345678901 has permission to sts:AssumeRole against arn:aws:iam::0123456789012:role/Role AWS::IAM::Role. See finding 00000000-0000-0000-0000-000000000000 in IAM Access Analyzer for more information or to archive this finding\n  "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_incident_response_with_aws_console_and_cli/1_getting_started/","title":"Getting Started","tags":[],"description":"","content":"1.1 Install the AWS CLI Although instructions in this lab are written for both AWS Management console and AWS CLI, its best to install the AWS CLI on the machine you will be using as you can modify the example commands to run different scenarios easily and across multiple AWS accounts.\n Install the AWS CLI on macOS  Install the AWS CLI on Linux  Install the AWS CLI on Windows   You will also need jq to parse json from the CLI:\n Install jq    A best practice is to enforce the use of MFA, so if you misplace your AWS Management console password and/or access/secret key, there is nothing anyone can do without your MFA credentials. You can follow the instructions here to configure AWS CLI to assume a role with MFA enforced.\n1.2 Amazon CloudWatch Logs Amazon CloudWatch Logs can be used to monitor, store, and access your log files from Amazon Elastic Compute Cloud (Amazon EC2) instances, AWS CloudTrail, Amazon Route 53, Amazon VPC Flow Logs, and other sources. It is a best practice to enable logging and analyze centrally, and develop investigation proceses. Using the AWS CLI and developing runbooks for investigation into different events can be significantly faster than using the console. If your logs are stored in Amazon S3 instead, you can use Amazon Athena to directly analyze data.\nTo list the Amazon CloudWatch Logs Groups you have configured in each region, you can describe them. Note you must specify the region, if you need to query multiple regions you must run the command for each. You must use the region ID such as us-east-1 instead of the region name of US East (N. Virginia) that you see in the console. You can obtain a list of the regions by viewing them in the AWS Regions and Endpoints or using the CLI command: aws ec2 describe-regions. To list the log groups you have in a region, replace the example us-east-1 with your region: aws logs describe-log-groups --region us-east-1 The default output is json, and it will give you all details. If you want to list only the names in a table: aws logs describe-log-groups --output table --query 'logGroups[*].logGroupName' --region us-east-1\n"},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_aws_resource_optimization/1_cloudwatch_intro/","title":"Getting to know Amazon Cloudwatch","tags":[],"description":"","content":"The first step to perform right sizing is to monitor and analyze your current use of services to gain insight into instance performance and usage patterns. To gather sufficient data, observe performance over at least a two-week period (ideally, over a one-month period) to capture the workload and business peak. The most common metrics that define instance performance are vCPU utilization, memory utilization, network utilization, and disk use.\n  Log into your AWS console via SSO, go to the Amazon CloudWatch service page:   Select EC2 under the Service Dashboard:   Observe the Service Dashboard and all of its different metrics, but focus on CPU Utilization and Network In and Out:   Select one of the EC2 resources by clicking on the little color icon to the left of the resource-id name:   Deselect the EC2 resource and now modify the time range on the top right, click custom and select the last 2 weeks:   Navigate to the CPU Utilization Average widget, click the three dots and launch the View in metrics page. Using the Graphed metrics section try to answer the following questions:\n   a. What is the instance with the highest CPU Average? b. What is the instance with the highest CPU Max? c. What is the instance with the lowest CPU Min?   function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/2_resource_opt\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_aws_resource_optimization/1_cloudwatch_intro/","title":"Getting to know Amazon Cloudwatch","tags":[],"description":"","content":"NOTE: In order to run this lab you will need to have at least one EC2 instance running and have AWS Cost Explorer and Amazon EC2 Resource Optimization enabled.\nDuring this lab we will create a custom metric at Amazon CloudWatch and install an agent in one EC2 instance to start collecting Memory utilization and improve the recommendation accuracy of the Amazon EC2 Resource Optimization report. Be aware that custom metrics are not part of the Amazon CloudWatch free tier  usage so additional costs will incur at your bill. For more information read the Amazon CloudWatch pricing page.\nAll custom metrics charges are prorated by the hour and metered only when you send metrics to Amazon CloudWatch. Each custom metrics costs $0.30 per metric/month for the first 10,000 metrics and can go down to $0.02 per metric/month at the lowest priced tier (US Virginia prices from November 2019).\n1. Getting to know Amazon Cloudwatch The first step to perform right sizing is to monitor and analyze your current use of services to gain insight into instance performance and usage patterns. To gather sufficient data, observe performance over at least a two-week period (ideally, over a one-month period) to capture the workload and business peak. The most common metrics that define instance performance are vCPU utilization, memory utilization, network utilization, and disk use.\n  Log into your AWS console, go to the Amazon CloudWatch service page:   Select EC2 under the Service Dashboard:   Observe the Service Dashboard and all of its different metrics, but focus on CPU Utilization and Network In and Out:   Select one of the EC2 resources by clicking on the little color icon to the left of the resource-id name:   Deselect the EC2 resource and now modify the time range on the top right, click custom and select the last 2 weeks:   Navigate to the CPU Utilization Average widget and launch the View Metrics detailed page. Using the Graphed metrics session try to answer the following questions:\n   a) What is the instance with the lowest CPU Average? b) What is the instance with the lowest CPU Max? c) What is the instance with the lowest CPU Min?   function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/2_create_iamrole\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_lambda_cross_account_bucket_policy/1_create_bucket/","title":"Identify (or create) S3 bucket in account 2","tags":[],"description":"","content":"This lab is best run using two AWS accounts  Identify the AWS account number for account 1 (no dashes) Identify the AWS account number for account 2 (no dashes)  If you only have one AWS account, then use the same AWS account number for both account1 and account2\n In account 2 sign in to the S3 Management Console as an IAM user or role in your AWS account, and open the S3 console at https://console.aws.amazon.com/s3 Choose an S3 bucket that contains some objects. You will enable the ability to list the objects in this bucket from the other account.  If you would rather create a new bucket to use, follow these directions  Record the bucketname    "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_incident_response_playbook_with_jupyter-aws_iam/1_prereq/","title":"Install Python &amp; AWS CLI","tags":[],"description":"","content":"1.1 Install Python and Modules Python 3 and a number of Python modules are required.\n Python downloads  Installing pip may be required also.  After installing Python, install the following packages by executing the following command in your command line or terminal:\npip install boto3 pandas jupyter 1.2 Install the AWS CLI AWS CLI is not directly used for this lab, however it makes configuration of the AWS IAM credentials easier, and is useful for testing and general use.\n Install AWS CLI:   Install the AWS CLI on macOS  Install the AWS CLI on Linux  Install the AWS CLI on Windows   In your command line or terminal run aws configure to configure your credentials. Note the user will require access to the IAM service.  A best practice is to enforce the use of MFA, so if you misplace your AWS Management console password and/or access/secret key, there is nothing anyone can do without your MFA credentials. You can follow the instructions here to configure AWS CLI to assume a role with MFA enforced.\n"},{"uri":"https://wellarchitectedlabs.com/operational-excellence/100_labs/100_inventory_patch_management/1_intro/","title":"Intro","tags":[],"description":"","content":"In the cloud, you can apply the same engineering discipline that you use for application code to your entire environment. You can define your entire workload (applications, infrastructure, etc.) as code and update it with code. You can script your operations procedures and automate their execution by triggering them in response to events. By performing operations as code, you limit human error and enable consistent execution of operations activities.\nIn this lab you will apply the concepts of Infrastructure as Code and Operations as Code to the following activities:\n Deployment of Infrastructure Inventory Management Patch Management  Included in the lab guide are bonus sections that can be completed if you have time or later if interested.\n Creating Maintenance Windows and Scheduling Automated Operations Activities Create and Subscribe to a Simple Notification Service Topic   Important You will be billed for any applicable AWS resources used in this lab that are not covered in the AWS Free Tier . At the end of the lab guide there is an additional section on how to remove all the resources you have created.\n  Removing Lab Resources   function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/2_setup_env\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_basic_ec2_with_waf_protection/1_launch_instance/","title":"Launch Instance","tags":[],"description":"","content":"For launching your first instance, we are going to use the launch wizard in the Amazon EC2 console.\n1.1 Launch Single Linux Instance You can launch a Linux instance using the AWS Management Console. This tutorial is intended to help you launch your first instance quickly, so it doesn\u0026rsquo;t cover all possible options. For more information about the advanced options, see Launching an Instance . Launch an instance:\n Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. From the console dashboard, choose Launch Instance.  The choose an Amazon Machine Image (AMI) page displays a list of basic configurations, called Amazon Machine Images (AMIs), that serve as templates for your instance. Select the HVM edition of the Amazon Linux AMI (not Amazon Linux 2).  On the Choose an Instance Type page, you can select the hardware configuration of your instance. Select the t2.micro type, which is selected by default. Notice that this instance type is eligible for the free tier. Then select Next: Configure Instance Details.  On the Configure Instance Details page, make the following changes:  5.1 Select Create new IAM role.\n5.2\tIn the new tab that opens, select Create role.\n5.3\tWith AWS service pre-selected, select EC2 from the top of the list, then click Next: Permissions.\n5.4\tEnter s3 in the search and select AmazonS3ReadOnlyAccess from the list of policies, then click Next: Review. This policy will give this EC2 instance access to read and list any objects in Amazon S3 within your AWS account.\n5.5\tEnter a role name, such as ec2-s3-read-only-role, and then click Create role.\n5.6\tBack on the EC2 launch web browser tab, select the refresh button next to Create new IAM role, and click the role you just created.\n5.7 Scroll down and expand the Advanced Details section. Enter the following in the User Data test box to automatically install Apache web server and apply basic configuration when the instance is launched:\n``` #!/bin/bash yum update -y yum install -y httpd service httpd start chkconfig httpd on groupadd www usermod -a -G www ec2-user chown -R root:www /var/www chmod 2775 /var/www find /var/www -type d -exec chmod 2775 {} + find /var/www -type f -exec chmod 0664 {} + ```   Accept defaults and Choose Next: Add tags.\n  Click Next: Configure Security Group.\n7.1 On type SSH, select Source as My IP\n7.2 Click Add Rule, select Type as HTTP and source as Anywhere\n Note that best practice is to have an Elastic Load Balancer inline or the EC2 instance not directly exposed. However, for simplicity in this lab, we are opening the access to anywhere. Later modules will secure access with Elastic Load Balancer.  7.3 Select Add Rule to add both SSH and HTTP, and on source, select My IP\n7.4 Click Review and Launch.\n  On the Review Instance Launch page, check the details, and then click Launch.\n  If you do not have an existing key pair for access instances, a prompt will appear. Click Create New, then type a name such as lab, click Download Key Pair, and then click Launch Instances.\n  This is the only chance to save the private key file. You\u0026rsquo;ll need to provide the name of your key pair when you launch an instance, and you\u0026rsquo;ll provide the corresponding private key each time you connect to the instance.\n Click View Instances. When your instance is launched, its status will change to running, and it will need a few minutes to apply patches and install Apache web server.  You can connect to the Apache test page by entering the public DNS, which you can find on the description tab or instances list. Take note of this public DNS value.  "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_cloudfront_with_waf_protection/1_launch_instance/","title":"Launch Instance","tags":[],"description":"","content":"You can launch a Linux instance using the AWS Management Console. This tutorial is intended to help you launch your first instance quickly, so it doesn\u0026rsquo;t cover all possible options. For more information about the advanced options, see Launching an Instance . Launch an instance:\n  Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/.\n  From the console dashboard, choose Launch Instance.   The choose an Amazon Machine Image (AMI) page displays a list of basic configurations, called Amazon Machine Images (AMIs), that serve as templates for your instance. Select the HVM edition of the Amazon Linux AMI, either version.   On the Choose an Instance Type page, you can select the hardware configuration of your instance. Select the t2.micro type, which is selected by default. Notice that this instance type is eligible for the free tier. Then select Next: Configure Instance Details.   On the Configure Instance Details page, make the following changes:\n5.1 Select Create new IAM role.\n5.2\tIn the new tab that opens, select Create role.\n5.3\tWith AWS service pre-selected, select EC2 from the top of the list, then click Next: Permissions.\n5.4\tEnter s3 in the search and select AmazonS3ReadOnlyAccess from the list of policies, then click Next: Review. This policy will give this EC2 instance access to read and list any objects in Amazon S3 within your AWS account.\n5.5 Enter a role name, such as ec2-s3-read-only-role, and then click Create role.\n5.6\tBack on the EC2 launch web browser tab, select the refresh button next to Create new IAM role, and click the role you just created.\n5.7 Scroll down and expand the Advanced Details section. Enter the following in the User Data test box to automatically install Apache web server and apply basic configuration when the instance is launched:\n  #!/bin/bash yum update -y yum install -y httpd service httpd start chkconfig httpd on groupadd www usermod -a -G www ec2-user chown -R root:www /var/www chmod 2775 /var/www find /var/www -type d -exec chmod 2775 {} + find /var/www -type f -exec chmod 0664 {} +  Accept defaults and click Next: Add tags.\n  Click Next: Configure Security Group. 7.1 Accept default option Create a new security group. 7.2 On the line of the first default entry SSH, select Source as My IP. 7.3 Click Add Rule, select Type as HTTP and Source as Anywhere. Note that best practice is to have an Elastic Load Balancer inline or the EC2 instance not directly exposed to the internet. However, for simplicity in this lab, we are opening the access to anywhere. Other lab modules secure access with Elastic Load Balancer.\n7.5 Click Review and Launch.\n  On the Review Instance Launch page, check the details, and then click Launch.\n  If you do not have an existing key pair for access instances, a prompt will appear. Click Create New,then type a name such as lab, click Download Key Pair, and then click Launch Instances.\n  This is the only chance to save the private key file. You\u0026rsquo;ll need to provide the name of your key pair when you launch an instance, and you\u0026rsquo;ll provide the corresponding private key each time you connect to the instance.\n Click View Instances. When your instance is launched, its status will change to running, and it will need a few minutes to apply patches and install Apache web server.  You can connect to the Apache test page by entering the public DNS, which you can find on the description tab or instances list. Take note of this public DNS value.  "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/100_labs/100_walkthrough_of_the_well-architected_tool/1_nav_console/","title":"Navigating to the console","tags":[],"description":"","content":"The AWS Well-Architected Tool is in the AWS Console. You simply need to login to the console and navigate to the tool.\n  Sign in to the AWS Management Console as an IAM user with MFA enabled or in a federated Role, and open the Well-Architected console at https://console.aws.amazon.com/wellarchitected/ .\n  If you are already in the console, click Services on the tool bar along the top of the console to bring up the service search. Start typing Well Architected into the search box and select the AWS Well-Architected Tool:\n   function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/2_create_workload\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_100_loft_introduction_to_security/1_new_acct_secure_root/","title":"New AWS Account Setup and Securing Root User","tags":[],"description":"","content":"Walkthrough This hands-on lab will guide you through the introductory steps to configure a new AWS account and secure the root user. AWS Account and Root User Further Considerations  Federate Identity Using SAML: Leveraging a SAML provider  Separate production, non-production and different workloads using different AWS accounts: AWS Multiple Account Billing Strategy   "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_automated_cur_query_and_email_delivery/1_architecture/","title":"Overview architecture","tags":[],"description":"","content":" function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/2_create_bucket\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_certificate_manager_request_public_certificate/1_request_certificate/","title":"Requesting a public certificate using the console","tags":[],"description":"","content":"  Sign into the AWS Management Console and open the ACM console at https://console.aws.amazon.com/acm/home . Select your prefferred region for regional certificates including Elastic Load Balancing, or US East (N. Virginia) for global services including Amazon CloudFront.\n  If you see a welcome page, click Get started under provision certificates area.\n  On the Request a certificate page, click Request a public certificate, then click Request a certificate.\n  Type your domain name. You can use a fully qualified domain name (FQDN) such as www.example.com or a bare or apex domain name such as example.com. You can also use an asterisk * as a wildcard in the leftmost position to protect several site names in the same domain. For example, *.example.com protects corp.example.com, and images.example.com. The wildcard name will appear in the Subject field and the Subject Alternative Name extension of the ACM certificate.\nNote: When you request a wildcard certificate, the asterisk * must be in the leftmost position of the domain name and can protect only one subdomain level. For example, *.example.com can protect login.example.com, and test.example.com, but it cannot protect test.login.example.com. Also note that *.example.com protects only the subdomains of example.com, it does not protect the bare or apex domain example.com. To protect both, see the next step.\n  To add more domain names to the ACM certificate, choose Add another name to this certificate and type another domain name in the text box that opens. This is useful for protecting both a bare or apex domain (like example.com) and its subdomains *.example.com.\n  After you have typed valid domain names, choose Next.\n  Before ACM issues a certificate, it validates that you own or control the domain names in your certificate request. You can use either email validation or DNS validation. If you choose email validation, ACM sends validation email to three contact addresses registered in the WHOIS database and to five common system administration addresses for each domain name. You or an authorized representative must approve one of these email messages. If you use DNS validation, you simply create a CNAME record provided by ACM to your DNS configuration. Choose your option, then click Review.\nNote: If you are able to edit your DNS configuration, we recommend that you use DNS domain validation rather than email validation. DNS validation has multiple benefits over email validation. See Use DNS to Validate Domain Ownership.\n  If the review page correctly contains the information that you provided for your request, choose Confirm and request. The following page shows that your request status is pending validation. You must approve the request either through email link or DNS record.\nImportant: Unless you choose to opt out, your certificate will be automatically recorded in at least two public certificate transparency databases. You cannot currently use the console to opt out. You must use the AWS CLI or the API. For more information, see Opting Out of Certificate Transparency Logging . For general information about transparency logs, see Certificate Transparency Logging .\n  Your certificate is now ready to associate with a supported service .\n  "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_splitting_sharing_cur_access/1_setup_s3_output/","title":"Setup Output S3 Bucket","tags":[],"description":"","content":"We need to provide a location to deliver the output from the Athena queries, so that it can be secured and restricted to the sub accounts. We\u0026rsquo;ll need to create the S3 bucket, and implement a Lambda function to re-write the object ACLs when new objects are delivered.\nSo what we\u0026rsquo;ll do is as follows:\n Create the output S3 bucket with the required bucket policy Create an IAM policy that will allow a Lambda function to re-write object ACLs Implement the Lambda function  1 - Login to the consolve via SSO.\n2 - Go to the S3 console\n3 - Create the output S3 bucket\n4 - The lab has been designed to allow multiple statements to output to a single bucket, each in a different folder. Create one folder for each Athena statement you will run, a convenient name for the folders is the Account ID of the sub account.\nDo not put any hyphens \u0026lsquo;-\u0026rsquo; in your folder name, as it will be an Athena table and there are restrictions on naming conventions.\n 5 - Go to Permissions, and implement a bucket policy to allow sub accounts access, ensure you follow security best practices and allow least privilege: You can modify this sample policy as a starting point:\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;AllowListingOfFolders\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;AWS\u0026quot;: \u0026quot;arn:aws:iam::(account ID):root\u0026quot; }, \u0026quot;Action\u0026quot;: \u0026quot;s3:ListBucket\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:s3:::(bucket)\u0026quot; }, { \u0026quot;Sid\u0026quot;: \u0026quot;AllowAllS3ActionsInSubFolder\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;AWS\u0026quot;: \u0026quot;arn:aws:iam::(account ID):root\u0026quot; }, \u0026quot;Action\u0026quot;: \u0026quot;s3:*\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:s3:::(bucket)/(folder)/*\u0026quot; } ] } 6 - Go to the IAM Dashboard\n7 - Create an IAM policy Lambda_S3Linked_PutACL to allow lambda to write ACLs: You can modify the following sample policy as a starting point:\nNOTE: replace (bucket name):\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;VisualEditor0\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;s3:PutObjectVersionAcl\u0026quot;, \u0026quot;s3:PutObjectAcl\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:s3:::(bucket name)/*\u0026quot; } ] } 8 - Create an IAM role for Lambda named Lambda_Put_Linked_S3ACL\n9 - Attach the Lambda_S3Linked_PutACL policy: 10 - Go to the Lambda service dashboard\n11 - Create the lambda function S3LinkedPutACL with the following details:\n Node.js Role: Lambda_Put_Linked_S3ACL Code: ./Code/S3LinkedPutACL.md   12 - Go to the S3 service dashboard\n13 - Select the Output Bucket, go to Properties, and add an S3 event to trigger on All object create events, and have it run the S3LinkedPutACL Lambda function: 14 - Test the configuration is working correctly by uploading a file into the S3 folder. Verify that it has multiple Grantees to the required accounts: 15 - Delete the file and ensure all folders are empty. The output bucket setup is now complete. Every time the Athena query runs and outputs a file into the S3 bucket, it will automatically have its permissions ACL updated to allow access to the sub account.\n function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/2_backfill_data\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_4_cost_and_usage_analysis/1_verify_cur/","title":"Verify your CUR files are being delivered","tags":[],"description":"","content":"We will verify the CUR files are being delivered, they are in the correct format and the region they are in.\n  Log into the console via SSO.\n  Get the bucket name which contains your Cost and Usage Report (CUR) files in the management/payer account. Modify the following address and replace (bucket name) with your bucket:\n https://s3.console.aws.amazon.com/s3/buckets/(bucket name)/     You should see a aws-programmatic-access-test-object which was put there to verify AWS can deliver reports, and also the folder which is the report prefix - cur. Click on the folder name for the prefix (here it is cur):   Click on the folder name which is also part of the prefix (here it is WorkshopCUR):   Click on the prefix folder, here it is WorkshopCUR, then drill down in the current year and month:   You can see the delivered CUR file, it is in the parquet format:   You have successfully verified that you have access to the CUR files, and they are being delivered in the correct format.\n Sample Files You may not have substantial or interesting usage, in this case there are sample files below. Create a folder structure, such as (bucket name)/cur/WorkshopCUR/WorkshopCUR/year=2018/month=x and copy the parquet files below into each months folder:\n October 2018 Usage  November 2018 Usage  December 2018 Usage    function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/2_setup_athena\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_monitoring_with_cloudwatch_dashboards/1_intro/","title":"View Amazon CloudWatch Automatic Dashboards","tags":[],"description":"","content":"1. View Amazon CloudWatch Automatic Dashboards  Amazon CloudWatch Automatic Dashboards allow you to easily monitor all AWS Resources, and is quick to get started. Explore account and resource-based view of metrics and alarms, and easily drill-down to understand the root cause of performance issues.\n  Open the Amazon CloudWatch console at https://console.aws.amazon.com/cloudwatch/ and select your region from the top menu bar.\n  If you are logging into a brand new AWS account, you will see the default Cloudwatch console such as this: You will need to deploy something into your account to see a Cloudwatch automatic dashboard.\n  Once you have services deployed into your AWS account, Cloudwatch will automatically populate the Overview tab with various metrics such as this:   The upper left shows a list of AWS services you use in your account, along with the state of alarms in those services. In this example, it is showing that we have an EC2 instance in use and it is marked as OK.   The upper right shows alarms in your account, which will contain up to four alarms that are in the ALARM state or it will show those that most recently changed state. These upper areas enable you to assess the health of your AWS services, by seeing the alarm states in every service and the alarms that most recently changed state. This helps you monitor and quickly diagnose issues.\n  Below these areas is spot for a custom default dashboard that you can create that is named CloudWatch-Default This is a convenient way for you to add metrics about your own custom services or applications to the overview page, or to bring forward additional key metrics from AWS services that you most want to monitor. In this example, we do not have a custom default dashboard created.  If you wish, you can click the \u0026ldquo;Create a new CloudWatch-Default dashboard\u0026rdquo; to generate a new dashboard and see it displayed in the overview screen.    If you use six or more AWS services, below the default dashboard is a link to the automatic cross-service dashboard. The cross-service dashboard automatically displays key metrics from every AWS service you use without requiring you to choose what metrics to monitor or create custom dashboards. You can also use it to drill down to any AWS service and see even more key metrics for that service. In this example, we see both EC2 metrics as well as EBS volume metrics for the test machines that were created.\n   function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/2_cleanup\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_3_pricing_models/1_ri_report/","title":"View an RI report","tags":[],"description":"","content":"We are going to view the RI reports within AWS Cost Explorer, to understand the potential savings and usage trends.\n  Log into the console via SSO, go to the management/payer account and go to the AWS Cost Explorer service page:   In the left menu under Reservations select Recommendations:   On the right select the filters: Select recommendation type non-EC2 service, RI term 1 year, Payment Option (your preference), Based on the past 7 days:   The top section will show the estimated savings and number of recommendations, take note of the Purchase Recommendations\nOn the right select the filter: Based on the past 30 days:   View the Purcahse Recommendations, if the 7 days recommendation is more than the 30 days recommendation - your usage is increasing and the recommendations are lower risk. If the 7 days recommendation is less than 30 days, then your usage is decreasing and you need to look further into your usage patterns to see which RI\u0026rsquo;s would be suitable.\nYou now have an overview of your potential savings and your usage trends.\n  function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/2_prepare_csv\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_4_cost_and_usage_analysis/1_view_invoice/","title":"View your AWS Invoices","tags":[],"description":"","content":"At the end of a billing cycle or at the time you make a purchase and incur a one-time fee, AWS charges the payment method you have and issues your invoice as a PDF file. You can view these invoices through the AWS console, which will show summary information of all usage and cost incurred for that one off item, or billing period.\n  Log into the console via SSO and go to the billing dashboard:   Select Orders and invoices from the menu on the left:   Click on an Invoice ID corresponding to the month you wish to view:   It will download a PDF version of your invoice similar to below:    function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/2_cost_usage_detail\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_5_cost_visualization/1_cost_usage_service/","title":"View your cost and usage by service","tags":[],"description":"","content":"AWS Cost Explorer is a free built in tool that lets you dive deeper into your cost and usage data to identify trends, pinpoint cost drivers, and detect anomalies. We will examine costs by service in this exercise.\n  Log into the console via SSO and go to the billing dashboard:   Select Cost Explorer from the menu on the left:   Click on Launch Cost Explorer:   Click on Reports from the left menu:   You will be presented a list of pre-configured and saved reports. Click on Monthly costs by service:   This is the monthly costs by service for the last 6 months, broken down by month (your usage will most likely be different):   We will change to a daily view to highlight trends. Select the Monthly drop down and click on Daily:   The bar graph is difficult to read, so we will switch to a line graph. Click on the Bar dropdown, then select Line:   This is the same data with daily granularity and shows trends much more clearly. There are monthly peaks - these are monthly recurring reservation fees from Reserved Instances (Purple line):   We will remove the Recurring reservation fees. Click on More filters then click Charge Type filter on the right, click the checkbox next to Recurring reservation fee, select Exclude only to remove the data. Then click Apply filters:   We have now excluded the monthly recurring fees and the peaks have been removed. We can see the largest cost for our usage during this period is EC2-Instances:   We will remove the EC2 service to show the other services with better clarity. Click on the Service filter from the right, click the checkbox next to EC2-Instances, select Exclude only, and click Apply filters:   EC2-Instances has now been excluded, and all the other services can been seen easily:   You have now viewed the costs by service and applied multiple filters. You can continue to modify the report by timeframe and apply other filters.\n function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/2_cost_usage_account\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_3_pricing_models/1_view_recommendations/","title":"View your Savings Plan recommendations","tags":[],"description":"","content":"Introduction Savings Plans are a commitment based discount model. By making a commitment of spend you will use for 1 or 3 years, you receive a discount of up to 72%. They offer the same discounts as Reserved Instances, however offer a great deal more flexibility, and do not have the same management overhead.\nIn this workshop we will take you through your recommendations, and help you choose the right savings plan for your future business requirements.\n  Log into the console via SSO and go to the Cost Explorer dashboard:   Click on Savings Plans on the left menu:   You can see a description and some examples of Savings Plans under the Savings Plans heading, and estimated savings at the bottom::   Click on Recommendations on the left menu:   You can see the default options at the top, its a Compute savings plan, for 3-year, paying All upfront and based on the previous 30 days of usage.   At the bottom you can see your estimated before and after spend, along with the percentage saving, this is an ideal starting point to understand the overall return you can get on your commitment:   Click on Compute, 1-year and No upfront from the options above, and see the changes in before and after below. Note down the % saving, in this example it is 22%:   Click on EC2 Instance, 3-year and All upfront, and note the % saving, in this example it is now 46%:   This will typically be the highest and lowest savings you can achieve on the previous usage that was analyzed. You can vary those options to achieve the discount and features that most suit your business. You can also combine the options by purchasing multiple savings plans, making some commitment for 1-year with an upfront component, and some with no upfront commitment for a 3-year term.\n  While the commitment is a full 1 or 3 years, a Savings Plan will typically be paid off much sooner. We will analyze this in the next step.\n function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/2_usage_trend\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/100_labs/100_inventory_patch_management/","title":"100 - Inventory and Patch Management","tags":[],"description":"","content":"Introduction In this lab you will apply the concepts of Infrastructure as Code and Operations as Code to the following activities:\n Deployment of Infrastructure Inventory Management Patch Management  Goals:  Automated deployment of infrastructure Dynamic management of resources Automated patch management  Prerequisites:  An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account with full access to CloudFormation, EC2, VPC, IAM.  Costs NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\n  function prevStep(){ window.open(\"\", \"_self\") } function nextStep(){ window.open(\".\\/1_intro\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool  Start Lab  Steps:  Intro   Setup   Deploy an Environment Using Infrastructure as Code   Inventory Management using Operations as Code   Patch Management   Creating Maintenance Windows and Scheduling Automated Operations Activities   Creating a Simple Notification Service Topic   Removing Lab Resources   "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/200_labs/200_automating_operations_with_playbooks_and_runbooks/","title":"200 - Automating your operations with Playbooks and Runbooks","tags":[],"description":"","content":"Authors  Stephen Salim, Well-Architected Geo Solutions Architect. Brian Carlson, Well-Architected Operational Excellence Pillar Lead.  Introduction This hands-on lab will guide you through the steps to automate your operational activities using playbook and runbooks built with AWS tools.\nAt a glance, both Playbook and Runbooks appears to be similar documents that any adequately skilled team members (who are unfamiliar with the workload) can use to execute operational activities. However, the main difference between them is that; a Playbook is intended to document the process /guide to gather applicable information, identify potential sources of failure, isolate faults, and determine root cause of issues. Runbooks contain instructions necessary to successfully complete an activity to resolve the issue.\nExecuting both Playbook and Runbook in an automated fashion is critical to achieve operational excellence for your workload. Isolating human element and streamlining the process will make significant impact in the reliability, scalability, traceability of your operations.\nIn this lab, we will show how you can build an automated Playbook to investigate an issue in a workload, and a Runbook to remediate the issue using AWS tools, services we will utilize in this lab includes.\n Event Bridge Rules Systems Manager Automation Document Simple Notification Service  Goals:  Build \u0026amp; Execute Automated Investigative Playbook \u0026amp; Remediative Runbook Build \u0026amp; Execute Automated Issue remediation Runbook Enabling traceability of Operational Activity in environment.  Prerequisites:  An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account with full access to CloudFormation, EC2, VPC, IAM.  Costs NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\n  function prevStep(){ window.open(\"\", \"_self\") } function nextStep(){ window.open(\".\\/1_deploy_the_lab_network_infrastructure\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool  Start Lab  Steps:  Deploy Lab Network Infrastructure   Configure ECS Respository and Deploy The Application Stack   Simulate Application Issue   Build \u0026amp; Execute Investigative Playbook   Buiild \u0026amp; Execute Remediation Playbook   Visibility on Operational activities.   Teardown   "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_1_aws_account_setup/","title":"Level 100: AWS Account Setup: Lab Guide","tags":[],"description":"","content":" Your browser doesn't support video, or if you're on GitHub head to https://wellarchitectedlabs.com to watch the video.  Last Updated October 2020\nAuthors  Nathan Besh, Cost Lead Well-Architected  Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com\nIntroduction This hands-on lab will guide you through the steps to configure your accounts and get them prepared for Cost Optimization work. It will create and setup an initial account structure, enable access to billing information and create a cost optimization team. This will ensure that you can complete the Well-Architected Cost workshops, and enable you to optimize your workloads inline with the Well-Architected Framework.\nGoals  Implement an account structure Configure billing services Enable detailed cost and usage information Create a cost optimization team  Prerequisites  Multiple AWS accounts already created (at least three)  Permissions required  Root user and administrator access to the management and member accounts  Costs  https://aws.amazon.com/aws-cost-management/pricing/ Variable costs will be incurred Cost Explorer: $0.01 per 1,000 usage records S3: Storage of CUR file, refer to S3 pricing https://aws.amazon.com/s3/pricing/ Estimated costs should be \u0026lt;$5 a month for small accounts  Time to complete  The lab should take approximately 30 minutes to complete  Steps:  Configure IAM access   Create an account structure   Configure Cost and Usage reports   Enable Single Sign On (SSO)   Configure account settings   Setup Amazon QuickSight   Enable AWS Cost Explorer   Enable AWS-Generated Cost Allocation Tags   Tear down    function prevStep(){ window.open(\"\", \"_self\") } function nextStep(){ window.open(\".\\/1_iam_access\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool  Start Lab  "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_monitoring_with_cloudwatch_dashboards/","title":"Level 100: Monitoring with CloudWatch Dashboards","tags":[],"description":"How to configure an Amazon CloudWatch Dashboard to get aggregated views of the health and performance of all AWS resources","content":"Authors  Eric Pullen, Performance Efficiency Lead Well-Architected  Introduction This hands-on lab will guide you through configuring an Amazon CloudWatch Dashboard to get aggregated views of the health and performance of all AWS resources. This enables you to quickly get started with monitoring, explore account and resource-based view of metrics and alarms, and easily drill-down to understand the root cause of performance issues. You can find more best practices by reading the Performance Efficiency Pillar of the AWS Well-Architected Framework . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nGoals  Monitor resources to ensure they are performing as expected  Prerequisites  An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .  Costs  https://aws.amazon.com/cloudwatch/pricing/  You can create 3 dashboards for up to 50 metrics per month on the free tier Outside of the free tier, it is $3.00 per dashboard per month   This lab creates one dashboard, so the maximum cost would be $3.00 per month if you have already consumed the free tier.  NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\n  function prevStep(){ window.open(\"\", \"_self\") } function nextStep(){ window.open(\".\\/1_intro\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool  Start Lab  Steps:  View Amazon CloudWatch Automatic Dashboards   Teardown   "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/100_labs/100_walkthrough_of_the_well-architected_tool/","title":"Level 100: Walkthrough of the Well-Architected Tool","tags":[],"description":"","content":"Authors  Rodney Lester, Principal Solutions Architect, Well-Architected, AWS  Introduction The purpose of this lab is to walk you through the features of the AWS Well-Architected Tool. You will create a workload, review the Reliability Pillar questions, save the workload, take a milestone, and examine and download the Well-Architected Review report.\nThe knowledge you acquire will help you build Well-Architected workloads in alignment with the AWS Well-Architected Framework Goals:  Learn where resources about the questions and best practices are located. Learn how to use milestones to track your progress again high and medium risks over time. Learn how to generate a report or view the results of the review in the Well-Architected Tool.  Prerequisites:  An AWS Account that you are able to use for testing, that is not used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to use Well-Architected Tool (WellArchitectedConsoleFullAccess managed policy).  Costs:  There are no costs for this lab AWS Pricing   Time to complete  The lab should take approximately 30 minutes to complete  Steps:  Navigating to the console   Creating a workload   Performing a review   Saving a milestone   Viewing and downloading the report   Tear down this lab    function prevStep(){ window.open(\"\", \"_self\") } function nextStep(){ window.open(\".\\/1_nav_console\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool  Start Lab  "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_bidirectional_replication_for_s3/","title":"Level 200: Implementing Bi-Directional Cross-Region Replication (CRR) for Amazon Simple Storage Service (Amazon S3)","tags":["data_backup"],"description":"Improve reliability using automatic asynchronous backup of encrypted data in Amazon S3","content":"Author  Seth Eliot, Principal Reliability Solutions Architect, AWS Well-Architected  Introduction This hands-on lab will guide you through the steps to improve reliability of your service and its data using automatic asynchronous backup of encrypted data you store in Amazon S3. Your Amazon S3 data will be securely backed up to a different AWS region.\nThe skills you learn will help you build resilient workloads in alignment with the AWS Well-Architected Framework Goals By the end of this lab, you will be able to:\n Perform data backup automatically for objects in Amazon S3 buckets Secure and encrypt backups of objects in Amazon S3 Automate disaster recovery (DR) of your objects in Amazon S3 Query CloudTrail logs to improve your understanding of how cross-region replication works for Amazon S3  Prerequisites If you are running this at an AWS sponsored workshop then you may be provided with an AWS Account to use, in which case the following pre-requisites will be satisfied by the provided AWS account. If you are running this using your own AWS Account, then please note the following prerequisites:\n An AWS Account that you are able to use for testing. This account MUST NOT be used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to create IAM Polices and Roles, create S3 buckets and bucket policies, get and put objects into S3 buckets, and create and read CloudTrail trails and CloudWatch Log Groups.  NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\n  function prevStep(){ window.open(\"\", \"_self\") } function nextStep(){ window.open(\".\\/1_deploy_infra\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool  Start Lab  Steps:  Deploy the Infrastructure   Configure bi-directional cross-region replication (CRR) for S3 buckets   Test bi-directional cross-region replication (CRR)   Tear down this lab   References \u0026amp; useful resources   Costs NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\n  In this lab you will do three S3 PUTs, store under 1MB of data in two S3 buckets, and transfer under 1MB of data from one region to another. The total of these operations will not exceed $0.01. Under normal conditions there should be no accrued charges for this lab. Please follow the directions for Tear Down to remove all deployed resources when you are done with this lab.  "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_automated_cur_query_and_email_delivery/","title":"Level 300: Automated Athena CUR Query and E-mail Delivery","tags":[],"description":"","content":"Authors  Na Zhang, Sr. Technical Account Manager, AWS  Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com\nIntroduction This hands-on lab will guide you through deploying an automatic CUR query \u0026amp; E-mail delivery solution using Athena, Lambda, SES and CloudWatch. The Lambda function is triggered by a CloudWatch event, it then runs saved queries in Athena against your CUR file. The queries are grouped into a single report file (xlsx format), and sends report via SES. This solution provides automated reporting to your organization, to both consumers of cloud and financial teams.\nGoals  Provide automated financial reports across your organization  Prerequisites  CUR is enabled and delivered into S3, with Athena integration. Recommend to complete 200_4_Cost_and_Usage_Analysis  If your account is in the SES sandbox(default), verify your email addresses in SES to assure you can send or receive emails via verified mail addresses: https://docs.aws.amazon.com/ses/latest/DeveloperGuide/verify-email-addresses.html  Permissions required  Create IAM policies and roles Write and read to/from S3 Buckets Create and modify Lambda functions Create, save and execute Athena queries Verify e-mail address, send mail in SES  Costs  Variable, dependent on the amount of data scanned and report frequency Approximately \u0026lt;$5 a month for small to medium accounts  Time to complete  The lab should take approximately 15 minutes to complete  Steps:  Overview architecture   Create S3 Bucket   Create an IAM policy and role for Lambda function   Configure parameters of function code and upload code to S3   Create a Lambda function   Customize query strings and create scheduled CloudWatch event   Teardown    function prevStep(){ window.open(\"\", \"_self\") } function nextStep(){ window.open(\".\\/1_architecture\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool  Start Lab  "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_autonomous_patching_with_ec2_image_builder_and_systems_manager/","title":"Level 300: Autonomous Patching With EC2 Image Builder And Systems Manager","tags":[],"description":"","content":"Authors  Tim Robinson, Well-Architected Geo Solutions Architect. Stephen Salim, Well-Architected Geo Solutions Architect.  Introduction Patching is a vital component to any security strategy which ensures that your compute environments are operating with the latest code revisions available. This in turn means that you are running with the latest security updates for the system, which reduces the potential attack surface of your workload.\nThe majority of compliance frameworks require evidence of patching strategy or some sort. This means that patching needs to be performed on a regular basis. Depending on the criticality of the workload, the operational overhead will need to be managed in a way that poses minimal impact to the workload\u0026rsquo;s availability.\nEnsuring that you have an automated patching solution, will contribute to building a good security posture, while at the same time reducing the operational overhead, together with allowing traceability that can potentially be useful for future compliance audits.\nThere are multiple different approaches available to automate operating system patching using a combination of AWS services.\nOne approach is to utilize a blue/green deployment methodology to build an entirely new Amazon Machine Image (AMI) that contains the latest operating system patch, which can be deployed into the application cluster. This lab will walk you through this approach, utilizing a combination of the following services and features:\n EC2 Image Builder to automate creation of the AMI  Systems Manager Automated Document to orchestrate the execution. CloudFormation with AutoScalingReplacingUpdate update policy, to gracefully deploy the newly created AMI into the workload with minimal interruption to the application availability.  We will deploy section 1 and 2 of the lab with CloudFormation templates to get your environment built as efficiently as possible. This will allow the base infrastructure and application deployment to be completed quickly so you can focus on the main lab objectives which are covered in sections 3 and 4. In these sections, we will give you the choice of either using additional pre-built templates or manual steps to complete the EC2 Image Builder and Systems Manager Document configuration.\nThe skills you learn from this lab will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nNote: For simplicity, we have used Sydney \u0026lsquo;ap-southeast-2\u0026rsquo; as the default region for this lab. Please ensure all lab interaction is completed from this region.\n Goals  EC2 Image Builder configuration experience. Systems Manager Automated Document experience.  Prerequisites  An AWS account that you are able to use for testing, that is not used for production or other purposes.  NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\n Steps:  Deploy The Lab Base Infrastructure   Deploy The Application Infrastructure   Deploy The AMI Builder Pipeline   Deploy The Build Automation With SSM   Teardown   "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_iam_permission_boundaries_delegating_role_creation/","title":"Level 300: IAM Permission Boundaries Delegating Role Creation","tags":[],"description":"","content":"Authors  Ben Potter, Security Lead, Well-Architected  Introduction This hands-on lab will guide you through the steps to configure an example AWS Identity and Access Management (IAM) permission boundary. AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature in which you use a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by the policy. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as a developer, the developer can then use this role to create additional user roles that are restricted to specific services and regions. This allows you to delegate access to create IAM roles and policies, without them exceeding the permissions in the permission boundary. We will also use a naming standard with a prefix, making it easier to control and organize policies and roles that your developers create.\nThe skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nGoals  IAM permission boundaries IAM policy conditions  Prerequisites  An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user with MFA enabled that can assume roles in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .  Steps:  Create IAM policies   Create and Test Developer Role   Create and Test User Role   Knowledge Check   Tear down   "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_health_checks_and_dependencies/","title":"Level 300: Implementing Health Checks and Managing Dependencies to improve Reliability","tags":["mitigate_failure"],"description":"Improve reliability of a service by decoupling service dependencies, using health checks, and demonstrating when to use fail-open and fail-closed behaviors","content":"Author  Seth Eliot, Principal Reliability Solutions Architect, AWS Well-Architected  Introduction This hands-on lab will guide you through the steps to improve reliability of a service by decoupling service dependencies, using health checks, and demonstrating when to use fail-open and fail-closed behaviors.\nThe skills you learn will help you build resilient workloads in alignment with the AWS Well-Architected Framework Amazon Builders' Library This lab additionally illustrates best practices as described in the Amazon Builders' Library article: Implementing health checks Goals After you have completed this lab, you will be able to:\n Implement graceful degradation to transform applicable hard dependencies into soft dependencies Monitor all layers of the workload to detect failures Route traffic only to healthy application instances Configure fail-open and fail-closed behaviors as appropriate in response to detected faults Use AWS services to reduce mean time to recovery (MTTR)  Prerequisites If you are running this at an AWS sponsored workshop then you may be provided with an AWS Account to use, in which case the following pre-requisites will be satisfied by the provided AWS account. If you are running this using your own AWS Account, then please note the following prerequisites:\n An AWS Account that you are able to use for testing. This account MUST NOT be used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to create Amazon Virtual Private Cloud(s) (VPCs), including subnets and route tables, Security Groups, Internet Gateways, NAT Gateways, Elastic IP Addresses, IAM Roles, instance profiles, AWS Auto Scaling launch configurations, Application Load Balancers, Auto Scaling Groups, DynamoDB tables, SSM Parameters, and EC2 instances.   function prevStep(){ window.open(\"\", \"_self\") } function nextStep(){ window.open(\".\\/1_deploy_app\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool  Start Lab  Steps:  Deploy the Infrastructure and Application   Handle failure of service dependencies   Implement deep health checks   Fail open when appropriate   Tear down this lab   Costs NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\n  This lab will cost approximately $5.50 per day when deployed The majority of this cost is the charge for NatGateway-Hours Please follow the directions for Tear Down to avoid unwanted costs after you have concluded this lab  "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_100_loft_introduction_to_security/","title":"Quest: Loft - Introduction to Security","tags":[],"description":"Introduction to AWS security basics, used as the workshop in AWS loft events.","content":"About this Guide This quest is the guide for an AWS Loft Well-Architected Security introduction workshop. You can check your local loft schedule for upcoming Well-Architected events, or you can also run it on your own! The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nPrerequisites  An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .  Steps:  New AWS Account Setup and Securing Root User   Basic Identity and Access Management User, Group, Role   CloudFront with WAF Protection   Automated Deployment of Detective Controls   "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/100_labs/","title":"100 Labs","tags":[],"description":"","content":"List of labs available  100 - Inventory and Patch Management   100 - Dependency Monitoring   "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/","title":"100 Labs","tags":[],"description":"","content":"AWS Well-Architected Performance Efficiency Labs 100 Level Labs:  Level 100: Monitoring with CloudWatch Dashboards  How to configure an Amazon CloudWatch Dashboard to get aggregated views of the health and performance of all AWS resources\n Level 100: Calculating differences in clock source  How various linux clock sources can affect the performance of your application on EC2\n Level 100: Monitoring Windows EC2 instance with CloudWatch Dashboards  How to configure an Amazon CloudWatch Dashboard to get aggregated views of the health and performance of a Windows EC2 instance.\n Level 100: Monitoring an Amazon Linux EC2 instance with CloudWatch Dashboards  How to configure an Amazon CloudWatch Dashboard to get aggregated views of the health and performance of a Amazon Linux EC2 instance.\n "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/100_labs/","title":"100 Labs","tags":[],"description":"","content":"List of labs available  Level 100: Walkthrough of the Well-Architected Tool   Contributing to Well-Architected Labs   "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/200_labs/","title":"200 Labs","tags":[],"description":"","content":"List of labs available  200 - Automating your operations with Playbooks and Runbooks   "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/","title":"200 Labs","tags":[],"description":"","content":"List of labs available  Level 200: Implementing Bi-Directional Cross-Region Replication (CRR) for Amazon Simple Storage Service (Amazon S3)  Improve reliability using automatic asynchronous backup of encrypted data in Amazon S3\n Level 200: Deploy and Update CloudFormation  Improve reliability of a service by using automation to make changes in your cloud infrastructure\n Level 200: Testing Backup and Restore of Data  Create a strategy to backup data sources periodically using AWS Backup, and automate the testing of the restore process\n Level 200: Testing for Resiliency of EC2 instances  Use code to inject faults simulating EC2 failures. These are used as part of Chaos Engineering to test workload resiliency\n "},{"uri":"https://wellarchitectedlabs.com/","title":"AWS Well-Architected Labs","tags":[],"description":"","content":"Introduction The Well-Architected framework has been developed to help cloud architects build the most secure, high-performing, resilient, and efficient infrastructure possible for their applications. This framework provides a consistent approach for customers and partners to evaluate architectures, and provides guidance to help implement designs that will scale with your application needs over time.\nThis repository contains documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices. The labs are categorized into levels, where 100 is introductory, 200/300 is intermediate and 400 is advanced.\nPrerequisites: An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\nLabs: The labs are structured around the five pillars of the Well-Architected Framework :  Operational Excellence   Security   Reliability   Performance Efficiency   Cost Optimization   Well-Architected Tool   Contributing Click here for the process to perform a pull request to contribute to the labs "},{"uri":"https://wellarchitectedlabs.com/cost/fundamentals/","title":"Fundamentals","tags":[],"description":"","content":"About cost optimization fundamentals The first step in your Cost Optimization journey is to setup your account correctly, and get to know the tools and data available for Cost Optimization. These are a collection of labs that are accessible to anyone that will be working with the cloud, including non-technical roles.\n Step 1 - Account Setup Administrative privlieges required This first step will set your organization up correctly for Cost Optimization. It will build an account structure, configure the required data sources and settings, and create a Cost Optimization team with the required permissions to perform Cost Optimization activities.\n           This lab requires root access. It is for inital setup so completed only once. If you have a very large or diverse organization with multilpe management/Payer accounts, you should complete it for each management account or cost optimization team that you have.     Step 2 - Cost and Usage Governance - Notifications Configuring notifications allows you to receive an email when usage or cost is above a defined amount.\n           100 Level Lab: This lab will show you how to implement AWS Budgets to provide notifications on usage and spend.     Step 3 - Pricing Models - Savings Plans and Reserved Instances By using the right pricing model for your workload resources, you pay the lowest price for that resource.\n           100 Level Lab: This lab will introduce you to working with Savings Plans (SP\u0026rsquo;s), utilizing AWS Cost Explorer to make low risk, high return SP purchases for your business.     200 Level Lab: This lab will introduce you to working with Reserved Instances (RI\u0026rsquo;s), utilizing AWS Cost Explorer to make low risk, high return RI purchases for your business.     Step 4 - Monitor Usage and Cost - Analysis Cost and Usage Analysis will enable you to understand how you consumed the cloud, and what your costs are for that consumption.\n           100 Level Lab: This lab introduces you to the billing console, allowing you to view your current and past bills, and also inspect your usage across services and accounts.     Step 5 - Monitor Usage and Cost - Visualization Visualizing cost and usage highlights trends and allows you to gain further insights.\n           100 Level Lab: This lab will introduce AWS Cost Explorer, and demonstrate how to use its features to provide insights.     Step 6 - Govern Usage and Cost - Controls Implementing usage controls will ensure excess usage and accompanying costs does not occur.\n           200 Level Lab: This lab will extend the permissions of the Cost Optimization team, then utilize Identity and Access Management (IAM) policies to control and restrict usage.     Step 7 - Monitor Usage and Cost - Advanced Analysis Advanced analysis using your Cost and Usage Report (CUR) will allow you to answer the most challenging questions on your usage and cost. It is the most detailed source of information on your cost and usage available.\n           200 Level Lab: This lab will utilize Amazon Athena to provide an interface to query the CUR, provide you the most common customer queries, and help you to build your own queries.     Step 8 - Monitor Usage and Cost - Advanced Visualization Utilizing the CUR data source in the previous step, you can provide more detailed and custom visualizations and dashboards.\n           200 Level Lab: This Lab extends the previous step, utilizing Amazon Quicksight to visualize the CUR data source.    "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_goals_and_targets/1_cloud_financial_management/","title":"Cloud Financial Management","tags":[],"description":"","content":"Practice Cloud Financial Management Cost-Aware Processes CFM Reporting  Goal: Regular reporting on CFM implemented across the business Target: Within 6 months all DevOps teams must report bi-weekly on CFM activities Best Practice: Cost-Aware Processes  Measures: Number of reports delivered, number of teams delivering Good/Bad: Good Why? When does it work well or not?: Actively drives CFM throughout the organization, ensures a thorough \u0026amp; structured approach to CFM Contact/Contributor: natbesh@amazon.com  CFM included in business processes  Goal: CFM included in all relevant business processes Target: Change control process must include CFM, the impact to workload efficiency, effort, and risk, must be evaluated and documented on all changes to any production workload Best Practice: Cost-Aware Processes  Measures: % of changes with complete/full CFM evaluation (exceptions are NOT included) Good/Bad: Good Why? When does it work well or not?: Actively drives CFM throughout the organization, minimizes fritciont as changes that impact cost/efficiency are known and communicated to all parts of the business Contact/Contributor: natbesh@amazon.com  Include CFM into internal training programs  Goal: Improve CFM capability company wide Target: CFM training to be incorporated into internal training courses for all roles. Best Practice: Cost-Aware Processes  Measures: % of courses with CFM included Good/Bad: Good Why? When does it work well or not?: Ensures employees have the capability to perform CFM in their jobs/roles Contact/Contributor: natbesh@amazon.com  New hire training programs  Goal: Improve practical CFM capability company wide Target: All new hires to complete the Well-Architected labs fundamentals within their first 3 months Best Practice: Cost-Aware Processes  Measures: % of new hires completed the series Good/Bad: Good Why? When does it work well or not?: Ensures employees have the hands on capability to perform CFM in their jobs/roles Contact/Contributor: natbesh@amazon.com  Update CFM guidance  Goal: Ensure you keep up to date with best practices Target: Review Well-Architected Cost Optimization Whitepaper yearly, and update all training \u0026amp; process inline with any changes Best Practice: Cost-Aware Processes  Measures: % of processes updated Good/Bad: Good Why? When does it work well or not?: Ensures new best practices are implemented, and you\u0026rsquo;re at the forefront of CFM Contact/Contributor: natbesh@amazon.com   Cost-Aware Culture Perform CFM, accountability  Goal: Ensure CFM is a part of everyones job/role Target: All job descriptions must include CFM within 6 months, applies to all builder/devops/technical/finance \u0026amp; relevant management roles Best Practice: Cost Aware Culture  Measures: Number of job descriptions updated, % of relevant descriptions with CFM Good/Bad: Good Why? When does it work well or not?: Actively drives CFM throughout the organization by driving accountability and ensuring all employees are aware CFM is part of their job/role. Ensures accountability can be enforced, as not performing Cost Opt is not performing their job \u0026amp; standard processes can be used as corrective action Contact/Contributor: natbesh@amazon.com  Hire CFM Capabilities  Goal: Include CFM as a core capability when hiring Target: All job postings to include CFM within 6 months, applies to all builder/devops/technical/finance \u0026amp; relevant management roles Best Practice: Cost Aware Culture  Measures: Number of job descriptions updated Good/Bad: Good Why? When does it work well or not?: Actively drives CFM throughout the organization by hiring CFM capability, also informs the industry \u0026amp; potiential hires that CFM matters to your organization Contact/Contributor: natbesh@amazon.com    Goal: Include CFM as a core capability when hiring Target: All job postings to include CFM within 6 months, applies to all builder/devops/technical/finance \u0026amp; relevant management roles Best Practice: Cost Aware Culture  Measures: Number of job descriptions updated Good/Bad: Good Why? When does it work well or not?: Actively drives CFM throughout the organization by hiring CFM capability, also informs the industry \u0026amp; potiential hires that CFM matters to your organization Contact/Contributor: natbesh@amazon.com    Goal: Ensure CFM capability when hiring Target: As part of the hiring process, all relevant roles (tech, finance \u0026amp; some management) must obtain a pass mark on a CFM test during the phone screen Best Practice: Cost Aware Culture  Measures: Number of tests performed, number of candidates passed Good/Bad: Good Why? When does it work well or not?: Ensures that CFM capability is established across your relevant teams, also informs the industry \u0026amp; potiential hires that CFM matters to your organization Contact/Contributor: natbesh@amazon.com  Certification  Goal: Ensure CFM capability is retained in the organization Target: Maintain a level of 10 certified employees within the organization Best Practice: Cost-Aware Processes  Measures: number of certified employees, number of teams that have certified employees Good/Bad: Good Why? When does it work well or not?: Ensures a verified level of capability throughout the organization Contact/Contributor: natbesh@amazon.com   function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/2_govern_usage\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cost_journey/2_create_journey/","title":"Create Journey","tags":[],"description":"","content":"Create the Journey We will now run the Lambda function and create the Well-Architected Cost Journeys. We will manually run the function\n  Go to the Lambda service page, select the Cost_W-A_Journey function\n  Scroll down to the Code source, click Deploy, click Test\n  Set the event name to CreateJourney, click Create\n  Click Test,   You will see a null response and there should be some log messages in the lambda console   Go to the s3 console and select the bucket you configured, you will see a W-A Workload Journeys.html file which contains the index of all the workload journeys. The WorkloadReports/ folder contains each workload journey.   Open up the files in a web browser and view the image:   You have successfully created your Well-Architected Cost Journeys. You can configure the S3 bucket to serve web pages to easily distribute the journeys across your organization.\n  function prevStep(){ window.open(\"..\\/1_pricing_sources\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/3_tear_down\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_testing_backup_and_restore_of_data/2_configure_backup_plan/","title":"Create Backup Plan","tags":[],"description":"","content":"A well thought out backup strategy is key to an organization\u0026rsquo;s success and is determined by a variety of factors. The biggest factors influencing a backup strategy is the Recovery Time Objective (RTO) and Recovery Point Objective (RPO) set for the workload. RTO and RPO are determined based on the criticality of the workload to the business, the SLAs that have been agreed upon, and the cost associated with achieving the RTO and RPO. RTO and RPO should be specific to each workload and not set for the entire organization/infrastructure.\nIn this lab, you will create a backup strategy by leveraging AWS Backup, a fully managed backup service that can automatically backup data from various data sources such as EC2 Instances, EBS Volumes, RDS Databases, and more. You can view a complete list of supported services here .\n  Sign in to the AWS Backup console - https://us-east-1.console.aws.amazon.com/backup/home?region=us-east-1#backupplan.\n  Choose CREATE BACKUP PLAN.\n  Select the option to BUILD A NEW PLAN.\n  Specify a Backup plan name such as BACKUP-LAB.\n  Under BACKUP RULE CONFIGURATION, enter a RULE NAME such as BACKUP-LAB-RULE.\n  To set a SCHEDULE for the backup, you can specify the FREQUENCY at which backups are taken. You can enter frequency as every 12 hours, Daily, Weekly, or Monthly. Alternatively, you can specify a custom CRON EXPRESSION for your backup frequency. For this exercise, select the FREQUENCY as DAILY.\n  Once frequency has been established, you will need to specify a backup window - a period of time during which data is being backed up from your data sources. Keep in mind that creating backups could cause you data sources to be temporarily unavailable depending on the underlying configuration of those resources. It is best to create backups during scheduled downtimes/maintenance windows when user impact is minimal. For this exercise, select Use backup window defaults - recommended. The default backup window is set to start at 5 AM UTC time and last 8 hours. If you need to schedule backups at a different time, you can customize the backup window.\n  You can set lifecycle policies for your backups to transition them to cold storage or to expire them after a period of time to reduce costs and operational overhead. This is currently supported for backups of EFS only. For this exercise, set the values for Transition to cold storage and Expire both to NEVER.\n  Under BACKUP VAULT, click on CREATE NEW BACKUP VAULT. It is recommended to use different Backup Vaults for different workloads.\n  On the pop-up screen, specify a BACKUP VAULT NAME such as BACKUP-LAB-VAULT.\n  You can choose to encrypt your backups for additional security by specifying a KMS key. You can choose the default key created and managed by AWS Backup or specify your own custom key. For this exercise, select the default key (default) aws/backup.\n  Click CREATE BACKUP VAULT.\n  Additionally, you can choose to have your backups automatically copied to a different AWS Region.\n  You can add tags to your recovery points to help identify them.\n  Click CREATE PLAN.\n  Once the backup plan and the backup rule has been created, you can specify resources to back up. You can select individual resources to be backed up, or specify a tag (key-value) associated with the resource. AWS Backup will execute backup jobs on all resources that match the tags specified.\n  Click on BACKUP PLANS from the menu on the left side of the screen.\n  Select the backup plan BACKUP-LAB that you just created.\n  Scroll down to the section titled RESOURCE ASSIGNMENTS and click on ASSIGN RESOURCES.\n  Specify a RESOURCE ASSIGNMENT NAME such as BACKUP-RESOURCES to help identify the resources that are being backed up.\n  Leave the DEFAULT ROLE selected for IAM ROLE. If a role does not already exist, the AWS Backup service will create one with the necessary permissions.\n  Under ASSIGN RESOURCES, you can specify resources to be backed up individually by specifying the RESOURCE TYPE and RESOURCE ID, or select TAGS and enter the TAG KEY and the TAG VALUE. For this lab, select TAGS as the value for ASSIGN BY, and enter workload as the KEY and myapp as the VALUE. This tag and value was created by the CloudFormation stack. Remember that tags are case sensitive and ensure that the values you enter are all in lower case.\n  Click on ASSIGN RESOURCES.\n  You have successfully created a backup plan for your data sources, and all supported resources with the tags workload=myapp will be backed up automatically, at the frequency specified. In case of a disaster, these backups can be used to recover data to ensure business continuity. Since the entire process is automated, it will save considerable operational overhead for your Operations teams.\n function prevStep(){ window.open(\"..\\/1_prerequisite\", \"_self\") } function nextStep(){ window.open(\"..\\/3_enable_notifications\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_pricing_model_analysis/2_usage_source/","title":"Create the Usage Data Source","tags":[],"description":"","content":"We will combine the pricing information with our Cost and Usage Report (CUR). This will give us a usage data source which contains a summary of your usage at an hourly level, with multiple pricing dimensions.\n  Go to the Athena service page:   Run the following query to create a single pricing data source, combining the OD and SP pricing:\n CREATE VIEW pricing.pricing AS SELECT sp.col18 AS Region, sp.col9 AS OS, REPLACE(od.col19, '\u0026quot;') AS InstanceType, REPLACE(od.col36, '\u0026quot;') AS Tenancy, REPLACE(od.col9, '\u0026quot;') AS ODRate, sp.col4 AS SPRate FROM pricing.sp_pricedata sp JOIN pricing.od_pricedata od ON ((sp.col8 = REPLACE(od.col47, '\u0026quot;')) AND (sp.col9 = REPLACE(od.col48, '\u0026quot;'))) WHERE od.col9 IS NOT NULL AND sp.col18 NOT LIKE 'Any' AND sp.col10 LIKE 'No Upfront' AND sp.col11 like '1'    Next we\u0026rsquo;ll join the CUR file with that pricing source as a view. Edit the following query, replace costmaster.costmasterfile with your existing database name and tablename of your CUR, then run the rollowing query:\n CREATE VIEW costmaster.SP_Usage AS SELECT costmaster.line_item_usage_account_id, costmaster.line_item_usage_start_date, to_unixtime(costmaster.line_item_usage_start_date) AS EpochTime, costmaster.product_instance_type, costmaster.product_location, costmaster.product_operating_system, costmaster.product_tenancy, SUM(costmaster.line_item_unblended_cost) AS ODPrice, SUM(costmaster.line_item_unblended_cost*(cast(pr.SPRate AS double)/cast(pr.ODRate AS double))) SPPrice, abs(SUM(cast(pr.SPRate AS double)) - SUM (cast(pr.ODRate AS double))) / SUM(cast(pr.ODRate AS double))*100 AS DiscountRate, SUM(costmaster.line_item_usage_amount) AS InstanceCount FROM costmaster.costmasterfile costmaster JOIN pricing.pricing pr ON (costmaster.product_location = pr.Region) AND (costmaster.line_item_operation = pr.OS) AND (costmaster.product_instance_type = pr.InstanceType) AND (costmaster.product_tenancy = pr.Tenancy) WHERE costmaster.line_item_product_code LIKE '%EC2%' AND costmaster.product_instance_type NOT LIKE '' AND costmaster.product_operating_system NOT LIKE 'NA' AND costmaster.line_item_unblended_cost \u0026gt; 0 AND costmaster.line_item_line_item_type like 'Usage' GROUP BY costmaster.line_item_usage_account_id, costmaster.line_item_usage_start_date, costmaster.product_instance_type, costmaster.product_location, costmaster.product_operating_system, costmaster.product_tenancy ORDER BY costmaster.line_item_usage_start_date ASC, DiscountRate DESC    The code above will capture ONLY on-demand usage, as defined by costmaster.line_item_line_item_type like \u0026lsquo;Usage\u0026rsquo;. You can remove this to include Savings Plan usage, to see total commitment you should have, instead of additional commitment required.\n  Verify the data source is setup by editing the following query, replace costmaster. with the name of the database and run the following query:\n SELECT * FROM costmaster.sp_usage limit 10;    You now have your usage data source setup with your pricing dimensions. You can modify the queries above to add or remove any columns you want in the view, which can later be used to visualize the data, for example tags.\n  function prevStep(){ window.open(\"..\\/1_pricing_sources\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/3_quicksight_setup\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_using_awscli_to_manage_wa_reviews/2_create_workload/","title":"Create a Well-Architected Workload","tags":[],"description":"","content":"Overview Well-Architected Reviews are conducted per workload. A workload identifies a set of components that deliver business value. The workload is usually the level of detail that business and technology leaders communicate about. Workloads vary in levels of architectural complexity, from static websites to architectures with multiple data stores and many components.\nCreating a workload  We will start with creating a Well-Architected workload to use throughout this lab. Using the create-workload API , you can create a new Well-Architected workload: aws wellarchitected create-workload --workload-name \u0026quot;WA Lab Test Workload\u0026quot; --description \u0026quot;Test Workload for WA Lab\u0026quot; --review-owner \u0026quot;John Smith\u0026quot; --environment \u0026quot;PRODUCTION\u0026quot; --aws-regions \u0026quot;us-east-1\u0026quot; --lenses \u0026quot;wellarchitected\u0026quot; \u0026quot;serverless\u0026quot;  The following are the required parameters for the command:  workload-name - This is a uniquie identifier for the workload. Must be between 3 and 100 characters. description - A brief description of the workload to document its scope and intended purpose. Must be between 3 and 250 characters. review-owner - The name, email address, or identifier for the primary individual or group that owns the review process. Must be between 3 and 255 characters. environment - The environment in which your workload runs. This must either be PRODUCTION or PREPRODUCTION aws-regions - The aws-regions in which your workload runs (us-east-1, etc). lenses - The list of lenses associated with the workload. All workloads must include the \u0026ldquo;wellarchitected\u0026rdquo; lens as a base, but can include additional lenses. For this lab, we are also including the serverless lens.  Using the list-lenses API you can get a list of lenses: aws wellarchitected list-lenses       Once the command is run, you should get a response that contains the workload json structure. This will include the following items:  WorkloadId - The ID assigned to the workload. This ID is unique within an AWS Region. WorkloadArn - The ARN for the workload.    Finding your WorkloadId  Assume you created a new workload, but you did not write down the WorkloadId to use in subsequent API calls. Using the list-workloads API , you can find the WorkloadId by using a search for the workload name prefix: aws wellarchitected list-workloads --workload-name-prefix \u0026quot;WA Lab\u0026quot; You should get back a response that includes the WorkloadId along with other information about the workload that starts with \u0026ldquo;WA Lab\u0026rdquo;  If you want to only return the WorkloadId, you can use the AWS CLI query parameter to query for the value: aws wellarchitected list-workloads --workload-name-prefix \u0026quot;WA Lab\u0026quot; --query 'WorkloadSummaries[].WorkloadId' --output text   Using WorkloadId to remove and add lenses  In the first step, we added the serverless lens to our new workload. Next, we will remove and then re-add this lens to the workload. Make sure you have the WorkloadId from the previous step and replace WorkloadId with it Using the get-workload API , lets check which lenses are associated with our workload. aws wellarchitected get-workload --workload-id \u0026quot;\u0026lt;WorkloadId\u0026gt;\u0026quot; --query 'Workload.Lenses[]'  You should see serverless listed as a lens. Using the disassociate-lenses API we will remove the serverless lens. aws wellarchitected disassociate-lenses --workload-id \u0026quot;\u0026lt;WorkloadId\u0026gt;\u0026quot; --lens-aliases \u0026quot;serverless\u0026quot; When you use disassociate-lenses, it will be destructive and irreversible to any questions you have answered if you have not saved a milestone. Saving a milestone is recommended before you use the disassociate-lenses API call.\n  You will not get a response to this command, but using the get-workload API you can verify that the lens was removed. aws wellarchitected get-workload --workload-id \u0026quot;\u0026lt;WorkloadId\u0026gt;\u0026quot; You should see a response such as this, showing that you no longer have serverless listed.  Using the associate-lenses API we can add the serverless lens back into the workload. aws wellarchitected associate-lenses --workload-id \u0026quot;\u0026lt;WorkloadId\u0026gt;\u0026quot; --lens-aliases \u0026quot;serverless\u0026quot; Again, you will not see a response to this command, but we can verify that it was added by doing another get-workload aws wellarchitected get-workload --workload-id \u0026quot;\u0026lt;WorkloadId\u0026gt;\u0026quot;    function prevStep(){ window.open(\"..\\/1_configure_env\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/3_perform_review\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/200_labs/","title":"200 Labs","tags":[],"description":"","content":"List of labs available  Level 200: Using AWSCLI to Manage WA Reviews   "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_fault_isolation_with_shuffle_sharding/2_impact_of_failures/","title":"Scope of Impact of failures","tags":[],"description":"","content":"Break the application In the scenario used in the lab, the application has a known issue which is triggered by passing a \u0026ldquo;bad\u0026rdquo; query string. If such a request is received, the EC2 instance that handles the request will become unresponsive and the application will crash on the instance. The \u0026ldquo;bad\u0026rdquo; query string that triggers this is bug with a value of true. The development team is aware of this bug and are working on a fix, however, the issue exists today and customers might accidentally or intentionally trigger it. This is referred to as a \u0026ldquo;poison pill\u0026rdquo;, a bug or issue which when introduced into a system could compromise the functionality of the system.\n  Imagine a situation where a customer accidentally triggers the bug in the application that causes it to shutdown on the instance where the request was received. This can be done by including the query-string bug=true. The modified URL should look like this - http://shuffle-alb-1p2xbmzo541rr-1602891463.us-east-1.elb.amazonaws.com/?name=Alpha\u0026amp;bug=true\n  You should see an Internal Server Error response on the browser indicating that the application has stopped working as expected on the instance that processed this request\n  At this point, there are 3 healthy instances still available so other customers are not impacted. You can verify this by opening another browser tab and specifying the URL with a different customer name and without the bug query string as shown below. Try with at least two other customers, you may try them all if you want to, but it is not necessary.\n http://shuffle-alb-1p2xbmzo541rr-1602891463.us-east-1.elb.amazonaws.com/?name=Bravo http://shuffle-alb-1p2xbmzo541rr-1602891463.us-east-1.elb.amazonaws.com/?name=Charlie http://shuffle-alb-1p2xbmzo541rr-1602891463.us-east-1.elb.amazonaws.com/?name=Delta http://shuffle-alb-1p2xbmzo541rr-1602891463.us-east-1.elb.amazonaws.com/?name=Echo http://shuffle-alb-1p2xbmzo541rr-1602891463.us-east-1.elb.amazonaws.com/?name=Foxtrot  If you refresh the browser for requests from the other customers listed above, you will see that responses are being returned only from 3 EC2 instances instead of 4.\nNote: If you see a response that says \u0026ldquo;This site can\u0026rsquo;t be reached\u0026rdquo;, please make sure you are using the URL obtained from the outputs section of the CloudFormation stack and not the sample URL provided in this lab guide.\n  Customer Alpha, not aware of this bug in the application, will retry the request. Refresh the page with customer Alpha\u0026rsquo;s request with the bug=true query string to simulate this. This request is then routed to one of the 3 remaining healthy instances. The bug is triggered again and another instance goes down leaving only 2 healthy instances. This can be verified by sending requests from one of the other customers without including the query string bug=true and seeing responses from only 2 EC2 instances.\n  This process continues with customer Alpha retrying requests until all instances are unhealthy. Refresh the page a few more times as customer Alpha with the query string bug=true. You will eventually see the response change to 502 Bad Gateway because there are no healthy instances to handle requests. You can verify this by sending requests from other customers, you should see a 502 Bad Gateway response received for all requests from all customers.\n http://shuffle-alb-1p2xbmzo541rr-1602891463.us-east-1.elb.amazonaws.com/?name=Bravo http://shuffle-alb-1p2xbmzo541rr-1602891463.us-east-1.elb.amazonaws.com/?name=Charlie http://shuffle-alb-1p2xbmzo541rr-1602891463.us-east-1.elb.amazonaws.com/?name=Delta http://shuffle-alb-1p2xbmzo541rr-1602891463.us-east-1.elb.amazonaws.com/?name=Echo http://shuffle-alb-1p2xbmzo541rr-1602891463.us-east-1.elb.amazonaws.com/?name=Foxtrot  This is what is known as a \u0026ldquo;retry-storm\u0026rdquo;, where a customer is unknowingly making bad requests and retrying the request every time it fails because they are not aware of the bug within the application.\n  In this situation, a buggy request made by one customer has taken down all instances on the backend resulting in complete downtime and all customers are now affected. This is a widespread scope of impact with 100% of customers affected.\n  Fix the application As previously mentioned, the development team is aware of this bug within the application and are working on a fix, however, the fix will not be rolled out for several weeks/months. They have been able to identify the root cause of the issue and provided a temporary manual fix for it. Whenever this issue is encountered, the Operations team executes the temporary fix to bring the application back up again. They have codified this process into a Systems Manager Document and use Systems Manager to implement the fix on their fleet if outages occur.\n  Go to the Outputs section of the CloudFormation stack and open the link for SSMDocument. This will take you to the Systems Manager console.\n  Click on Run command which will open a new tab on the browser\n  Scroll down to the Targets section and select Choose instances manually\n  Check the box next to the EC2 instances with the names Worker-1, Worker-2, Worker-3, and Worker-4\n  Scroll down to the Output options section and uncheck the box next to Enable an S3 bucket. This will prevent Systems Manager from writing log files based on the command execution to S3.\n  Click on Run\n  You should see the command execution succeed in a few seconds\n  Once the command has finished execution, you can go back to the application and test it to verify it is working as expected. Make sure that the query-string bug is not included in the request. For example, http://shuffle-alb-1p2xbmzo541rr-1602891463.us-east-1.elb.amazonaws.com/?name=Alpha should return a valid response. Refresh the page a few times to make sure all 4 instances are up and running. You can also change the customer name in the query string to see that functionality has returned to all customers.\n http://shuffle-alb-1p2xbmzo541rr-1602891463.us-east-1.elb.amazonaws.com/?name=Alpha http://shuffle-alb-1p2xbmzo541rr-1602891463.us-east-1.elb.amazonaws.com/?name=Bravo http://shuffle-alb-1p2xbmzo541rr-1602891463.us-east-1.elb.amazonaws.com/?name=Charlie http://shuffle-alb-1p2xbmzo541rr-1602891463.us-east-1.elb.amazonaws.com/?name=Delta http://shuffle-alb-1p2xbmzo541rr-1602891463.us-east-1.elb.amazonaws.com/?name=Echo http://shuffle-alb-1p2xbmzo541rr-1602891463.us-east-1.elb.amazonaws.com/?name=Foxtrot     function prevStep(){ window.open(\"..\\/1_deploy_workload\", \"_self\") } function nextStep(){ window.open(\"..\\/3_implement_sharding\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_monitoring_linux_ec2_cloudwatch/2_deploy_instance/","title":"Deploying an instance","tags":["Linux","Amazon Linux","EC2","CloudWatch","CloudWatch Dashboard"],"description":"","content":" The CloudFormation template that accompanies this lab requires the ability to create IAM Roles and IAM Instance Profiles. If the account you are using does not have these capabilities, you will not be able to complete this lab.\n Deploy CloudFormation Template   Download the LinuxMachineDeploy.yaml CloudFormation template to your machine.\n  Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and click Create Stack \u0026gt; With new resources   Leave Prepare template setting as-is\n For Template source select Upload a template file Click Choose file and supply the CloudFormation template you downloaded: LinuxMachineDeploy.yaml     Click Next\n  For Stack name use LinuxMachineDeploy\n  Parameters\n  Look over the Parameters and their default values.\n  None of these parameters need to be changed, but are available if you wish to try different settings\n  Stack name  Use LinuxMachineDeploy (case sensitive)\n  CloudWatchNameSpace - The CloudWatch NameSpace to use instead of the default\n  InstanceAMI  This will auto-populate with the latest version of the Amazon Linux 2 AMI\n  InstanceType - Instance Type, defaults to t3.large but can use any size supported by Linux in the region you have chosen\n    MetricAggregationInterval - How often should the CloudWatch Agent send data into CloudWatch.\n  MetricCollectionInterval - How often should the CloudWatch Agent collect information from the Operating System.\n  PrimaryNodeLabel - The additional label assigned to the EC2 instance to use for searching within CloudWatch Explorer. This is done by adding an extra Tag to the EC2 instance.\n  VPCImportName - The name of the stack you created in the previous step that will be used to launch the instance into.\n  Click Next\n    For Configure stack options we recommend configuring tags, which are key-value pairs, that can help you identify your stacks and the resources they create. For example, enter Owner in the left column which is the key, and your email address in the right column which is the value. We will not use additional permissions or advanced options so click Next. For more information, see Setting AWS CloudFormation Stack Options .\n  For Review\n Review the contents of the page At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Create stack     This will take you to the CloudFormation stack status page, showing the stack creation in progress.\n Click on the Events tab Scroll through the listing. It shows the activities performed by CloudFormation (newest events at top), such as starting to create a resource and then completing the resource creation. Any errors encountered during the creation of the stack will be listed in this tab.     When it shows status CREATE_COMPLETE, then you are finished with this step.\n  This template will take between 2-5 minutes to fully deploy using a t3.large. A smaller instance size may take longer.\n  function prevStep(){ window.open(\"..\\/1_deploy_vpc\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/3_creating_cloudwatch_dashboard\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_monitoring_windows_ec2_cloudwatch/2_deploy_instance/","title":"Deploying an instance","tags":["Windows Server","Windows","EC2","CloudWatch","CloudWatch Dashboard"],"description":"","content":" The CloudFormation template that accompanies this lab requires the ability to create IAM Roles and IAM Instance Profiles. If the account you are using does not have these capabilities, you will not be able to complete this lab.\n Deploy CloudFormation Template   Download the WindowsMachineDeploy.yaml CloudFormation template to your machine.\n  Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and click Create Stack \u0026gt; With new resources   Leave Prepare template setting as-is\n For Template source select Upload a template file Click Choose file and supply the CloudFormation template you downloaded: WindowsMachineDeploy.yaml     Click Next\n  For Stack name use WindowsMachineDeploy\n  Parameters\n  Look over the Parameters and their default values.\n  Stack name  Use WindowsMachineDeploy (case sensitive)\n  VPCImportName - The subnet you wish to deploy the EC2 instance into for testing. For this lab, we use a default VPC subnet within the account.\n  InstanceAMI  This will auto-populate with the latest version of the Windows 2019 Base AMI\n  InstanceType - Instance Type, defaults to t3.large but can use any size supported by Windows in the region you have chosen\n   MetricAggregationInterval - How often should the Windows CloudWatch Agent send data into CloudWatch. No need to change this. MetricCollectionInterval - How often should the CloudWatch Agent collect information from the Operating System. No need to change this. PrimaryNodeLabel - The additional label assigned to the EC2 instance to use for searching within CloudWatch Explorer   Click Next    For Configure stack options we recommend configuring tags, which are key-value pairs, that can help you identify your stacks and the resources they create. For example, enter Owner in the left column which is the key, and your email address in the right column which is the value. We will not use additional permissions or advanced options so click Next. For more information, see Setting AWS CloudFormation Stack Options .\n  For Review\n Review the contents of the page At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Create stack     This will take you to the CloudFormation stack status page, showing the stack creation in progress.\n Click on the Events tab Scroll through the listing. It shows the activities performed by CloudFormation (newest events at top), such as starting to create a resource and then completing the resource creation. Any errors encountered during the creation of the stack will be listed in this tab.     When it shows status CREATE_COMPLETE, then you are finished with this step.\n  This template will take between 10-15 minutes to fully deploy using a t3.large. A smaller instance size may take longer.\n  function prevStep(){ window.open(\"..\\/1_deploy_vpc\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/3_creating_cloudwatch_dashboard\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_organization_data_cur_connection/2_create_automation_resources_source/","title":"Create Automation Resources","tags":[],"description":"","content":"Setup an AWS Lambda function to retrieve AWS Organizations information Create the On-Demand AWS Lambda function to get the AWS Organizations information, and extract the required parts from it then write to our bucket in Amazon S3.\n Return to your Sub account for the rest of this lab. Go to the Lambda service page :  Click Create function:   Enter the following details:\n Select: Author from scratch Function name: Lambda_Org_Data Runtime: Python (Latest) Open Change default execution role Execution Role: Use an existing role Role name: LambdaOrgRole    Click Create function\n   Copy and paste the following code into the Function code section and change (account id) to your Management Account ID and (Region) to the Region you are deploying in:\n#!/usr/bin/env python3 import argparse import boto3 from botocore.exceptions import ClientError from botocore.client import Config import os def list_accounts(): bucket = os.environ[\u0026quot;BUCKET_NAME\u0026quot;] #Using environment variables below the Lambda will use your S3 bucket sts_connection = boto3.client('sts') acct_b = sts_connection.assume_role( RoleArn=\u0026quot;arn:aws:iam::(account id):role/OrganizationLambdaAccessRole\u0026quot;, RoleSessionName=\u0026quot;cross_acct_lambda\u0026quot; ) ACCESS_KEY = acct_b['Credentials']['AccessKeyId'] SECRET_KEY = acct_b['Credentials']['SecretAccessKey'] SESSION_TOKEN = acct_b['Credentials']['SessionToken'] # create service client using the assumed role credentials client = boto3.client( \u0026quot;organizations\u0026quot;, region_name=\u0026quot;us-east-1\u0026quot;, #Using the Organizations client to get the data. This MUST be us-east-1 regardless of region you have the Lamda in aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY, aws_session_token=SESSION_TOKEN, ) paginator = client.get_paginator(\u0026quot;list_accounts\u0026quot;) #Paginator for a large list of accounts response_iterator = paginator.paginate() with open('/tmp/org.csv', 'w') as f: # Saving in the temporay folder in the lambda for response in response_iterator: # extracts the needed info for account in response[\u0026quot;Accounts\u0026quot;]: aid = account[\u0026quot;Id\u0026quot;] name = account[\u0026quot;Name\u0026quot;] time = account[\u0026quot;JoinedTimestamp\u0026quot;] status = account[\u0026quot;Status\u0026quot;] line = \u0026quot;%s, %s, %s, %s\\n\u0026quot; % (aid, name, time, status) f.write(line) print(\u0026quot;respose gathered\u0026quot;) try: s3 = boto3.client('s3', '(Region)', config=Config(s3={'addressing_style': 'path'})) s3.upload_file( '/tmp/org.csv', bucket, \u0026quot;organisation-data/org.csv\u0026quot;) #uploading the file with the data to s3 print(\u0026quot;org data in s3\u0026quot;) except Exception as e: print(e) def lambda_handler(event, context): list_accounts()    Edit Basic settings below:\n Memory: 512MB Timeout: 2min Click Save    Scroll down to Environment variable and click Edit   Add environment variable:\n In Key paste BUCKET_NAME In Value paste your S3 Bucket name where the Organizations data should be saved.  Click Save\n  Scroll to the function code and click Deploy. Then Click Test.  Enter an Event name of Test, click Create:   Click Test\n  The function will run, it will take a minute or two given the size of the Organizations files and processing required, then return success. Click Details and verify there is headroom in the configured resources and duration to allow any increases in Organizations file size over time:\n  Go to your S3 bucket and into the organisation-data folder and you should see a file of non-zero size is in it:  Amazon CloudWatch Events Setup We will setup a Amazon CloudWatch Event to periodically run the Lambda functions, this will update the Organizations and include any newly created accounts.\n Go to the CloudWatch service page:  Click on Events, then click Rules:  Click Create rule   For the Event Source\n Select Schedule and set the required period Select 7 days Add the Lambda_Org_Data Lambda function  Click Configure details\n  Add the name Lambda_Org_Data, optionally add a description and click Create rule:  You have now created your lambda function to gather your organization data and place it into the S3 Bucket we made earlier. Using Cloudwatch this will now run every 7 days updating the data.\n  function prevStep(){ window.open(\"..\\/1_create_static_resources_source\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/3_utilize_organization_data_source\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/100_labs/100_basic_identity_and_access_management_user_group_role/","title":"Creating your first Identity and Access Management User, Group, Role","tags":[],"description":"","content":"Last Updated: February 2021\nAuthor: Ben Potter, Security Lead, Well-Architected\nThis lab has been retired. It is strongly recommended you centralize your identities instead of using IAM Users. If you have more than a single test account for personal use, use AWS Single Sign-On or an identity provider configured in IAM, instead of IAM users. IAM users should not have access keys, for Command Line Interface (CLI) you should instead assume a role, or use integration with AWS Single Sign-on making it easy to get short term credentials for CLI use without needing to store long lived credentials. Use separate accounts for development/test and production, If you dont have an existing organizational structure with AWS Organizations , AWS Control Tower is the easiest way to get started. For more information see Security Foundations and Identity and Access Management in the AWS Well-Architected security whitepaper.\n "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/100_labs/100_dependency_monitoring/2_understand_metrics/","title":"Understanding Metrics","tags":[],"description":"","content":"Workloads can and should be designed to emit a variety of metrics such as application metrics (number of requests, error codes), infrastructure metrics (CPU, memory, disk usage), etc. These metrics should be tied back to KPI(s) that are used to measure a certain aspect of the business. Understanding different workload metrics will allow you to select the right metric to monitor, and understand the availability or status of your workload and its dependencies.\nIn this case, a Lambda function gets invoked after every data write using the S3 PutObject API call. Understanding metrics related to the Lambda function will provide insight on what the right metrics are to determine the health of the dependent service i.e. the external data write service.\n\u0026ldquo;AWS Lambda automatically monitors Lambda functions on your behalf and reports metrics through Amazon CloudWatch. To help you monitor your code as it executes, Lambda automatically tracks the number of requests, the execution duration per request, and the number of requests that result in an error. It also publishes the associated CloudWatch metrics and you can leverage these metrics to set CloudWatch custom alarms to enable automated responses to changes in metrics.\u0026rdquo; - https://docs.aws.amazon.com/lambda/latest/dg/lambda-monitoring.html\nAWS Lambda provides default metrics on CloudWatch across three categories:\n Invocation Metrics  Performance Metrics  Concurrency Metrics   For the use-case in this lab, invocation metrics provided by Lambda will be used. The two key invocation metrics to focus on here are Invocations and Errors.\nMonitoring the Errors metric will provide visibility into function errors. Function errors include exceptions thrown by your code and exceptions thrown by the Lambda runtime. The runtime returns errors for issues such as timeouts and configuration errors. This is a valuable metric to identify issues with the function code, or configuration, as well as certain issues related to the external service.\nFor example, if the Lambda function is expecting the file written by the external service to be in a certain format, and if the written file does not match this format, it could result in a Lambda function error. Creating an alarm on Errors will provide visibility into the function execution.\nThere might be a situation where the external service is running at reduced capacity, some capabilities may be impaired, or it could be experiencing a complete outage with no data writes to S3. If no new files are being written to S3 monitoring for Lambda function errors will not suffice.\nIf the external service is no longer writing files to S3 at 50 second intervals, there would be no S3 notification, and subsequently, no Lambda invocations. As the Lambda function was never invoked, the Errors metric will show no change. A different approach is necessary.\nThe SLA with the external service expects new files to be written at 50 second intervals. The expected Lambda invocation rate is a minimum of 1 invocation per minute as a results of the S3 notifications. Now that a baseline has been established as to what the Invocations metric should look like, you can create an alarm to alert when the metric deviates from the baseline.\n  Go to the Amazon CloudWatch console at https://console.aws.amazon.com/cloudwatch and click on Metrics\n  In the search bar under All metrics, enter the name of the data read function - WA-Lab-DataReadFunction and press enter\n  In the metric breakdown, select Lambda \u0026gt; By Resource and you will see a list of Lambda metrics available\n  Check the box for the metric Invocations and observe the graph, you will see all the function invocations\n  Change the range by clicking on the custom option and selecting 15 Minutes\n  Change the period to 1 Minute by going to the Graphed metrics tab\n  You should now see a graph that has a data point at 1 minute intervals representing a Lambda function invocation. Each invocation represents a write to the S3 bucket. By monitoring the number and frequency of lambda invocations, you are monitoring the availability of the external service. If the number of invocations drops below the expected value, there may be an issue with the external service.\n function prevStep(){ window.open(\"..\\/1_deploy_infrastructure\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/3_create_alarm\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/100_labs/100_dependency_monitoring/","title":"100 - Dependency Monitoring","tags":[],"description":"","content":"Author: Mahanth Jayadeva, Solutions Architect, Well-Architected\nIntroduction In this lab, you will become familiar with dependency monitoring and how to apply it to gain insights on resources that your workload depends on. You will learn how to create alarms and notifications to determine when a response is required.\nIt is important to design and configure your workload to emit information about the status (for example, reachability or response time) of resources it depends on. Examples of external dependencies can include, external databases, DNS, and network connectivity. By monitoring resources that your workload is dependent on, you will be able to quickly take action and ensure business continuity even when the dependent service is experiencing issues or downtime.\nIn this lab, you will create a CloudWatch alarm to monitor a dependency for a workload, and automate notifications so that your teams are aware of a potential impact to your workload due to a failing/degraded external dependency.\nThe skills you learn will help you define a dependency monitoring strategy in alignment with the AWS Well-Architected Framework Goals:  Create alarms to monitor external dependencies Alert relevant stakeholders when outcomes are at risk due to a failed external dependency Learn how to automate this process  Best Practices Covered: Implement dependency telemetry: Design and configure your workload to emit information about the status of resources it depends on. Examples of these are external databases, DNS, and network connectivity. Use this information to determine when a response is required.\nAlert when workload outcomes are at risk: Raise an alert when workload outcomes are at risk so that you can respond appropriately if required.\nEnable push notifications: Communicate directly with your users (for example, with email or SMS) when the services they use are impacted, and when the services return to normal operating conditions, to enable users to take appropriate action.\nRequirements  An AWS Account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account that has Administrator privileges.  Costs NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\n  function prevStep(){ window.open(\"\", \"_self\") } function nextStep(){ window.open(\".\\/1_deploy_infrastructure\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool  Start Lab  Steps  Deploy the Infrastructure   Understanding Metrics   Create an Alarm   Test Fail Condition   Bonus Content   Tear down this lab   "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_licensing/2_analyze_understand/","title":"Analyze and Understand Licensing","tags":[],"description":"","content":"Analyze and understand licensing costs in your workload We will analyze the CUR file for any software that includes licensing. First we will see what columns could give us the information we need. Then we look for the amount we are spending on items containing licenses, and how much we are spending on the actual licenses by comparing to a similar non-licensed option.\nWe take this information and decide if the effort required to make the change will be less than what we save, and if the required functionality is still met.\nThe queries below are for the provided data set created previously. If you use your own CUR you will need to modify these queries accordingly.\n Analyze CUR columns The first step is to understand what information we may have that can show us costs associated with licenses.\n  Go to the Athena Console:\n  Select the costmaster database, you should see the before and after tables\n  We want to see all the columns available, and some sample data, so paste the following command and click Run query:\n SELECT * FROM \u0026quot;costmaster\u0026quot;.\u0026quot;before\u0026quot; limit 10;    You can see each column and 10 rows of data below. There are over 100 columns - take a minute or two and slowly scroll through them and view the data.\n  The supplied datasource has multiple workloads in it, we want to ensure we are only looking at costs for our ordering workload. There is a column at the far right resource_tags_user_application, this contains the tags we put on our resources. Lets limit our search to costs tagged with ordering. Run the query:\n SELECT * FROM \u0026quot;costmaster\u0026quot;.\u0026quot;before\u0026quot; where resource_tags_user_application like 'ordering' limit 30    Now we will look for EC2 Operating System licenses. Look through the data and see if you can find a column which indicates the operating system used.\n  There are multiple columns we can use, look at the columns line_item_line_item_description and product_operating_system. Note: columns are close to alphabetical order.\n  We will focus on line_item_line_item_description, you can see Linux, Windows and RHEL. We will get the costs of RHEL licensing and compare it to a similar operating system, AWS Linux. To get a sample of the RHEL costs, paste the following query and click Run query:\n SELECT * FROM \u0026quot;costmaster\u0026quot;.\u0026quot;before\u0026quot; where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%RHEL%' limit 30    We will now get the costs for each hour for running RHEL. We dont need 100+ columns, so we\u0026rsquo;ll focus on just the following columns: line_item_usage_start_date, linte_item_line_item_description, line_item_unblended_cost. We will sum the unblended_cost column, and group it by usage_start_date, this will give us cost per hour. The order by line orders the output by date. Run the following query:\n SELECT line_item_usage_start_date, line_item_line_item_description, sum(line_item_unblended_cost) as cost FROM \u0026quot;costmaster\u0026quot;.\u0026quot;before\u0026quot; where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%RHEL%' group by line_item_usage_start_date, line_item_line_item_description order by line_item_usage_start_date asc    You can see that the cost per hour of running the RHEL instances was 0.8128000000000001 per hour. To get the pricing for a non-licensed operating system we will need the instance size and location. We will use the line_item_usage_type and line_item_availability_zone columns to get this, run the following query:\nSELECT line_item_usage_type, line_item_availability_zone, sum(line_item_unblended_cost) as cost FROM \u0026quot;costmaster\u0026quot;.\u0026quot;before\u0026quot; where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%RHEL%' group by line_item_usage_type, line_item_availability_zone limit 30    Look at the columns: line_item_usage_type, line_item_availability_zone, you will see the values BoxUsage:t3.medium and us-east-1a/b:\n  We see that we are running t3.medium RHEL instances in us-east-1. Open the pricing pages in a new tab: https://aws.amazon.com/ec2/pricing/on-demand/\n  Pricing is correct as of August 2020, we may have had a price drop since then and the prices below may be higher than what is currently in the console. Please disregard and proceed through the lab.\n  Ensure the Linux tab is selected, and the region is US East (N. Virginia). The pricing for a t3.medium with AWS Linux is: $0.0416 per hour. Now click the RHEL tab:\n  The pricing will change, the price for RHEL is: $0.1016.\n  Lets see exactly how many hours of usage we were running in total (line_item_usage_amount), and the price. In the Athena console paste the following query:\nSELECT line_item_line_item_description, sum(line_item_usage_amount) usage_hours, sum(line_item_unblended_cost) as cost FROM \u0026quot;costmaster\u0026quot;.\u0026quot;before\u0026quot; where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%RHEL%' group by line_item_line_item_description    You can see we consumed 176 instance hours of licensed RHEL ($0.1016 an hour), for a total cost of $17.8815999\u0026hellip;.\n  Our sample CUR is less than a month, to get monthly costs - lets see how many instances we are running with RHEL. We will get the resource IDs and how long they ran for with the following query. In the Athena Console paste the following:\nSELECT distinct line_item_resource_id, sum(line_item_usage_amount) as Hours_ran FROM \u0026quot;costmaster\u0026quot;.\u0026quot;before\u0026quot; where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%RHEL%' group by line_item_resource_id    We are running 8 instances of RHEL at a cost of $0.1016 per hour. If we changed these to AWS Linux at a cost of $0.0416 per hour, we would save $0.06 each hour, per instance.\n  8 Instances at saving of $0.06 per hour, for an average of 730 hours each month, is a monthly saving of $350.4.\n  As with all optimization opportunities, weigh up the savings gained from making the change, against the effort required to change. Is $350 each month worth making the change? How long before you pay off your effort? How long will the workload run? What if your instance usage increases or decreases - does the answer change?\n Understand the costs of running licensed software in your workload  There can be associated costs with running licensed software. Additional resources may be mandated by the software in addition to a base configuration. We will continue our example by looking at operating systems.\nWe will now see if additional resources are required to run RHEL compared to AWS Linux, and the cost of these additional resources.\nIf you do not have the additional privileges the EC2 console you will have to read through first 10 steps below.\n   Go into the EC2 console, and click Launch Instance, then Launch instance\n  Select the Amazon Linux 2 AMI\n  Select a t3.medium instance size, and click Next: Configure Instance Details\n  Click Next: Add Storage\n  You can see that the default configuration is for 8GiB GP2 of EBS storage. Click Cancel.   From the EC2 Console click Launch Instance, then Launch instance\n  Select the Red Hat Enterprise Linux AMI:\n  Select a t3.medium instance and click Next: Configure Instance Details:\n  Click Next: Add Storage\n  You will see that there is 10GiB GP2 of storage required for RHEL as a default. Click Cancel.   Confirm how much storage we are using and the cost:\n SELECT line_item_usage_start_date, line_item_line_item_description, sum(line_item_unblended_cost) as cost, sum(line_item_usage_amount) as amount FROM \u0026quot;costmaster\u0026quot;.\u0026quot;before\u0026quot; where resource_tags_user_application like 'ordering' and line_item_usage_type like '%EBS%' and resource_tags_user_tier like 'front' group by line_item_usage_start_date, line_item_line_item_description order by line_item_usage_start_date asc limit 5    Each hour we are consuming 0.111111 for a cost of $0.011111.\n  You need an additional 2Gb of GP2 storage per instance. Go to the EBS pricing page here: https://aws.amazon.com/ebs/pricing/\n  The price in US East(N. Virginia) is $0.1 per GB-month for SSD gp2 volumes. We will now calculate the storage savings across our workload.\n  Lets see how many instances we are running with RHEL. We will get the resource IDs and how long they ran for with the following query. In the Athena Console paste the following:\nSELECT distinct line_item_resource_id, sum(line_item_usage_amount) as Hours_ran FROM \u0026quot;costmaster\u0026quot;.\u0026quot;before\u0026quot; where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%RHEL%' group by line_item_resource_id    We are running 8 instances, which each require an additional 2GB GP2. This is 16GB-month of storage (at $0.10 per GB-month), which is an additional cost of $1.60 each month on our workload.\n  We have total additional monthly costs due to licensing of $350.4 (licenses) + $1.60 (storage) = $352\n  We have total monthly savings due to licensing. Lets think long term, whats the impact if our workload grows over time? whats the saving?\n Simulate the change and validate We have performed the change in our environment and created new Cost and Usage Reports. We will now analyze the after table which contains these changes.\n  Lets see the amount and cost of our new AWS Linux instances, and the price. In the Athena console paste the following query:\n SELECT distinct line_item_resource_id, line_item_line_item_description, sum(line_item_usage_amount) as Hours_ran, sum(line_item_unblended_cost) as cost FROM \u0026quot;costmaster\u0026quot;.\u0026quot;after\u0026quot; where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%t3.medium%' and resource_tags_user_tier like 'front' group by line_item_resource_id, line_item_line_item_description    You can see we ran 8 Linux instances in a sample of 23hrs at a cost of $0.09568 for each instance.\n  The total cost for 8 instances over 730 hours ($0.9568 / 23 * 730 * 8), would be $242.944. Our original cost was $593.344, and we predicted a saving of $350.4, our final cost of $242.944 verifies this prediction.\n  Lets analyze the storage with the following query:\n SELECT line_item_usage_start_date, line_item_line_item_description, sum(line_item_unblended_cost) as cost, sum(line_item_usage_amount) as amount FROM \u0026quot;costmaster\u0026quot;.\u0026quot;after\u0026quot; where resource_tags_user_application like 'ordering' and line_item_usage_type like '%EBS%' and resource_tags_user_tier like 'front' group by line_item_usage_start_date, line_item_line_item_description order by line_item_usage_start_date asc limit 5    You can see there was an hourly amount of 0.088888 and an hourly cost of $0.008888, which is 8/10 of the original amount of 0.111111 and cost of 0.011111, which validates changing to 8Gb volumes from 10Gb volumes. This validates our $1.60 saving prediction.\n  In this step you discovered your Licensing costs, and associated additional resource costs. You analyzed them to make sure it was worth the effort to make a change, then verified the savings after making (simulating) the change.\n  function prevStep(){ window.open(\"..\\/1_pricing_sources\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/3_tear_down\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/100_labs/contributing/","title":"Contributing to Well-Architected Labs","tags":[],"description":"","content":"Authors  Nathan Besh. Cost-Lead, Well-Architected, AWS  Introduction The Well-Architected labs are open source and we welcome feedback and contributions from the community. If there are errors or you wish to make improvements to the labs, you can perform a pull request. To do a pull request follow the steps below.\nRequirements  GitHub account Git setup locally on your PC Install Hugo   Create a fork You will create a fork to work on, make the edits and then submit it to be merged back into production. A local fork will be your local copy of the Well-Architected Labs repository.\n Go to the production Well-Architected Labs repository  On the right side of the page, click on the Fork icon Select your GitHub account to fork to You will then have a remote repository of /aws-well-architected-labs  This is effectively your own version of the labs, stored on GitHub\nCreate a local repository from the fork You will create a local copy of the fork to work on, and make the required edits, his allows you to work locally and view the changes.\n  Create a directory on your PC to hold the local repository\n  Change that to directory\n  Replace the repository name (username)/aws-well-architected-labs below with your repository name, and clone it:\n git clone git@github.com:(username)/aws-well-architected-labs.git    It will download the repository\n  You now have your local copy setup \u0026amp; have your own GitHub repository to push to\n  Make changes and verify Make the required updates to the content, test and verify locally\n  Change to the labs directory aws-well-architected-labs\n  Navigate to the content folder and make the required edits\n  Navigate back to the aws-well-architected-labs parent folder\n  Serve the content locally:\n hugo serve -D    Open a browser and navigate to http://localhost:1313/\n  Verify the change you made was correct and there were no problems introduced\n  Push your changes to the remote repository: Push your changes to GitHub, so that they are stored and backed up by GitHub. Use the following commands:\n git add -A git commit -m \u0026quot;your comment here\u0026quot; git push  All your changes will be in the remote repository in GitHub, which can now be merged into the Well-Architected Labs repository.\nPerform a pull request All the changes are now in your remote repository, lets do a pull request to merge it into the public Well-Architected Labs repository:\n Go to the Well-Architected Labs pull requests  Click New pull request Click compare across forks Select your fork on the right side as head repository Review the changes, and click Create pull request Edit the info (this is public  be careful) Click Create pull request  The W-A team will receive a notification of the pull request, will review \u0026amp; include the changes.\nCleanup Clean up your local and remote repositories, delete them if there is not additional work.\n Go to your remote repository, modify the link: https://github.com/(username)/aws-well-architected-labs Click Settings Scroll down under Danger Zone and click Delete this repository Confirm \u0026amp; click delete Clean up your local repository by deleting the directory created previously  "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_cur_queries/queries/application_integration/","title":"Application Integration","tags":[],"description":"","content":"These are queries for AWS Services under the Application Integration product family .\nUse the clipboard in the top right of the text boxes below to copy all of the text to be pasted.\n You may need to change variables used as placeholders in your query. ${table_Name} is a common variable which needs to be replaced. Example: cur_db.cur_table\n Table of Contents   Amazon MQ   Query Description This query will provide daily unblended and amortized cost as well as usage information per linked account for Amazon MQ. The output will include detailed information about the resource id (broker), usage type, and API operation. The usage amount and cost will be summed and the cost will be in descending order.\nPricing Please refer to the Amazon MQ pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, product_broker_engine, line_item_usage_type, product_product_family, pricing_unit, pricing_term, SPLIT_PART(line_item_usage_type, ':', 2) AS split_line_item_usage_type, SPLIT_PART(line_item_resource_id, ':', 7) AS split_line_item_resource_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d') AS day_line_item_usage_start_date, line_item_operation, SUM(CAST(line_item_usage_amount AS double)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS decimal(16,8))) AS sum_line_item_unblended_cost FROM ${table_Name} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND product_product_name = 'Amazon MQ' AND line_item_line_item_type NOT IN ('Tax','Credit','Refund','EdpDiscount','Fee','RIFee') GROUP BY bill_payer_account_id, line_item_usage_account_id, product_broker_engine, product_product_family, pricing_unit, pricing_term, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d'), line_item_usage_type, line_item_resource_id, line_item_operation ORDER BY day_line_item_usage_start_date, sum_line_item_unblended_cost DESC, split_line_item_usage_type;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         Amazon SES   Query Description This query will provide daily unblended and usage information per linked account for Amazon SES. The output will include detailed information about the product family (Sending Attachments, Data Transfer, etc\u0026hellip;) and usage type. The usage amount and cost will be summed and the cost will be in descending order.\nPricing Please refer to the Amazon SES pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d') AS day_line_item_usage_start_date, product_product_family, CASE WHEN line_item_usage_type LIKE '%%DataTransfer-In-Bytes%%' THEN 'Data Transfer GB (IN) ' WHEN line_item_usage_type LIKE '%%DataTransfer-Out-Bytes%%' THEN 'Data Transfer GB (Out)' WHEN line_item_usage_type LIKE '%%AttachmentsSize-Bytes%%' THEN 'Attachments GB' WHEN line_item_usage_type LIKE '%%Recipients' THEN 'Recipients' WHEN line_item_usage_type LIKE '%%Recipients-EC2' THEN 'Recipients' WHEN line_item_usage_type LIKE '%%Recipients-MailboxSim' THEN 'Recipients (MailboxSimulator)' WHEN line_item_usage_type LIKE '%%Message%%' THEN 'Messages' WHEN line_item_usage_type LIKE '%%ReceivedChunk%%' THEN 'Received Chunk' ELSE 'Others' END as case_line_item_usage_type, SUM(CAST(line_item_usage_amount AS double)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS decimal(16,8))) AS sum_line_item_unblended_cost FROM ${table_Name} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND product_product_name = 'Amazon Simple Email Service' AND line_item_line_item_type NOT IN ('Tax','Credit','Refund','EdpDiscount','Fee','RIFee') GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d'), product_product_family, line_item_usage_type ORDER BY day_line_item_usage_start_date, sum_line_item_usage_amount, sum_line_item_unblended_cost DESC;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         Amazon SNS   Query Description This query will provide daily unblended cost and usage information per linked account for Amazon SNS. The output will include detailed information about the product family, API Operation, and usage type. The usage amount and cost will be summed and the cost will be in descending order.\nPricing Please refer to the Amazon SNS pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d') AS day_line_item_usage_start_date, CONCAT(product_product_family,' - ',line_item_operation) AS concat_product_product_family, SUM(CAST(line_item_usage_amount AS double)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS decimal(16,8))) AS sum_line_item_unblended_cost FROM ${table_Name} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND product_product_name = 'Amazon Simple Notification Service' AND line_item_line_item_type NOT IN ('Tax','Credit','Refund','EdpDiscount','Fee','RIFee') GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d'), CONCAT(product_product_family,' - ',line_item_operation) ORDER BY day_line_item_usage_start_date, sum_line_item_unblended_cost DESC;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback       CUR queries are provided as is. We recommend validating your data by comparing it against your monthly bill and Cost Explorer prior to making any financial decisions. If you wish to provide feedback on these queries, there is an error, or you want to make a suggestion, please email: curquery@amazon.com\n "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_enterprise_dashboards/2_modify_cost_intelligence/","title":"Modify Cost Intelligence Dashboard","tags":[],"description":"","content":"Authors  Alee Whitman, Commercial Architect (AWS)  Add Account Mapping Data This section is optional and shows how you can add your Business Unit or Enterprise Account mapping data to your dashboard using Account Id as the identifier.\nThis example will show you how to replace the AccountID, with a name that is meaningful to your organization.\n  Create an account mapping file locally, you can use the sample here as a starting point: mapping_document.csv   In QuickSIght, select the summary_view Data Set\n  Select Edit data set\n  Select Add Data:   Select upload a file   Navigate to your mapping data file and click Open\n  Confirm the mappings are correct, click Next:   Select the two circles to open the Join configuration then select Left to change your join type:   Create following join clause then click Apply:\n linked_account_id = (Your linked Account Id field name)     Scroll down in the field list, and confirm the new fields have the correct data types. The Account ID must be Int:   Select Save\n  Repeat steps 2-11, creating mapping joins for your remaining QuickSight data sets:\n s3_view ec2_running_cost compute_savings_plan_eligible_spend    You now have new fields that can be used on the visuals - we will now use them\n  Go to the Cost Intelligence Analysis\n  Edit the calculated field Account:   Change the formula from toString({linked_account_id}) to {Account Name}   You can now select a visual, select the Account field, and you will see the account names in your visuals, instead of the Account number:   You now have successfully utilized mapping data on your dashboard\n Customize your Summary View Cost value This section is optional and shows how you can customize the analysis from using Amortized Cost to using Unblended/Invoiced Cost.\n  From the QuickSight Analysis dashboard, click on the Cost Intelligence Analysis   Edit the calculated field Cost:   Change the formula from {Cost_Amortized} to {Cost_Unblended}:   If you have usage with upfront charges - such as partial or full upfront Savings Plans or Reserved Instances, you will notice changes in the cost values of your dashboards.\n  You have successfully updated your Cost value and customized the Summary View.\n  function prevStep(){ window.open(\"..\\/1_create_cost_intelligence\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/3_create_data_transfer_cost_analysis\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_automated_deployment_of_vpc/1_create_vpc_stack/","title":"Create VPC Stack","tags":[],"description":"","content":"This step will create the VPC and all components using the example CloudFormation template.\n  Download the latest version of the CloudFormation template here: vpc-alb-app-db.yaml   Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and click Create Stack \u0026gt; With new resources   Leave Prepare template setting as-is\n For Template source select Upload a template file Click Choose file and supply the CloudFormation template you downloaded: vpc-alb-app-db.yaml     Click Next\n  For Stack name use WebApp1-VPC\n  Parameters\n  Look over the Parameters and their default values.\n  Leave all parameters as their default values unless you are experimenting.\n  Click Next\n    For Configure stack options we recommend configuring tags, which are key-value pairs, that can help you identify your stacks and the resources they create. For example, enter Owner in the left column which is the key, and your email address in the right column which is the value. We will not use additional permissions or advanced options so click Next. For more information, see Setting AWS CloudFormation Stack Options .\n  For Review\n Review the contents of the page At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Create stack     This will take you to the CloudFormation stack status page, showing the stack creation in progress.\n Click on the Events tab Scroll through the listing. It shows the activities performed by CloudFormation (newest events at top), such as starting to create a resource and then completing the resource creation. Any errors encountered during the creation of the stack will be listed in this tab.     When it shows status CREATE_COMPLETE, then you are finished with this step.\n  Now that you have a new VPC, check out 200_Automated_Deployment_of_EC2_Web_Application to deploy an example web application inside it.\n"},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_clock_source_performance/","title":"Level 100: Calculating differences in clock source","tags":[],"description":"How various linux clock sources can affect the performance of your application on EC2","content":"Authors  Eric Pullen, Performance Efficiency Lead Well-Architected  Introduction This hands-on lab will teach you the fundamentals of how various linux clock sources can affect the performance of your application on EC2. AWS has introduced new capabilities within our Nitro system on certain instance types, which takes advantage of better clock timing as well as offloading many other hypervisor related tasks.\nIn this lab, you will deploy three distinct EC2 instances running Amazon Linux, each configured with a different instance type and all with SSM enabled. You will also deploy a set of SSM documents, which will be used to enable and disable various clock timing changes to the machines. Lastly, a set of test scripts will be deployed which allow you to see the differences as the clock changes are made.\nThe skills you learn will help you learn the various clock sources available in EC2, as well as ways to test those changes for your applications in alignment with the AWS Well-Architected Framework.\nFor a list of Nitro-based instances currently available, head to https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#ec2-nitro-instances for the latest list.\nGoals  Deploy EC2 instances on both Nitro and non-nitro backed hypervisors Deploy SSM documents to allow for modifications to the Linux clock source Learn how SSM documents can be used to apply configuration changes to your Linux instances, such as changing the clock source Learn how to use SSM session manager to gain shell access to run your own test against the various instance types  Prerequisites  An AWS account that you are able to use for testing, that is not used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to create an IAM Role, instance profiles, and EC2 instances. An IAM user or federated credentials into that account that has permissions to access AWS System Manager  Costs  https://aws.amazon.com/ec2/pricing/on-demand/ This lab will create 2 EC2 instances in the default VPC  The cost per hour to run this lab would be $0.196 ($4.704/day) if you accept the default parameters in us-east-1   Refer to the link above for pricing if you deploy in a different region There is no additional cost for the AWS Systems Manager run commands used during the lab.  NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\n  function prevStep(){ window.open(\"\", \"_self\") } function nextStep(){ window.open(\".\\/1_deploy\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool  Start Lab  Steps:  Deploying the infrastructure   Testing Performance of both nodes before clock changes   Changing clock type on the Xen based EC2 instance   Teardown   "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_clock_source_performance/2_testing_before/","title":"Testing Performance of both nodes before clock changes","tags":[],"description":"","content":"Now that we have two EC2 machines, one Xen based EC2 and one Nitro/KVM based EC2, we can run a simple test to see the speed in which is can return time of day. This test program was installed on each machine under /tmp/time_test.py. The program will simply request the time of day from a local c-library one million times.\nThe time_test.py code #!/usr/bin/python3 import time _gettimeofday = None def gettimeofday(): import ctypes global _gettimeofday if not _gettimeofday: _gettimeofday = ctypes.cdll.LoadLibrary(\u0026quot;libc.so.6\u0026quot;).gettimeofday class timeval(ctypes.Structure): _fields_ = \\ [ (\u0026quot;tv_sec\u0026quot;, ctypes.c_long), (\u0026quot;tv_usec\u0026quot;, ctypes.c_long) ] tv = timeval() _gettimeofday(ctypes.byref(tv), None) return float(tv.tv_sec) + (float(tv.tv_usec) / 1000000) start_time = time.time() for x in range(0,1000000): gettimeofday() print(\u0026quot;--- %s seconds ---\u0026quot; % (time.time() - start_time)) print(\u0026quot;Done\u0026quot;) If you wish to bypass using the pre-defined AWS System Manager documents below, you can also run this script interactively on each EC2 node using AWS Systems Manager Session Manager .\n Open the AWS Console (https://console.awa.amazon.com) In the Find Services search bar, type systems manager and press enter  Under Instances \u0026amp; Nodes click on Run Command and on the left side of the screen again click on Run a command  In the search bar under Command Document, select Owner from the pulldown and then select Owned by me.  You should see 2 SSM documents that have been created for you by the Cloudformation Template. Click on the one with runTestScriptdocument in the name.  Scroll down and under Targets, select the checkbox next to the two instances that were created from the template. The should be labeled XenTimeInstanceTest and KVMTimeInstanceTest  Scroll down and uncheck the box that says Enable writing to an S3 bucket and then click the Run button at the bottom of the screen.  You should see the command running on both nodes as In Progress. Click the refresh button every few seconds until both boxes show Success in their status column. It will take longer to complete for one node to run than the other. The total time for both to complete should be about 2 minutes if you use the default EC2 instance types.\n  You can now click on each Instance ID to see the output of the command that was run. Just click the pulldown for Step 1 - Output and you should see the following. As you noticed, it tells you at the top of the output how long the script took to run in total, as well as the syscalls used during its execution. In this first example, we can see it ran in 16 seconds and the stat call is the most frequently used.  In this second instance, we can see that it took 110 seconds and the gettimeofday call used over 99% of the time during its run.    function prevStep(){ window.open(\"..\\/1_deploy\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/3_change_clock\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_automated_deployment_of_iam_groups_and_roles/2_assume_roles_iam_user/","title":"Assume Roles from an IAM user","tags":[],"description":"","content":"We will assume the roles previously created in the web console and command line interface (CLI) using an existing IAM user.\n2.1 Use Restricted Administrator Role in Web Console A role specifies a set of permissions that you can use to access AWS resources that you need. In that sense, it is similar to a user in AWS Identity and Access Management (IAM). A benefit of roles is they allow you to enforce the use of an MFA token to help protect your credentials. When you sign in as a user, you get a specific set of permissions. However, you don\u0026rsquo;t sign in to a role, but once signed in (as a user) you can switch to a role. This temporarily sets aside your original user permissions and instead gives you the permissions assigned to the role. The role can be in your own account or any other AWS account. By default, your AWS Management Console session lasts for one hour.\nImportant\nThe permissions of your IAM user and any roles that you switch to are not cumulative. Only one set of permissions is active at a time. When you switch to a role, you temporarily give up your user permissions and work with the permissions that are assigned to the role. When you exit the role, your user permissions are automatically restored.    Sign in to the AWS Management Console as an IAM user https://console.aws.amazon.com .\n  In the console, click your user name on the navigation bar in the upper right. It typically looks like this: username@account_ID_number_or_alias. Alternatively you can paste the link in your browser that you recorded earlier.\n  Click Switch Role. If this is the first time choosing this option, a page appears with more information. After reading it, click Switch Role. If you clear your browser cookies, this page can appear again.\n  On the Switch Role page, type the account ID number or the account alias and the name of the role that you created for the Administrator in the previous step, for example, arn:aws:iam::account_ID:role/Administrator.\n  (Optional) Type text that you want to appear on the navigation bar in place of your user name when this role is active. A name is suggested, based on the account and role information, but you can change it to whatever has meaning for you. You can also select a color to highlight the display name. The name and color can help remind you when this role is active, which changes your permissions. For example, for a role that gives you access to the test environment, you might specify a Display Name of Test and select the green Color. For the role that gives you access to production, you might specify a Display Name of Production and select red as the Color.\n  Click Switch Role. The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you.\nTip\nThe last several roles that you used appear on the menu. The next time you need to switch to one of those roles, you can simply choose the role you want. You only need to type the account and role information manually if the role is not displayed on the Identity menu.\n  You are now using the role with the granted permissions! To stop using a role In the IAM console, choose your role\u0026rsquo;s Display Name on the right side of the navigation bar. Choose Back to UserName. The role and its permissions are deactivated, and the permissions associated with your IAM user and groups are automatically restored.\n  2.2 Use Restricted Administrator Role in Command Line Interface (CLI) Coming soon, for now check out: https://docs.aws.amazon.com/cli/latest/userguide/cli-roles.html "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_100_loft_introduction_to_security/2_basic_iam/","title":"Basic Identity and Access Management User, Group, Role","tags":[],"description":"","content":"This hands-on lab will guide you through the introductory steps to configure AWS Identity and Access Management (IAM). You will use the AWS Management Console to guide you through how to configure your first IAM user, group and role for administrative access.\nWalkthrough Basic Identity and Access Management User, Group, Role "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_100_quick_steps_to_security_success/2_identity/","title":"Centralize Identities","tags":[],"description":"","content":"Every user must leverage unique credentials so we can trace actions within your accounts. Setup your identity structure in the management account and use cross account access to access the child accounts. As you create roles for your users ensure that you are implementing least privilege access by ensuring that users only have access to perform actions required for their role. Be careful who you give permission to perform IAM actions as they can create their own permissions.\nControl Tower sets up your landing zone to leverage AWS Single Sign-On as a central place for your users to log on and access AWS accounts. In this step we will federate that access to your existing identity store.\nWalk through  In your existing AWS account perform a credentials audit, add multi factor authentication to root and ensure that details are up to date  Configure AWS SSO to federate identity . If you are not using SSO you can still federate Identity leveraging a SAML provider and then use cross account access roles to access the accounts that we setup in step 1.  "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_automated_deployment_of_web_application_firewall/2_config_cloudfront/","title":"Configure Amazon CloudFront","tags":[],"description":"","content":"Using the AWS Management Console, we will create a CloudFront distribution, and link it to the AWS WAF ACL we previously created.\n Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. From the console dashboard, choose Create Distribution.  Click Get Started in the Web section.  Specify the following settings for the distribution:   In Origin Domain Name enter the DNS or domain name from your elastic load balancer or EC2 instance.   In the distribution Settings section, click AWS WAF Web ACL, and select the one you created previously.   Click Create Distrubution. For more information on the other configuration options, see Values That You Specify When You Create or Update a Web Distribution in the CloudFront documentation.  After CloudFront creates your distribution, the value of the Status column for your distribution will change from In Progress to Deployed.  When your distribution is deployed, confirm that you can access your content using your new CloudFront URL or CNAME. Copy the Domain Name into a web browser to test.  For more information, see Testing a Web Distribution in the CloudFront documentation.\nYou have now configured Amazon CloudFront with basic settings and AWS WAF.  For more information on configuring CloudFront, see Viewing and Updating CloudFront Distributions in the CloudFront documentation.\n"},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_cloudfront_with_waf_protection/2_config_waf/","title":"Configure AWS WAF","tags":[],"description":"","content":"Using AWS CloudFormation , we are going to deploy a basic example AWS WAF configuration for use with CloudFront.\n Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create stack.  Enter the following Amazon S3 URL: https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/waf-global.yaml and click Next.  Enter the following details:   Stack name: The name of this stack. For this lab, use waf. WAFName: Enter the base name to be used for resource and export names for this stack. For this lab, you can use Lab1. WAFCloudWatchPrefix: Enter the name of the CloudWatch prefix to use for each rule using alphanumeric characters only. For this lab, you can use Lab1. The remainder of the parameters can be left as defaults.  At the bottom of the page click Next. In this lab, we won\u0026rsquo;t add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you\u0026rsquo;re satisfied with the configuration, click Create stack. After a few minutes the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE. You have now set up a basic AWS WAF configuration ready for CloudFront to use!  "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_bidirectional_replication_for_s3/2_configure_replication/","title":"Configure bi-directional cross-region replication (CRR) for S3 buckets","tags":[],"description":"","content":"Amazon S3 replication enables automatic, asynchronous copying of objects across Amazon S3 buckets. Buckets that are configured for object replication can be owned by the same AWS account or by different accounts. You can copy objects between different AWS Regions or within the same Region. You will setup bi-directional replication between S3 buckets in two different regions, owned by the same AWS account.\nReplication is configured via rules. There is no rule for bi-directional replication. You will however setup a rule to replicate from the S3 bucket in the east AWS region to the west bucket, and you will setup a second rule to replicate going the opposite direction. These two rules will enable bi-directional replication across AWS regions.\n2.1 Setup rule #1 to replicate objects from east bucket to west bucket   Go to the Amazon S3 console   Click on the name of the east bucket\n if you used Ohio the name will be \u0026lt;your_naming_prefix\u0026gt;-crrlab-us-east-2    Click on the Management tab (Step A in screenshot)\n  Click Replication (Step B in screenshot)\n  Click + Add Rule (Step C in screenshot)\n  For Set source select Entire bucket\n  For Replication criteria leave Replicate objects encrypted with AWS KMS not selected\n Our objects are encrypted using server-side encryption However since you used SSE-S3 encryption, you do not need to select this option and do not need to provide a KMS key SSE-S3 uses KMS keys, but these managed by Amazon S3 for the user For more detail see What Does Amazon S3 Replicate?     Click Next\n  For Destination bucket leave Buckets in this account selected, and select the name of the west bucket from the drop-down\n If you used Oregon the name will be \u0026lt;your_naming_prefix\u0026gt;-crrlab-us-west-2 Troubleshooting: If you get an error saying The bucket doesnt have versioning enabled then you have chosen the wrong bucket. Double check the bucket name.    Click Next\n  For IAM Role select \u0026lt;your-naming-prefix\u0026gt;-S3-Replication-Role-us-east-2 from the search results box\n (If you chose a different region as your east region, then look for that region at the end of the IAM role name)    For Rule name enter east to west\n  Leave Status set to enabled\n  Click Next\n  Review the configuration\n  Click Save\n  The screen should say Replication configuration updated successfully. and display the Source, Destination, and Permissions of your replication rule\n2.2 Test replication rule #1 - replicate object from east bucket to west bucket To test this rule you will upload an object into the east bucket and observe that it is replicated into the west bucket. For this step you will need a test object:\n This is a file that you will upload into the east S3 bucket. It should not be too big, as this will increase the time to upload it from your computer. If you do not have a file to use, you can download this file .  Right-click and Save image as\u0026hellip;   Go to the Amazon S3 console , or if you are already there click on Amazon S3 in the upper left corner\n  Click on the name of the east bucket\n if you used Ohio the name will be \u0026lt;your_naming_prefix\u0026gt;-crrlab-us-east-2    Click on  Upload\n  Upload the file you will use as an object\n Drag and drop the file or click Add files Click Upload (note there is a Next button, but you do not need to click it)    When the file is finished uploading, click on the filename\n It will look like the left side of the screenshot below If Replication status is PENDING, wait and refresh until it says COMPLETED which should be just a few seconds.    At the top of the console click on Amazon S3 and then click on the name of the west bucket\n If you used Oregon the name will be \u0026lt;your_naming_prefix\u0026gt;-crrlab-us-west-2    Click on the filename of the file that you just uploaded to the other bucket (yes, it is here now too!)\n It will look like the right side of the screenshot below    Note the following in from the object details:\n Replication status: Note the different values for the source (east) and destination (west) S3 buckets. The value REPLICA in the west bucket is part of the solution how the system recognizes it should not replicate this object back again to the east bucket, which would cause an infinite loop. Server-side encryption: The object was encrypted in the source (east) bucket, and remains encrypted in the destination (west) bucket.    2.3 Setup rule #2 to replicate objects from west bucket to east bucket After setting up the second rule, you will have completed configuration of bi-directional replication between our two Amazon S3 buckets.\n Go to the Amazon S3 console , or if you are already there click on Amazon S3 in the upper left corner Click on the name of the west bucket  if you used Oregon the name will be \u0026lt;your_naming_prefix\u0026gt;-crrlab-us-west-2   Click on the Management tab Click Replication Click + Add Rule For Set source select Entire bucket For Replication criteria leave Replicate objects encrypted with AWS KMS not selected  Our objects are encrypted using server-side encryption However since you used SSE-S3 encryption, you do not need to select this option and do not need to provide a KMS key SSE-S3 uses KMS keys, but these managed by Amazon S3 for the user For more detail see What Does Amazon S3 Replicate?    Click Next For Destination bucket leave Buckets in this account selected, and select the name of the east bucket from the drop-down  If you used Ohio the name will be \u0026lt;your_naming_prefix\u0026gt;-crrlab-us-east-2 Troubleshooting: If you get an error saying The bucket doesnt have versioning enabled then you have chosen the wrong bucket. Double check the bucket name.   Click Next For IAM Role select \u0026lt;your-naming-prefix\u0026gt;-S3-Replication-Role-us-west-2 from the search results box  (If you chose a different region as your west region, then look for that region at the end of the IAM role name)   For Rule name enter west to east Leave Status set to enabled Click Next Review the configuration Click Save  The screen should say Replication configuration updated successfully. and display the Source, Destination, and Permissions of your replication rule\n function prevStep(){ window.open(\"..\\/1_deploy_infra\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/3_test_replication\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/200_labs/200_automating_operations_with_playbooks_and_runbooks/2_configure_ecs_repository_and_deploy_application_stack/","title":"Configure ECS Respository and Deploy The Application Stack","tags":[],"description":"","content":"In this section, we are going to prepare our sample application API. Our API will be hosted inside docker containers orchestrated using Amazon Elastic Compute Service (ECS) with Application Load Balancer fronting it. The API will take 2 actions ; encrypt / decrypt. :\n encrypt action will allow the user to pass on a secret message along with it\u0026rsquo;s key identifier, and it will return a secret key id that they can use to decrypt. decrypt action will allow the user to pass the key identifier along with the secret key id to obtain the secret message encrypted before.  Both actions will make a write and read call to the RDS database where encrypted messages are being stored.\nIn preparation for the deployment, we will need to package our application as a docker image and push it into ECR . When this is completed, we will use the image which we placed in ECR to build our application cluster.\nFor more information on how ECS works, please refer to this guide .\nWhen our application stack is completed, our architecture will look like this:\nMove through the sections below to complete the repository configuration and application stack deployment:\n2.1. Configure the ECS Container Repository. Our sample application preparation will require running several docker commands to create a local image in your computer which we will push into Amazon ECR. The following diagram shows the image creation process:\nTo make this process simple for you, we have created a basic application and script to build the container.\nComplete the instructions as follows to download the application and deploy them to the repository:\nYou can either execute the following commands from your own laptop, or follow the steps using AWS Cloud9. If you are running this from your own machine, please ensure to have Docker version 18.09.9 or above installed.\n 2.1.1. From the main console, launch the Cloud9 service.\nWhen you get to the welcome screen, select Create Environment as shown here:\n2.1.2. Now we will enter naming details for the environment. To do this enter the following into the name environment dialog box:\nName: waopslab-environment Description: Well Architected Operations Lab\nWhen you are ready, click on Next Step to continue as shown:\n2.1.4. On the Configure Settings dialog box, leave defaults and click Next Step\n2.1.5. On the Review dialog box, click Create Environment\nThe Cloud9 IDE environment will now build, integrating the AWS command line and all docker components that we require to build out our lab.\nThis step can take a few minutes, so please be patient.\n2.1.6. Once our environment is built, you will be greeted with a command prompt to your environment.\nWe will use this to build our application for upload to the repository.\nFirstly we will need to download the files which contain all of the application dependencies. To do this, run the following command within the Cloud9 IDE:\ncurl -o sample_app.zip https://github.com/stephensalim/walabs-opsexcellence-labs/raw/main/sample_app.zip The command should show the file download as follows:\n2.1.7. When you have downloaded the application, unzip it as follows:\nunzip sample_app.zip 2.1.8. Now we will build our application and upload to the repository. We have built a script to help you with this process, which will query the previous CloudFormation stack which you created for the necessary repository information, build an image and then upload to the new repository.\nExecute the script with the argument of waopslab-base-vpc and v1 as follows:\n./build-container.sh waopslab-base-vpc v1 Once your command runs successfully, you should be seeing the image being pushed to ECR and URI marked as shown here:\nTake note of the ECS Image URI produced at the end of the script as we will require it later. This is highlighted in the screenshot above.\n 2.1.9. Confirm that the ECR repository exists in the ECR console. To do this, launch ECR in your AWS Console.\nYou can then follow this guide to check to your repository as shown:\n2.2. Deploy The Application Stack Now that we have pushed the docker image into our Amazon ECR repository, we will now deploy it within Amazon ECS .\nOur sample application is configured as follows:\n The service will expose a REST API wth /encrypt and /decrypt action. The /encrypt will take an input of a JSON payload with key and value as below '{\u0026quot;Name\u0026quot;:\u0026quot;Bob\u0026quot;,\u0026quot;Text\u0026quot;:\u0026quot;Run your operations as code\u0026quot;}' The Name Key will be the identifier that we will use to store the encrypted value of Text Value. The application will then call the KMS Encrypt API and encrypt it again using a KMS key that we designate. (For simplicity, in this mock app we will be using the same KMS key for every Name you put in, ideally you want to use individual key for each name) The encrypted value of Text key will then be stored in an RDS database, and the app will return a Encryption Key value that the user will have to pass on to decrypt the Text later The decrypt API will do the reverse, taking the Encryption Key you pass to decrypt the text {\u0026quot;Text\u0026quot;:\u0026quot;Run your operations as code\u0026quot;}  Note: In this section we will be deploying a CloudFormation Stack which will launch an ECS cluster. If this is the first time you are working with the ECS service, you will need to deploy a service linked role which will be able to assume the IAM role to perform the required activities within your account. To do this, run the following from the command line using appropriate profile flags: aws iam create-service-linked-role --aws-service-name ecs.amazonaws.com \n Download the application template from here and deploy according to your preference below.\n  Click here for CloudFormation command-line deployment steps   Command Line: To deploy from the command line, ensure that you have installed and configured AWS CLI with the appropriate credentials.\n2.2.1. Execute below command to create the application stack. Ensure that you pass the ECR Image URI you noted at the end of section 1.2 as follows:\naws cloudformation create-stack --stack-name waopslab-base-app \\ --template-body file://base-app.yml \\ --parameters ParameterKey=BaselineVpcStack,ParameterValue=waopslab-base-vpc \\ ParameterKey=ECRImageURI,ParameterValue=\u0026lt;ECR Image URI\u0026gt; \\ ParameterKey=NotificationEmail,ParameterValue=testyser@domain.com \\ --capabilities CAPABILITY_NAMED_IAM \\ --tags Key=Application,Value=OpsExcellence-Lab Note: Our example below shows sample arguments passed into the command for your reference:\naws cloudformation create-stack --stack-name waopslab-base-app \\ --template-body file://base-app.yml \\ --parameters ParameterKey=BaselineVpcStack,ParameterValue=waopslab-base-vpc \\ ParameterKey=ECRImageURI,ParameterValue=111111111111.dkr.ecr.region.amazonaws.com/pattern1appcontainerrepository-cu9vft86ml5e:latest \\ ParameterKey=NotificationEmail,ParameterValue=testyser@domain.com \\ --capabilities CAPABILITY_NAMED_IAM \\ --tags Key=Application,Value=OpsExcellence-Lab      Click here for CloudFormation console deployment steps   Console: If you decide to deploy the stack from the console, ensure that you follow below requirements \u0026amp; step:\n2.2.1. Follow this guide for information on how to deploy the cloudformation template via the console.\n2.2.2. Enter the following details into the stack details:\n Use waopslab-base-app as the Stack Name. Use waopslab-base-vpc as the BaselineVpcStack. Use the URI which you recorded in the application build as the ECRImageURI Enter an email address you would like to receive notification about this Application as the NotificationEmail  An example would be as follows:\nWhen you are ready, click next to continue.\n2.2.3. On the Configure Stack Options place in a Tag with Key Application and OpsExcellence-Lab for it\u0026rsquo;s value as per screenshot below\nclick Next\n2.2.4. On the Review waopslab-base-app click Create Stack.\nNote Don\u0026rsquo;t forget to tick the Capabilities acknowledgement at the bottom of the screen.\n  2.3. Confirm Stack Status. 2.3.1. Once the command deployed successfully, go to your Cloudformation console to locate the stack named waopslab-base-app.\n2.3.2. Confirm that the stack is in a \u0026lsquo;CREATE_COMPLETE\u0026rsquo; state.\n2.3.3. Record the following output details as they will be required later:\n Take note of this stack name Take note of the DNS value specified under OutputPattern1ApplicationEndpoint of the Outputs.  The following diagram shows the output from the cloudformation stack:\n2.3.4. Check for an email on the address you\u0026rsquo;ve specified, in NotificationEmail parameter. Click confirm subscription to start confirm subscription to the application alarm.\n2.4. Test the Application launched. In this part of the Lab, we will be testing the encrypt API of the sample application we just deployed. Our application will basically take a JSON payload with Name and Text key, and it will encrypt the value under text key with a designated KMS key. Once the text is encrypted, it will store the encrypted text in the RDS database with the Name as the primary key.\nNote: For simplicity our sample application is not generating individual KMS keys for each record generated. Should you wish to deploy this pattern to production, we recommend that you use a separate KMS key for each record.\n From your Cloud9 terminal, replace the \u0026lt; Application Endpoint URL \u0026gt; with the OutputPattern1ApplicationEndpoint from previous step.\nALBURL=\u0026quot;\u0026lt; Application Endpoint URL \u0026gt;\u0026quot; curl --header \u0026quot;Content-Type: application/json\u0026quot; --request POST --data '{\u0026quot;Name\u0026quot;:\u0026quot;Bob\u0026quot;,\u0026quot;Text\u0026quot;:\u0026quot;Run your operations as code\u0026quot;}' $ALBURL/encrypt Once you\u0026rsquo;ve executed this you should see an output similar to this:\n{\u0026quot;Message\u0026quot;:\u0026quot;Data encrypted and stored, keep your key save\u0026quot;,\u0026quot;Key\u0026quot;:\u0026quot;\u0026lt;encrypt key (take note) \u0026gt;\u0026quot;} Take note of the encrypt key value under Key and place it under the same Key section in the command below to test the decrypt API.\ncurl --header \u0026quot;Content-Type: application/json\u0026quot; --request GET --data '{\u0026quot;Name\u0026quot;:\u0026quot;Bob\u0026quot;,\u0026quot;Key\u0026quot;:\u0026quot;\u0026lt;encrypt key (taken from previous command ) \u0026gt;\u0026quot;}' $ALBURL/decrypt If the the application is functioning as it should, then you should see response like below\n{\u0026quot;Text\u0026quot;:\u0026quot;Run your operations as code\u0026quot;} This completes section 2 of the lab. Proceed to section 3 to continue with the lab.\n function prevStep(){ window.open(\"..\\/1_deploy_the_lab_network_infrastructure\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/3_simulate_application_issue\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     END OF SECTION 2\n "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_autonomous_monitoring_of_cryptographic_activity_with_kms/2_configure_ecs_repository_and_deploy_application_stack/","title":"Configure ECS Respository and Deploy The Application Stack","tags":[],"description":"","content":"2.0. Introduction. In this section, we are going to prepare our sample application. We will package this as a docker image and push to a repository.\nAs we mentioned in our introduction, our sample application will be running in a docker container which we will deploy using Amazon ECS .\nIn preparation for the deployment, we will need to package our application as a docker image and push it into ECR . When this is completed, we will use the image which we placed in ECR to build our application cluster.\nFor more information on how ECS works, please refer to this guide .\nWhen our application stack is completed, our architecture will look like this:\nMove through the sections below to complete the repository configuration and application stack deployment:\n2.1. Configure the ECS Container Repository. Our initial sample application preparation will require running several docker commands to create a local image in your computer which we will push into Amazon ECR. The following diagram shows the image creation process:\nTo make this process simple for you, we have created a basic application and script to build the container for you.\nComplete the instructions as follows to download the :\n2.1.1. Our environment used to push the container to the repository will need to be running Docker version 18.09.9 or above. To achieve this on a laptop such as a mac involves installing not only Docker itself, but also the Docker Machine and VirtualBox. Because of this, we will use AWS Cloud9 IDE in the console which has all of the dependencies pre-configured for you.\n2.1.2. From the main console, launch the Cloud9 service.\nWhen you get to the welcome screen, select Create Environment as shown here:\n2.1.3. Now we will enter naming details for the environment. To do this enter the following into the name environment dialog box:\nName: pattern1-environment Description: re:Invent 2020 pattern1 environment!\nWhen you are ready, click on Next Step to continue as shown:\n2.1.4. On the Configure Settings dialog box, leave defaults and click Next Step\n2.1.5. On the Review dialog box, click Create Environment\nThe Cloud9 IDE environment will now build, integrating the AWS command line and all docker components that we require to build out our lab.\nThis step can take a few minutes, so please be patient.\n2.1.6. Once our environment is built, you will be greeted with a command prompt to your environment.\nWe will use this to build our application for upload to the repository.\nFirstly we will need to download the files which contain all of the application dependencies. To do this, run the following command within the Cloud9 IDE:\ncurl -o sample-app.zip https://d3h9zoi3eqyz7s.cloudfront.net/Security/sample_app.zip The command should show the file download as follows:\n2.1.7. When you have downloaded the application, unzip it as follows:\nunzip sample-app.zip 2.1.8. Now we will build our application and upload to the repository. We have built a script to help you with this process, which will query the previous CloudFormation stack which you created for the necessary repository information, build an image and then upload to the new repository.\nExecute the script with the argument of pattern1-base as follows:\ncd app/ ./build-container.sh pattern1-base Once your command runs successfully, you should be seeing the image being pushed to ECR and URI marked as shown here:\nTake note of the ECS Image URI produced at the end of the script as we will require it later. This is highlighted in the screenshot above.\n 2.1.9. Confirm that the ECR repository exists in the ECR console. To do this, launch ECR in your AWS Console.\nYou can then follow this guide to check to your repository as shown:\n2.2. Deploy The Application Stack Now that we have pushed the docker image into our Amazon ECR repository, we will now deploy it within Amazon ECS .\nOur sample application is configured as follows:\n Our application is built using nodejs express ( You can find the source code under app/app.js file of the github repository ) The service will expose a REST API wth /encrypt and /decrypt action. The /encrypt will take an input of a JSON payload with key and value as below '{\u0026quot;Name\u0026quot;:\u0026quot;Andy Jassey\u0026quot;,\u0026quot;Text\u0026quot;:\u0026quot;Welcome To ReInvent 2020!\u0026quot;}' The Name Key will be the identifier that we will use to store the encrypted value of Text Value. The application will then call the KMS Encrypt API and encrypt it again using a KMS key that we designate. (For simplicity, in this mock app we will be using the same KMS key for every Name you put in, ideally you want to use individual key for each name) The encrypted value of Text key will then be stored in an RDS database, and the app will return a Encryption Key value that the user will have to pass on to decrypt the Text later The decrypt API will do the reverse, taking the Encryption Key you pass to decrypt the text {\u0026quot;Text\u0026quot;:\u0026quot;Welcome To ReInvent 2020!\u0026quot;}  Note: In this section we will be deploying a CloudFormation Stack which will launch an ECS cluster. If this is the first time you are working with the ECS service, you will need to deploy a service linked role which will be able to assume the IAM role to perform the required activities within your account. To do this, run the following from the command line using appropriate profile flags: aws iam create-service-linked-role --aws-service-name ecs.amazonaws.com \n Download the application template from here and deploy according to your preference below.\n  Click here for CloudFormation command-line deployment steps   Command Line: To deploy from the command line, ensure that you have installed and configured AWS CLI with the appropriate credentials.\n2.2.1. Execute below command to create the application stack. Ensure that you pass the ECR Image URI you noted at the end of section 1.2 as follows:\naws cloudformation create-stack --stack-name pattern1-app \\ --template-body file://pattern1-app.yml \\ --parameters ParameterKey=BaselineVpcStack,ParameterValue=pattern1-base \\ ParameterKey=ECRImageURI,ParameterValue=\u0026lt;ECR Image URI\u0026gt; \\ --capabilities CAPABILITY_NAMED_IAM \\ --region ap-southeast-2 Note: Our example below shows sample arguments passed into the command for your reference:\naws cloudformation create-stack --stack-name pattern1-app \\ --template-body file://pattern1-app.yml \\ --parameters ParameterKey=BaselineVpcStack,ParameterValue=pattern1-base \\ ParameterKey=ECRImageURI,ParameterValue=111111111111.dkr.ecr.ap-southeast-2.amazonaws.com/pattern1appcontainerrepository-cu9vft86ml5e:latest \\ --capabilities CAPABILITY_NAMED_IAM \\ --region ap-southeast-2      Click here for CloudFormation console deployment steps   Console: If you decide to deploy the stack from the console, ensure that you follow below requirements \u0026amp; step:\n2.2.1. Follow this guide for information on how to deploy the cloudformation template via the console.\n2.2.2. Enter the following details into the stack details:\n Use pattern1-app as the Stack Name. Use pattern1-base as the BaselineVpcStack. Use the URI which you recorded in the application build as the ECRImageURI  An example would be as follows:\nWhen you are ready, click next to continue.\n2.2.3. On the Configure Stack Options click Next\n2.2.4. On the Review pattern1-app click Create Stack.\nNote Dont forget to tick the Capabilities acknowledgement at the bottom of the screen.\n  2.3. Confirm Stack Status. 2.3.1. Once the command deployed successfully, go to your Cloudformation console to locate the stack named pattern1-app.\n2.3.2. Confirm that the stack is in a \u0026lsquo;CREATE_COMPLETE\u0026rsquo; state.\n2.3.3. Record the following output details as they will be required later:\n Take note of this stack name Take note of the DNS value specified under OutputPattern1ApplicationEndpoint of the Outputs. Take note of the ECS Task Role Arn value specified under OutputPattern1ECSTaskRole of the Outputs. Take note of the OutputPattern1ECSTaskRole.  The following diagram shows the output from the cloudformation stack:\n2.4. Test the Application launched. In this part of the Lab, we will be testing the encrypt API of the sample application we just deployed. Our application will basically take a JSON payload with Name and Text key, and it will encrypt the value under text key with a designated KMS key. Once the text is encrypted, it will store the encrypted text in the RDS database with the Name as the primary key.\nNote: For simplicity our sample application is not generating individual KMS keys for each record generated. Should you wish to deploy this pattern to production, we recommend that you use a separate KMS key for each record.\n From your Cloud9 terminal, replace the \u0026lt; Application Endpoint URL \u0026gt; with the OutputPattern1ApplicationEndpoint from previous step.\nALBURL=\u0026quot;\u0026lt; Application Endpoint URL \u0026gt;\u0026quot; curl --header \u0026quot;Content-Type: application/json\u0026quot; --request POST --data '{\u0026quot;Name\u0026quot;:\u0026quot;Andy Jassey\u0026quot;,\u0026quot;Text\u0026quot;:\u0026quot;Welcome to ReInvent 2020!\u0026quot;}' $ALBURL/encrypt Once you\u0026rsquo;ve executed this you should see an output similar to this:\n{\u0026quot;Message\u0026quot;:\u0026quot;Data encrypted and stored, keep your key save\u0026quot;,\u0026quot;Key\u0026quot;:\u0026quot;\u0026lt;encrypt key (take note) \u0026gt;\u0026quot;} Take note of the encrypt key value under Key from your output as we will need it for decryption later in the lab.\nThis completes section 2 of the lab. Proceed to section 3 where we will be configuring CloudTrail.\n END OF SECTION 2\n "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_testing_for_resiliency_of_ec2/2_configure_env/","title":"Configure Execution Environment","tags":[],"description":"","content":"Failure injection is a means of testing resiliency by which a specific failure type is simulated on a service and its response is assessed.\nYou have a choice of environments from which to execute the failure injections for this lab. Bash scripts are a good choice and can be used from a Linux command line. If you prefer Python, Java, Powershell, or C# instructions for these are also provided.\n2.1 Setup AWS credentials and configuration Your execution environment needs to be configured to enable access to the AWS account you are using for the workshop. This includes\n  Credentials\n AWS access key AWS secret access key AWS session token (used in some cases)    Configuration\n Region: us-east-2 (or region where you deployed your WebApp) Default output: JSON    Note: us-east-2 is the Ohio region\n If you already know how to configure these, please do so now and proceed to the next step 2.2 Set up the bash environment  If you need help then follow the instructions in either Option A or Option B below  Option A - AWS CLI This option uses the AWS CLI. If you do not have this installed, or do not want to install it, then use Option B\n  To see if the AWS CLI is installed:\n $ aws --version aws-cli/1.16.249 Python/3.6.8...   AWS CLI version 1.1 or higher is fine If you instead got command not found then either install the AWS CLI or use Option B    Run aws configure and provide the following values:\n $ aws configure AWS Access Key ID [*************xxxx]: \u0026lt;Your AWS Access Key ID\u0026gt; AWS Secret Access Key [**************xxxx]: \u0026lt;Your AWS Secret Access Key\u0026gt; Default region name: [us-east-2]: us-east-2 (or your chosen region) Default output format [None]: json    Option B - Manually creating credential files If you already did Option A, then skip this\n  create a .aws directory under your home directory\n mkdir ~/.aws    Change directory to there\n cd ~/.aws    Use a text editor (vim, emacs, notepad) to create a text file (no extension) named credentials. In this file you should have the following text.\n [default] aws_access_key_id = \u0026lt;Your access key\u0026gt; aws_secret_access_key = \u0026lt;Your secret key\u0026gt;    Create a text file (no extension) named config. In this file you should have the following text:\n [default] region = us-east-2 (or your chosen region) output = json    2.2 Set up the bash environment   Click here for instructions if using bash:   Using bash is an effective way to execute the failure injection tests for this workshop. The bash scripts make use of the AWS CLI. If you will be using bash, then follow the directions in this section. If you cannot use bash, then skip to the next section .\n  Prerequisites\n  awscli AWS CLI installed\n  If you already installed AWS CLI as part of the AWS credentials and configuration setup, you can skip this and proceed to installing jq\n $ aws --version aws-cli/1.16.249 Python/3.6.8...    Version 1.1 or higher is fine\n  If you instead got command not found then see instructions here to install awscli     jq command-line JSON processor installed.\n $ jq --version jq-1.5-1-a5b5cbe   Version 1.4 or higher is fine If you instead got command not found then see instructions here to install jq       Download the fail_instance.sh script from the resiliency bash scripts on GitHub to a location convenient for you to execute it. You can use the following link to download the script:\n bash/fail_instance.sh     Set the script to be executable.\n chmod u+x fail_instance.sh      2.3 Set up the programming language environment (for Python, Java, C#, or PowerShell) Choose the appropriate section below for your language\n   Click here for instructions if using Python:     The scripts are written in python with boto3. On Amazon Linux, this is already installed. Use your local operating system instructions to install boto3: https://github.com/boto/boto3\n  Download the fail_instance.py from the resiliency Python scripts on GitHub to a location convenient for you to execute it. You can use the following link to download the script:\n python/fail_instance.py         Click here for instructions if using Java:     The command line utility in Java requires Java 8 SE.\n $ java -version openjdk version \u0026quot;1.8.0_222\u0026quot; OpenJDK Runtime Environment (build 1.8.0_222-8u222-b10-1ubuntu1~18.04.1-b10) OpenJDK 64-Bit Server VM (build 25.222-b10, mixed mode)    If you have java 1.7 installed (as will be the case for In Amazon Linux), you need to install Java 8 and remove Java 7.\n  For Amazon Linux and RedHat\n$ sudo yum install java-1.8.0-openjdk $ sudo yum remove java-1.7.0-openjdk    For Debian, Ubuntu\n$ sudo apt install openjdk-8-jdk $ sudo apt install openjdk-7-jdk    Next choose one of the following options: Option A or Option B\n    Option A: If you are comfortable with git\n  Clone the aws-well-architected-labs repo\n $ git clone https://github.com/awslabs/aws-well-architected-labs.git Cloning into 'aws-well-architected-labs'... ... Checking out files: 100% (1935/1935), done.    go to the build directory\n cd static/Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Code/FailureSimulations/java/appresiliency      Option B:\n Download the zipfile of the executables at the following URL https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/javaresiliency.zip go to the build directory: cd java/appresiliency    Build: mvn clean package shade:shade\n  cd target - this is where your jar files were built and where you can run from the command line\n      Click here for instructions if using C#:     Download the zipfile of the executables at the following URL. https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/csharpresiliency.zip   Unzip the folder in a location convenient for you to execute the command line programs.\n      Click here for instructions if using PowerShell:     If you do not have the AWS Tools for Powershell, download and install them following the instructions here. https://aws.amazon.com/powershell/\n  Download the fail_instance.sh script from the resiliency PowerShell scripts on GitHub to a location convenient for you to execute it. You can use the following link to download the script:\n powershell/fail_instance.sh     If your PowerShell script is refused authorization to access your AWS account, consult Getting Started with the AWS Tools for Windows PowerShell    \n function prevStep(){ window.open(\"..\\/1_prerequisite\", \"_self\") } function nextStep(){ window.open(\"..\\/3_failure_injection\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/2_configure_env/","title":"Configure Execution Environment","tags":[],"description":"","content":"Failure injection is a means of testing resiliency by which a specific failure type is simulated on a service and its response is assessed.\nYou have a choice of environments from which to execute the failure injections for this lab. Bash scripts are a good choice and can be used from a Linux command line. If you prefer Python, Java, Powershell, or C#, then instructions for these are also provided.\n2.1 Setup AWS credentials and configuration Your execution environment needs to be configured to enable access to the AWS account you are using for the workshop. This includes\n  Credentials - You identified these credentials back in step 1  AWS access key AWS secret access key AWS session token (used in some cases)    Configuration\n Region: us-east-2 Default output: JSON    Note: us-east-2 is the Ohio region\nIf you already know how to configure these, please do so now. If you need help or if you are planning to use PowerShell for this lab, then follow these instructions 2.2 Set up the bash environment   Click here for instructions if using bash:   Using bash is an effective way to execute the failure injection tests for this workshop. The bash scripts make use of the AWS CLI. If you will be using bash, then follow the directions in this section. If you cannot use bash, then skip to the next section .\n  Prerequisites\n  awscli AWS CLI installed\n $ aws --version aws-cli/1.16.249 Python/3.6.8...   Version 1.1 or higher is fine If you instead got command not found then see instructions here to install awscli     jq command-line JSON processor installed.\n $ jq --version jq-1.5-1-a5b5cbe   Version 1.4 or higher is fine If you instead got command not found then see instructions here to install jq       Download the resiliency bash scripts from GitHub to a location convenient for you to execute them. You can use the following links to download the scripts:\n bash/fail_instance.sh  bash/failover_rds.sh  bash/fail_az.sh     Set the scripts to be executable.\n chmod u+x fail_instance.sh chmod u+x failover_rds.sh chmod u+x fail_az.sh      2.3 Set up the programming language environment (for Python, Java, C#, or PowerShell) Choose the appropriate section below for your language\n   Click here for instructions if using Python:     The scripts are written in python with boto3. On Amazon Linux, this is already installed. Use your local operating system instructions to install boto3: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html#installation\n  Download the resiliency Python scripts from GitHub to a location convenient for you to execute them. You can use the following links to download the scripts:\n python/fail_instance.py  python/fail_rds.py  python/fail_az.py         Click here for instructions if using Java:     The command line utility in Java requires Java 8 SE.\n $ java -version openjdk version \u0026quot;1.8.0_222\u0026quot; OpenJDK Runtime Environment (build 1.8.0_222-8u222-b10-1ubuntu1~18.04.1-b10) OpenJDK 64-Bit Server VM (build 25.222-b10, mixed mode)    If you have java 1.7 installed (as will be the case for In Amazon Linux), you need to install Java 8 and remove Java 7.\n  For Amazon Linux and RedHat\n$ sudo yum install java-1.8.0-openjdk $ sudo yum remove java-1.7.0-openjdk    For Debian, Ubuntu\n$ sudo apt install openjdk-8-jdk $ sudo apt install openjdk-7-jdk    Next choose one of the following options: Option A or Option B.\n    Option A: If you are comfortable with git\n  Clone the aws-well-architected-labs repo\n $ git clone https://github.com/awslabs/aws-well-architected-labs.git Cloning into 'aws-well-architected-labs'... ... Checking out files: 100% (1935/1935), done.    go to the build directory\n cd aws-well-architected-labs/Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Code/FailureSimulations/java/appresiliency      Option B:\n Download the zipfile of the executables at the following URL https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/javaresiliency.zip go to the build directory: cd java/appresiliency    Build: mvn clean package shade:shade\n  cd target - this is where your jar files were built and where you can run from the command line\n      Click here for instructions if using C#:     Download the zipfile of the executables at the following URL. https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/csharpresiliency.zip   Unzip the folder in a location convenient for you to execute the command line programs.\n      Click here for instructions if using PowerShell:     If you do not have the AWS Tools for Powershell, download and install them following the instructions here. https://aws.amazon.com/powershell/\n  Download the resiliency PowerShell scripts from GitHub to a location convenient for you to execute them. You can use the following links to download the scripts:\n powershell/fail_instance.ps1  powershell/failover_rds.ps1  powershell/fail_az.ps1     If your PowerShell scripts are refused authorization to access your AWS account, consult Getting Started with the AWS Tools for Windows PowerShell    \n function prevStep(){ window.open(\"..\\/1_deploy_infra\", \"_self\") } function nextStep(){ window.open(\"..\\/3_failure_injection_prep\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_1_aws_account_setup/2_account_structure/","title":"Create an account structure","tags":[],"description":"","content":"NOTE: Do NOT do this step if you already have an organization and consolidated billing setup.\nYou will create an AWS Organization, and join two or more accounts to the management account. An organization will allow you to centrally manage multiple AWS accounts efficiently and consistently. It is recommended to have a management account that is used for security and administration, with access provided for limited billing tasks. A dedicated member account will be created for the Cost Optimization team or function, and another (or multiple) member account/s created to contain workload resources.\nYou will need organizations:CreateOrganization access, and 2 or more AWS accounts. When you join a member account to a management account, it will contain all billing information for that member account. Member accounts will no longer have any billing information, including historical billing information. Ensure you backup or export any reports or data before joining accounts to a management account.\nCreate an AWS Organization You will create an AWS Organization with the management account.\n  Login to the AWS console as an IAM user with the required permissions, start typing AWS Organizations into the Find Services box and click on AWS Organizations:   Click on Create organization:   To create a fully featured organization, Click on Create organization   You will receive a verification email, click on Verify your email address to verify your account:   You will then see a verification message in the console for your organization:   You now have an organization that you can join other accounts to.\nJoin member accounts You will now join other accounts to your organization. You need to create and join an account that will be used to perform Cost Optimization work, as well as other member accounts used to run workloads.\n  From the AWS Organizations console click on Add account:   Click on Invite account:   Enter in the Email or account ID, enter in any relevant Notes and click Invite:   You will then have an open request:   Log in to your member account, and go to AWS Organizations:   You will see an invitation in the menu, click on Invitations:   Verify the details in the request (they are blacked out here), and click on Accept:   Verify the Organization ID (blacked out here), and click Confirm:   You are shown that the account is now part of your organization:   The member account will receive an email showing success:   The management account will also receive email notification of success:   Repeat the steps above for each additional member account in your organization.\nEnable Service Control Policies We will enable Service control policies, which offer central control over the maximum permissions available - for cost governance, and Tag policies which assist to standardize tags across your organization.\n  From the AWS Organizations console in the management account click on Policies:   By default both policies are disabled, Click on Service control policies:   Click on Enable service control policies:   You will see Service control policies are enabled, click Policies:   We will enable tag policies, Click Tag policies:   Click Enable tag policies:   You will see Tag policies are enabled, click Policies:   You will now see that both policies are enabled:    function prevStep(){ window.open(\"..\\/1_iam_access\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/3_cur\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_2_cost_and_usage_governance/2_ec2_restrict_region/","title":"Create an IAM Policy to restrict service usage by region","tags":[],"description":"","content":"To manage costs you need to manage and control your usage. AWS offers multiple regions, so depending on your business requirements you can limit access to AWS services depending on the region. This can be used to ensure usage is only allowed in specific regions which are more cost effective, and minimize associated usage and cost, such as data transfer.\nWe will create a policy that allows all EC2, RDS and S3 access in a single region only. NOTE: it is best practice to provide only the minimum access required, the policy used here is for brevity and simplicity, and should only be implemented as a demonstration before being removed.\nCreate the Region restrictive IAM Policy   Go to the IAM service page:   Select Policies from the left menu:   Click Create Policy:   Click the JSON tab:   Copy and paste the policy into the console:   IAM Policy   { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;ec2:*\u0026quot;, \u0026quot;rds:*\u0026quot;, \u0026quot;s3:*\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;Condition\u0026quot;: {\u0026quot;StringEquals\u0026quot;: {\u0026quot;aws:RequestedRegion\u0026quot;: \u0026quot;us-east-1\u0026quot;}} } ] }   \n  Click Review policy:   Create the policy with the following details:\n Name: RegionRestrict Description: EC2, RDS, S3 access in us-east-1 only Click Create policy:     You have successfully created an IAM policy to restrict usage by region.\n Apply the policy to your test group   Select Groups from the left menu:   Click on the CostTest group (created previously):   Select the Permissions tab:   Click Attach Policy:   Click Policy Type and select Customer Managed:   Select the checkbox next to Region_Restrict (created above) and click Attach Policy:   You have successfully attached the policy to the CostTest group.\n Log out from the console\n Verify the policy is in effect   Logon to the console as the TestUser1 user, go to the EC2 Service dashboard:   Click the current region in the top right, and select US West (N.California):   Try to launch an instance by clicking Launch Instance, select Launch Instance:   Click on Select next to the Amazon Linux 2 AMI, You will receive an error when you select an AMI as you do not have permissions:   You have successfully verified that you cannot launch any instances outside of the N.Virginia region. We will now verify we have access in us-east-1 (N.Virginia):\n Change the region by clicking the current region, and selecting US East (N.Virginia):   Now attempt to launch an instance, choose the Amazon Linux 2 AMI, leave 64-bit (x86) selected, click Select:   Scroll down and select a c5.large, and click Review and Launch:   Take note of the security group created (as you need to delete it), Click Launch:   Select Proceed without a key pair, and click I acknowledge.. checkbox, and click Launch Instances:   You will get a success message, click on the instance id:   Ensure the correct instance is selected, click Actions, then Instance State, then Terminate:   Confirm the instance ID is correct, click Yes, Terminate:   Log out of the console as TestUser1.\n  You have successfully implemented an IAM policy that restricts all EC2, RDS and S3 operations to a single region. You have also successfully launched a c5 Instance Type.\n  function prevStep(){ window.open(\"..\\/1_create_test_group\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/3_ec2_restrict_family\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_aws_resource_optimization/2_create_iamrole/","title":"Create an IAM Role to use with Amazon CloudWatch Agent","tags":[],"description":"","content":"Access to AWS resources requires permissions. You will now create an IAM role to grant permissions that the agent needs to write metrics to CloudWatch. Amazon created two new default policies called CloudWatchAgentServerPolicy and CloudWatchAgentAdminPolicy only for that purpose.\n  To create the IAM role first you will need to sign in to the AWS Management Console and open the IAM console   In the navigation pane on the left, choose Roles and then Create role.   Under Choose the service that will use this role, choose EC2 Allows EC2 instances to call AWS services on your behalf. Choose Next: Permissions.   In the list of policies, select the check box next to CloudWatchAgentServerPolicy. If necessary, use the search box to find the policy, click Next: Tags:\n    Add tags (optional) for this policy, click Next: Review.   Confirm that CloudWatchAgentServerPolicy appears next to Policies. In Role name, enter a name for the role, such as CloudWatchAgentServerRole. Optionally give it a description. Then click Create role.   The role is now created.\n function prevStep(){ window.open(\"..\\/1_cloudwatch_intro\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/3_attach_iamrole\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_2_cost_and_usage_governance/2_budget_ec2actual/","title":"Create and implement an AWS Budget for EC2 actual cost","tags":[],"description":"","content":"We will create a monthly EC2 actual cost budget, which will notify if the actual costs of EC2 instances exceeds the specified amount.\n  Click Create budget:   Select Cost budget, and click Set your budget \u0026gt;:   Create a cost budget, enter the following details:\n Name: EC2_actual Period: Monthly Budget effective dates: Recurring Budget Start Month: (current month) Budget amount: Fixed Budgeted amount: $1 (enter an amount a lot LESS than last months cost), Other fields: leave a defaults Under FILTERING click on Service:     Type Elastic in the search field, then select the checkbox next to EC2-Instances(Elastic Compute Cloud - Compute) and Click Apply filters:   De-select Upfront reservation fees, and click Configure alerts \u0026gt;:   Select:\n Send alert based on: Actual Costs Alert threshold: 100% of budgeted amount Email contacts: (your email address) Click on Confirm budget \u0026gt;:     Review the configuration, and click Create:   You can see the current amount exceeds the budget (it is red, you may need to refresh your browser):   You will receive an email similar to the previous budget within a few minutes.   You have created an actual cost budget for EC2 usage. You can extend this budget by adding specific filters such as linked accounts, tags or instance types. You can also create budgets for services other than EC2.\n  function prevStep(){ window.open(\"..\\/1_budget_forecast\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/3_budget_spcoverage\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_iam_permission_boundaries_delegating_role_creation/2_developer_role/","title":"Create and Test Developer Role","tags":[],"description":"","content":"2.1 Create Developer Role Create a role for developers that will have permission to create roles and policies, with the permission boundary and naming prefix enforced:\n Sign in to the AWS Management Console as an IAM user with MFA enabled that can assume roles in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . In the navigation pane, click Roles and then click Create role.  Click Another AWS account, then enter your account ID and tick Require MFA, then click Next: Permissions. We enforce MFA here as it is a best practice.  In the search field start typing createrole then check the box next to the createrole-restrict-region-boundary policy.  Erase your previous search and start typing iam-res then check the box next to the iam-restricted-list-read policy and then click Next: Tags.  For this lab we will not use IAM tags, click Next: Review. Enter the name of developer-restricted-iam for the Role name and click Create role.  Check the role you have created by clicking on developer-restricted-iam in the list. Record both the Role ARN and the link to the console. The role is now created, ready to test!  2.2. Test Developer Role Now you will use an existing IAM user with MFA enabled to assume the new developer-restricted-iam role.\n Sign in to the AWS Management Console as an IAM user with MFA enabled. https://console.aws.amazon.com . In the console, click your user name on the navigation bar in the upper right. It typically looks like this: username@account_ID_number_or_aliasthen click Switch Role. Alternatively you can paste the link in your browser that you recorded earlier. On the Switch Role page, type the account ID number or the account alias and the name of the role developer-restricted-iam that you created in the previous step. (Optional) Type text that you want to appear on the navigation bar in place of your user name when this role is active. A name is suggested, based on the account and role information, but you can change it to whatever has meaning for you. You can also select a color to highlight the display name. Click Switch Role. If this is the first time choosing this option, a page appears with more information. After reading it, click Switch Role. If you clear your browser cookies, this page can appear again.   The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you replacing the permission that you had as the IAM user.\nTip\nThe last several roles that you used appear on the menu. The next time you need to switch to one of those roles, you can simply click the role you want. You only need to type the account and role information manually if the role is not displayed on the Identity menu.\n  You are now using the developer role with the granted permissions, stay logged in using the role for the next section.\n  "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_basic_ec2_with_waf_protection/2_create_waf_rules/","title":"Create AWS WAF Rules","tags":[],"description":"","content":"2.1 AWS CloudFormation to create AWS WAF ACL for Application Load Balancer Using AWS CloudFormation , we are going to deploy a basic example AWS WAF configuration for use with Application Load Balancer.\n Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create New Stack. Select Specify an Amazon S3 template URL and enter the following URL for the template: https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/waf-regional.yaml and click Next. Enter the following details:   Stack name: The name of this stack. For this lab, use lab-waf-regional. WAFName: Enter the base name to be used for resource and export names for this stack. For this lab, you can use WAFLabReg. WAFCloudWatchPrefix: Enter the name of the CloudWatch prefix to use for each rule using alphanumeric characters only. For this lab, you can use WAFLabReg. The remainder of the parameters can be left as defaults.  Click Next. In this scenario, we won\u0026rsquo;t add any tags or other options. Click Next. Review the information for the stack. When you\u0026rsquo;re satisfied with the settings, click Create. After a few minutes, the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE. You have now set up a basic AWS WAF configuration ready for Application Load Balancer to use!  "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_iam_tag_based_access_control_for_ec2/2_create_role/","title":"Create Role","tags":[],"description":"","content":"Create a role for EC2 administrators, and attach the managed policies previously created.\n Sign in to the AWS Management Console as an IAM user with MFA enabled that can assume roles in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . In the navigation pane, click Roles and then click Create role.   Click Another AWS account, then enter the account ID of the account you are using now and tick Require MFA, then click Next: Permissions. We enforce MFA here as it is a best practice.   In the search field start typing ec2- then check the box next to the policies you just created: ec2-create-tags, ec2-create-tags-existing, ec2-list-read, ec2-manage-instances, ec2-run-instances. and then click Next: Tags.\n  For this lab we will not use IAM tags, click Next: Review. Enter the name of ec2-admin-team-alpha for the Role name and click Create role.  Check the role you have created by clicking on ec2-admin-team-alpha in the list. Record both the Role ARN and the link to the console. The role is now created, ready to test!  "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_lambda_cross_account_bucket_policy/2_create_role/","title":"Create role for Lambda in account 1","tags":[],"description":"","content":"  In account 1 sign in to the AWS Management Console as an IAM user or role in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/\n  Click Roles on the left, then create role\n  AWS service will be pre-selected, select Lambda, then click Next: Permissions\n  Do not select any managed policies, click Next: Tags\n  Click Next: Review\n  Enter Lambda-List-S3-Role for the Role name then click Create role\n  From the list of roles click the name of Lambda-List-S3-Role\n  Click Add inline policy, then click JSON tab\n  Replace the sample json with the following\n  Replace account1 with the AWS Account number (no dashes) of account 1\n  Replace bucketname with the S3 bucket name from account 2\n  Then click Review Policy\n { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;S3ListBucket\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;s3:ListBucket\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:s3:::bucketname\u0026quot; }, { \u0026quot;Sid\u0026quot;: \u0026quot;logsstreamevent\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;logs:CreateLogStream\u0026quot;, \u0026quot;logs:PutLogEvents\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:logs:us-east-1:account1:log-group:/aws/lambda/Lambda-List-S3*/*\u0026quot; }, { \u0026quot;Sid\u0026quot;: \u0026quot;logsgroup\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;logs:CreateLogGroup\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] }      Name this policy Lambda-List-S3-Policy, then click Create policy\n  "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_lambda_cross_account_iam_role_assumption/2_create_role_acct_1/","title":"Create role for Lambda in account 1","tags":[],"description":"","content":"  Sign in to the AWS Management Console as an IAM user or role in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ .\n  Click Roles on the left, then create role.\n  AWS service will be pre-selected, select Lambda, then click Next: Permissions.\n  Do not select any managed policies, click Next: Tags.\n  Click Next: Review.\n  Enter Lambda-Assume-Roles for the Role name then click Create role.\n  From the list of roles click the name of Lambda-Assume-Roles.\n  Copy the Role ARN and store for use later in this lab.\n  Click Add inline policy, then click JSON tab.\n  Replace the sample json with the following, replacing account1 and account2 with your respective account id\u0026rsquo;s, us-east-1 region with the region you are using, then click Review Policy.\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;stsassumerole\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:iam::account2:role/LambdaS3ListBuckets\u0026quot;, \u0026quot;Condition\u0026quot;: { \u0026quot;StringLike\u0026quot;: { \u0026quot;aws:UserAgent\u0026quot;: \u0026quot;*AWS_Lambda_python*\u0026quot; } } }, { \u0026quot;Sid\u0026quot;: \u0026quot;logsstreamevent\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;logs:CreateLogStream\u0026quot;, \u0026quot;logs:PutLogEvents\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:logs:us-east-1:account1:log-group:/aws/lambda/Lambda-Assume-Roles*/*\u0026quot; }, { \u0026quot;Sid\u0026quot;: \u0026quot;logsgroup\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;logs:CreateLogGroup\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] }    Name this policy Lambda-Assume-Roles-Policy, then click Create policy.\n  "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_automated_cur_query_and_email_delivery/2_create_bucket/","title":"Create S3 Bucket","tags":[],"description":"","content":"The first step is to create an S3 bucket which will hold the lambda code and also used for storage of the reports. NOTE: the bucket must be in the same region as the Lambda function, it is advised to use a single region for all resources within this lab.\nThis bucket will store the reports and Athena CUR query results. These will not be deleted, to enable historical reporting, so delete these periodically if you do not require them.\n Login via SSO, go to the s3 dashboard and create an S3 bucket in the required region:    function prevStep(){ window.open(\"..\\/1_architecture\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/3_iam_policy_and_role\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_workload_efficiency/2_efficiency_data/","title":"Create the efficiency data source","tags":[],"description":"","content":"We will now build the efficiency data source, by combining the application logs with the cost data. When using your own application logs, you need to look through the logs and discover what the application is doing, and capture the log messages that indicate its various outputs, and what could consume resources of the system - as this will potentially indicate cost and usage of the system. Things to look for:\n Successful requests: Valid requests that produce an output Unsuccessful requests: Requests that require processing and resources, but dont produce an output Errors: Do not procduce an output, but consume application resources Types of requests: Different requests may require different resourcing, and costs Data transfer: Similar to types of requests, the data going into a system may indicate processing requirements or cost  Review application logs We will use Athena to analyze the application logs, and discover the relevant data and fields.\n  Go into Athena console\n  Run the following query to see a sample of all data available:\n SELECT * FROM \u0026quot;webserverlogs\u0026quot;.\u0026quot;applogfiles_workshop\u0026quot; limit 10;    We can see interesting fields could be: request, response, bytes, as these indicate requests to the workload and could indicate the amount of processing the workload performs.\n  Run the following query to see the different types of requests:\n SELECT distinct request, count(*) FROM \u0026quot;webserverlogs\u0026quot;.\u0026quot;applogfiles_workshop\u0026quot; group by request order by count(*) desc limit 1000;    We can see there are: health checks, errors, image_file requests, index.html requests. Successful requests should make up most of the work and cost, however errors may also consume resources and costs.\n  Run the following query to see the different types of responses:\n SELECT distinct response, count(*) FROM \u0026quot;webserverlogs\u0026quot;.\u0026quot;applogfiles_workshop\u0026quot; group by response limit 100;    We can see most are 200 - success, but there are a lot of 400 series which are client errors. So there could be considerable load from errors on the workload.\n  Data transfer also contributes to cost, so lets look at bytes. A large total amount of bytes may come from small numbers of large byte requests, or large numbers of small byte requests. Lets look at the distribution and run the following query:\n SELECT distinct bytes, count(*) FROM \u0026quot;webserverlogs\u0026quot;.\u0026quot;applogfiles_workshop\u0026quot; group by bytes order by count(*) desc limit 100;    We have both lots of small requests (55, 91 bytes) and some large sized requests also.\n  We will choose the following fields for our efficiency data:\n Request Response Bytes    Review the Cost and Usage Reports We already know how to analyse the Cost and Usage reports, so lets use Athena to discover the relevant data and fields.\n  Go into Athena console\n  Run the following query to see a sample of all data available:\n SELECT * FROM \u0026quot;costusage\u0026quot;.\u0026quot;costusagefiles_workshop\u0026quot; limit 10;    We know we need the unblended cost, the usage date and time, and also ensure that it is only costs for this specific workload. We tagged our resources, so include the tag: user application = ordering\n  Run the following query to get a sample of our cost data for the application:\n SELECT line_item_usage_start_date, sum(try_cast(line_item_unblended_cost as double)) as cost FROM \u0026quot;costusage\u0026quot;.\u0026quot;costusagefiles_workshop\u0026quot; where resource_tags_user_application like 'ordering' group by line_item_usage_start_date limit 10    We now have our workload hourly costs, so lets combine that with our application logs and create an efficiency table.\n  Create the efficiency data source We will combine the application logs and the hourly cost data with a view, to get an efficiency data source. First we\u0026rsquo;ll create an hourly cost data set, then combine this with the application logs in another view.\n  Run the following query in Athena to create the hourly cost view:\n create view costusage.hourlycost as SELECT line_item_usage_start_date, sum(try_cast(line_item_unblended_cost as double)) as cost FROM \u0026quot;costusage\u0026quot;.\u0026quot;costusagefiles_workshop\u0026quot; where resource_tags_user_application like 'ordering' group by line_item_usage_start_date    Lets confirm its setup correctly \u0026amp; sample it, run the following query:\n select * from costusage.hourlycost    We can see the workload cost for every hour\n  We will combine the hourly cost table and the application log table using a union. This will basically copy the lines together in a single table. However, the columns wont match between the tables, so we will add NULL values where required. We will also divide the bytes by 1048576 to get a more readable MBytes value. Copy the following query into Athena to create the efficiency table:\n create view costusage.efficiency AS SELECT date_parse(concat(logdate, ' ', logtime), '%d/%b/%Y %H:%i:%S') as Datetime, request, response, try_cast(bytes as double)/1048576 as MBytes, NULL AS Cost from webserverlogs.applogfiles_workshop union select date_parse(line_item_usage_start_date, '%Y-%m-%d %H:%i:%s') as Datetime, NULL AS request, NULL AS response, NULL AS MBytes, Cost from costusage.hourlycost    Lets check our new efficiency table. Run the following query:\n SELECT * FROM \u0026quot;costusage\u0026quot;.\u0026quot;efficiency\u0026quot; order by datetime asc limit 100;    We have our efficiency data source:   The first line is from our cost table, note the NULL values for requets, response and MBytes. The remaining lines will be from our application logs, and contain the data we need to measure efficiency.\n function prevStep(){ window.open(\"..\\/1_data_sources\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/3_visualizations\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_5_cost_visualization/2_create_visualizations/","title":"Create visualizations","tags":[],"description":"","content":"We will now start to visualize our costs and usage, and create a dashboard.\nCost by account and product The first visualization of the dashboard will do is a visualization of costs by linkedaccountID, and product. This will highlight top spend by account and product.\n  Select line_item_unblendedcost from the Fields list, and it will show Sum of Line_item_unblended_cost:   Select line_item_usage_account_id, which will add it to the graph:   Expand the field wells by clicking on the two arrows in the top right. Drag line_item_product_code into the Group/Color field:   Select the dropdown next to the title, and chose Format visual:   Click on the down arrows under format visual, change:\n   Y-Axis label: Linked Account ID X-Axis label: Cost Double click the title to set it: Title: Cost by Account and Product    Modify the graph so that all elements are visible, with the lower corner and vertical bars: (you may need to increase the size of the graph)   Sort the accounts by cost, click the dropdown under the X-Axis (Cost label), and select Sort by descending:   The visualization is complete and the layout should look similar to:   Click on the highest usage bar, in this example it is AWSGlue, and select Exclude AWSGlue:   You will notice that AWSGlue (or the service you selected) is no longer showing, and on the left it has automatically created and applied a filter:   Elasticity The next visualization on the dashboard we will create is a visualization that shows usage for every hour, by purchase type (On Demand, Spot, Reserved Instance). In the CUR file there is no single field which shows the purchase type for EC2 Instances  so well make one with a calculated field.\n  Click Add in the top left corner, then select Add calculated field:   Copy and paste this formula into the Formula box:\n  ifelse(split({line_item_usage_type},':',1) = 'SpotUsage','Spot',ifelse(right(split({product_usagetype},':',1), 8) = 'BoxUsage',{pricing_term},'other')) Description:\n Ifelse(, , ) If statement evaluated and returns if true, otherwise  Right(, ) Returns the right most characters from a string Split(, , ) Returns the substring when is split by , position is the index of the array starting at 1  Formula Logic: If the first part of lineitem/usagetype is SpotUsage then PurchaseOption = Spot, otherwise check part of product/usagetype is BoxUsage, if it is then PurchaseOption = pricing/term, otherwise PurchaseOption = other.\nEnter a Calculated field name of PurchaseOption, and click Create:   The new field will appear in the list of fields in the data source\n Click Add then select Add visual from the top left:   Click the field line_item_usage_amount to add it to the visualization:   Click line_item_usage_start_date to add it to the visualization x-axis:   Change the aggregation of time to hourly, expand the field wells wih the arrows at the top right, click the *down arrow next to line_item_usage_start_date, click the arrow next to Aggregate: Day, and click Hour:   Click and drag PurchaseOption to the Color field:   Now we will filter out other, click Filter on the left, and click Create One\u0026hellip;:   Select PurchaseOption:   Click on the filter name PurchaseOption to edit it:   Change the filter type to Custom filter list, enter other and click the +, change the Current list to Exclude:   Click Apply:   Select the empty line, and right click and select exclude:   Update the title to Usage Elasticity, and you now have your elasticity graph, showing hourly usage by purchase option:   In the top left it states SHOWING TOP 200, and on the x-axis it has changed the range from Nov 10th to Nov 18th (most recent data points)\n Line charts show up to 2500 data points on the X axis when no color field is selected. When color is populated, line charts show up to 200 data points on the X axis and up to 25 data points for color. To work within this limitation, you can to add a filter to see each purchase option (OnDemand, Reserved, Spot) and remove the color field, we will do that next.\n We will now add instance type to the visualization, to be able to further drill down on usage. We will use another calculated field to get the instance type. Click on Add, and click Add calculated field:   Copy and paste the following formula:\n  split({line_item_usage_type},':',2)  Name the field InstanceType, click Create:   Drag InstanceType across to the Color field, the bottom of the box so it says Add drill-down layer:   Select InstanceType and it will display the hourly usage by instance type (which is all usage regardless of purchase option):   Now select PurchaseOption:   Now well focus only on ondemand. Click on the blue line \u0026amp; select Focus only on OnDemand:   You can see it automatically added a filter on the left, now click InstanceType:   It will now only show hourly usage of OnDemand instances:   You can enable/disable the filter to quickly cycle through the different options, by clicking on the checkbox next to the filter:    This is also useful to work within the limitations of the number of data points on visuals. Remove the color field \u0026amp; enable/disable the filters to switch between data. Hourly usage of on demand instances is useful when making Reserved Instance purchase decisions and verifying usage to confirm if a purchase should be made.  Cost by line item description The previous visual showed instance usage, however instances vary in cost and your organization may have significant spend in other services and other components of EC2. So now well create a visualization that looks at daily costs by line_item_line_item_descrption, this will help to identify exactly where your costs are by within each service, across all services on a daily basis.\n  Click Add and select Add visual:   Click on line_item_unblendedcost to add it to the visualization:   Click on line_item_usage_start_date to add it to the visualization, and you will have the Sum of Line_item_unblended_cost by line_item_usage_start_date:   The data source for our workshop is 3 months of data, so well narrow that down with a filter to make it faster. Click on Filter and click Create one   Select bill_billing_period_start_date:   Click on the filter name, bill_billing_period_start_date:   Select a Relative dates filter, by Months and select This month, then click Apply:   Click Visualize:   Drag line_item_line_item_description to the Color field well, to add it to the visualization:   You may have a visualization similar to below, which doesnt look very meaningful:   Click on the Vertical stacked bar chart icon under Visual Types:   You should get a graph similar to below which highlights cost more efficiently:   Hover over the large usage and you can see the actual costs. To use this graph, observe the top costs, then exclude them and continue to drill down on the highest cost visible.\nDashboard Complete Your dashboard is now complete, you should have a similar dashboard to below:  function prevStep(){ window.open(\"..\\/1_create_dataset\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/3_share_analysis\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_automated_deployment_of_ec2_web_application/1_create_web_stack/","title":"Create Web Stack","tags":[],"description":"","content":"Please note a prerequisite to this lab is that you have deployed the CloudFormation VPC stack in the lab Automated Deployment of VPC with the default parameters and recommended stack name.\n Choose the version of the CloudFormation template and download to your computer, or by cloning this repository:  wordpress.yaml to create a WordPress site, including an RDS database. staticwebapp.yaml to create a static web application that simply displays the instance ID for the instance it is running upon.   Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create Stack, then With new resources (standard).  Click Upload a template file and then click Choose file.  Choose the CloudFormation template you downloaded in step 1, return to the CloudFormation console page and click Next. Enter the following details:   Stack name: The name of this stack. For this lab, for the WordPress stack use WebApp1-WordPress or for the static web stack use WebApp1-Static and match the case.  ALBSGSource: Your current IP address in CIDR notation which will be allowed to connect to the application load balancer, this secures your web application from the public while you are configuring and testing. The remaining parameters may be left as defaults, you can find out more in the description for each.  At the bottom of the page click Next. In this lab, we won\u0026rsquo;t add any tags, permissions or advanced options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you\u0026rsquo;re satisfied with the configuration, check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack.  After a number of minutes the final stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE.  You have now created the WordPress stack (well actually CloudFormation did it for you).\nIn the stack click the Outputs tab, and open the WebsiteURL value in your web browser, this is how to access what you just created. After you have played and explored with your web application, don\u0026rsquo;t forget to tear it down to save cost.  Further Considerations  Enable TLS (SSL) on application load balancer to encrypt communications, using Amazon Certificate Manager. WordPress that is deployed stores the database password in clear text in a configuration file and is not rotated, best practice (that is supported by WordPress) would be to encrypt and automatically rotate preferably accessing the Secrets Manager API. Use EBS encryption by default to encrypting the EBS volumes for the web instances. Implementing a Web Application Firewall such as AWS WAF, and a content delivery service such as Amazon CloudFront to help protect the application. Create an automated process for patching the AMI\u0026rsquo;s and scanning for vulnerabilities before updating in production. Create a pipeline that verifies the CloudFormation template for misconfigurations before creating or updating the stack.  "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/100_labs/100_walkthrough_of_the_well-architected_tool/2_create_workload/","title":"Creating a workload","tags":[],"description":"","content":"Well-Architected Reviews are conducted per workload . A workload identifies a set of components that deliver business value. The workload is usually the level of detail that business and technology leaders communicate about.\n  Click the Define Workload button on the landing page:   If you had existing workloads, then you will land at the Workloads listing. In this interface, click the Define Workload button:   On the Define Workload interface, enter the necessary information:    Name: Workload for AWS Workshop Description: This is an example for the AWS Workshop Review owner: Your name Environment: Select \u0026ldquo;Pre-production\u0026rdquo; Regions: Select AWS Regions, and choose US West (Oregon)/us-west-2   Click on the Next button:   This will land you at the Apply lenses page. In this interface, the \u0026ldquo;AWS Well-Architected Framework\u0026rdquo; is always selected as it is not technically a Lens. We are not using any lens, so click on the Define Workload button:    function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/3_perform_review\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_autonomous_patching_with_ec2_image_builder_and_systems_manager/2_deploy_the_application_infrastructure/","title":"Deploy The Application Infrastructure","tags":[],"description":"","content":"The second section of the lab will build out the sample application stack what will run in the VPC which was build in section 1.\nThis application stack will comprise of the following :\n Application Load Balancer (ALB) . Autoscaling Group along with it\u0026rsquo;s Launch Configuration.  Once you completed below steps, you base architecture will be as follows:\nBuilding each components in this section manually will take a bit of time, and because our objective in this lab is to show you how to automate patching through AMI build and deployment. To save time, we have created a cloudformation template that you can deploy to expedite the process.\nPlease follow the steps below to do so :\n2.1. Get the Cloudformation Template. To deploy the second CloudFormation template, you can either deploy directly from the command line or via the console.\nYou can get the template here .\n  Click here for CloudFormation command line deployment steps   Command Line: 2.1.1. To deploy from the command line, ensure that you have installed and configured AWS CLI with the appropriate credentials.\naws cloudformation create-stack --stack-name pattern3-app \\ --template-body file://pattern3-application.yml \\ --parameters ParameterKey=AmazonMachineImage,ParameterValue=ami-0f96495a064477ffb\t\\ ParameterKey=BaselineVpcStack,ParameterValue=pattern3-base \\ --capabilities CAPABILITY_IAM \\ --region ap-southeast-2 Important Note:  For simplicity, we have used Sydney \u0026lsquo;ap-southeast-2\u0026rsquo; as the default region for this lab. We have also pre configured the Golden Amazon Machine Image Id to be the AMI id of Amazon Linux 2 AMI (HVM) in Sydney region ami-0f96495a064477ffb. If you choose to to use a different region, please change the AMI Id accordingly for your region.      Click here for CloudFormation console deployment steps   Console: 2.1.1. If you need detailed instructions on how to deploy CloudFormation stacks from within the console, please follow this guide. To deploy the pattern1-app stack from the console, ensure that you follow below requirements:\n Use pattern3-app as the Stack Name. Provide the name of the VPC CloudFormation stack you create in section 1 ( we used pattern3-base as default ) as the BaselineVpcStack parameter value. Use the AMI Id of Amazon Linux 2 AMI (HVM) as the AmazonMachineImage parameter value. ( In Sydney region ami-0f96495a064477ffb if you choose to to use a different region, please change the AMI Id accordingly for your region. )    2.2. Confirm Successful Application Installation Once the stack creation is complete, let\u0026rsquo;s check that the application deployment has been successful. To do this follow below steps:\n  Go to the Outputs section of the cloudformation stack you just deployed.\n  Note the value of OutputPattern3ALBDNSName and you can find the DNS name as per screen shot below:   Copy the value and paste it into a web browser.\n  If you have configured everything correctly, you should be able to view a webpage with \u0026lsquo;Welcome to Re:Invent 2020 The Well Architected Way\u0026rsquo; as the page title.\n  Adding /details.php to the end of your DNS address will list the packages currently available, together with the AMI which has been used to create the instance as follows:   Take note of the installed packages and AMI Id (Copy and paste this elsewhere we will use this to confirm the changes later).\n  When you have confirmed that the application deployment was successful, move to section 3 which will deploy your AMI Builder Pipeline.\n   END OF SECTION 2\n "},{"uri":"https://wellarchitectedlabs.com/reliability/100_labs/100_deploy_cloudformation/2_deploy_webapp/","title":"Deploy Web Application and Infrastructure using CloudFormation","tags":[],"description":"","content":"Wait until the VPC CloudFormation stack status is CREATE_COMPLETE, then continue. This will take about four minutes.\n Download the CloudFormation template: staticwebapp.yaml  You can right-click then choose Save link as; or you can right click and copy the link to use with wget      Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and click Create Stack \u0026gt; With new resources   Leave Prepare template setting as-is\n For Template source select Upload a template file Click Choose file and supply the CloudFormation template you downloaded: staticwebapp.yaml     Click Next\n  For Stack name use CloudFormationLab\n  Parameters\n  Look over the Parameters and their default values.\n  Click Next\n    For Configure stack options we recommend configuring tags, which are key-value pairs, that can help you identify your stacks and the resources they create. For example, enter Owner in the left column which is the key, and your email address in the right column which is the value. We will not use additional permissions or advanced options so click Next. For more information, see Setting AWS CloudFormation Stack Options .\n  For Review\n Review the contents of the page At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Create stack     This will take you to the CloudFormation stack status page, showing the stack creation in progress.\n Click on the Events tab Scroll through the listing. It shows the activities performed by CloudFormation (newest events at top), such as starting to create a resource and then completing the resource creation. Any errors encountered during the creation of the stack will be listed in this tab.     When it shows status CREATE_COMPLETE, then you are finished with this step.\n   function prevStep(){ window.open(\"..\\/1_deploy_vpc\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/3_explore_webapp\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_3_pricing_models/2_prepare_csv/","title":"Download and prepare the RI CSV files","tags":[],"description":"","content":" Download the CSV for both the 7 day and 30 day recommendation files, by selecting the filter 7 days or 30 days, and clicking on Download CSV:   The next steps MUST be followed carefully, ensure you name everything exactly as specified or the formulas will not work.\n If you do not have sufficient usage, you can download the two sample files:  Open both files in a spreadsheet application. Paste the 30day recommendations into one worksheet, and the 7day recommendations into another worksheet called 7Day Rec, in the same spreadsheet.\n 7_day_EC2_R_Rec.csv  30_day_EC2_R_Rec.csv    Create a new column called RI ID to the left of the Recommended Instance Quantity Purchase column on both 30Day and 7Day sheets, which is a unique identifier of the RI Type, the formula for this cell will concatenate the columns: Instance Type, Location,OS and Tenancy. On row 2 of the sample files, paste the formula below into the first row of data and fill the remaining rows below.\n=CONCATENATE(C2,L2,M2,N2)\n  Add a column in the 30Day worksheet to the right of the Recommended Instance Quantity Purchase column. Label it 7Day recommendation. Add a VLOOKUP formula to get the values from the 7Day worksheet, paste the formula below into the first row of data and fill the remaining rows below.  If using your own data, modify the U$48 component to the number of rows in your data.\n =VLOOKUP(T2,'7Day Rec'!T$2:U$48,2,FALSE)   We will now create a Fully Paid Day column. This shows us how long it will take to pay off the full term of the RI, and will help to measure risk. The closer to 12months the fully paid day is, the higher the risk. The break even is the wrong measure, as it only shows how quickly you pay off the upfront component, and not the full amount. Paste the following formula into the last column z:\n=(R2+S2*12)/(R2/12+S2+W2)    The formula for the fully paid day is: (yearly RI cost) / (monthly on-demand cost)\n Delete the following columns as they are not necessary:   Recommendation Date Size Flexible Recommendation Max hourly normalized unit usage in Historical Period Min hourly normalized unit usage in Historical Period Average hourly normalized unit usage in Historical Period Projected RI Utilization Payment Option Break Even Months.  You have compiled a complete set of recommendations with the required data to be able to analyse and make low risk high return recommendations.\n  function prevStep(){ window.open(\"..\\/1_ri_report\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/3_filter_csv\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_deploy_and_update_cloudformation/2_understand_deploy/","title":"Explore your Deployed Infrastructure","tags":[],"description":"","content":"Here you will explore the CloudFormation stack you deployed in the previous step.\nHow many resources were created?  From the CloudFormation console, click the Resources tab for the CloudFormationLab stack.  The listing shows all the resources that were created.   Now, look at the simple_stack.yaml template (in your text editor) and compare. How many resources are defined there? Investigate:  Why did the deployment not create all of the resources? You may click below for the answer. Try to figure this out before continuing.      click here for answer:   The deployed stack only has one resource, the VPC. But the CloudFormation Template contains many more in its Resources section. Why are these different?\n The Condition PublicEnabled is set using the Parameter PublicEnabledParam Similarly the Condition EC2SecurityEnabled is set using the Parameter EC2SecurityEnabledParam The Default value for both of these Parameters is false And therefore both conditions PublicEnabled and EC2SecurityEnabled evaluate to false Look in the template at how the PublicEnabled and EC2SecurityEnabled Conditions are used  IGWAttach: Condition: PublicEnabled #https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-ec2-vpc-gateway-attachment.html Type: AWS::EC2::VPCGatewayAttachment Properties: VpcId: !Ref SimpleVPC InternetGatewayId: !Ref IGW  The Condition: \u0026lt;Condition_Name\u0026gt; statement on a resource means  If this condition is true  then create this resource   else  do not create this resource     All resources except the VPC have a Condition statement. Since the conditions were false only the VPC was created    Complare the CloudFormation template to the VPC resource that was created  Return to the AWS CloudFormation console  Click the Resources tab for the CloudFormationLab stack. The listing shows all the resources that were created. In this case just the VPC Note the Logical ID for the VPC is SimpleVPC. Look at the CloudFormation template file and determine where this name came from Under the Resources tab click on the Physical ID link for SimpleVPC  This takes you to the VPC console where you can see the VPC you created Select the checkbox next to your VPC (if not already selected) Look at the VPC attributes under the Description tab. How do these compare to the CloudFormation template?     For more information see the syntax and properties for a VPC in Cloudformation here .   function prevStep(){ window.open(\"..\\/1_deploy_infra\", \"_self\") } function nextStep(){ window.open(\"..\\/3_cfn_params\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_health_checks_and_dependencies/2_handle_dependency/","title":"Handle failure of service dependencies","tags":[],"description":"","content":"2.1 System dependency initially healthy   You already observed that all three EC2 instances are successfully serving requests\n  In a new tab navigate to ELB Target Groups console\n By clicking here to open the AWS Management Console  or navigating through the AWS Management Console: Services \u0026gt; EC2 \u0026gt; Load Balancing \u0026gt; Target Groups Leave this tab open as you will be referring back to it multiple times    Click on the Targets tab (bottom half of screen)\n  Under Registered Targets observe the three EC2 instances serving your web service\n  Note that they are all healthy (see Status and Description)\n In this state the ELB will route traffic to any of the three servers    From the Target Groups console, now click on the the Health checks tab\n Note here that the Path is configured to /healthcheck    Copy the URL of the web service to a new tab and append /healthcheck to the end of the URL\n  The new URL should look like:\nhttp://healt-alb1l-\u0026lt;...\u0026gt;.elb.amazonaws.com/healthcheck    Refresh several times and observe the health check on the three servers\n  Note the check is successful\n    The EC2 servers receive user requests (for a TV show recommendation) on the path / and they receive health check requests from the Elastic Load Balancer on the path /healthcheck\n The health check always returns an http 200 code for any request to it. The server code running on each EC2 instance can be viewed here , or you can view the health check code excerpt below:    # Healthcheck request - will be used by the Elastic Load Balancer elif self.path == \u0026#39;/healthcheck\u0026#39;: # Return a healthy code self.send_response(200) self.send_header(\u0026#39;Content-type\u0026#39;, \u0026#39;text/html\u0026#39;) self.end_headers() 2.2 Simulate dependency not available 2.2.1 Disable RecommendationService You will now simulate a complete failure of the RecommendationService. Every request in turn makes a (simulated) call to the getRecommendation API on this service. These will all fail for every request on every server.\n In a new tab, navigate to the Parameter Store on the AWS Systems Manager console  By clicking here to open the AWS Management Console  or navigating through the AWS Management Console: Services \u0026gt; Systems Manager \u0026gt; Parameter Store Leave this tab open as you will be referring back to it one additional time   Click on RecommendationServiceEnabled Click Edit In the Value box, type false Click Save Changes  A status message should say Edit parameter request succeeded    The RecommendationServiceEnabled parameter is used only for this lab. The server code reads its value, and simulates a failure in RecommendationService (all reads to the DynamoDB table simulating the service will fail) when it is false.\n2.2.2 Observe behavior when dependency not available   Refresh the test web service multiple times\n Note that it fails with 502 Bad Gateway For each request one of the servers receiving the request attempts to call the RecommendationService but catastrophically fails and fails to return a reply (empty reply) to the load balancer, which in turn presents this as a http 502 failure.    You can observe this by opening a new tab and navigating to ELB Load Balancers console:\n By clicking here to open the AWS Management Console  or navigating through the AWS Management Console: Services \u0026gt; EC2 \u0026gt; Load Balancing \u0026gt; Load Balancers    Click on the Monitoring tab (bottom half of screen)\n Observe the ELB 5XXs (Count) and HTTP 502s (Count) errors for the load balancer It will take a minute for the metrics to show up. Make sure you refresh the web service page multiple times in your browser These are the error codes the load balancer returns on every request during this simulated outage    Compare these metrics to those for the target group (the EC2 servers themselves)\n Return to the Target Groups console and click the Monitoring tab there Observe HTTP 5XXs ( Count ) errors shows no data The servers themselves are not returning actual http error codes, they are failing to return any data at all   We need to update the server code to handle when the dependency is not available    2.3 Update server code to handle dependency not available The getRecommendation API is actually a get_item call on a DynamoDB table. Examine the server code to see how errors are currently handled\n  The server code running on each EC2 instance can be viewed here   Search for the call to the RecommendationService. It looks like this:\n response = call_getRecommendation(self.region, user_id)   What happens if this call fails?    Choose one of the options below (Option 1 - Expert or Option 2 - Assisted) to improve the code and handle the failure\n  2.3.1 Option 1 - Expert option: make and deploy your changes to the code You may choose this option, or skip to Option 2 - Assisted option\nThis option requires you have access to place a file in a location accessible via https/https via a URL. For example a public readable S3 bucket, gist (use the raw option to get the URL), or your private webserver.\n Download the existing server code from here: server_basic.py  Modify the code to handle the call to the RecommendationService When the call to RecommendationService fails then instead of using the response data you requested and did not get, return a static response:  Instead of user first name return Valued Customer Instead of a personalized recommended TV show, return I Love Lucy Try to also return some diagnostic information on the cause of the error   Put your updated server code in a location where it can be downloaded via its URL using wget In the AWS Console go the HealthCheckLab CloudFormation stack and Update it:  Leave Use current template selected and click Next Find the ServerCodeUrl parameter and enter the URL for your new code When stack status is CREATE_COMPLETE (about four minutes) then continue    If you completed the Option 1 - Expert option, then skip the Option 2 - Assisted option section and continue with 2.3.3 Error handling code\n2.3.2 Option 2 - Assisted option: deploy workshop provided code  The new server code including error handling can be viewed here  Search for Error handling in the comments (occurs twice). What will this code do now if the dependency call fails?  Deploy the new error handling code   Navigate to the AWS CloudFormation console\n  Click on the HealthCheckLab stack\n  Click Update\n  Leave Use current template selected and click Next\n  Find the ServerCodeUrl parameter and enter the following:\n https://aws-well-architected-labs-ohio.s3.us-east-2.amazonaws.com/Healthcheck/Code/server_errorhandling.py    Click Next until the last page\n  At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names\n  Click Update stack\n  Click on Events, and click the refresh icon to observe the stack progress\n New EC2 instances running the error handling code are being deployed When stack status is CREATE_COMPLETE (about four minutes) then continue    2.3.3 Error handling code This is the error handling code from server_errorhandling.py . The Option 2 - Assisted option uses this code. If you used the Option 1 - Expert option, you can consult this code as a guide.\nCode:   Click here to see the code:   # Error handling: # surround the call to RecommendationService in a try catch try: # Call the getRecommendation API on the RecommendationService response = call_getRecommendation(self.region, user_id) # Parses value of recommendation from DynamoDB JSON return value # {\u0026#39;Item\u0026#39;: { # \u0026#39;ServiceAPI\u0026#39;: {\u0026#39;S\u0026#39;: \u0026#39;getRecommendation\u0026#39;}, # \u0026#39;UserID\u0026#39;: {\u0026#39;N\u0026#39;: \u0026#39;1\u0026#39;}, # \u0026#39;Result\u0026#39;: {\u0026#39;S\u0026#39;: \u0026#39;M*A*S*H\u0026#39;}, ... tv_show = response[\u0026#39;Item\u0026#39;][\u0026#39;Result\u0026#39;][\u0026#39;S\u0026#39;] user_name = response[\u0026#39;Item\u0026#39;][\u0026#39;UserName\u0026#39;][\u0026#39;S\u0026#39;] message += recommendation_message (user_name, tv_show, True) # Error handling: # If the service dependency fails, and we cannot make a personalized recommendation # then give a pre-selected (static) recommendation # and report diagnostic information except Exception as e: message += recommendation_message (\u0026#39;Valued Customer\u0026#39;, \u0026#39;I Love Lucy\u0026#39;, False) message += \u0026#39;\u0026lt;br\u0026gt;\u0026lt;br\u0026gt;\u0026lt;br\u0026gt;\u0026lt;h2\u0026gt;Diagnostic Info:\u0026lt;/h2\u0026gt;\u0026#39; message += \u0026#39;\u0026lt;br\u0026gt;We are unable to provide personalized recommendations\u0026#39; message += \u0026#39;\u0026lt;br\u0026gt;If this persists, please report the following info to us:\u0026#39; message += str(traceback.format_exception_only(e.__class__, e))   \n2.3.4 Observe behavior of web service with added error handling   After the new error-handling code has successfully deployed, refresh the test web service page multiple times. Observe:\n It works. It no longer returns an error All three EC2 instances and Availability Zones are being used A default recommendation for Valued Customer is displayed instead of a user-personalized one There is now Diagnostic Info. What does it mean?    Refer back to the newly deployed code to understand why the website behaves this way now\n  The Website is working again, but in a degraded capacity since it is no longer serving personalized recommendations. While this is less than ideal, it is much better than when it was failing with http 502 errors. The RecommendationService is not available, so the app instead returns a static response (the default recommendation) instead of the data it would have obtained from RecommendationService.\n   Well-Architected for Reliability: Best practice     Implement graceful degradation to transform applicable hard dependencies into soft dependencies: When a component\u0026rsquo;s dependencies are unhealthy, the component itself can still function, although in a degraded manner. For example, when a dependency call fails, instead use a predetermined static response.     function prevStep(){ window.open(\"..\\/1_deploy_app\", \"_self\") } function nextStep(){ window.open(\"..\\/3_deep_healthcheck\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_incident_response_with_aws_console_and_cli/2_iam/","title":"Identity &amp; Access Management","tags":[],"description":"","content":"2.1 Investigate AWS CloudTrail As AWS CloudTrail logs API activity for supported services , it provides an audit trail of your AWS account that you can use to track history of an adversary. For example, listing recent access denied attempts in AWS CloudTrail may indicate attempts to escalate privilege unsuccessfully. Note that some services such as Amazon S3 have their own logging, for example read more about Amazon S3 server access logging . You can enable AWS CloudTrail by following the Automated Deployment of Detective Controls lab.\n2.1.1 AWS Console The AWS console provides a visual way of querying Amazon CloudWatch Logs, using CloudWatch Logs Insights and does not require any tools to be installed.\n Open the Amazon CloudWatch console at https://console.aws.amazon.com/cloudwatch/ and select your region. From the left menu, choose Insights under Logs. From the dropdown near the top select your CloudTrail Logs group, then the relative time to search back on the right. Copy the following example queries below into the query input, then click Run query.  IAM access denied attempts:\nTo list all IAM access denied attempts you can use the following example. Each of the line item results allows you to drill down to reveal further details:\nfilter errorCode like /Unauthorized|Denied|Forbidden/ | fields awsRegion, userIdentity.arn, eventSource, eventName, sourceIPAddress, userAgent\nIAM access key:\nIf you need to search for what actions an access key has performed you can search for it e.g. AKIAIOSFODNN7EXAMPLE:\nfilter userIdentity.accessKeyId =\u0026quot;AKIAIOSFODNN7EXAMPLE\u0026quot; | fields awsRegion, eventSource, eventName, sourceIPAddress, userAgent\nIAM source ip address:\nIf you suspect a particular IP address as an adversary you can search such as 192.0.2.1:\nfilter sourceIPAddress = \u0026quot;192.0.2.1\u0026quot; | fields awsRegion, userIdentity.arn, eventSource, eventName, sourceIPAddress, userAgent\nIAM access key created\nAn access key id will be part of the responseElements when its created so you can query that:\nfilter responseElements.credentials.accessKeyId =\u0026quot;AKIAIOSFODNN7EXAMPLE\u0026quot; | fields awsRegion, eventSource, eventName, sourceIPAddress, userAgent\nIAM users and roles created\nListing users and roles created can help identify unauthorized activity:\nfilter eventName=\u0026quot;CreateUser\u0026quot; or eventName = \u0026quot;CreateRole\u0026quot; | fields requestParameters.userName, requestParameters.roleName, responseElements.user.arn, responseElements.role.arn, sourceIPAddress, eventTime, errorCode\nS3 List Buckets\nListing buckets may indicate someone trying to gain access to your buckets. Note that Amazon S3 server access logging needs to be enabled on each bucket to gain further S3 access details:\nfilter eventName =\u0026quot;ListBuckets\u0026quot; | fields awsRegion, eventSource, eventName, sourceIPAddress, userAgent\n2.1.2 AWS CLI Remember you might need to update the \u0026ndash;log-group-name, \u0026ndash;region and/or \u0026ndash;start-time parameter to a millisecond epoch start time of how far back you wish to search. You can use a web conversion tool such as www.epochconverter.com .\nIAM access denied attempts:\nTo list all IAM access denied attempts you can use CloudWatch Logs with \u0026ndash;filter-pattern parameter of AccessDenied for roles and Client.UnauthorizedOperation for users:\naws logs filter-log-events --region us-east-1 --start-time 1551402000000 --log-group-name CloudTrail/DefaultLogGroup --filter-pattern AccessDenied --output json --query 'events[*].message'| jq -r '.[] | fromjson | .userIdentity, .sourceIPAddress, .responseElements'\nIAM access key:\nIf you need to search for what actions an access key has performed you can modify the \u0026ndash;filter-pattern parameter to be the access key to search such as AKIAIOSFODNN7EXAMPLE:\naws logs filter-log-events --region us-east-1 --start-time 1551402000000 --log-group-name CloudTrail/DefaultLogGroup --filter-pattern AKIAIOSFODNN7EXAMPLE --output json --query 'events[*].message'| jq -r '.[] | fromjson | .userIdentity, .sourceIPAddress, .responseElements'\nIAM source ip address:\nIf you suspect a particular IP address as an adversary you can modify the \u0026ndash;filter-pattern parameter to be the IP address to search such as 192.0.2.1:\naws logs filter-log-events --region us-east-1 --start-time 1551402000000 --log-group-name CloudTrail/DefaultLogGroup --filter-pattern 192.0.2.1 --output json --query 'events[*].message'| jq -r '.[] | fromjson | .userIdentity, .sourceIPAddress, .responseElements'\nS3 List Buckets\nListing buckets may indicate someone trying to gain access to your buckets. Note that Amazon S3 server access logging needs to be enabled on each bucket to gain further S3 access details:\naws logs filter-log-events --region us-east-1 --start-time 1551402000000 --log-group-name CloudTrail/DefaultLogGroup --filter-pattern ListBuckets --output json --query 'events[*].message'| jq -r '.[] | fromjson | .userIdentity, .sourceIPAddress, .responseElements'\n2.2 Block access in AWS IAM Blocking access to an IAM entity, that is a role, user or group can help when there is unauthorized activity as it will no longer be able to perform any actions. Be careful as blocking access may disrupt the operation of your workload, which is why it is important to practice in a non-production environment. Note that the AWS IAM entity may have created another entity, or other resources that may allow access to your account. You can use AWS CloudTrail that logs activity in your AWS account to determine the IAM entity that is performing the unauthorized operations. Additionally service last accessed data in the AWS Console can help you audit permissions.\n2.3 List AWS IAM roles/users/groups If you need to confirm the name of a role, user or group you can list:\n2.3.1 AWS Console  Sign in to the AWS Management Console as an IAM user or role in your AWS account, and open the AWS IAM console at https://console.aws.amazon.com/iam/ . Click Roles on the left, the role will be displayed and you can use the search field.  2.3.2 AWS CLI aws iam list-roles This provides a full json formatted list of all roles, if you only want to display the RoleName use an output of table and query:\naws iam list-roles --output table --query 'Roles[*].RoleName' List all users:\naws iam list-users --output table --query 'Users[*].UserName' List all groups:\naws iam list-groups --output table --query 'Groups[*].GroupName'\n2.4 Attach inline deny policy Attaching an explicit deny policy to an AWS IAM role, user or group will quickly block ALL access for that entity which is useful if it is performing unauthorized operations. Please note that the role will still be able to call the sts API to obtain information on itself, e.g. using get-caller-identity will return the account ID, user ID and ARN.\n2.4.1 AWS Console  Sign in to the AWS Management Console as an AWS IAM user or role in your AWS account, and open the AWS IAM console at https://console.aws.amazon.com/iam/ . Click either Groups, Users or Roles on the left, then click the name to modify. Click Permissions tab. Click Add inline policy. Click the JSON tab then replace the example with the following: { \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Deny\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] } Click Review policy. Enter Name of DenyAll then click Create policy. Note that the console may incorrectly display the access level.  2.4.2 AWS CLI Block a role, modify ROLENAME to match your role name:\naws iam put-role-policy --role-name ROLENAME --policy-name DenyAll --policy-document '{ \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Deny\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] }' Block a user, modify USERNAME to match your user name:\naws iam put-user-policy --user-name USERNAME --policy-name DenyAll --policy-document '{ \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Deny\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] }' Block a group, modify GROUPNAME to match your user name:\naws iam put-group-policy --group-name GROUPNAME --policy-name DenyAll --policy-document '{ \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Deny\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] }'\n2.5 Delete inline deny policy To delete the policy you just attached and restore the original permissions the entity had:\n2.5.1 AWS Console  Sign in to the AWS Management Console as an IAM user or role in your AWS account, and open the AWS IAM console at https://console.aws.amazon.com/iam/ . Click Roles on the left. Click the checkbox next to the role to delete. Click Delete role. Confirm the role to delete then click Yes, delete  2.5.2 AWS CLI Delete policy from a role: aws iam delete-role-policy --role-name ROLENAME --policy-name DenyAll Delete policy from a user: aws iam delete-user-policy --user-name USERNAME --policy-name DenyAll Delete policy from a group: aws iam delete-group-policy --group-name GROUPNAME --policy-name DenyAll\n"},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_remote_configuration_installation_and_viewing_cloudwatch_logs/2_install_cw_agent/","title":"Install the CloudWatch Agent","tags":[],"description":"","content":"The CloudWatch agent monitors activity on your EC2 instance to collect logs and metrics. This improves your security posture by providing detailed records you can use to investigate security incidents. The CloudWatch agent needs to be installed on the EC2 instance using AWS Systems Manager Run Command. Run Command enables you to perform actions on EC2 instances remotely. This tool is especially helpful at scale, where you can manage the configuration of many instances with a single command. It is possible to completely automate this process using user data scripts, but that is beyond the scope of this lab.\n Open the Systems Manager console . Choose Run Command from the left side menu under Instances \u0026amp; Nodes. Click Run Command on the page that opens up.  In the Command document box, click in the search bar. Select Document name prefix, then Equals, and enter AWS-ConfigureAWSPackage. Select the command that appears below. This command allows you to install packages on EC2 instances without directly accessing the instance; the AmazonCloudWatchAgent package we will use in this lab is one of these packages.  Under Command parameters:  Set Action to Install Set Installation Type to Uninstall and Reinstall In the Name field, enter AmazonCloudWatchAgent In the Version field, enter latest Do not modify the Additional Arguments field    Under Targets:  Select Choose instances manually.  For the purpose of this lab, there is only one EC2 Instance you need to run a command on. If you have a large fleet of EC2 instances, you can assign a tag to those instances and choose Specify instance tags to run a command on many tagged instances easily. You should see a list of running instances. Select the instance that was launched by the CloudFormation template you deployed for this lab, which will be named Security-CW-Lab-Instance. In order to use Systems Manager with an instance, the instance needs certain IAM permissions. The initial CloudFormation stack you deployed created and assigned an IAM role to this instance. The policy document AmazonSSMManagedInstanceCore is attached to this role, allowing Systems Manager to perform operations on the instance.      Under Output Options, deselect Enable writing to an S3 bucket. Choose Run. Optionally, in the Targets and outputs areas, select the button next to an instance name and choose View output. Systems Manager should show that the agent was successfully installed.  Recap: In this portion of the lab, you installed the AWS CloudWatch agent on an EC2 Instance using AWS Systems Manager Run Command. Run Command facilitated installing the package on the instance without directly accessing it using SSH - exemplifying the Well-Architected Best Practice of enabling people to perform actions at a distance and reducing attack surface.\n function prevStep(){ window.open(\"..\\/1_deploy_cfn_stack\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/3_create_cw_config\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_automated_cur_updates_and_ingestion/2_multiple_curs/","title":"Multiple CURs","tags":[],"description":"","content":"This step is used when there are multiple CURs being delivered into the same bucket - for example a CUR with hourly granularity and one with daily granularity. This will automatically update Athena/Glue when there are new versions and new months data for both reports.\nThe easiest way to work with multiple CURs is to deliver each CUR to a different S3 bucket, and follow the previous process. If you must deliver to a single bucket, configure your CURs with different prefixes or folders and follow this process.\n Log into the console as an IAM user with the required permissions, verify you have multiple CURs with different prefixes being delivered into the same bucket. We will have the following configuration:  Format: \u0026lt;bucket name\u0026gt;/\u0026lt;prefix\u0026gt;/\u0026lt;report_name\u0026gt;/ Configuration: \u0026lt;bucket name\u0026gt;/DailyCUR/daily/ \u0026lt;bucket name\u0026gt;/HourlyCUR/hourly/  Open the S3 console, and navigate to one of the directories where CURs are stored. Open and save the crawler-cfn.yml file:   Open the file in your favourite text editor\n  Modify the following lines to remove all references to the prefix or report name. Replace the first line with the second in each case:Under AWSCurDatabase:\n  Name: 'athenacurcfn_daily' Name: 'athenacurcfn' Under AWSCURCrawlerComponentFunction:\nResource: arn:aws:s3:::\u0026lt;bucket name\u0026gt;/DailyCUR/daily/daily* Resource: arn:aws:s3:::\u0026lt;bucket name\u0026gt;* Under AWSCURCrawler:\nName: AWSCURCrawler-daily Name: AWSCURCrawler and\nPath: 's3://\u0026lt;bucket name\u0026gt;/DailyCUR/daily/daily' Path: 's3://\u0026lt;bucket name\u0026gt;' and under Exclusions after .zip add:\n'aws-programmatic-access-test-object' Under AWSPutS3CURNotification:\nReportKey: 'DailyCUR/daily/daily' ReportKey: '' Under AWSCURReportStatusTable:\nDatabaseName: athenacurcfn_daily DatabaseName: athenacurcfn and\nLocation: 's3://\u0026lt;bucket name\u0026gt;/DailyCUR/daily/cost_and_usage_data_status/' Location: 's3://\u0026lt;bucket name\u0026gt;/cost_and_usage_data_status/' A modified sample is provided here: Code/crawler-cfn.yml Look for the comments: ### New line\n Save the template file.\n  Go to the CloudFormation dashboard and execute the template you just created   Go to the Glue dashboard and verify that there is a single database, containing multiple tables:    function prevStep(){ window.open(\"..\\/1_cf_stack\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/3_cleanup\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_splitting_sharing_cur_access/2_backfill_data/","title":"Perform one off Fill of Member/Linked Data","tags":[],"description":"","content":"Perform this step if you want to generate data for all previous months available in your current CUR files. This is a one off step that is performed manually. We create a temporary table in Athena, and write the output to the S3 location created above, for the member/linked account to access it. We then delete the temporary table - which does not delete the S3 output data.\n1 - In the management/payer account go into the Athena service dashboard\n2 - Create your query using the template below:\nThe following statement will copy all columns from the source table if the line_item_usage_account_id matches a specific Account ID. It will output each month into a separate folder by using partitioning on the year and month, and output it to the S3 output folder.\nCREATE TABLE (database).temp_table WITH ( format = 'Parquet', parquet_compression = 'GZIP', external_location = 's3://(bucket)/(folder)', partitioned_by=ARRAY['year_1','month_1']) AS SELECT *, year as year_1, month as month_1 FROM \u0026quot;(database)\u0026quot;.\u0026quot;(table)\u0026quot; where line_item_usage_account_id like '(account ID)' Some key points for your queries:\n Partitioning will allow us to write only the current months data each time, and not write all the data Parquet format is used, which allows faster access and reduced costs through reduced data scanning GZIP compression produces smaller output files than SNAPPY SNAPPY is faster than GZIP to run  Example of performance with a source CUR of 6.3Gb:\n Using Parquet and GZIP, it will take approximate 11min 16sec, and produce 8.4Gb of output files Using Parquet and SNAPPY, it will take approximately 7min 8sec, and produce 12.2Gb of output files  3 - Execute the statement in Athena: 4 - Go into the S3 service dashboard\n5 - Go to the output bucket and folder\n6 - Verify the data has been populated into the S3 folders 7 - Verify the permissions are correct on the files - there should be multiple Grantees: 8 - Then delete the temp table from Athena by modifying the following code: (this will NOT delete the s3 data)\nDROP TABLE (database).temp_table   function prevStep(){ window.open(\"..\\/1_setup_s3_output\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/3_athena_queries\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_incident_response_playbook_with_jupyter-aws_iam/2_playbook_run/","title":"Playbook Run","tags":[],"description":"","content":"2.1 Download Playbook and Helper Download the example version of the notebook Incident_Response_Playbook_AWS_IAM.ipynb and helper incident_response_helpers.py , place them in the same directory.\n2.2 Run the Playbook  In your command line or terminal change directory to where you downloaded or cloned the notebook and helper. Enter jupyter notebook to start the local webserver, and connect to the url provided in the console e.g. The Jupyter Notebook is running at:, a web browser may automatically open to the correct url. Click on the Incident_Response_Playbook_AWS_IAM.ipynb file to execute the playbook. Follow the instructions in the playbook.  "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/100_labs/100_inventory_patch_management/2_setup_env/","title":"Setup","tags":[],"description":"","content":"Requirements You will need the following to be able to perform this lab:\n Your own device for console access An AWS account that you are able to use for testing, that is not used for production or other purposes An available region within your account with capacity to add 2 additional VPCs  User and Group Management When you create an Amazon Web Services (AWS) account, you begin with a single sign-in identity that has complete access to all AWS services and resources in the account. This identity is called the AWS account root user. It is accessed by signing in with the email address and password that you used to create the account.\nWe strongly recommend that you do not use the root user for your everyday tasks, even the administrative ones. Instead, adhere to the best practice of using the root user only to create your first IAM user. Securely store the root user credentials and use them to perform only a few account and service management tasks. To view the tasks that require you to sign in as the root user, see AWS Tasks That Require Root User .\nIAM Users \u0026amp; Groups As a best practice, do not use the AWS account root user for any task where it\u0026rsquo;s not required. Instead, create a new IAM user for each person that requires administrator access. Then grant administrator access by placing the users into an \u0026ldquo;Administrators\u0026rdquo; group to which the AdministratorAccess managed policy is attached.\nUse administrators group members to manage permissions and policy for the AWS account. Limit use of the root user to only those actions that require it .\n2.1 Create Administrator IAM User and Group To create an administrator user for yourself and add the user to an administrators group:\n Use your AWS account email address and password to sign in as the AWS account root user to the IAM console at https://console.aws.amazon.com/iam/. In the IAM navigation pane, choose Users and then choose Add user. In Set user details for User name, type a user name for the administrator account you are creating. The name can consist of letters, digits, and the following characters: plus (+), equal (=), comma (,), period (.), at (@), underscore (_), and hyphen (-). The name is not case sensitive and can be a maximum of 64 characters in length. In Select AWS access type for Access type, select the check box next to AWS Management Console access, select Custom password, and then type your new password in the text box. If you\u0026rsquo;re creating the user for someone other than yourself, you can leave Require password reset selected to force the user to create a new password when first signing in. Clear the box next to Require password reset and then choose Next: Permissions. In set permissions for user ensure Add user to group is selected. Under Add user to group choose Create group. In the Create group dialog box, type a Group name for the new group, such as Administrators. The name can consist of letters, digits, and the following characters: plus (+), equal (=), comma (,), period (.), at (@), underscore (_), and hyphen (-). The name is not case sensitive and can be a maximum of 128 characters in length. In the policy list, select the check box next to AdministratorAccess and then choose Create group. Back at Add user to group, in the list of groups, ensure the check box for your new group is selected. Choose Refresh if necessary to see the group in the list. choose Next: Review to see the list of group memberships to be added to the new user. When you are ready to proceed, choose Create user. At the confirmation screen you do not need to download the user credentials for programmatic access at this time. You can create new credentials at any time.  You can use this same process to create more groups and users and to give your users access to your AWS account resources. To learn about using policies that restrict user permissions to specific AWS resources, see Access Management and Example Policies . To add additional users to the group after it\u0026rsquo;s created, see Adding and Removing Users in an IAM Group .\n2.2 Log in to the AWS Management Console using your administrator account  You can now use this administrator user instead of your root user for this AWS account. Choose the link https://\u0026lt;yourAccountNumber\u0026gt;.signin.aws.amazon.com/console and log in with your administrator user credentials. Select the region you will use for the lab from the the list in the upper right corner. Verify that you have 2 available VPCs (3 or less in use) in the selected region by navigating to the VPC Console (https://console.aws.amazon.com/vpc/) and in the Resources section reviewing the number of VPCs.  2.3 Create an EC2 Key Pair Amazon EC2 uses public-key cryptography to encrypt and decrypt login information. Public-key cryptography uses a public key to encrypt a piece of data, such as a password, then the recipient uses the private key to decrypt the data. The public and private keys are known as a key pair. To log in to the Amazon Linux instances we will create in this lab, you must create a key pair, specify the name of the key pair when you launch the instance, and provide the private key when you connect to the instance.\n Use your administrator account to access the Amazon EC2 console at https://console.aws.amazon.com/ec2/. In the EC2 navigation pane under Network \u0026amp; Security, choose Key Pairs and then choose Create Key Pair. In the Create Key Pair dialog box, type a Key pair name such as OELabIPM and then choose Create. Save the keyPairName.pem file for optional later use accessing the EC2 instances created in this lab.   function prevStep(){ window.open(\"..\\/1_intro\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/3_deploy_env_iaac\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/100_labs/100_aws_account_and_root_user/2_cleanup/","title":"Tear down","tags":[],"description":"","content":"Please note that the changes you made to secure your account and root user should remain in place, and have no charges associated with them.\n "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_certificate_manager_request_public_certificate/2_tear_down/","title":"Tear down","tags":[],"description":"","content":"The following instructions will remove the certificate you have created.\n Sign into the AWS Management Console and open the ACM console at https://console.aws.amazon.com/acm/home . Select the region where you created the certificate. Click the check box for the domain name of the certificate to delete. Click Actions then Delete. Verify this is the certificate to delete and click Delete. Note: You cannot delete an ACM Certificate that is being used by another AWS service. To delete a certificate that is in use, you must first remove the certificate association.  References \u0026amp; useful resources  AWS Certificate Manager  Create an HTTPS Listener for Your Application Load Balancer   "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_cloudfront_for_web_application/2_tear_down/","title":"Tear down","tags":[],"description":"","content":"The following instructions will remove the resources that have a cost for running them. Please note that Security Groups and SSH key will exist. You may remove these also or leave for future use.\nDelete the CloudFront distribution:\n Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. From the console dashboard, select the distribution you created earlier and click the Disable button. To confirm, click the Yes, Disable button. After approximately 15 minutes when the status is Deployed, select the distribution and click the Delete button, and then to confirm click the Yes, Delete button.   References \u0026amp; useful resources Amazon CloudFront Developer Guide AWS WAF, AWS Firewall Manager, and AWS Shield Advanced Developer Guide "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_monitoring_with_cloudwatch_dashboards/2_cleanup/","title":"Teardown","tags":[],"description":"","content":"Remove the dashboard you created The AWS free tier allows for 3 Dashboards for up to 50 metrics per month for free, but to ensure you are not charged for the dashboard, you should remove it if you created one in the previous step.\nReferences \u0026amp; useful resources  See Key Metrics From All AWS Services  Focus on Metrics and Alarms in a Single AWS Service  Focus on Metrics and Alarms in a Resource Group    function prevStep(){ window.open(\"..\\/1_intro\\/\", \"_self\") } function nextStep(){ window.open(\"\\/cost\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with PERF7 - \u0026ldquo;How do you monitor your resources to ensure they are performing?\u0026quot;\nClick here to access the Well-Architected Tool   Previous Step Complete this lab     "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_3_pricing_models/2_usage_trend/","title":"Understand your usage trend","tags":[],"description":"","content":"In large organizations usage can be distributed across many teams, and could take significant effort to collect. We can assist this effort by using tooling to understand your overall trends in usage to make an informed choice on Savings Plan commitments.\nYou can use the Compute Savings Plan for this exercise if you plan on purchasing a compute plan. However we will use an EC2 Instance plan to provide more granularity and insights into usage.\n  Click on Recommendations and then select EC2 Instance Savings Plans type, 1-year Savings Plans term, All upfront, and 60 days time period:   Scroll down to Recommended EC2 Instance Savings Plans, take note of the Recommended commitment:   Scroll up and change it to 30 days analysis:   Scroll down to Recommended EC2 Instance Savings Plans, take note of the Recommended commitment:   Scroll up and change it to 7 days analysis:   Scroll down to Recommended EC2 Instance Savings Plans, take note of the Recommended commitment:   Compare the trends in usage to see if your usage is increasing or decreasing. If usage is decreasing make a smaller initial hourly commitment, then re-analyze in 2-4 weeks. If usage is steady or increasing make a commitment closer to the recommended commitment:\n  You now have an understanding of your overall usage trend, and can use this information to make a commitment that is matched to your business requirements.\n function prevStep(){ window.open(\"..\\/1_view_recommendations\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/3_analyze_recommendations\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/100_labs/100_cloudfront_with_s3_bucket_origin/2_upload_file/","title":"Upload example index.html file","tags":[],"description":"","content":" Create a simple index.html file, you can create by coping the following text into your favourite text editor.  \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Example\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Example Heading\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;Example paragraph.\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Open the Amazon S3 console at https://console.aws.amazon.com/s3/ . In the console click the name of your bucket you just created. Click the Upload button.  Click the Add files button, select your index.html file, then click the Upload button.  Your index.html file should now appear in the list.  "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_4_cost_and_usage_analysis/2_setup_athena/","title":"Use AWS Glue to enable access to CUR files via Amazon Athena","tags":[],"description":"","content":"We will use AWS Glue and setup a scheduled Crawler, which will run each day. This crawler will scan the CUR files and create a database and tables for the delivered files. If there are new versions of a CUR, or new months delivered - they will be automatically included.\nWe will use Athena to access and view our CUR files via SQL. Athena is a serverless solution to be able to execute SQL queries across very large amounts of data. Athena is only charged for data that is scanned, and there are no ongoing costs if data is not being queried, unlike a traditional database solution.\n  Go to the Glue console:   Click on Get started if you have not used Glue before\n  Ensure you are in the region where your CUR files are delivered, click on Crawlers and click Add crawler:   Enter a Crawler name starting with Cost, and click Next:   Select Data stores, and click Next:   Ensure you select Specified path in another account, and enter the S3 path of your bucket s3://(CUR bucket), expand Exclude patterns, enter the following patterns one line at a time and click next:\n **.json, **.yml, **.sql, **.csv, **.gz, **.zip     Add another data store, click Next:   Select Create an IAM role, enter a role name of Cost_MasterCrawler, and click Next:   Click the Down arrow, and select a Daily Frequency:   Enter in a Start Hour and Start Minute, then click Next:   Click Add database:   Enter a Database name of costmaster, and click Create:   Click Next:   Review the crawler and click Finish:   Select the checkbox next to the crawler, click Run crawler:   You will see the Crawler was successful and created a table:   Click Databases   Select the costmaster database that Glue created:   Click Tables in costmaster:   Click the table name:   Verify the recordCount is not zero, if it is - go back and verify the steps above:   Go to the Athena Console:   Select the drop down arrow, and click on the new database:   A new table will have been created (named after the CUR), we will now load the partitions. Click on the 3 dot menu and select Load partitions:   You will see it execute the command MSCK REPAIR TABLE, and in the results it may add partitions to the metastore for each month that has a billing file:   NOTE: It may or may not add partitions and show the messages above.\n If you are using the supplied files for this lab, check:\n The folder names year and month are in S3 and the case matches There are parquet files in each of the month folders   We will now preview the data. Click on the 3 dot menu and select Preview table:   It will execute a Select * from query, and in the results you will see the first 10 lines of your CUR file:   You have successfully setup your CUR file to be analyzed. You can now query your usage and costs via SQL.\n  function prevStep(){ window.open(\"..\\/1_verify_cur\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/3_cur_analysis\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_aws_resource_optimization/2_resource_opt/","title":"Using Amazon EC2 Resource Optimization Recommendations","tags":[],"description":"","content":"NOTE: In order to complete this step you need to have Amazon EC2 Resource Optimization enabled, you can do that by going to AWS Cost Explorer, Recommendations (left bar) section.\nAmazon EC2 Resource Optimization offers rightsizing recommendations in AWS Cost Explorer without any additional cost. These recommendations identify likely idle and underutilized instances across your accounts, regions and tags. To generate these recommendations, AWS analyzes your historical EC2 resource usage (using Amazon CloudWatch) and your existing reservation footprint to identify opportunities for cost savings (e.g., by terminating idle instances or downsizing active instances to lower-cost options within the same family/generation).\n  Navigate to the AWS Cost Explorer page   Select Recommendations in the left menu bar   Click on the View All link associated with the Amazon EC2 Resource Optimization Recommendations section.   In case you havent enabled the Amazon EC2 Resource Optimization please do so (no additional cost), it may take up to 24 hours in order to generate your first recommendations. Only regular or payer accounts can enable it and by default both linked and payer accounts will be able to access their rightsizing recommendations unless the payer account specifically prohibits it on the settings page (top right). To improve the quality of recommendations, AWS might use other utilization metrics that you might be collecting, such as disk or memory utilization. All resource utilization metrics are anonymized and aggregated before AWS uses them for model training. If you would like to opt out of this experience and request your metrics not be stored and used for model improvement, please submit an AWS support ticket . For more information, see AWS Service Terms .\nAssuming you had enabled the Amazon EC2 Resource Optimization Recommendations, you will be presented with a screen that provides recommendations (if any exists):    Optimization opportunities  The number of recommendations available based on your resources and usage Estimated monthly savings  The sum of the projected monthly savings associated with each of the recommendations provided Estimated savings (%)  The available savings relative to the direct Amazon EC2 costs (On-Demand) associated with the instances in the recommendation list  You can also filter your recommendations by the type of action (Idle and Underutilized), Linked Account, Region and Tag.\nClick on view next to a recommendation, to view the details:   How are the potential savings calculated? AWS will first examine the instance running during the last 14 days to identify if it was partially or fully covered by an RI or running On-Demand. Another factor is whether the RI is instance size-flexible. The cost to run the instance is calculated based on the On-Demand hours and the hourly rate for the instance type. For each recommendation, AWS calculates the cost to operate a new instance. AWS assumes that an instance size-flexible Reserved Instance will cover the new instance in the same way as the previous instance. Savings are calculated based on the number of On-Demand running hours and the difference in On-Demand rates. If the Reserved Instance isn\u0026rsquo;t instance size-flexible, the savings calculation is based on whether the instance hours during the last 14 days are operated as On-Demand. AWS will only provide recommendations with estimated savings greater than or equal to $0.\nAmazon EC2 Resource Optimization recommendations already excludes Spot usage and takes into consideration the existing Reserved Instances and Savings Plan footprint to provide recommendations. There are two types of recommended actions: Terminate if the instance is considered idle (max CPU utilization is at or below 1%) or Downsize if the instance is underutilized (max CPU utilization is between 1% and 40%).\nYou can enable Amazon CloudWatch to report memory utilization and improve the recommendation accuracy. Please check the 200 level EC2 Right Sizing lab for more information on how to enable memory utilization metrics. Finally, Amazon EC2 Resource Optimization recommendations will only ever recommend up to three, same-family instances as target instances for downsizing.\n function prevStep(){ window.open(\"..\\/1_cloudwatch_intro\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/3_prio_resource_opt\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_5_cost_visualization/2_cost_usage_account/","title":"View your cost and usage by account","tags":[],"description":"","content":"We will now view usage by account. This helps to highlight where the costs and usage are by linked account. NOTE: you will need one or more multiple accounts for this exercise to be effective.\n  Select Saved reports from the left menu:   Click on Monthly costs by linked account:   It will show the default last 6 months, with a monthly granularity.   As above, change the graph to Daily granularity and from a bar graph to a Line graph:   Here is the daily granularity line graph. You can see there is one account which has the most cost, so lets focus on that by applying a filter:   On the right click on Linked Account, select the checkbox next to the account we want to focus on, then click Include only and Apply filters:   You can now see this one accounts usage:   Lets see the services breakdown for this account, click on Service to group by services and change it to a line graph:   You can see the service breakdown for this account. Lets see the instance type breakdown for this account, click on Instance Type and change it to a line graph:   You can see the instance type breakdown for this account. Lets see the usage type breakdown for this account, click on Usage Type and change it to a line graph:   Here is the usage type breakdown:   You have now viewed the costs by account and applied multiple filters. You can continue to modify the report by timeframe and apply other filters.\n function prevStep(){ window.open(\"..\\/1_cost_usage_service\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/3_sp_coverage\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_4_cost_and_usage_analysis/2_cost_usage_detail/","title":"View your cost and usage in detail","tags":[],"description":"","content":"You can view past and present costs and usage through the console, which also provides more detailed information on cost and usage. We will go through accessing your cost and usage by service, and by linked account (if applicable). We will then drill down into a specific service.\n  Go to the billing dashboard:   Click on Bills from the left menu:   Select the Date you require from the drop down menu, by clicking on the menu item:   You will be shown Bill details by service, where you can dynamically drill down into the specific service cost and usage. Pick your largest cost service and look into the region and line items:   Select Bill details by account to see cost and usage for each account separately. Select the Account name, then drill down into the specific service cost and usage:    function prevStep(){ window.open(\"..\\/1_view_invoice\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/3_cost_usage_download\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_2_cost_and_usage_governance/","title":"Level 100: Cost and Usage Governance","tags":[],"description":"","content":"Last Updated May 2020\nAuthors  Nathan Besh, Cost Lead Well-Architected  Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com\nIntroduction This hands-on lab will guide you through the steps to implement cost and usage governance. The skills you learn will help you control your cost and usage in alignment with your business requirements.\nGoals  Implement AWS Budgets to notify on usage and spend Create an AWS Budget report to notify users every week on budget status  Prerequisites  A small amount of usage in your account, less than $5 to trigger the budget AWS Account Setup has been completed  Permissions required  Access to the Cost Optimization team created in AWS Account Setup   Costs  https://aws.amazon.com/aws-cost-management/pricing/ Less than $1 per month if the tear down is not performed  Time to complete  The lab should take approximately 15 minutes to complete  Steps:  Create and implement an AWS Budget for monthly forecasted cost    Create and implement an AWS Budget for EC2 actual cost   Create and implement an AWS Budget for EC2 Savings Plan coverage   Create and implement an AWS Budget Report   Tear down    function prevStep(){ window.open(\"\", \"_self\") } function nextStep(){ window.open(\".\\/1_budget_forecast\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool  Start Lab  "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_2_cost_and_usage_governance/","title":"Level 200: Cost and Usage Governance","tags":[],"description":"","content":"Last Updated May 2020\nAuthors  Nathan Besh, Cost Lead Well-Architected Spencer Marley, Commercial Architect  Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com\nIntroduction This hands-on lab will guide you through the steps to implement cost and usage governance. The skills you learn will help you control your cost and usage in alignment with your business requirements.\nGoals  Implement IAM Policies to control usage  Prerequisites  AWS Account Setup has been completed  Permissions required  Log in as the Cost Optimization team, created in AWS Account Setup  Add the following IAM Policy for this lab NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required.  Costs  A small number of instances will be started \u0026amp; then immediately terminated Costs will be less than $5 if all steps including the teardown are performed  Time to complete  The lab should take approximately 15 minutes to complete  Steps:  Create a group of users for testing   Create an IAM Policy to restrict service usage by region   Create an IAM Policy to restrict EC2 usage by family   Extend an IAM Policy to restrict EC2 usage by instance size   Create an IAM policy to restrict EBS Volume creation by volume type   Teardown    function prevStep(){ window.open(\"\", \"_self\") } function nextStep(){ window.open(\".\\/1_create_test_group\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool  Start Lab  "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_deploy_and_update_cloudformation/","title":"Level 200: Deploy and Update CloudFormation","tags":["implement_change"],"description":"Improve reliability of a service by using automation to make changes in your cloud infrastructure","content":"Author  Seth Eliot, Principal Reliability Solutions Architect, AWS Well-Architected  Introduction This hands-on lab will guide you through the steps to improve reliability of a service by using automation to make changes in your cloud infrastructure. When this lab is completed, you will have deployed and edited a CloudFormation template. Using this template you will deploy and modify a VPC, an S3 bucket and an EC2 instance running a simple web server.\nAWS Well-Architected offers two different CloudFormation labs illustrating Reliability best practices. Choose which lab you prefer (or do both):\n This is the 200 level lab where you create an infrastructure using CloudFormation and then make several modifications to it. Because this 200 level lab includes modification and update as part of the exercise, it uses a simplified, single-tier architecture, which does not follow best practices for reliability If you prefer a simpler lab that does deployment only, or want to see how to use CloudFormation to deploy a a multi-tier reliable architecture using Amazon EC2, see this 100 level lab: Deploy a Reliable Multi-tier Infrastructure using CloudFormation   The skills you learn will help you build resilient workloads in alignment with the AWS Well-Architected Framework Goals By the end of this lab, you will be able to:\n Automate change for your workload Document and track changes in code Implement infrastructure as a service  Prerequisites If you are running this at an AWS sponsored workshop then you may be provided with an AWS Account to use, in which case the following pre-requisites will be satisfied by the provided AWS account. If you are running this using your own AWS Account, then please note the following prerequisites:\n An AWS Account that you are able to use for testing. This account MUST NOT be used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to create IAM Roles, EC2 instances, S3 buckets, VPCs, Subnets, and Internet Gateways  NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\n  function prevStep(){ window.open(\"\", \"_self\") } function nextStep(){ window.open(\".\\/1_deploy_infra\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool  Start Lab  Steps:  Deploy Infrastructure using a CloudFormation Stack   Explore your Deployed Infrastructure   Configure Deployed Resources using Parameters   Add an Amazon S3 Bucket to the Stack   Add an Amazon EC2 Instance to the Stack   Tear down this lab   Costs NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\n  This lab will cost approximately $1.00 per day when deployed It may be less (or zero) if you have remaining AWS Free Tier usage on your account The majority of this cost is the charge for EC2 BoxUsage (per hour usage charge) for the single EC2 instance you deploy Please follow the directions for Tear Down to avoid unwanted costs after you have concluded this lab  "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_automated_cur_updates_and_ingestion/","title":"Level 300: Automated CUR Updates and Ingestion","tags":[],"description":"","content":"Authors  Nathan Besh, Cost Lead, Well-Architected Derrick Gold, Software Development Engineer, AWS Insights  Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com\nIntroduction This hands-on lab will guide you through the steps to enable automated updates of your CUR files into Athena. The skills you learn will help you perform cost and usage analysis in alignment with the AWS Well-Architected Framework.\nGoals  Automatically update the CUR table in Athena/Glue when a new report arrives Automatically update the CUR table for multiple Cost and Usage Reports in the same bucket  Prerequisites  An AWS Account CUR enabled and delivered into S3, with Athena integration 6-12 months AWS experience, able to navigate the console, and have an understanding of the underlying services and features  Steps:  Create the CloudFormation Stack   Multiple CURs   Teardown    function prevStep(){ window.open(\"\", \"_self\") } function nextStep(){ window.open(\".\\/1_cf_stack\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool  Start Lab  "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_iam_tag_based_access_control_for_ec2/","title":"Level 300: IAM Tag Based Access Control for EC2","tags":[],"description":"","content":"Introduction This hands-on lab will guide you through the steps to configure example AWS Identity and Access Management (IAM) policies, and a AWS IAM role with associated permissions to use EC2 resource tags for access control. Using tags is powerful as it helps you scale your permission management, however you need to be careful about the management of the tags which you will learn in this lab. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as an EC2 administrator. This allows the EC2 administrator to create tags when creating resources only if they match the requirements, and control which existing resources and values they can tag.\nThe skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nGoals  IAM least privilege IAM policy conditions  Prerequisites  An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user with MFA enabled that can assume roles in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .  Steps:  Create IAM policies   Create Role   Test Role   Knowledge Check   Tear down   References \u0026amp; useful resources AWS Identity and Access Management User Guide IAM Best Practices and Use Cases Become an IAM Policy Master in 60 Minutes or Less Actions, Resources, and Condition Keys for Identity And Access Management "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/","title":"Level 300: Testing for Resiliency of EC2, RDS, and AZ","tags":["test_resiliency"],"description":"Use code to inject faults simulating EC2, RDS, and Availability Zone failures. These are used as part of Chaos Engineering to test workload resiliency","content":"  Your browser doesn't support video, or if you're on GitHub head to [wellarchitectedlabs.com](https://www.wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/) to watch the video.   Authors  Rodney Lester, Senior Solutions Architect Manager, AWS Well-Architected Adrian Hornsby, Principal Tech Evangelist, AWS Seth Eliot, Principal Reliability Solutions Architect, AWS Well-Architected  Introduction The purpose if this lab is to teach you the fundamentals of using tests to ensure your implementation is resilient to failure by injecting failure modes into your application. This may be a familiar concept to companies that practice Failure Mode Engineering Analysis (FMEA). It is also a key component of Chaos Engineering, which uses such failure injection to test hypotheses about workload resiliency. One primary capability that AWS provides is the ability to test your systems at a production scale, under load.\nIt is not sufficient to only design for failure, you must also test to ensure that you understand how the failure will cause your systems to behave. The act of conducting these tests will also give you the ability to create playbooks how to investigate failures. You will also be able to create playbooks for identifying root causes. If you conduct these tests regularly, then you will identify changes to your application that are not resilient to failure and also create the skills to react to unexpected failures in a calm and predictable manner.\nIn this lab, you will deploy a 3-tier resource, with a reverse proxy (Application Load Balancer), Web Application on Amazon Elastic Compute Cloud (EC2), and MySQL database using Amazon Relational Database Service (RDS). There is also an option to deploy the same stack into a different region, which provides you the ability to progress from simpler component failure testing to failure testing under a simulated AWS regional failure.\nThe skills you learn will help you build resilient workloads in alignment with the AWS Well-Architected Framework Goals:  Reduce fear of implementing resiliency testing by providing examples in common development and scripting languages Resilience testing of EC2 instances Resilience testing of RDS Multi-AZ instances Resilience testing using Availability Zones failures Resilience testing of S3 objects Learn how to implement resiliency using those tests Learn how to think about what a failure will cause within your infrastructure Learn how common AWS services can reduce mean time to recovery (MTTR)  Prerequisites:  An AWS Account that you are able to use for testing, that is not used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to create Amazon Virtual Private Cloud(s) (VPCs), including subnets, security groups, internet gateways, NAT Gateways, Elastic IP Addresses, and route tables. The credentials must also be able to create the database subnet group needed for a Multi-AZ RDS instance. The credential will need permissions to create IAM Role, instance profiles, AWS Auto Scaling launch configurations, application load balancers, auto scaling group, and EC2 instances. An IAM user or federated credentials into that account that has permissions to deploy the deployment automation, which consists of IAM service linked roles, AWS Lambda functions, and an AWS Step Functions state machine to execute the deployment.  Note: This 300 level lab covers multiple failure injection scenarios. If you would prefer a simpler 200 level lab that demonstrates only EC2 failure injection, then see Level 200: Testing for Resiliency of EC2 instances . This 300 level lab here includes everything in the 200 level lab, plus additional failure simulations.\n function prevStep(){ window.open(\"\", \"_self\") } function nextStep(){ window.open(\".\\/1_deploy_infra\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool  Start Lab  Steps:  Deploy the Infrastructure and Application   Configure Execution Environment   Preparation for Failure Injection   Test Resiliency Using EC2 Failure Injection   Test Resiliency Using RDS Failure Injection   Test Resiliency Using Availability Zone (AZ) Failure Injection   Test Resiliency Using Failure Injection - Optional steps   Tear down this lab   Costs NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\n  This lab will cost approximately $6.50 per day when deployed About half of this cost is the charge for NatGateway-Hours Please follow the directions for Tear Down to avoid unwanted costs after you have concluded this lab  Additional lab resources:  Troubleshooting Guide for common problems encountered while deploying and conducting this lab Builders Guide for building the AWS Lambda functions and the web server and where to make changes in the lab guide to use the code you built instead of the publicly available executables.  "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_100_quick_steps_to_security_success/","title":"Quest: Quick Steps to Security Success","tags":[],"description":"In just one day (or an hour a day for a week!) implement some foundational security controls to immediately improve your security posture.","content":"Authors  Byron Pogson, Solutions Architect  About this Guide This quest is for you to improve your security posture. Every stakeholder involved in your organization and product or service is entitled to make use of a secure platform. Security is important to earn the trust with your customers and your providers. A secure environment also helps to protect your intellectual property. Each set of activities can be done in one day or split over a week in your lunch break. Further discussion can be found in best practices for your AWS environment For more context on this quest see Essential Security Patterns from Public Sector Summit Canberra 2019 and the associated slide deck on SlideShare Implementing multiple AWS accounts for your workload improves your security by isolating parts of your workload to limit the blast radius. Understanding cross account access ensures that common resources can continue to be shared among separate workloads. This quest will guide you to setting up a foundational multi-account environment which allows you to implement appropriate controls on top of it while still maintaining a centralized view and flexibility to adapt to your business processes.\nThis quest leverages AWS Control Tower to implement your best practice landing zone. AWS Control Tower is a managed service to setup up and govern secure multi-account AWS environment. Control Tower is not currently available in all regions so instructions are also provided for an alternate approach too. It is strongly recommend that you set up your account landing zone with Control Tower as it is a managed service supported directly by AWS and includes many best practices and guardrails.\nSteps:  Control Tower   Centralize Identities   Enable Additional Guardrails   Monitoring and Alerting   Operating   "},{"uri":"https://wellarchitectedlabs.com/cost/expenditureawareness/","title":"Expenditure Awareness","tags":[],"description":"","content":"About expenditure awareness The capability to attribute resource costs to the individual organization or product owners drives efficient usage behavior and helps reduce waste. Accurate cost attribution allows you to know which products are truly profitable, and allows you to make more informed decisions about where to allocate budget.\n Step 1 - Cost and Usage Governance - Notifications Configuring notifications allows you to receive an email when usage or cost is above a defined amount.\n           100 Level Lab: This lab will show you how to implement AWS Budgets to provide notifications on usage and spend.     Step 2 - Monitor Usage and Cost - Analysis Cost and Usage Analysis will enable you to understand how you consumed the cloud, and what your costs are for that consumption.\n           100 Level Lab: This lab introduces you to the billing console, allowing you to view your current and past bills, and also inspect your usage across services and accounts.     Step 3 - Monitor Usage and Cost - Visualization Visualizing cost and usage highlights trends and allows you to gain further insights.\n           100 Level Lab: This lab will introduce AWS Cost Explorer, and demonstrate how to use its features to provide insights.     Step 4 - Govern Usage and Cost - Controls Implementing usage controls will ensure excess usage and accompanying costs does not occur.\n           200 Level Lab: This lab will extend the permissions of the Cost Optimization team, then utilize Identity and Access Management (IAM) policies to control and restrict usage.     Step 5 - Monitor Usage and Cost - Advanced Analysis Advanced analysis using your Cost and Usage Report (CUR) will allow you to answer the most challenging questions on your usage and cost. It is the most detailed source of information on your cost and usage available.\n           200 Level Lab: This lab will utilize Amazon Athena to provide an interface to query the CUR, provide you the most common customer queries, and help you to build your own queries.     Step 6 - Monitor Usage and Cost - Advanced Visualization Utilizing the CUR data source in the previous step, you can provide more detailed and custom visualizations and dashboards.\n           200 Level Lab: This Lab extends the previous step, utilizing Amazon Quicksight to visualize the CUR data source.     Step 7 - Monitor Workload Efficiency This hands-on lab will guide you through the steps to measure the efficiency of a workload. It shows you how to get the overall efficiency, then look deeper for patterns in usage to be able to allocate different weights to different outputs of a system.\n           200 Level Lab: This lab combines your application logs with cost data, to provide an efficiency metric and insights for your workloads.     Step 8 (Optional) - Automated CUR Updates and Ingestion This hands-on lab will guide you through the steps to enable automated updates of your CUR files into Athena, every time a new CUR file is delivered.\n           300 Level Lab: This lab uses s3 events and Lambda to trigger a Glue crawler and update Athena when a new CUR is delivered.     Step 9 (Optional) - Splitting and sharing the CUR This hands-on lab will guide you on how to automatically extract part of your CUR file, and then deliver it to another S3 bucket and folder to allow another account to access it. Ideal for partners to deliver a sub-account only CUR to each of its customers, or large enterprises.\n           300 Level Lab: This Lab uses S3 events, Lambda and Athena to extract part of a CUR file and deliver it to an S3 bucket for another account.    Step 10 (Optional) - Automated CUR Reports and email delivery            300 Level Lab: This Lab uses CloudWatch events, Lambda, Athena and SES to run queries against the CUR file, and send financial reports to recipients.    Step 11 (Optional) - Enterprise Dashboards            200 Level Lab: Build an Enterpise QuickSight dashboard for cost and usage analysis.    "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_goals_and_targets/2_govern_usage/","title":"Govern Usage","tags":[],"description":"","content":"Govern Usage Controls Notifications  Goal: Ensure relevant people are notified when costs are predicted to exceed the set budget Target: All accounts have a forecasted budget set, with notifications sent to management, finance and technical leads Best Practice: Controls - Notifications  Measures: % of accounts with budgets set Good/Bad: Good Why? When does it work well or not?: Ensure relevant staff are made aware of potential deviations from expected costs and usage Contact/Contributor: natbesh@amazon.com    function prevStep(){ window.open(\"..\\/1_cloud_financial_management\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/3_monitor_cost_usage\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cost_journey/3_tear_down/","title":"Teardown","tags":[],"description":"","content":"The following resources were created in this lab:\n S3 Bucket: (custom name) Lambda Function: Cost_W-A_Journey IAM Role: extract-wa-reports_role IAM Policy: WAReportAccess   function prevStep(){ window.open(\"..\\/5_format_dashboard\\/\", \"_self\") } function nextStep(){ window.open(\"\\/cost\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Congratulations! Now that you have your Cost Optimization Journey, you can continue to implement the best practices within your organization and workloads.\nClick here to access the Well-Architected Tool   Previous Step Complete this lab     Suggested definitions These have been used to configure the templated journey file, and should help guide you to making modifications.\nEffort    Effort Level People Involved Time Taken non-tech example tech example     1 2 \u0026lt;3min execute a step click on something   2 1 \u0026lt;15min execute a simple process, follow a few steps single click, multiple times   3 1-5 \u0026lt;1hr execute an easy process execute a lab, no discussions   4 1-5 \u0026lt;4hrs execute an easy, well defined, long process bulid something simple, a script, complete a 100level lab solo   5 3-10 \u0026lt;4-8hrs create a simple process, execute a process with non-diverse roles, multile inputs/consideration with no contention 100 lab with a few people, 200-300 lab solo   6  \u0026lt;3days refactor a component 400+ level solo, highly technical   7  \u0026lt;3days thorough \u0026amp; deep analysis - one session, create a complex/large process with non-diverse roles, execute a process with diverse roles, multiple inputs/contention    8  3-5days multiple sessions of thought \u0026amp; deep analysis    9  1month refactor multiple components    10  6months refactor a workload, create a complex and large process with multiple inputs/contention     Frequency    Frequency Example     1 once   2 yearly   3 bi-annual   4 quarterly   5 monthly   6 multiple times a month   7 weekly   8 multiple times a week   9 daily   10 multiple times a day   11 on event    Duration    Duration Example - time to build capability     1 can be done at the same time as the previous best practice   2 an hour   3 a few hours   4 1 day   5 2-3 days   6 1 week   7 2-3 weeks   8 1 month   9 3 months   10 6 months    "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_testing_backup_and_restore_of_data/3_enable_notifications/","title":"Enable Notifications","tags":[],"description":"","content":"In the cloud, setting up notifications to be aware of events within your workload is easily achieved. AWS Backup leverages AWS SNS to send notifications related to backup activities that are occurring. This will allow visibility into backup job statuses, restore job statuses, or any failures that may have occurred, allowing your Operations teams to respond appropriately.\n  Open a terminal where you have access to the AWS CLI. Ensure that the CLI is up to date and that you have AWS Administrator Permissions to run AWS CLI commands. https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html\n  Edit the following AWS CLI command and include the ARN of the SNS TOPIC that you created. Replace  with the ARN of the SNS TOPIC obtained from the outputs section of the CloudFormation Stack. Note that the backup vault name is case sensitive.\naws backup put-backup-vault-notifications --region us-east-1 --backup-vault-name BACKUP-LAB-VAULT --backup-vault-events BACKUP_JOB_COMPLETED RESTORE_JOB_COMPLETED --sns-topic-arn \u0026lt;YOUR SNS TOPIC ARN\u0026gt;\n  Once edited, run the above command, it will enable notifications with messages published to the SNS TOPIC every time a backup or restore job is completed. This will ensure the Operations team is aware of any failures with backing up or restoring data.\n  You can verify that notifications have been enabled by running the following command. The output will include a section called SNSTopicArn followed by the ARN of the SNS Topic that was created as part of the lab.\naws backup get-backup-vault-notifications --backup-vault-name BACKUP-LAB-VAULT --region us-east-1\n  You have now successfully enabled notifications for the backup vault BACKUP-LAB-VAULT, ensuring that the Operations team is aware of completion of backup and restore activities involving this vault, and any failures associated with those activities.\n function prevStep(){ window.open(\"..\\/2_configure_backup_plan\", \"_self\") } function nextStep(){ window.open(\"..\\/4_test_restore\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_testing_backup_and_restore_of_data/","title":"Level 200: Testing Backup and Restore of Data","tags":["data_backup"],"description":"Create a strategy to backup data sources periodically using AWS Backup, and automate the testing of the restore process","content":"Authors  Mahanth Jayadeva, Solutions Architect, Well-Architected  Introduction It is not sufficient to just create backups of data sources, you must also test these backups to ensure they can be used to recover data. A backup is useless if you are unable to restore your data from it. Testing the restore process after each backup will ensure you are aware of any issues that might arise during a restore down the line.\nIn this lab, you will create an EC2 Instance as a data source. You will then create a strategy to backup these data sources periodically using AWS Backup, and finally, automate the testing of the restore process as well as cleanup of resources using AWS Lambda.\nThe skills you learn will help you define a backup and restore plan in alignment with the AWS Well-Architected Framework Goals:  Create a Backup Strategy to ensure mission-critical data is being backed up regularly Test restoring from backups to ensure there are no data recovery issues Learn how to automate this process  Prerequisites:  An AWS Account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account that has Administrator privileges. Launch the CloudFormation Stack to provision resources that will act as data sources.  NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\n  function prevStep(){ window.open(\"\", \"_self\") } function nextStep(){ window.open(\".\\/1_prerequisite\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool  Start Lab  Steps:  Deploy the Infrastructure   Create Backup Plan   Enable Notifications   Test Restore   Teardown   "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_using_awscli_to_manage_wa_reviews/3_perform_review/","title":"Performing a review","tags":[],"description":"","content":"Overview Now that we have created a workload, we will answer the question OPS 5. How do you reduce defects, ease remediation, and improve flow into production. For this question, we will select a subset of the best practices, affirming them as true (turning them from unchecked to checked):\n Use version control Use configuration management systems Use build and deployment management systems Perform patch management Use multiple environments  Step 1 - Find the QuestionId and ChoiceID for a particular pillar question and best practice   Make sure you have the WorkloadId from the previous step and replace WorkloadId with it\n  To find the OPS 5 question listed above, we will search through the pillar for any question that begins with the string of the question.\n  The simplest way to find the QuestionId for a pillar is to search the output from the list-answers API call.\n  Using the list-answers API , we can search the output for the question we want to answer.\naws wellarchitected list-answers --workload-id \u0026quot;\u0026lt;WorkloadId\u0026gt;\u0026quot; --lens-alias \u0026quot;wellarchitected\u0026quot; --pillar-id \u0026quot;operationalExcellence\u0026quot; --query 'AnswerSummaries[?starts_with(QuestionTitle, `How do you reduce defects, ease remediation, and improve flow into production`) == `true`].QuestionId'   This will return the value of the QuestionId, in this case dev-integ   Next, using the get-answer API we can find the ChoiceId values of each answer. For this first command, we will get the ChoiceId for \u0026ldquo;Use version control\u0026rdquo;\naws wellarchitected get-answer --workload-id \u0026quot;\u0026lt;WorkloadId\u0026gt;\u0026quot; --lens-alias \u0026quot;wellarchitected\u0026quot; --question-id \u0026quot;dev-integ\u0026quot; --query 'Answer.Choices[?starts_with(Title, `Use version control`) == `true`].ChoiceId'   This will return the value of the ChoiceId, in this case ops_dev_integ_version_control   Now we need to get the rest of the ChoiceID\u0026rsquo;s for each of the best practices we want to select for the question\naws wellarchitected get-answer --workload-id \u0026quot;\u0026lt;WorkloadId\u0026gt;\u0026quot; --lens-alias \u0026quot;wellarchitected\u0026quot; --question-id \u0026quot;dev-integ\u0026quot; --query 'Answer.Choices[?starts_with(Title, `Use configuration management systems`) == `true`].ChoiceId' aws wellarchitected get-answer --workload-id \u0026quot;\u0026lt;WorkloadId\u0026gt;\u0026quot; --lens-alias \u0026quot;wellarchitected\u0026quot; --question-id \u0026quot;dev-integ\u0026quot; --query 'Answer.Choices[?starts_with(Title, `Use build and deployment management systems`) == `true`].ChoiceId' aws wellarchitected get-answer --workload-id \u0026quot;\u0026lt;WorkloadId\u0026gt;\u0026quot; --lens-alias \u0026quot;wellarchitected\u0026quot; --question-id \u0026quot;dev-integ\u0026quot; --query 'Answer.Choices[?starts_with(Title, `Perform patch management`) == `true`].ChoiceId' aws wellarchitected get-answer --workload-id \u0026quot;\u0026lt;WorkloadId\u0026gt;\u0026quot; --lens-alias \u0026quot;wellarchitected\u0026quot; --question-id \u0026quot;dev-integ\u0026quot; --query 'Answer.Choices[?starts_with(Title, `Use multiple environments`) == `true`].ChoiceId'   This will return the rest of the values we need for ChoiceID:\nops_dev_integ_conf_mgmt_sys ops_dev_integ_build_mgmt_sys ops_dev_integ_patch_mgmt ops_dev_integ_multi_env   Step 2 - Use the QuestionID and ChoiceID to update the answer in well-architected review  After finding the ChoiceID\u0026rsquo;s in the previous step, we can \u0026ldquo;check\u0026rdquo; each of those items off as achieved using the CLI as well. The next step will set the choiceID\u0026rsquo;s to true, but leave the rest of the best practices as false (unchecked). Using the update-answer API we can update the well-architected workload using the the QuestionId and the ChoiceId\u0026rsquo;s we gathered above. aws wellarchitected update-answer --workload-id \u0026quot;\u0026lt;WorkloadId\u0026gt;\u0026quot; --lens-alias \u0026quot;wellarchitected\u0026quot; --question-id \u0026quot;dev-integ\u0026quot; --selected-choices ops_dev_integ_version_control ops_dev_integ_conf_mgmt_sys ops_dev_integ_build_mgmt_sys ops_dev_integ_patch_mgmt ops_dev_integ_multi_env  This will return the JSON object for the question, and at the bottom you will see SelectedChoices is now populated with the answers we have provided. Because we still have have not checked all critical best practices, this question has still been identified as a high risk item (HRI).   OPTIONAL: Repeat steps 1 and 2 but for the other pillar questions and best practices listed below  SEC 1. How do you securely operate your workload?  Separate workloads using accounts Secure AWS account Keep up to date with security threats   REL 2. How do you plan your network topology?  Use highly available network connectivity for your workload public endpoints Provision redundant connectivity between private networks in the cloud and on-premises environments Ensure IP subnet allocation accounts for expansion and availability Prefer hub-and-spoke topologies over many-to-many mesh Enforce non-overlapping private IP address ranges in all private address spaces where they are connected   PERF 5. How do you configure your networking solution?  Understand how networking impacts performance Evaluate available networking features Choose appropriately sized dedicated connectivity or VPN for hybrid workloads Leverage load-balancing and encryption offloading Choose network protocols to improve performance Choose your workloads location based on network requirements   COST 3. How do you monitor usage and cost?  Configure detailed information sources Identify cost attribution categories Establish organization metrics Configure billing and cost management tools Add organization information to cost and usage     function prevStep(){ window.open(\"..\\/2_create_workload\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/4_save_milestone\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_fault_isolation_with_shuffle_sharding/3_implement_sharding/","title":"Implement sharding","tags":[],"description":"","content":"In this section we will update the architectural design of the workload and implement sharding. Similar to sharding a database where a large database or table is broken up into smaller chunks distributed across multiple servers, we will shard the overall capacity of the workload and segment it so that each shard is responsible for handling a subset of customers only. By minimizing the number of \u0026ldquo;components\u0026rdquo; a single customer is able to interact with within the workload, we will be able to reduce the impact of a potential posion pill. This will result in a much smaller scope of impact depending on the number of shards within the workload. In a sharded system, scope of impact of failures can be calculated using the following formula:\nFor example if there were 100 customers, and the workload was divided into 10 shards, then the failure of any 1 shard will only impact 10% of customers.\nUpdate the workload architecture   Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and select the stack that was created as part of this lab - Shuffle-sharding-lab\n  Click on Update\n  Under Prerequisite - Prepare template, select Replace current template\n For Template source select Amazon S3 URL In the text box under Amazon S3 URL specify https://aws-well-architected-labs-virginia.s3.amazonaws.com/Reliability/300_Fault_Isolation_with_Shuffle_Sharding/sharding.yaml    Click Next\n  No changes are required for Parameters. Click Next\n  For Configure stack options click Next\n  On the Review page:\n Scroll to the end of the page and select I acknowledge that AWS CloudFormation might create IAM resources with custom names. This ensures CloudFormation has permission to create resources related to IAM. Additional information can be found here .  Note: The template creates an IAM role and Instance Profile for EC2. These are the minimum permissions necessary for the instances to be managed by AWS Systems Manager. These permissions can be reviewed in the CloudFormation template under the \u0026ldquo;Resources\u0026rdquo; section - InstanceRole.\n Click Update stack    This will take you to the CloudFormation stack status page, showing the stack update in progress. The stack takes about 1 minute to go through all the updates. Periodically refresh the page until you see that the Stack Status is in UPDATE_COMPLETE.\nWith this stack update, the architecture of the workload has been updated by introducing 2 Application Load Balancer listener rules and Target Groups. These listener rules have been configured to inspect the incoming request for a query-string name. Depending on the value provided, the request is routed to one of two target groups where each target group consists of 2 EC2 instances.\nTest the sharded application Now that the application has been deployed, it is time to test it to understand how it works. The sample application used in this lab is the same as before, a simple web application that returns a message with the Worker that responded to the request. Customers pass in a query string with the request to identify themselves. The query string used here is name.\n  Copy the URL provided in the Outputs section of the CloudFormation stack created in the previous string.\n  Append the query string /?name=Alpha to the URL and paste it into a web browser. The full string should look similar to this - http://shuffle-alb-1p2xbmzo541rr-1602891463.us-east-1.elb.amazonaws.com/?name=Alpha.\n  Refresh the web browser a few times to see that responses are being returned from different EC2 instances on the back-end. Notice that after implementing sharding, you are seeing responses being returned from only 2 instances for customer Alpha\u0026rsquo;s requests. No matter how many times you refresh the page or try a different browser, customer Alpha will only receive responses from 2 EC2 instances. This is because we have created Application Load Balancer listener rules that divert traffic to a specific subset of the overall capacity of the workload, also known as a shard. In this case, customers Alpha, Bravo, and Charlie are mapped to Shard 1 containing Worker 1 and Worker 2, and customers Delta, Echo, and Foxtrot are mapped to Shard 2 containing Worker 3 and Worker 4.\n The list of EC2 instances in your workload can be viewed in the AWS Console here     Update the value for the query string to one of the other customers, the possible values are - Alpha, Bravo, Charlie, Delta, Echo, and Foxtrot\n http://shuffle-alb-8vonmf2ywl5z-682850122.us-east-1.elb.amazonaws.com/?name=Alpha http://shuffle-alb-8vonmf2ywl5z-682850122.us-east-1.elb.amazonaws.com/?name=Bravo http://shuffle-alb-8vonmf2ywl5z-682850122.us-east-1.elb.amazonaws.com/?name=Charlie http://shuffle-alb-8vonmf2ywl5z-682850122.us-east-1.elb.amazonaws.com/?name=Delta http://shuffle-alb-8vonmf2ywl5z-682850122.us-east-1.elb.amazonaws.com/?name=Echo http://shuffle-alb-8vonmf2ywl5z-682850122.us-east-1.elb.amazonaws.com/?name=Foxtrot    Refresh the web browser multiple times to verify that customers are receiving responses only from EC2 instances in the shard they are mapped to\n   function prevStep(){ window.open(\"..\\/2_impact_of_failures\", \"_self\") } function nextStep(){ window.open(\"..\\/4_impact_of_failures_sharding\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_fault_isolation_with_shuffle_sharding/","title":"Level 300: Fault Isolation with Shuffle Sharding","tags":["mitigate_failure"],"description":"Implement shuffle sharding to minimize scope of impact of failures","content":"Authors  Mahanth Jayadeva, Solutions Architect, AWS Well-Architected  Introduction In this lab, you will become familiar with the concept of shuffle sharding and how it can help reduce blast radius during failures. You will learn how to distribute user requests to resources in a combinatorial way so that any failures affect only a small subset of users. \u0026ldquo;Users\u0026rdquo; in this case refers to any source of requests to your workload. This can be other services that call your workload, in addition to actual human users.\nWithout any sharding, any worker (such as servers, queues, or databases) in your workload can handle any request. With this architecture a poisonous request or a flood of requests from one user will spread unabated through the entire fleet. Sharding is the practice of providing logical isolation of capacity (one ore more workers) for each set of users, limiting the propagation of such failures. Shuffle sharding further limits the impact of any one \u0026ldquo;bad actor\u0026rdquo; among your users.\nIn this lab, you will create a workload with a sample application running on EC2 instances. You will then create a strategy to distribute traffic using shuffle sharding and reduce the blast radius when a failure occurs.\nThe skills you learn will help you use bulkhead architectures in alignment with Reliability best practices of the AWS Well-Architected Framework Goals:  Deploy a workload with a bulkhead architecture to limit propagation of failures Implement shuffle sharding to further limit failure propagation Test failure within the workload and ensure the scope of impact is minimized  Prerequisites:  An AWS Account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account that has Administrator privileges.  NOTE: You will be billed for any applicable AWS resources used as part of this lab, that are not covered in the AWS Free Tier.\n function prevStep(){ window.open(\"\", \"_self\") } function nextStep(){ window.open(\".\\/1_deploy_workload\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool  Start Lab  Steps:  Deploy the workload   Scope of Impact of failures   Implement sharding   Impact of failures with sharding   Implement shuffle sharding   Impact of failures with shuffle sharding   Teardown   References \u0026amp; useful resources   "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_monitoring_linux_ec2_cloudwatch/3_creating_cloudwatch_dashboard/","title":"Create CloudWatch Dashboard","tags":["Linux","Amazon Linux","EC2","CloudWatch","CloudWatch Dashboard"],"description":"","content":"We have deployed a single Amazon Linux 2 EC2 instance and we will now create a CloudWatch Dashboard to monitor the memory and CPU resources consumed by the instance.\n From the AWS Console, click the search box and type in CloudWatch (or you can open this link directly https://console.aws.amazon.com/cloudwatch/home)  Click on Dashboards link on the left side  Click on Create Dashboard button If you have not done so already, make sure to click \u0026ldquo;Try out the new interface\u0026rdquo; to see the updated CloudWatch interface.\n  Under Dashboard Name, type in \u0026ldquo;LinuxEC2Server and then click Create Dashboard  You will be presented with dialog box to Add to the dashboard. Select Line and click Next  Select Metrics and click Configure  Under All metrics, scroll down to Custom Namespaces and click on PerfLab  You will find multiple metrics, but we will start with each of the CPUs in the machine. Click on ImageId, InstanceId, InstanceType, cpu  Make sure the Instance Name starts with LinuxMachineDeploy, then click on one of the InstanceId and select Search for this only. This will ensure we are only looking at instances created for this lab.  Under Metric Name, look for cpu_usage_user and then click Add to search. This will limit your choices again to only the CPU metrics for this machine. In this case, you should see 2 processors listed, but if the machine you have deployed has more, you will see them each listed.  On the left side of the screen, select the box right above the list, this will add both metrics to your graph.  Click on the Graphed Metrics (2) tab, and on the right side next to Period, select 5 seconds  The metric graph will now update with more data points. Click Create Widget You should now see the dashboard we have created displayed with the first metric widget.    function prevStep(){ window.open(\"..\\/2_deploy_instance\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/4_adding_metrics_to_dashboard\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_monitoring_windows_ec2_cloudwatch/3_creating_cloudwatch_dashboard/","title":"Create CloudWatch Dashboard","tags":["Windows Server","Windows","EC2","CloudWatch","CloudWatch Dashboard"],"description":"","content":"We have deployed a single Windows 2 EC2 instance and we will now create a CloudWatch Dashboard to monitor the memory and CPU resources consumed by the instance.\n  From the AWS Console, click the search box and type in CloudWatch (or you can open this link directly https://console.aws.amazon.com/cloudwatch/home)   Click on Dashboards link on the left side   Click on Create Dashboard button If you have not done so already, make sure to click \u0026ldquo;Try out the new interface\u0026rdquo; to see the updated CloudWatch interface.\n   Under Dashboard Name, type in \u0026ldquo;WindowsEC2Server\u0026rdquo; and then click \u0026ldquo;Create Dashboard\u0026rdquo;   You will be presented with dialog box to \u0026ldquo;Add to the dashboard\u0026rdquo;. Select Line and click Next   Select Metrics and click Configure   Under \u0026ldquo;All metrics\u0026rdquo;, scroll down to \u0026ldquo;Custom Namespaces\u0026rdquo; and click on \u0026ldquo;CWAgent\u0026rdquo;   You will find multiple metrics, but we will start with each of the CPU\u0026rsquo;s in the machine.\n  Click on \u0026ldquo;ImageId, InstanceId, InstanceType, instance, o\u0026hellip;\u0026rdquo;   Make sure the Instance Name starts with WindowsMachineDeploy, then click on one of the InstanceId and select \u0026ldquo;Search for this only\u0026rdquo;. This will ensure we are only looking at instances created for this lab.   Under Metric Name, look for \u0026ldquo;Processor % User Time\u0026rdquo; and then click \u0026ldquo;Add to search\u0026rdquo;. This will limit your choices again to only the CPU metrics for this machine. In this case you should see 2 processors listed, but if the machine you have deployed has more, you will see them each listed.   On the left side of the screen, select the box right above the list, this will add both metrics to your graph.   Click on the \u0026ldquo;Graphed Metrics (2)\u0026rdquo; tab, and on the right side next to Period, select \u0026ldquo;5 seconds\u0026rdquo;   The metric graph will now update with more data points. Click \u0026ldquo;Create Widget\u0026rdquo;   You should now see the dashboard we have created displayed with the first metric widget.    function prevStep(){ window.open(\"..\\/2_deploy_instance\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/4_adding_metrics_to_dashboard\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_monitoring_windows_ec2_cloudwatch/","title":"Level 100: Monitoring Windows EC2 instance with CloudWatch Dashboards","tags":["Windows Server","Windows","EC2","CloudWatch","CloudWatch Dashboard"],"description":"How to configure an Amazon CloudWatch Dashboard to get aggregated views of the health and performance of a Windows EC2 instance.","content":"Authors  Eric Pullen, Performance Efficiency Lead Well-Architected  Introduction This hands-on lab will guide you through creating an Amazon EC2 instance for Windows and then configuring a Amazon CloudWatch Dashboard to get aggregated views of the health and performance information for that instance. This lab should enables you to quickly get started with CloudWatch monitoring and explore account and resource-based views of metrics. You can find more best practices by reading the Performance Efficiency Pillar of the AWS Well-Architected Framework . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nGoals  Monitor a Windows EC2 machine to identify CPU and memory bottlenecks  Prerequisites  An AWS account that you are able to use for testing, that is not used for production or other purposes An IAM role in your AWS account  Costs  https://aws.amazon.com/cloudwatch/pricing/  You can create 3 dashboards for up to 50 metrics per month on the free tier and then it is $3.00 per dashboard per month This lab creates one dashboard, so the maximum cost would be $3.00 per month if you have already consumed the free tier.   The default lab uses a t3.large EC2 instance which will consume approximately $3.00 for every day the lab is running The VPC that is created for this lab will build a Nat Gateway, and will consume $5.50 per day when deployed. Using defaults, the total cost of the lab would be at least $8.50 per day  NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\n  function prevStep(){ window.open(\"\", \"_self\") } function nextStep(){ window.open(\".\\/1_deploy_vpc\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool  Start Lab  Steps:  Deploying the infrastructure   Deploying an instance   Create CloudWatch Dashboard   Add metrics to Dashboard   Generate CPU and Memory load   Teardown   "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_1_aws_account_setup/3_cur/","title":"Configure Cost and Usage reports","tags":[],"description":"","content":"Cost and Usage Reports provide the most detailed information on your usage and bills. They can be configured to deliver 1 line per resource, for every hour of the day. They must be configured to enable you to access and analyze your usage and billing information.\nConfigure a Cost and Usage Report If you configure multiple Cost and Usage Reports (CURs), then it is recommended to have 1 CUR per bucket. If you must have multiple CURs in a single bucket, ensure you use a different report path prefix so it is clear they are different reports.\n  Log in to your management account as an IAM user with the required permissions, and go to the Billing console:   Select Cost \u0026amp; Usage Reports from the left menu:   Click on Create report:   Enter a Report name (it can be any name), ensure you have selected Include resource IDs and Data refresh settings, then click on Next:   Click on Configure:   Enter a unique bucket name, and ensure the region is correct, click Next:   Read and verify the policy, this will allow AWS to deliver billing reports to the bucket. Click on I have confirmed that this policy is correct, then click Save:   Verify the settings:\n   Ensure your bucket is a Valid Bucket (if not, verify the bucket policy) Enter a Report path prefix (it can be any word) without any \u0026lsquo;/\u0026rsquo; characters Ensure the Time Granularity is Hourly Report Versioning is set to Overwrite existing report Under Enable report data integration for select Amazon Athena, and click Next:   Review the configuration, scroll to the bottom and click on Review and Complete:   You have successfully configured a Cost and Usage Report to be delivered. It may take up to 24hrs for the first report to be delivered.\nThere will be S3 Costs incurred to store the CUR, however the CUR is compressed to minimize costs.\n Configure the CUR Bucket We will update the CUR bucket so that the Cost Optimization linked account can access the CURs.\n  Go to the S3 console, select the CUR Bucket, select Permissions:   Select Bucket Policy:   Add S3 read access to the Cost Optimization account by adding the following statements under the current bucket policy. Edit (Cost Optimization Member account ID) and (CUR bucket) and update the bucket policy:\n { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;AWS\u0026quot;: \u0026quot;arn:aws:iam::(sub account ID):root\u0026quot; }, \u0026quot;Action\u0026quot;: \u0026quot;s3:ListBucket\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:s3:::(CUR bucket)\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;AWS\u0026quot;: \u0026quot;arn:aws:iam::(Cost Optimization Member account ID):root\u0026quot; }, \u0026quot;Action\u0026quot;: \u0026quot;s3:GetObject\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:s3:::(CUR bucket)/*\u0026quot; }      Click here for a completed example policy   { \u0026quot;Version\u0026quot;: \u0026quot;2008-10-17\u0026quot;, \u0026quot;Id\u0026quot;: \u0026quot;123\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;Stmt1335892150622\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;AWS\u0026quot;: \u0026quot;arn:aws:iam::386209384616:root\u0026quot; }, \u0026quot;Action\u0026quot;: [ \u0026quot;s3:GetBucketAcl\u0026quot;, \u0026quot;s3:GetBucketPolicy\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:s3:::(CUR Bucket)\u0026quot; }, { \u0026quot;Sid\u0026quot;: \u0026quot;Stmt1335892526596\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;AWS\u0026quot;: \u0026quot;arn:aws:iam::386209384616:root\u0026quot; }, \u0026quot;Action\u0026quot;: \u0026quot;s3:PutObject\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:s3:::(CUR Bucket)/*\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;AWS\u0026quot;: \u0026quot;arn:aws:iam::(Cost Optimization Member Account ID):root\u0026quot; }, \u0026quot;Action\u0026quot;: \u0026quot;s3:ListBucket\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:s3:::(CUR Bucket)\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;AWS\u0026quot;: \u0026quot;arn:aws:iam::(Cost Optimization Member Account ID):root\u0026quot; }, \u0026quot;Action\u0026quot;: \u0026quot;s3:GetObject\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:s3:::(CUR Bucket)/*\u0026quot; } ] }     Allow bucket owner object ownership for the CUR files. Cick on Object Ownership   Click Edit   Select Bucket owner preferred, click Save   When CUR files are delivered they will now automatically have permissions allowing the bucket owner full control. Re-write of the object ACLs is no longer necessary.\nUpdate existing CURs If there are existing CURs from other reports that need permissions to be updated, you can use the following CLI - which will copy the objects over themselves and update the permissions as it copies.\naws s3 cp --recursive s3://(CUR bucket) s3://(CUR bucket) --grants read=id=(sub account canonical ID) full=id=(management account canonical ID) --storage-class STANDARD  Congratulations - you will now have CURs delivered and accessible by your Cost Optimization account.\n  function prevStep(){ window.open(\"..\\/2_account_structure\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/4_configure_sso\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_automated_deployment_of_detective_controls/","title":"Automated Deployment of Detective Controls","tags":[],"description":"","content":"Last Updated: September 2020\nAuthor: Ben Potter, Security Lead, Well-Architected\nIntroduction This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail, AWS Config, and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to automate the configuration of each service. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nAWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. CloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services.\nAWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. AWS Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations.\nAmazon GuardDuty is a threat detection service that continuously monitors for malicious or unauthorized behavior to help you protect your AWS accounts and workloads. It monitors for activity such as unusual API calls or potentially unauthorized deployments that indicate a possible account compromise. GuardDuty also detects potentially compromised instances or reconnaissance by attackers.\nPrerequisites  An AWS account that you are able to use for testing. Permissions to create resources in CloudFormation, CloudTrail, GuardDuty, Config, S3, CloudWatch.  Costs  Typically less than $2 per month if the account is only used for personal testing or training, and the tear down is not performed AWS CloudTrail pricing  Amazon GuardDuty pricing  AWS Config pricing  Amazon S3 pricing  AWS Pricing   Steps:  Create Stack   Tear down   References \u0026amp; Useful Resources  Automate alerting on key indicators AWS Cloudtrail, AWS Config and Amazon GuardDuty provide insights into your environment. Implement new security services and features: New features such as Amazon GuardDuty have been adopted. Automate configuration management: CloudFormation is being used to configure AWS CloudTrail, AWS Config and Amazon GuardDuty. Implement managed services: Managed services are utilized to increase your visibility and control of your environment.  "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_automated_deployment_of_ec2_web_application/","title":"Automated Deployment of EC2 Web Application","tags":[],"description":"","content":"Last Updated: September 2020\nAuthors: Ben Potter, Security Lead, Well-Architected \u0026amp; Rodney Lester, Manager, Well-Architected\nIntroduction This hands-on lab will guide you through the steps to configure a web application in Amazon EC2 with a defense in depth approach incorporating a number of AWS security best practices. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . The WordPress example CloudFormation template will deploy a basic WordPress content management system, This example is not intended to be a comprehensive WordPress system, please consult Build a WordPress Website for more information.\nThis lab will create the web application and all components using the example CloudFormation template, inside the VPC you have created previously. The components created include:\n Application load balancer Auto scaling group of web instances A role attached to the auto-scaled instances allows temporary security credentials to be used Instances use Systems Manager instead of SSH for administration Amazon Aurora serverless datbase cluster Secrets manager secret for database cluster AWS Key Management Service is used for key management of Aurora database Security groups for load balancer and web instances to restrict network traffic Custom CloudWatch metrics and logs for web instances IAM role for web instances that grants permission to Systems Manager and CloudWatch Instances are configured from the latest Amazon Linux 2 Amazon Machine Image at boot time using user data to install agents and configure services  Overview of wordpress stack architecture: An SSH key is not configured in this lab, instead AWS Systems Manager should be used to manage the EC2 instances as a more secure and scalable method.\nThe Application Load Balancer will listen on unencrypted HTTP (port 80), it is a best practice to encrypt data in transit, you can configure a HTTPS listener after completion of this lab.\nAn example amazon-cloudwatch-agent.json file is provided and automatically downloaded by the instances to configure CloudWatch metrics and logs, this requires that you follow the example naming prefix of WebApp1.\n Prerequisites  An AWS account that you are able to use for testing. Permissions to create resources in CloudFormation, EC2, VPC, IAM, Elastic Load Balancing, CloudWatch, Aurora RDS, KMS, Secrets Manager, Systems Manager. Basic understanding of AWS CloudFormation , visit the Getting Started section of the user guide. Deployed the CloudFormation VPC stack in the lab Automated Deployment of VPC .  Costs Typically less than $20 per month if the account is only used for personal testing or training, and the tear down is not performed:\n EC2 instance default t3.micro X2 (default value) is $0.0208 per hour in us-east-1 region Aurora serverless database default of 2 capacity units is $0.12 per hour in us-east-1 region AWS KMS key for Aurora database is $1.00 per month plus $0.03 per 10,000 requests in us-east-1 region Elastic Load Balancing, Application Load Balancer for Application Load Balancer is $0.0225 per hour in us-east-1 region AWS Secrets Manager secret for database password is $0.40 per month Amazon CloudWatch custom metrics X8 is $2.40 per month per instance in us-east-1 region Amazon CloudWatch logs is $0.50 per GB in us-east-1 region AWS Pricing   Steps:  Create Web Stack   Tear down this lab   "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_automated_deployment_of_iam_groups_and_roles/","title":"Automated Deployment of IAM Groups and Roles","tags":[],"description":"","content":"Last Updated: September 2020\nAuthor: Ben Potter, Security Lead, Well-Architected\nIntroduction This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure AWS Identity and Access Management (IAM) Groups and roles for cross-account access. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of a new or existing AWS account with IAM best practices. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nGoals  Fine-grained authorization Automate security best practices  Prerequisites  An AWS account that you are able to use for testing. Permissions to create resources in IAM.  Costs  There are no costs for this lab AWS Pricing   Steps:  AWS CloudFormation to Create Groups, Policies and Roles with MFA Enforced   Assume Roles from an IAM user   Tear down   "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/","title":"300 Level Advanced Labs","tags":[],"description":"","content":"List of labs available  Level 300: Autonomous Monitoring Of Cryptographic Activity With KMS   Level 300: Autonomous Patching With EC2 Image Builder And Systems Manager   Level 300: IAM Permission Boundaries Delegating Role Creation   Level 300: IAM Tag Based Access Control for EC2   Level 300: Incident Response Playbook with Jupyter - AWS IAM   Level 300: Incident Response with AWS Console and CLI   Level 300: Lambda Cross Account Using Bucket Policy   Level 300: Lambda Cross Account IAM Role Assumption   "},{"uri":"https://wellarchitectedlabs.com/security/100_labs/100_cloudfront_with_s3_bucket_origin/","title":"CloudFront with S3 Bucket Origin","tags":[],"description":"","content":"Last Updated: September 2020\nAuthor: Ben Potter, Security Lead, Well-Architected\nIntroduction This hands-on lab will guide you through the steps to host static web content in an Amazon S3 bucket , protected and accelerated by Amazon CloudFront . Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nPrerequisites  An AWS account that you are able to use for testing. Permissions to Amazon S3 and Amazon CloudFront.  Costs  Typically less than $1 per month (depending on the number of requests) if the account is only used for personal testing or training, and the tear down is not performed. Amazon S3 pricing Amazon CloudFront Pricing  Amazon CloudFront pricing  AWS Pricing   Steps:  Create S3 bucket   Upload example index.html file   Configure Amazon CloudFront   Tear down   References \u0026amp; useful resources Amazon S3 Developer Guide Amazon CloudFront Developer Guide "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/100_labs/100_dependency_monitoring/3_create_alarm/","title":"Create an Alarm","tags":[],"description":"","content":"Now that the right metric has been identified to monitor the dependency, it is time to create an alarm to monitor the metric and send notifications based on thresholds defined. CloudWatch Alarms can be used to automatically initiate actions on your behalf. An alarm watches a single metric over a specified time period, and performs one or more specified actions, based on the value of the metric relative to a threshold over time. The action is a notification sent to an Amazon SNS topic or an Auto Scaling policy.\nAn alarm needs to be created that checks the Lambda function invocation every minute to ensure that it has been invoked at least one time, and treats missing data as an indication that the function has not not been invoked, and as such the dependency that is being monitoring has failed. If the alarm is triggered, a notification should be sent to an SNS topic so that someone can be notified and respond, or an automatic remediation activity can be triggered as a result.\n  Go to the Amazon CloudWatch console at https://console.aws.amazon.com/cloudwatch, click on Alarms, and then Create alarm\n  Click on Select metric\n  In the search bar under All metrics, enter the name of the data read function - WA-Lab-DataReadFunction and press enter\n  In the metric breakdown, select Lambda \u0026gt; By Resource and you will see a list of Lambda metrics available\n  Check the box for the metric Invocations and click Select metric\n  On the Specify metric and conditions page, make the following changes for the Metric:\n Change Statistic from Average to Sum Change Period from 5 minutes to 1 minute    Scroll down to the Conditions section and configure it as follows:\n Threshold type - set to Static Whenever invocations is\u0026hellip; - set to Lower than\u0026hellip; - enter 1 since this is the minimum number of invocations that is expected every minute    Click on the arrow next to Additional Configuration to expand that section and make the following configuration changes:\n Datapoints to alarm should be 1 out of 1 Missing data treatment should be set to Treat missing data as bad (breaching threshold) since Lambda will not report any metrics if a function was not invoked    Click Next to go to the Configure actions page\n  Under Notification, make the following changes:\n Alarm state trigger - select In alarm Select an SNS topic - choose Select an existing SNS topic since a topic was created as part of the CloudFormation stack Send a notification to\u0026hellip; - select WA-Lab-Dependency-Notification and verify that the Email (endpoints) listed below is the same that was specified during stack creation    Click Next to go to the Add name and description page\n  Under Name and description, specify the following:\n Alarm name - enter a recognizable name for the alarm such as WA-Lab-Dependency-Alarm Alarm description - this is an optional field that can be left blank for this exercise    Click Next to go to the Preview and create page\n  Click Create alarm\n  Once the alarm has been created, you will be returned to the Alarms page on the CloudWatch console. In the search bar, enter the name of the alarm that was just created WA-Lab-Dependency-Alarm. The alarm will be listed with a state of Insufficient data. This is because CloudWatch is currently evaluating the underlying metric to determine the current state. In a few minutes, you will see the alarm transition from Insufficient data to OK. Click on the alarm name to go to the alarm details page. Review contents of the page to understand the configuration of the alarm such as metric used, threshold set, evaluation interval, etc. The red line on the graph indicates the threshold that has been set for the alarm. Based on the alarm conditions, the alarm will go into an In alarm state if the metric graph falls below this threshold.\nUnder the History section, you will be able to view all state changes with respect to the alarm. Once the alarm is in an OK state, you will see a State update event under History.\nAn alarm has now been created on a suitable metric to identify if the external service that the workload is dependent on is experiencing outage. Additionally, notifications have been configured to alert relevant stakeholders when the workload outcome is at risk due to failure/unavailability of the external service.\n function prevStep(){ window.open(\"..\\/2_understand_metrics\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/4_test_fail_condition\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_enterprise_dashboards/3_create_data_transfer_cost_analysis/","title":"Create Data Transfer Cost Analysis Dashboard","tags":[],"description":"","content":"Authors  Chaitanya Shah, Sr. Technical Account Manager (AWS)  FAQ The FAQ for this dashboard is here. Introduction The Data Transfer Dashboard is an interactive, customizable and accessible QuickSight dashboard to help customers gain insights into their data transfer. It will analyze any data transfer that incurs a cost such as outbound internet and regional data transfer from all services.\nThis dashboard contains data transfer breakdowns with the following visuals:\n Amount and cost by service and region Between regions Internet data transfer Regional Data transfer  Request Template Access Ensure you have requested access to the Cost Intelligence template here. Create Athena Views The data source for the dashboard will be an Athena view of your existing Cost and Usage Report (CUR).\n  Login via SSO in your Cost Optimization account, go into the Athena console:\n  Create the Data Transfer view by modifying the following code, and executing it in Athena or using aws cli:\n Data Transfer View     The Athena Views are updated to reflect any additions in the cost and usage report. If you created your dashboard prior to July 26, 2020 you will want to update to the latest views.\n Create QuickSight Data Sets We will now create the data sets in QuickSight from the Athena views.\n  Go to the QuickSight service homepage\n  Click on the Datasets and then click on New dataset   Click Athena   Enter a data source name of DataTransfer_Cost_Dashboard and click Create data source:   Select the costmaster database, and the data_transfer_view table, click Edit/Preview data:   If you have large data for data transfer in CUR, we do NOT recommend using SPICE when setting up your data set in QuickSight, you can quickly use up the 10GB/user allocation and start to incur additional charges. Please use your judgment before enabling it.\n  Select SPICE to change your Query mode:   Hover over linked_account_id to get the drop down arrow and click on it, then hover over Change data type then select # Int:   Repeat step 7 for:\n payer_account_id    Hover over region to get the drop down arrow and click on it, then hover over Change data type then select # String\n  Hover over blended_cost to get the drop down arrow and click on it, then hover over Change data type then select # Decimal:   Ensure the following fields are all # Decimal, repeat step 10 if necessary for:\n blended_cost unblended_cost public_cost unblended_rate public_ondemand_rate    Select Save:   Select the data_transfer_view Data Set:   Click Schedule refresh:   Click Create:   Enter a schedule, it needs to be refreshed daily, and click Create:   Click Cancel to exit:   Click the x in the top corner:   You now have your data set setup ready to create a visualization.\n Create the Dashboard We will now use the CLI to create the dashboard from the Data Transfer Cost and Usage Analysis Dashboard template, then create an Analysis you can customize and modify in the next step.\n  If you have not requested access, go to this we page to request access to the template: Template Access   Edit the following command, replacing AccountID with your account ID, and region with the region you are working in, then using the CLI list the QuickSight datasets and copy the Name and Arn for the dataset: data_transfer_view:\n aws quicksight list-data-sets --aws-account-id (AccountID) --region (region)   { \u0026quot;Arn\u0026quot;: \u0026quot;arn:aws:quicksight:us-east-1:\u0026lt;your account id\u0026gt;:dataset/fc0cf1eb-173b-4aca-93b6-f58784637732\u0026quot;, \u0026quot;DataSetId\u0026quot;: \u0026quot;fc0cf1eb-173b-4aca-93b6-f58784637732\u0026quot;, \u0026quot;Name\u0026quot;: \u0026quot;data_transfer_view\u0026quot;, \u0026quot;CreatedTime\u0026quot;: \u0026quot;2020-08-09T23:06:41.666000-04:00\u0026quot;, \u0026quot;LastUpdatedTime\u0026quot;: \u0026quot;2020-08-11T23:15:35.438000-04:00\u0026quot;, \u0026quot;ImportMode\u0026quot;: \u0026quot;SPICE\u0026quot; }     Get your users Arn by editing the following command, replacing AccountID with your account ID, and region with the region you are working in, then using the CLI run the command:\n aws quicksight list-users --aws-account-id (AccountID) --namespace default --region (region)   { \u0026quot;Arn\u0026quot;: \u0026quot;arn:aws:quicksight:us-east-1:\u0026lt;your account id\u0026gt;:user/default/\u0026lt;your user\u0026gt;\u0026quot;, \u0026quot;UserName\u0026quot;: \u0026quot;\u0026lt;your user\u0026gt;\u0026quot;, \u0026quot;Email\u0026quot;: \u0026quot;\u0026lt;your user email\u0026gt;\u0026quot;, \u0026quot;Role\u0026quot;: \u0026quot;ADMIN\u0026quot;, \u0026quot;Active\u0026quot;: true, \u0026quot;PrincipalId\u0026quot;: \u0026quot;\u0026lt;principal id\u0026gt;\u0026quot; }     Create a local file create-data-transfer-dashboard.json with the text below, replace the values (Account ID) with your account ID on line 2 and line 25, (User ARN) with your user ARN on line 7, and (DataTransfer view Dataset ARN) with your dataset ARN on line 25:\n { \u0026quot;AwsAccountId\u0026quot;: \u0026quot;(Account ID)\u0026quot;, \u0026quot;DashboardId\u0026quot;: \u0026quot;data_transfer_cost_analysis_dashboard\u0026quot;, \u0026quot;Name\u0026quot;: \u0026quot;DataTransfer Cost Analysis Dashboard\u0026quot;, \u0026quot;Permissions\u0026quot;: [ { \u0026quot;Principal\u0026quot;: \u0026quot;(User ARN)\u0026quot;, \u0026quot;Actions\u0026quot;: [ \u0026quot;quicksight:DescribeDashboard\u0026quot;, \u0026quot;quicksight:ListDashboardVersions\u0026quot;, \u0026quot;quicksight:UpdateDashboardPermissions\u0026quot;, \u0026quot;quicksight:QueryDashboard\u0026quot;, \u0026quot;quicksight:UpdateDashboard\u0026quot;, \u0026quot;quicksight:DeleteDashboard\u0026quot;, \u0026quot;quicksight:DescribeDashboardPermissions\u0026quot;, \u0026quot;quicksight:UpdateDashboardPublishedVersion\u0026quot; ] } ], \u0026quot;SourceEntity\u0026quot;: { \u0026quot;SourceTemplate\u0026quot;: { \u0026quot;DataSetReferences\u0026quot;: [ { \u0026quot;DataSetPlaceholder\u0026quot;: \u0026quot;data_transfer_view\u0026quot;, \u0026quot;DataSetArn\u0026quot;: \u0026quot;arn:aws:quicksight:us-east-1:(Account ID):dataset/(DataTransfer view Dataset ARN)\u0026quot; } ], \u0026quot;Arn\u0026quot;: \u0026quot;arn:aws:quicksight:us-east-1:869004330191:template/data-transfer-cost-analysis-template\u0026quot; } }, \u0026quot;VersionDescription\u0026quot;: \u0026quot;1\u0026quot; }    To create the dashboard from the template, edit then run the following command, replacing (region) with the region you are working in, and you should receive a 202 response:\n aws quicksight create-dashboard --cli-input-json file://create-data-transfer-dashboard.json --region (region)   Response:     After a few minutes the dashboard will become available in QuickSight under All dashboard, click on the Dashboard name:     Click here - if you do not see your dashboard   Edit and run the following command:\n aws quicksight describe-dashboard --aws-account-id (Account ID) --dashboard-id data_transfer_cost_analysis_dashboard --region (region)  Correct the listed errors and run the delete-dashboard command followed by the original create-dashboard command:\n aws quicksight delete-dashboard --aws-account-id (Account ID) --dashboard-id data_transfer_cost_analysis_dashboard --region (region)      Click Share, click Share dashboard:,   Click Manage dashboard access:   Add the required users, or share with all users, ensure you check Save as for each user, then click the x to close the window:    Click Save as:   Enter an Analysis name and click Create:   Perform steps 11 and 12 above to create additional analyses for other teams, this will allow each team to have their own customizable analysis.\n  You will now have an analysis created from the template that you can edit and modify:   To view data transfer charts for desired period change From and To date filters. Dashboard will automatically refresh upon date changes.   You have successfully created the analysis from a template. For a detailed description of the dashboard read the FAQ   function prevStep(){ window.open(\"..\\/2_modify_cost_intelligence\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/4_distribute_dashboards\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_licensing/3_tear_down/","title":"Teardown","tags":[],"description":"","content":"The following resources were created in this lab:\n  Remove Athena before and after tables:\n drop table costmaster.before drop table costmaster.after    Delete the before and after folders from S3.\n  Delete s3 folder starting with the name cost-, ensure you select the correct folder.\n  Additional user permissions in SSO (if configured):\n ec2:DescribeImages ec2:DescribeVpcs ec2:DescribeSubnets     function prevStep(){ window.open(\"..\\/2_analyze_understand\\/\", \"_self\") } function nextStep(){ window.open(\"\\/cost\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with COST 5 - \u0026ldquo;How do you evaluate cost when you select services?\u0026quot;\nClick here to access the Well-Architected Tool   Previous Step Complete this lab     "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_cur_queries/queries/aws_cost_management/","title":"AWS Cost Management","tags":[],"description":"","content":"These are queries for AWS Services under the AWS Cost Management product family .\nUse the clipboard in the top right of the text boxes below to copy all of the text to be pasted.\n You may need to change variables used as placeholders in your query. ${table_Name} is a common variable which needs to be replaced. Example: cur_db.cur_table\n Table of Contents   AWS Marketplace   Query Description This query provides AWS Marketplace subscription costs including subscription product name, associated linked account, and monthly total unblended cost. This query includes tax, however this can be filtered out in the WHERE clause. Please refer to the CUR Query Library Helpers section for assistance.\nPricing Please refer to the AWS Marketplace FAQ .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, IF(line_item_usage_start_date IS NULL, DATE_FORMAT(DATE_PARSE(CONCAT(SPLIT_PART('${table_name}','_',5),'01'),'%Y%m%d'),'%Y-%m-01'), DATE_FORMAT((line_item_usage_start_date),'%Y-%m-01') ) AS month_line_item_usage_start_time, bill_billing_entity, product_product_name, SUM(CAST(line_item_unblended_cost AS decimal(16,8))) AS sum_line_item_unblended_cost FROM ${table_name} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND bill_billing_entity = 'AWS Marketplace' GROUP BY 1,2,3,4,5 ORDER BY month_line_item_usage_start_time ASC, sum_line_item_unblended_cost DESC;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback       CUR queries are provided as is. We recommend validating your data by comparing it against your monthly bill and Cost Explorer prior to making any financial decisions. If you wish to provide feedback on these queries, there is an error, or you want to make a suggestion, please email: curquery@amazon.com\n "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_cur_queries/","title":"Level 300: CUR Queries","tags":[],"description":"","content":"Last Updated January 2021\nFeedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: curquery@amazon.com\nIntroduction Your browser doesn't support video, or if you're on GitHub head to https://wellarchitectedlabs.com to watch the video.  The CUR Query Library is a collection of curated SQL queries to get you started with analyzing your Cost and Usage Report (CUR) data. Cost analysis is unique to each business and these queries are intended to be modified to suit your specific needs. The library will be updated periodically as we build new queries and receive feedback from customers.\nCUR queries are provided as is. We recommend validating your data by comparing it against your monthly bill and Cost Explorer prior to making any financial decisions. If you wish to provide feedback on these queries, there is an error, or you want to make a suggestion, please email: curquery@amazon.com\n Query Help Your browser doesn't support video, or if you're on GitHub head to https://wellarchitectedlabs.com to watch the video.  The CUR Query Library Help section is intended to provide tips and information about navigating the CUR dataset. We will cover beginner topics like getting started with querying the CUR, filtering query results, common query format, links to public documentation, and retrieving product information. We will also cover advanced topics like understanding your AWS Cost Datasets while working with the CUR data.\nCUR Library Query Help Queries Analytics Application Integration AWS Cost Management Compute Container Customer Engagement Database End User Computing Global Machine Learning Management \u0026amp; Governance Networking \u0026amp; Content Delivery Security Identity \u0026amp; Compliance Storage Prerequisites  An AWS Account Completed the AWS Account Setup lab  Completed the Cost and Usage Analysis lab  Usage in your AWS account  Contributing Community contributions are encouraged and welcome. Please follow the Contribution Guide . The goal is to pull together useful CUR queries in to a single library that is open, standardized, and maintained.\nContributors Please refer to the CUR Query Library Contributors section .\n"},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_organization_data_cur_connection/3_utilize_organization_data_source/","title":"Utilize Organization Data Source","tags":[],"description":"","content":"Create the Organisations Data Table In this section we will create the AWS Organizations table in Amazon Athena. This can then be used to connect to the AWS Cost \u0026amp; Usage Report (CUR) or other data sets you have, to show you the names and emails of your accounts.\n Go to the Athena service page   We are going to create the Organizations table. This can be done in any of the databases that holds your Cost \u0026amp; Usage Report. Copy and paste the below query replacing the ( bucket-name) with the S3 bucket name which holds the Organizations data, into the query box. Click Run query.\nCREATE EXTERNAL TABLE IF NOT EXISTS managementcur.organisation_data (\r`account_number` string,\r`account_name` string,\r`creation_date` string,\r`status` string )\rROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'\rWITH SERDEPROPERTIES (\r'serialization.format' = ',',\r'field.delim' = ','\r) LOCATION 's3://(bucket-name)/organisation-data/'\rTBLPROPERTIES ('has_encrypted_data'='false');     Athena should report Query Successful. Run the below query, to view your data in Amazon S3. As you can see, we have the account number, the name, when it was created and the current status of that account.\nSELECT * FROM \u0026quot;managementcur\u0026quot;.\u0026quot;organisation_data\u0026quot; limit 10;\r   You have now created your Athena table that will query the organization data in the S3 Bucket.\n Join with Cost and Usage Report We will be running an example query on how you can connect your CUR to this Organizations data as a one off. In this query you will see the service costs split by account names.\n In the Athena service page run the below query to join the Organizations data with the CUR table. Make the below changes as needed:    Change managementcur if your named your database differently\n  month = Chosen Month\n  year = Chosen Year\n SELECT line_item_usage_account_id,\rline_item_product_code,\raccount_name,\rsum(line_item_unblended_cost) AS line_item_unblended_cost_cost\rFROM \u0026quot;managementcur\u0026quot;.\u0026quot;cur\u0026quot; cur\rJOIN \u0026quot;managementcur\u0026quot;.\u0026quot;organisation_data\u0026quot;\rON \u0026quot;cur\u0026quot;.line_item_usage_account_id = organisation_data.account_number\rWHERE month = '10'\rAND year = '2020'\rGROUP BY line_item_usage_account_id, account_name, line_item_product_code\rlimit 10;\r   The important part of this query is the join. The line_item_usage_account_id from your Cost \u0026amp; Usage Report should match a account_number from the Organizations data. You can now see the account name in your data.  Create a View with Cost and Usage Report If you would like to always have your Organizations data connected to your CUR then we can create a view.\n  In the Athena service page run the below query to join the Organizations data with the CUR table as a view.\nCREATE OR REPLACE VIEW org_cur AS\rSELECT *\rFROM (\u0026quot;managementcur\u0026quot;.\u0026quot;cur\u0026quot; cur\rINNER JOIN \u0026quot;managementcur\u0026quot;.\u0026quot;organisation_data\u0026quot;\rON (\u0026quot;cur\u0026quot;.\u0026quot;line_item_usage_account_id\u0026quot; = \u0026quot;organisation_data\u0026quot;.\u0026quot;account_number\u0026quot;))    Going forward you will now be able to run your queries from this view and have the data connected to your Organizations data. To see a preview where your org data is, which is at the end of the returned data, run the below query.\n SELECT * FROM \u0026quot;managementcur\u0026quot;.\u0026quot;org_cur\u0026quot; limit 10;\r   Having run these queries you can now see how the Organization data connects to your Cost and Usage Report.\n  function prevStep(){ window.open(\"..\\/2_create_automation_resources_source\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/4_visualize_organization_data_in_quicksight\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_clock_source_performance/3_change_clock/","title":"Changing clock type on the Xen based EC2 instance","tags":[],"description":"","content":"We can change the clock type on Xen based EC2 instances to get better performance than the standard Xen clock source. You will notice that the speed for Xen and Nitro/KVM instances will be close to identical after this change.\nCode to change the clock source echo \u0026quot;tsc\u0026quot; \u0026gt; /sys/devices/system/clocksource/clocksource0/current_clocksource This changes the default time clock from xen to tsc (Time Stamp Counter), which is considered the best practice for Xen based EC2 instances.\nUsing SSM to run the clock change document  Open the AWS Console (https://console.awa.amazon.com) In the Find Services search bar, type systems manager and press enter  Under Instances \u0026amp; Nodes click on Run Command and on the left side of the screen again click on Run a command  In the search bar under Command Document, select Owner from the pulldown and then select Owned by me.  You should see 2 SSM documents that have been created for you by the CloudFormation Template. Click on the one with setTSCdocument in the name.  Scroll down and under Targets, select the checkbox next to the instance that is labeled XenTimeInstanceTest  Scroll down and uncheck the box that says Enable writing to an S3 bucket and then click the Run button at the bottom of the screen.  You should see the command running on the node selected. Click Refresh until the command is in the Success state  You can now go back to Step 2 and re-run the tests. You should see the time for the Xen based EC2 instance has dramatically improved from 110 to 24 seconds.    function prevStep(){ window.open(\"..\\/2_testing_before\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/4_cleanup\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_incident_response_with_aws_console_and_cli/3_vpc/","title":"Amazon VPC","tags":[],"description":"","content":"A Amazon VPC that has VPC Flow Logs enabled captures information about the IP traffic going to and from network interfaces in your Amazon VPC. This log information may help you investigate how Amazon EC2 instances and other resources in your VPC are communicating, and what they are communicating with. You can follow the Automated Deployment of VPC lab for creating a Amazon VPC with Flow Logs enabled.\n3.1 Investigate Amazon VPC Flow Logs 3.1.1 AWS Management Console The AWS Management console provides a visual way of querying CloudWatch Logs, using CloudWatch Logs Insights and does not require any tools to be installed.\n Open the Amazon CloudWatch console at https://console.aws.amazon.com/cloudwatch/ and select your region. From the left menu, choose Insights under Logs. From the dropdown near the top select your CloudTrail Logs group, then the relative time to search back on the right. Copy the following example queries below into the query input, then click Run query.  Rejected requests by IP address:\nRejected requests indicate attempts to gain access to your VPC, however there can often be noise from internet scanners. To count the rejected requests by source IP address: filter action=\u0026quot;REJECT\u0026quot; | stats count(*) as numRejections by srcAddr | sort numRejections desc\nReject requests originating from inside your VPC\nRejected requests that originate from inside your VPC may indicate your infrastructure in your VPC is attempting to connect to something it is not allowed to, e.g. a database instance is trying to connect to the internet and is blocked. This example uses regex to match the start of your VPC as 10.: filter action=\u0026quot;REJECT\u0026quot; and srcAddr like /^10\\./ | stats count(*) as numRejections by srcAddr | sort numRejections desc\nRequests from an IP address\nIf you suspect an IP address and want to list all requests that originate, replace 192.0.2.1 with the IP you suspect: filter srcAddr = \u0026quot;192.0.2.1\u0026quot; | fields @timestamp, interfaceId, dstAddr, dstPort, action\nRequest count from a private IP address by destination address\nIf you want to list and count all connections by a private IP address, replace 10.1.1.1 with your private IP: filter srcAddr = \u0026quot;10.1.1.1\u0026quot; | stats count(*) as numConnections by dstAddr | sort numConnections desc\n"},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_3_pricing_models/3_analyze_recommendations/","title":"Analyze your Savings Plan recommendations","tags":[],"description":"","content":"We have an understanding of the potential savings available to us, and how we can adjust that based on our business requirements. We also know our usage trends across the business, which will help us with our initial commitment. We will now go deeper to help you understand exactly what Savings Plan commitment is right for you.\nYou can think of a single Savings Plan as a highly flexible group of Reserved Instances (RI\u0026rsquo;s), without the same management overhead. Depending on the discount level and your usage, Savings Plans can pay themselves off very quickly and offer large savings, or pay themselves off over a longer period with less savings. We will look into a Savings Plan to ensure our commitment pays off in the right amount of time and offers the amount of savings we need.\nWe will analyze our Savings Plan to further refine our initial commitment level to purchase, this commitment will be very low risk and high return. Once that is purchased you then re-analyze every fortnight or month and \u0026ldquo;top up\u0026rdquo; your commitment levels. This ensures you maintain high levels of discounts, and you can continually adjust as your business evolves.\n  Click on Recommendations and select EC2 Instance, 1-year, No upfront:   Scroll down and click Download CSV   There is a sample file here if you do not have data:\n Sample SavingsPlan     Add the new column headings O, P and Q. and put in the formulas for each:\n Monthly OnDemand Cost, Cell O2: =G2*730 Monthly Cost after SP, Cell P2: =O2-K2 Fully Paid Day, Cell Q2: =P2*12/O2 Updated Sample SavingsPlan     The Fully Paid Day is the number of months it takes to pay off the entire 12 month savings plan. It is a combination of the discount level and your utilization level. At your current usage, after this day even if you turn off all your usage you will not lose money and be better off than paying on demand. The sooner the period the lower risk the purchase.   We want the lowest risk purchases for our initial commitment, so sort by the Fully Paid Day, in order of smallest to largest, and insert some blank lines before a fully paid day of 9 months:\n Updated Sample SavingsPlan     Sort the top group and bottom group by Estimated monthly savings amount, in order of largest to smallest, insert some lines above anything less than $50 in savings:\n Updated Sample SavingsPlan     You now have four groups of Savings Plan usage:\n Very low risk, high return Very low risk, low return low to medium risk, high return low to medium risk, low return    Combining this information with the previous exercises, your initial purchase will be typically focused on the low risk and high return, and some of the medium risk high return. Add up the hourly commitment for the recommendations that match your business requirements. In this example we have taken all of the very low risk high return, and 40% of the medium risk high return:\n Updated Sample SavingsPlan     Take this commitment level, apply the findings from the previous exercises (type of Savings Plan and Usage Trend) to make your initial purchase.\n   function prevStep(){ window.open(\"..\\/2_usage_trend\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/4_visualize_recommendations\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_aws_resource_optimization/3_attach_iamrole/","title":"Attach CloudWatch IAM role to selected EC2 Instances","tags":[],"description":"","content":"  We are now going to attach the IAM Role created on the previous step in one of our EC2 Instances, to do that let\u0026rsquo;s go to the Amazon EC2 Dashboard.   On the left bar, click on Instances.   Click on Launch Instance and select Linux 2 AMI (HVM) and t2.micro (free tier eligible) on the following screens. Click Review and launch:   Click Launch\n  Select Proceed without a key pair\n  Complete this step by launching this testing t2.micro EC2 instance. In order to install the CloudWatch agent We will need to access this instance using the browser-based SSH native AWS connection tool so make sure that the port 22 is not blocked in the Security Group attached to this instance.\n After the instance is launched and runnning, select the instance you want to start collecting Memory data by going to Actions on the top bar, select Instance Settings\u0026raquo;Attach/Replace IAM Role.   Look for the created IAM role CloudWatchAgentServerRole under the IAM role box, select it and apply.   Confirm that the CloudWatchAgentServerRole was sucessfully attached to your EC2 instance   Validade if the IAM role CloudWatchAgentServerRole is attached to the desired instance    function prevStep(){ window.open(\"..\\/2_create_iamrole\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/4_memory_plugin\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_100_loft_introduction_to_security/3_cloudfront_waf/","title":"CloudFront with WAF Protection","tags":[],"description":"","content":"This hands-on lab will guide you through the steps to protect a workload from network based attacks using Amazon CloudFront and AWS Web Application Firewall (WAF). You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy CloudFront with WAF integration to apply defense in depth methods. As CloudFront takes some time to update configuration in all edge locations, consider starting step 4 while its deploying.\nWalkthrough CloudFront with WAF Protection "},{"uri":"https://wellarchitectedlabs.com/security/100_labs/100_cloudfront_with_s3_bucket_origin/3_config_cloudfront/","title":"Configure Amazon CloudFront","tags":[],"description":"","content":"Using the AWS Management Console, we will create a CloudFront distribution, and configure it to serve the S3 bucket we previously created.\n Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home . From the console dashboard, click Create Distribution.  Click Get Started in the Web section.  Specify the following settings for the distribution:   In the Origin Domain Name field Select the S3 bucket you created previously. In Restrict Bucket Access click the Yes radio then click Create a New Identity. Click the Yes, Update Bucket Policy Button.   Scroll down to the Distribution Settings section, in the Default Root Object field enter index.html Click Create Distribution. To return to the main CloudFront page click Distributions from the left navigation menu. For more information on the other configuration options, see Values That You Specify When You Create or Update a Web Distribution in the CloudFront documentation.  After CloudFront creates your distribution which may take approximately 10 minutes, the value of the Status column for your distribution will change from In Progress to Deployed.  When your distribution is deployed, confirm that you can access your content using your new CloudFront Domain Name which you can see in the console. Copy the Domain Name into a web browser to test.  For more information, see Testing a Web Distribution in the CloudFront documentation.\nYou now have content in a private S3 bucket, that only CloudFront has secure access to. CloudFront then serves the requests, effectively becoming a secure, reliable static hosting service with additional features available such as custom certificates and alternate domain names .  For more information on configuring CloudFront, see Viewing and Updating CloudFront Distributions in the CloudFront documentation.\n"},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_cloudfront_with_waf_protection/3_config_cloudfront/","title":"Configure Amazon CloudFront","tags":[],"description":"","content":"Using the AWS Management Console, we will create a CloudFront distribution, and link it to the AWS WAF ACL we previously created.\n Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. From the console dashboard, choose Create Distribution.  Click Get Started in the Web section.  Specify the following settings for the distribution:   In Origin Domain Name enter the EC2 public DNS name you recorded from your instance launch.   In the distribution Settings section, click AWS WAF Web ACL, and select the one you created previously.   Click Create Distrubution. For more information on the other configuration options, see Values That You Specify When You Create or Update a Web Distribution in the CloudFront documentation.  After CloudFront creates your distribution, the value of the Status column for your distribution will change from In Progress to Deployed.  When your distribution is deployed, confirm that you can access your content using your new CloudFront URL or CNAME. Copy the Domain Name into a web browser to test.  For more information, see Testing a Web Distribution in the CloudFront documentation. 7. You have now configured Amazon CloudFront with basic settings and AWS WAF.\nFor more information on configuring CloudFront, see Viewing and Updating CloudFront Distributions in the CloudFront documentation.\n"},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_autonomous_monitoring_of_cryptographic_activity_with_kms/3_configure_and_check_cloudtrail/","title":"Configure CloudTrail ","tags":[],"description":"","content":"We will now focus on the creation and configuration of the CloudTrail service. This represents the source of record for all API calls generated within our architecture which we will apply filters to later. Note in the architecture below how CloudTrail integrates with the other AWS services we will deploy:\n  Click here for CloudFormation command-line deployment steps   Command Line: 3.1. Command Line Deployment Firstly, download the logging template from here. To deploy from the command line, ensure that you have installed and configured AWS CLI with the appropriate credentials. When your environment is ready, run the following command, taking note of the points below.\naws cloudformation create-stack --stack-name pattern1-logging \\ --template-body file://pattern1-logging.yml \\ --parameters ParameterKey=AppECSTaskRoleArn,ParameterValue=\u0026quot;\u0026lt;ECS Task Role ARN\u0026gt;\u0026quot; ParameterKey=EmailAddress,ParameterValue=\u0026lt; Email Address \u0026gt; \\ --capabilities CAPABILITY_NAMED_IAM \\ --region ap-southeast-2 Note :  For simplicity, we have used Sydney \u0026lsquo;ap-southeast-2\u0026rsquo; as the default region for this lab. For \u0026lt; ECS Task Role ARN \u0026gt;, use the ECS Task Role Arn value you took note of from section 2.3.3 for AppECSTaskRoleArn parameter. Use the email address you would like to use to be notified with under EmailAddress parameter.      Click here for CloudFormation console deployment steps   Console: 3.1. CloudFormation Console Deployment Firstly, download the logging template from here. To deploy the template from the console, please follow this guide for information on how to deploy the cloudformation template, noting the following points before starting your deployment:\n Use pattern1-logging as the Stack Name. Use the ECS Task Role Arn value you took note from section 2.3 for AppECSTaskRoleArn parameter. Use email address you would like to use to be notified with under EmailAddress parameter.  Note: Dont forget to acknowledge the capabilities checkbox at the bottom of the screen.\n    Click here for manual console deployment   Manual Console Deployment 3.1. Create a Trail in CloudTrail Console To create a trail for use within this lab, complete the following steps:\n3.1.1. Navigate to CloudTrail within the console, then click on Create trail as shown here:\n3.1.2. Enter pattern1-logging-trail as the Trail name.\n3.1.3. Select Create new S3 bucket and enter a name for your logging s3 bucket.\nNote that the name needs to be globally unique, so you can use your accountid or uuid to keep it unique for you.\n3.1.4. Enter the remainder of the settings as per the following example:\n3.1.5. Complete the following configuration choices:\n Tick on Enabled under CloudWatch Logs. Select New on the Log group radio button, and enter your log group name as pattern1-logging-loggroup Select New on IAM Role, and enter your role name as CloudTrailRoleForCloudWatchLogs_pattern1-logging  Your configuration should match the screenshot below:\nWhen you are complete, click Next.\n3.1.6. On the next screen, complete the following configuration choices:\n Select management events Select read write API Ensure that exclude AWS KMS event is NOT selected  Check your selection against the following screenshot and then click Next.\n3.1.7. Review the settings and click Create Trail\n3.2. Confirm Your CloudWatch Log Group Is Operational. Now that your CloudWatch configuration is completed, we need to confirm that the log group is operational.\nFollow the steps below to confirm the state of the Log Group:\n3.2.1. Navigate to CloudWatch in your console and click on Log Groups on the side menu.\n3.2.2. Locate the pattern1-logging-loggroup you created before and click on the the log group as show:\n3.2.3. Click on the available log stream, and confirm that you are seeing logs being generated.\nIf you have completed the configuration correctly, you should see an ongoing record of all the API calls within your account as show here:\nIn the next section, we are going to filter out the Events which matter to us. In doing this we will be able to create an appropriate Alarm\n   END OF SECTION 3\n "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_deploy_and_update_cloudformation/3_cfn_params/","title":"Configure Deployed Resources using Parameters","tags":[],"description":"","content":"In this task, you will gain experience changing CloudFormation stack parameters and updating your CloudFormation stack\n Your objective is to deploy additional resources used by the VPC to enable connection to the internet  2.1 Update Parameters  Go to the AWS CloudFormation console (if not already there) Click on Stacks Click on the CloudFormationLab stack Click Update Leave Use current template selected. You have not yet changed the template Click Next On the Specify stack details screen you now have the opportunity to change the Parameters  Change PublicEnabledParam to true   Click Next Click Next again, until you arrive at the Review CloudFormationLab screen  Scroll down to Change set preview and note several resources are being added At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Create stack   When stack status is CREATE_COMPLETE for your update (about one minute) then continue  2.2 Understanding the deployment  You did not change any contents of the the CloudFormation Template Changing only one parameter, you re-deployed the stack which resulted in additional resources deployed   Go to the AWS CloudFormation console (if not already there) Click the Resources tab for the CloudFormationLab stack.  The listing now shows the VPC as before, plus additional resources required to enable us to deploy resources into the VPC that have access to the internet Click through on several of the Physical ID links and explore these resources    The current deployment is now represented by this architecture diagram:  function prevStep(){ window.open(\"..\\/2_understand_deploy\", \"_self\") } function nextStep(){ window.open(\"..\\/4_add_s3\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_4_cost_and_usage_analysis/3_cur_analysis/","title":"Cost and Usage analysis","tags":[],"description":"","content":"We will now perform some common analysis of your usage through SQL queries. You will be charged for Athena usage by the amount of data that is scanned - the source files are monthly, and in parquet format - which is compressed and partitioned to minimise cost. Be careful to include limit 10 or similar at the end of your queries to limit the amount of data that comes back.\nYou may need to change the database and table name depending on how you configured your CUR. Replace costmaster.workshop_c_u_r with your database and table name.\n   Click here to copy and paste all the examples into a text editor to replace the database and table name quickly.   SELECT * from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; LIMIT 10; SELECT distinct \u0026quot;line_item_line_item_description\u0026quot; from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; LIMIT 10; SELECT * from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; WHERE \u0026quot;line_item_line_item_type\u0026quot; like '%Usage%' LIMIT 10; SELECT distinct bill_billing_period_start_date FROM \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; LIMIT 10; SELECT \u0026quot;line_item_usage_account_id\u0026quot;, round(sum(\u0026quot;line_item_unblended_cost\u0026quot;),2) as cost from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; GROUP BY \u0026quot;line_item_usage_account_id\u0026quot; ORDER BY cost desc LIMIT 10; SELECT \u0026quot;line_item_product_code\u0026quot;, round(sum(\u0026quot;line_item_unblended_cost\u0026quot;),2) as cost from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; GROUP BY \u0026quot;line_item_product_code\u0026quot; ORDER BY cost desc LIMIT 10; SELECT \u0026quot;line_item_product_code\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot;, round(sum(\u0026quot;line_item_unblended_cost\u0026quot;),2) as cost from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; GROUP BY \u0026quot;line_item_product_code\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot; ORDER BY cost desc LIMIT 10; SELECT \u0026quot;line_item_product_code\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot;, round(sum(\u0026quot;line_item_unblended_cost\u0026quot;),2) as cost from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; WHERE \u0026quot;line_item_product_code\u0026quot; like '%AmazonEC2%' GROUP BY \u0026quot;line_item_product_code\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot; ORDER BY cost desc LIMIT 10; SELECT \u0026quot;line_item_product_code\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot;, round(sum(\u0026quot;line_item_unblended_cost\u0026quot;),2) as cost from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; WHERE \u0026quot;line_item_product_code\u0026quot; like '%AmazonEC2%' and \u0026quot;line_item_usage_type\u0026quot; like '%BoxUsage%' GROUP BY \u0026quot;line_item_product_code\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot; ORDER BY cost desc LIMIT 10; SELECT \u0026quot;bill_payer_account_id\u0026quot;, \u0026quot;product_product_name\u0026quot;, \u0026quot;line_item_usage_type\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot;, resource_tags_user_cost_center, round(sum(line_item_unblended_cost),2) as cost FROM \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; WHERE length(\u0026quot;resource_tags_user_cost_center\u0026quot;) \u0026gt;0 GROUP BY \u0026quot;resource_tags_user_cost_center\u0026quot;, \u0026quot;bill_payer_account_id\u0026quot;, \u0026quot;product_product_name\u0026quot;, \u0026quot;line_item_usage_type\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot; ORDER BY cost desc LIMIT 20 SELECT \u0026quot;bill_payer_account_id\u0026quot;, \u0026quot;product_product_name\u0026quot;, \u0026quot;line_item_usage_type\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot;, round(sum(line_item_unblended_cost),2) as cost FROM \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; WHERE length(\u0026quot;resource_tags_user_cost_center\u0026quot;) = 0 GROUP BY \u0026quot;bill_payer_account_id\u0026quot;, \u0026quot;product_product_name\u0026quot;, \u0026quot;line_item_usage_type\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot; ORDER BY cost desc LIMIT 20 SELECT \u0026quot;bill_payer_account_id\u0026quot;, \u0026quot;bill_billing_period_start_date\u0026quot;, \u0026quot;line_item_usage_account_id\u0026quot;, \u0026quot;reservation_reservation_a_r_n\u0026quot;, \u0026quot;line_item_product_code\u0026quot;, \u0026quot;line_item_usage_type\u0026quot;, sum(\u0026quot;line_item_usage_amount\u0026quot;) as Usage, \u0026quot;line_item_unblended_rate\u0026quot;, sum(\u0026quot;line_item_unblended_cost\u0026quot;) as Cost, \u0026quot;line_item_line_item_description\u0026quot;, \u0026quot;pricing_public_on_demand_rate\u0026quot;, sum(\u0026quot;pricing_public_on_demand_cost\u0026quot;) as PublicCost from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; WHERE \u0026quot;line_item_line_item_Type\u0026quot; like '%DiscountedUsage%' GROUP BY \u0026quot;bill_payer_account_id\u0026quot;, \u0026quot;bill_billing_period_start_date\u0026quot;, \u0026quot;line_item_usage_account_id\u0026quot;, \u0026quot;reservation_reservation_a_r_n\u0026quot;, \u0026quot;line_item_product_code\u0026quot;, \u0026quot;line_item_usage_type\u0026quot;, \u0026quot;line_item_unblended_rate\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot;, \u0026quot;pricing_public_on_demand_rate\u0026quot; LIMIT 20 SELECT \u0026quot;line_item_usage_type\u0026quot;, sum(\u0026quot;line_item_usage_amount\u0026quot;) as usage, round(sum(\u0026quot;line_item_unblended_cost\u0026quot;),2) as cost from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; WHERE \u0026quot;line_item_usage_type\u0026quot; like '%t2.%' GROUP BY \u0026quot;line_item_usage_type\u0026quot; ORDER BY \u0026quot;line_item_usage_type\u0026quot; LIMIT 20 SELECT \u0026quot;line_item_usage_type\u0026quot;, round(sum(\u0026quot;line_item_usage_amount\u0026quot;),2) as usage, round(sum(\u0026quot;line_item_unblended_cost\u0026quot;),2) as cost, round(avg(\u0026quot;line_item_unblended_cost\u0026quot;/\u0026quot;line_item_usage_amount\u0026quot;),4) as hourly_rate from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; WHERE \u0026quot;line_item_product_code\u0026quot; like '%AmazonEC2%' and \u0026quot;line_item_usage_type\u0026quot; like '%Usage%' GROUP BY \u0026quot;line_item_usage_type\u0026quot; ORDER BY \u0026quot;line_item_usage_type\u0026quot; LIMIT 20 SELECT bill_billing_period_start_date, product_region, line_item_usage_type, reservation_reservation_a_r_n, reservation_unused_quantity, reservation_unused_recurring_fee from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; WHERE length(reservation_reservation_a_r_n) \u0026gt; 0 and reservation_unused_quantity \u0026gt; 0 ORDER BY bill_billing_period_start_date, reservation_unused_recurring_fee desc LIMIT 20    For each of the queries below, copy and paste each query into the query window, click Run query and view the results. What data is available in the CUR file? We will learn how to find out what data is available for querying in the CUR files, this will show what columns there are and some sample data in those columns.\nExecute each of the statements below, then spend a minute to thoroughly read through the results and observe the data returned.\nUse the clipboard in the top right of the text boxes below to copy all of the text to be pasted.\n   What are all the columns and data are in the CUR table?\n SELECT * from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; LIMIT 10;    What are all the different values in a specific column? (the column we use is line_item_line_item_description)\n SELECT distinct \u0026quot;line_item_line_item_description\u0026quot; from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; LIMIT 10;    3 What are all the columns from the CUR, where a specific value is in the column (here the column line_item_line_item_type contains the word Usage, note the capital \u0026lsquo;U\u0026rsquo;):\n SELECT * from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; WHERE \u0026quot;line_item_line_item_type\u0026quot; like '%Usage%' LIMIT 10;   What billing periods are available?\n SELECT distinct bill_billing_period_start_date FROM \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; LIMIT 10;    Top Costs To efficiently optimize, it is useful to view the top costs in different categories, such as service, description or tags. Here are the most useful categories to get top costs by.\n  Top10 Costs by AccountID:\n SELECT \u0026quot;line_item_usage_account_id\u0026quot;, round(sum(\u0026quot;line_item_unblended_cost\u0026quot;),2) as cost from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; GROUP BY \u0026quot;line_item_usage_account_id\u0026quot; ORDER BY cost desc LIMIT 10;    Top10 Costs by Product:\n SELECT \u0026quot;line_item_product_code\u0026quot;, round(sum(\u0026quot;line_item_unblended_cost\u0026quot;),2) as cost from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; GROUP BY \u0026quot;line_item_product_code\u0026quot; ORDER BY cost desc LIMIT 10;    Top10 Costs by Line Item Description\n SELECT \u0026quot;line_item_product_code\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot;, round(sum(\u0026quot;line_item_unblended_cost\u0026quot;),2) as cost from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; GROUP BY \u0026quot;line_item_product_code\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot; ORDER BY cost desc LIMIT 10;    Top EC2 Costs\n SELECT \u0026quot;line_item_product_code\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot;, round(sum(\u0026quot;line_item_unblended_cost\u0026quot;),2) as cost from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; WHERE \u0026quot;line_item_product_code\u0026quot; like '%AmazonEC2%' GROUP BY \u0026quot;line_item_product_code\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot; ORDER BY cost desc LIMIT 10;    Top EC2 OnDemand Costs\n SELECT \u0026quot;line_item_product_code\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot;, round(sum(\u0026quot;line_item_unblended_cost\u0026quot;),2) as cost from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; WHERE \u0026quot;line_item_product_code\u0026quot; like '%AmazonEC2%' and \u0026quot;line_item_usage_type\u0026quot; like '%BoxUsage%' GROUP BY \u0026quot;line_item_product_code\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot; ORDER BY cost desc LIMIT 10;    Tagging and Cost Attribution Common in large organizations is the requirement to allocate costs back to specific business units. It is also critical for optimization to be able to allocate costs to workloads, to measure workload efficiency.\nThis will only work if you have tags enabled in your billing files, and they are the same as the examples here - resource_tags_user_cost_center. You may need to change the examples below to match your tags.\n   Top 20 Costs by line item description and CostCenter Tag\n SELECT \u0026quot;bill_payer_account_id\u0026quot;, \u0026quot;product_product_name\u0026quot;, \u0026quot;line_item_usage_type\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot;, resource_tags_user_cost_center, round(sum(line_item_unblended_cost),2) as cost FROM \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; WHERE length(\u0026quot;resource_tags_user_cost_center\u0026quot;) \u0026gt;0 GROUP BY \u0026quot;resource_tags_user_cost_center\u0026quot;, \u0026quot;bill_payer_account_id\u0026quot;, \u0026quot;product_product_name\u0026quot;, \u0026quot;line_item_usage_type\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot; ORDER BY cost desc LIMIT 20    Top 20 costs by line item description, without a CostCenter Tag\n SELECT \u0026quot;bill_payer_account_id\u0026quot;, \u0026quot;product_product_name\u0026quot;, \u0026quot;line_item_usage_type\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot;, round(sum(line_item_unblended_cost),2) as cost FROM \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; WHERE length(\u0026quot;resource_tags_user_cost_center\u0026quot;) = 0 GROUP BY \u0026quot;bill_payer_account_id\u0026quot;, \u0026quot;product_product_name\u0026quot;, \u0026quot;line_item_usage_type\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot; ORDER BY cost desc LIMIT 20    Savings Plans, Reserved Instance, On Demand and Spot Usage To improve the use of pricing models across a business, these queries can assist to highlight the top opportunities for Savings Plans and Reserved Instances, by finding top On Demand costs. It also identifies who is successful with pricing models by (Top users of spot).\nYou will need specific usage in your account that matches the instance types below, for this to work correctly.\n   Who used Savings Plan Identify which usage was covered by a savings plan.\n SELECT \u0026quot;bill_payer_account_id\u0026quot;, \u0026quot;bill_billing_period_start_date\u0026quot;, \u0026quot;line_item_usage_account_id\u0026quot;, \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot;, \u0026quot;line_item_product_code\u0026quot;, \u0026quot;line_item_usage_type\u0026quot;, sum(\u0026quot;line_item_usage_amount\u0026quot;) as Usage, \u0026quot;line_item_line_item_description\u0026quot;, \u0026quot;pricing_public_on_demand_rate\u0026quot;, sum(\u0026quot;pricing_public_on_demand_cost\u0026quot;) as PublicCost, savings_plan_savings_plan_rate, sum(savings_plan_savings_plan_effective_cost) as SavingsPlanCost from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; WHERE \u0026quot;line_item_line_item_Type\u0026quot; like 'SavingsPlanCoveredUsage' GROUP BY \u0026quot;bill_payer_account_id\u0026quot;, \u0026quot;bill_billing_period_start_date\u0026quot;, \u0026quot;line_item_usage_account_id\u0026quot;, \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot;, \u0026quot;line_item_product_code\u0026quot;, \u0026quot;line_item_usage_type\u0026quot;, \u0026quot;line_item_unblended_rate\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot;, \u0026quot;pricing_public_on_demand_rate\u0026quot;, \u0026quot;savings_plan_savings_plan_rate\u0026quot; LIMIT 20    Unused Savings Plan For each savings plan, look at the total commitment and total usage each month.\n SELECT \u0026quot;bill_payer_account_id\u0026quot;, \u0026quot;bill_billing_period_start_date\u0026quot;, \u0026quot;line_item_usage_account_id\u0026quot;, \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot;, sum(savings_plan_savings_plan_effective_cost) as SavingsPlanUsage, sum(\u0026quot;savings_plan_recurring_commitment_for_billing_period\u0026quot;) as SavingsPlanCommit from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; WHERE length(\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot;) \u0026gt; 0 GROUP BY \u0026quot;bill_payer_account_id\u0026quot;, \u0026quot;bill_billing_period_start_date\u0026quot;, \u0026quot;line_item_usage_account_id\u0026quot;, \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; order by bill_billing_period_start_date desc, SavingsPlanCommit desc, SavingsPlanUsage LIMIT 10    Who used Reserved Instances Identify which accounts used the available RIs, and what they would have paid with public pricing. Ideal for chargeback within an organization.\n SELECT \u0026quot;bill_payer_account_id\u0026quot;, \u0026quot;bill_billing_period_start_date\u0026quot;, \u0026quot;line_item_usage_account_id\u0026quot;, \u0026quot;reservation_reservation_a_r_n\u0026quot;, \u0026quot;line_item_product_code\u0026quot;, \u0026quot;line_item_usage_type\u0026quot;, sum(\u0026quot;line_item_usage_amount\u0026quot;) as Usage, \u0026quot;line_item_unblended_rate\u0026quot;, sum(\u0026quot;line_item_unblended_cost\u0026quot;) as Cost, \u0026quot;line_item_line_item_description\u0026quot;, \u0026quot;pricing_public_on_demand_rate\u0026quot;, sum(\u0026quot;pricing_public_on_demand_cost\u0026quot;) as PublicCost from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; WHERE \u0026quot;line_item_line_item_Type\u0026quot; like '%DiscountedUsage%' GROUP BY \u0026quot;bill_payer_account_id\u0026quot;, \u0026quot;bill_billing_period_start_date\u0026quot;, \u0026quot;line_item_usage_account_id\u0026quot;, \u0026quot;reservation_reservation_a_r_n\u0026quot;, \u0026quot;line_item_product_code\u0026quot;, \u0026quot;line_item_usage_type\u0026quot;, \u0026quot;line_item_unblended_rate\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot;, \u0026quot;pricing_public_on_demand_rate\u0026quot; LIMIT 20    Specific Instance family usage Observe how much is being spent on each different family (usage type) and how much is covered by Reserved instances.\n SELECT \u0026quot;line_item_usage_type\u0026quot;, sum(\u0026quot;line_item_usage_amount\u0026quot;) as usage, round(sum(\u0026quot;line_item_unblended_cost\u0026quot;),2) as cost from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; WHERE \u0026quot;line_item_usage_type\u0026quot; like '%t2.%' GROUP BY \u0026quot;line_item_usage_type\u0026quot; ORDER BY \u0026quot;line_item_usage_type\u0026quot; LIMIT 20    Costs By running type Divide the cost by usage (hrs), and see how much is being spent per hour on each of the usage types. Compare BoxUsage (On Demand), to HeavyUsage (Reserved instance), to SpotUsage (Spot).\n SELECT \u0026quot;line_item_usage_type\u0026quot;, round(sum(\u0026quot;line_item_usage_amount\u0026quot;),2) as usage, round(sum(\u0026quot;line_item_unblended_cost\u0026quot;),2) as cost, round(avg(\u0026quot;line_item_unblended_cost\u0026quot;/\u0026quot;line_item_usage_amount\u0026quot;),4) as hourly_rate from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; WHERE \u0026quot;line_item_product_code\u0026quot; like '%AmazonEC2%' and \u0026quot;line_item_usage_type\u0026quot; like '%Usage%' GROUP BY \u0026quot;line_item_usage_type\u0026quot; ORDER BY \u0026quot;line_item_usage_type\u0026quot; LIMIT 20    Show unused Reserved Instances This will show how much of your reserved instances are not being used, and sorts it via cost of unused portion (recurring fee). You can use this in two ways:\n    See where you have spare RI\u0026rsquo;s and modify instances to match, so they will use the RIs\n  Convert your existing RI\u0026rsquo;s if possible\n SELECT bill_billing_period_start_date, product_region, line_item_usage_type, reservation_reservation_a_r_n, reservation_unused_quantity, reservation_unused_recurring_fee from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; WHERE length(reservation_reservation_a_r_n) \u0026gt; 0 and reservation_unused_quantity \u0026gt; 0 ORDER BY bill_billing_period_start_date, reservation_unused_recurring_fee desc LIMIT 20    You have now setup and completed basic analysis of a Cost and Usage Report (CUR)\n  function prevStep(){ window.open(\"..\\/2_setup_athena\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/4_tear_down\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_automated_cur_query_and_email_delivery/3_iam_policy_and_role/","title":"Create an IAM policy and role for Lambda function","tags":[],"description":"","content":"This step is used to create an IAM policy and a role that allows Lambda function to perform Athena CUR query and deliver processed CUR report via SES.\n  Log into IAM console, click on Policies and click on Create Policy:   Click on the JSON tab, modify the following policy, replacing the your-cur-query-results-bucket string. Make sure you add \u0026ldquo;*\u0026rdquo; at the end of the bucket name so the whole bucket is writable:\n { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;VisualEditor0\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;s3:PutObject\u0026quot; ], \u0026quot;Resource\u0026quot;: [ \u0026quot;arn:aws:s3:::your-cur-query-results-bucket*\u0026quot; ] }, { \u0026quot;Sid\u0026quot;: \u0026quot;VisualEditor1\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;athena:List*\u0026quot;, \u0026quot;athena:*QueryExecution\u0026quot;, \u0026quot;athena:Get*\u0026quot;, \u0026quot;athena:BatchGet*\u0026quot;, \u0026quot;glue:Get*\u0026quot;, \u0026quot;glue:BatchGet*\u0026quot;, \u0026quot;s3:Get*\u0026quot;, \u0026quot;s3:List*\u0026quot;, \u0026quot;SES:SendRawEmail\u0026quot;, \u0026quot;SES:SendEmail\u0026quot;, \u0026quot;logs:CreateLogStream\u0026quot;, \u0026quot;logs:CreateLogGroup\u0026quot;, \u0026quot;logs:PutLogEvents\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] }    Copy the policy to JSON edit frame, ensure the bucket name has been changed, click Review policy:   Configure the name Lambda_Auto_CUR_Delivery_Access, and click Create policy.   Click on Roles, click Create Role:   Choose Lambda as the service that will use this role, click Next Permissions:   At Attach permissions policies page, search and choose Lambda_Auto_CUR_Delivery_Access policy created in the previous step. Click Next:Tags, click Next:Review.   At Review page, configure a name Lambda_Auto_CUR_Delivery_Role, click Create role. This role will be used for lambda function execution.    function prevStep(){ window.open(\"..\\/2_create_bucket\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/4_configure_function_parameters\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_2_cost_and_usage_governance/3_ec2_restrict_family/","title":"Create an IAM Policy to restrict EC2 usage by family","tags":[],"description":"","content":"AWS offers different instance families within EC2. Depending on your workload requirements - different types will be most cost effective. For non-specific environments such as testing or development, you can restrict the instance families in those accounts to the most cost effective generic types. It is also an effective way to increase Savings Plan or Reserved Instance utilization, by ensuring these accounts will consume any available commitment discounts.\nWe will create a policy that allows operations on specific instance families only. This will not only restrict launching an instance, but all other activities. NOTE: it is best practice to provide only the minimum access required, the policy used here is for brevity and simplicity, and should only be implemented as a demonstration before being removed.\nCreate the Instance family restrictive IAM Policy   Log on to the console as your regular user with the required permissions, Go to the IAM service page:   Select Policies from the left menu:   Click Create Policy:   Click on the JSON tab:   Copy and paste the policy into the console:   IAM Policy   { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;ec2:*\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;Condition\u0026quot;: { \u0026quot;ForAllValues:StringLike\u0026quot;: { \u0026quot;ec2:InstanceType\u0026quot;: [ \u0026quot;t3.*\u0026quot;, \u0026quot;a1.*\u0026quot;, \u0026quot;m5.*\u0026quot; ] } } } ] }   \n  Click Review policy:   Enter the details:\n Name: EC2_FamilyRestrict Description: Restrict to t3, a1 and m5 families Click on Create Policy:     You have successfully created an IAM policy to restrict usage by Instance Family.\n Apply the policy to your test group   Click on Groups from the left menu:   Click on the CostTest group (created previously):   We need to remove the RegionRestrict policy, as it permitted all EC2 actions. Click on Detach Policy for RegionRestrict:   Click on Detach:   Click on Attach Policy:   Click on Policy Type, then click Customer Managed:   Select the checkbox next to Ec2_FamilyRestrict, and click Attach Policy:   You have successfully attached the policy to the CostTest group.\n Log out from the console\n Verify the policy is in effect   Logon to the console as the TestUser1 user, go to the EC2 Service dashboard:   Try to launch an instance by clicking Launch Instance, select Launch Instance:   Click on Select next to the Amazon Linux 2 AMI:   We will select an instance we are not able to launch first, so select a c5.large instance, click Review and Launch:   Make note of the security group created, click Launch:   Select Proceed without a key pair, and click I acknowledge that I will not be able to\u0026hellip;, then click Launch Instances:   You will receive an error, notice the failed step was Initiating launches. Click Back to Review Screen:   Click Edit instance type:   We will select an instance type we can launch (t3, a1 or m5) select t3.micro, and click Review and Launch:   Select Yes, I want to continue with this instance type (t3.micro), click Next:   Click Launch:   Select Proceed without a key pair, and click I acknowledge that i will not be able to\u0026hellip;, then click Launch Instances:   You will receive a success message. Click on the Instance ID and terminate the instance as above:   Log out of the console as TestUser1.\n  You have successfully implemented an IAM policy that restricts all EC2 actions to T3, A1 and M5 instance types.\n  function prevStep(){ window.open(\"..\\/2_ec2_restrict_region\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/4_ec2_restrict_size\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_2_cost_and_usage_governance/3_budget_spcoverage/","title":"Create and implement an AWS Budget for EC2 Savings Plan coverage","tags":[],"description":"","content":"We will create a monthly savings plan coverage budget which will notify if the coverage of Savings Plan for EC2 is below the specified amount.\nYou should not set an arbitrary limit to alarm on (i.e. alarm if coverage is less than 80%) instead select your current level of coverage - so if coverage reduces, you can act and increase coverage if required.\n  From the AWS Budgets dashboard in the console, click Create budget:   Select Savings Plans budget, and click Set your budget \u0026gt;:   Create a cost budget, enter the following details:\n Name: SP_Coverage Period: Monthly Savings Plans budget type: Savings Plans Coverage Coverage threshold: 90% Leave all other fields as defaults NOTE: NEVER create a utilization budget, unless you are doing it for a single and specific discount rate by using filters. For example you want to track the utilization of m5.large Linux discount. A utilization budget across different discounts will most likely lead to confusion and unnecessary work.    Scroll down and click Configure alerts \u0026gt;:   Enter an address for Email contacts and click Confirm budget \u0026gt;:   Review the configuration, and click Create in the lower right:   You have created an Savings Plans Coverage budget:   You will receive an email similar to this within a few minutes:   You have created a Savings Plan budget. Use this type of budget to notify you if a change in a workload has reduced coverage, a Savings Plan has expired, or additional usage has been created and a new Savings Plan purchase may be required.\n  function prevStep(){ window.open(\"..\\/2_budget_ec2actual\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/4_budget_report\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_iam_permission_boundaries_delegating_role_creation/3_user_role/","title":"Create and Test User Role","tags":[],"description":"","content":"3.1 Create User Role While you are still assuming the developer-restricted-iam role you created in the previous step, create a new user role with the boundary policy attached and name it with the prefix. We will use AWS managed policies for this user role, however the createrole-restrict-region-boundary policy will allow us to create and attach our own policies, only if they have a prefix of app1.\n Verify that you are Using the developer role previously created by checking the top bar it should look like and open the IAM console at https://console.aws.amazon.com/iam/ . You will notice a number of permission denied messages as this developer role is restricted. Least privilege is a best practice! In the navigation pane, click Roles and then click Create role.  Click Another AWS account, then enter your account ID that you have been using for this lab and tick Require MFA, then click Next: Permissions.  In the search field start typing ec2full then check the box next to the AmazonEC2FullAccess policy.  Erase your previous search and start typing lambda then check the box next to the AWSLambdaFullAccess policy.  Expand the bottom section Set permissions boundary and click Use a permissions boundary to control the maximum role permissions. In the search field start typing boundary then click the radio button for restrict-region-boundary and then click Next: Tags.  For this lab we will not use IAM tags, click Next: Review. Enter the Role name of app1-user-region-restricted-services for the role and click Create role.  The role should create successfully if you followed all the steps. Record both the Role ARN and the link to the console. If you receive an error message a common mistake is not changing the account number in the policies in the previous steps.  3.2 Test User Role Now you will use an existing IAM user to assume the new app1-user-region-restricted-services role, as if you were a user who only needs to administer EC2 and Lambda in your allowed regions.\n In the console, click your role\u0026rsquo;s Display Name on the right side of the navigation bar. Click Back to your previous username. You are now back to using your original IAM user. In the console, click your user name on the navigation bar in the upper right. Alternatively you can paste the link in your browser that you recorded earlier for the app1-user-region-restricted-services role. On the Switch Role page, type the account ID number or the account alias and the name of the role app1-user-region-restricted-services that you created in the previous step. Select a different color to before, otherwise it will overwrite that profile in your browser. Click Switch Role. The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you.  You are now using the user role with the only actions allowed of EC2 and Lambda in us-east-1 (North Virginia) and us-west-1 (North California) regions! Navigate to the EC2 Management Console in the us-east-1 region https://us-east-1.console.aws.amazon.com/ec2/v2/home?region=us-east-1 . The EC2 Dashboard should display a summary list of resources with the only error being Error retrieving resource count from Elastic Load Balancing as that requires additional permissions.  Navigate to the EC2 Management Console in a region that is not allowed, such as ap-southeast-2 (Sydney) https://ap-southeast-2.console.aws.amazon.com/ec2/v2/home?region=ap-southeast-2 . The EC2 Dashboard should display a number of unauthorized error messages.  Congratulations! You have now learnt about IAM permission boundaries and have one working!  "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_basic_ec2_with_waf_protection/3_create_alb_with_waf/","title":"Create Application Load Balancer with WAF integration","tags":[],"description":"","content":"Using the AWS Management Console, we will create an Application Load Balancer, link it to the AWS WAF ACL we previously created and test.\n3.1 Create Application Load Balancer  Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. From the console dashboard, choose Load Balancers from the Load Balancing section. Click Create Load Balancer.  Click Create under the Application Load Balancer section.  Enter Name for Application Load Balancer such as lab-alb. Select all availability zones in your region then click Next. You will need to click Next again to accept your load balancer is using insecure listener.  Click Create a new security group and enter name and description such as lab-alb and accept default of open to internet.  Accept defaults and enter Name such as lab-alb and click Next.  From the list of instances click the check box and then Add to registered button. Then click Next.  Review the details and click Create.  A successful message should appear, click Close. Take not of the DNS name under the Description tab, you will need this for testing.  3.2 Configure Application Load Balancer with WAF  Open the AWS WAF console at https://console.aws.amazon.com/waf/. In the navigation pane, choose Web ACLs.  Choose the web ACL that you want to associate with the Application Load Balancer. On the Rules tab, under AWS resources using this web ACL, choose Add association.  When prompted, use the Resource list to choose the Application Load Balancer that you want to associate this web ACL such as lab-alb and click Add.  The Application Load Balancer should now appear under resources using.  You can now test access by entering the DNS name of your load balancer in a web browser.  "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_splitting_sharing_cur_access/3_athena_queries/","title":"Create Athena Saved Queries to Write new Data","tags":[],"description":"","content":"Next we setup your recurring Athena queries. These will run each time a new CUR file is delivered, separate out the information for the sub accounts, and write it to the output S3 location. These queries will be very similar to the one above, except it will only extract data for the current month.\nYou must write one query for the extraction of the data, which will create a temporary table, and then a second query to delete the table. As the system has been written for future expansion, you must adhere to the guidelines below when writing and naming statements (other wise you will need to change the code):\n The queries MUST start with: create_linked_ and delete_linked_ otherwise you\u0026rsquo;ll need to modify the Lambda function. As Lambda looks for this string to identify these queries to automatically run when new files are delivered The output location must also end in the actual word subfolder as this will be re-written by the lambda function, to the current year and month The queries must include the component CAST(bill_billing_period_start_date as VARCHAR) like concat(substr(CAST(current_date as VARCHAR),1,7),'-01%') which ensures the query only gets data from the current month There is no need to include the columns year as year_1 and month as month_1, as that was only used for partitioning  1 - Create the saved query in Athena named create_linked_folder-name, the following sample code is the accompanying query for the previous query above:\nCREATE TABLE (database).temp_table WITH ( format = 'Parquet', parquet_compression = 'GZIP', external_location = 's3://(bucket)/(folder)/subfolder') AS SELECT * FROM \u0026quot;(database)\u0026quot;.\u0026quot;(table)\u0026quot; WHERE line_item_usage_account_id = '(some value)' AND (year=CAST(year(current_date- INTERVAL '__interval__' MONTH) AS VARCHAR)) AND month=CAST(month(current_date- INTERVAL '__interval__' MONTH) AS VARCHAR) 2 - Create the accompanying delete statement named delete_linked_folder-name to delete the temporary table:\ndrop TABLE IF EXISTS (database).temp_table; 3 - Repeat the steps above for any additional create and delete queries as required.\n function prevStep(){ window.open(\"..\\/2_backfill_data\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/4_lambda_function\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_lambda_cross_account_bucket_policy/3_create_bucket_policy/","title":"Create bucket policy for the S3 bucket in account 2","tags":[],"description":"","content":"  In account 2 sign in to the S3 Management Console as an IAM user or role in your AWS account, and open the S3 console at https://console.aws.amazon.com/s3\n  Click on the name of the bucket you will use for this workshop\n  Go to the Permissions tab\n  Click Bucket Policy\n  Enter the following JSON policy\n  Replace account1 with the AWS Account number (no dashes) of account 1\n  Replace bucketname with the S3 bucket name from account 2\n  Note: This policy uses least privilege. Only resources using the IAM role from account 1 will have access\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;Stmt1565731301209\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;s3:ListBucket\u0026quot; ], \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:s3:::bucketname\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;AWS\u0026quot;:\u0026quot;arn:aws:iam::account1:role/Lambda-List-S3-Role\u0026quot; }, \u0026quot;Condition\u0026quot;: { \u0026quot;StringLike\u0026quot;: { \u0026quot;aws:UserAgent\u0026quot;: \u0026quot;*AWS_Lambda_python*\u0026quot; } } } ] }      Click Save\n  "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_lambda_cross_account_iam_role_assumption/3_create_lambda_acct_1/","title":"Create Lambda in account 1","tags":[],"description":"","content":"  Open the Lambda console.\n  Click Create a function.\n  Accept the default Author from scratch.\n  Enter function name as Lambda-Assume-Roles.\n  Select Python 3.6 runtime.\n  Expand Permissions, click Use an existing role, then select the Lambda-Assume-Roles role.\n  Click Create function.\n  Replace the example function code with the following, replacing the RoleArn with the one from account 2 you created previously.\n import json import boto3 import os import uuid def lambda_handler(event, context): try: client = boto3.client('sts') response = client.assume_role(RoleArn='arn:aws:iam::account2:role/LambdaS3ListBuckets',RoleSessionName=\u0026quot;{}-s3\u0026quot;.format(str(uuid.uuid4())[:5])) session = boto3.Session(aws_access_key_id=response['Credentials']['AccessKeyId'],aws_secret_access_key=response['Credentials']['SecretAccessKey'],aws_session_token=response['Credentials']['SessionToken']) s3 = session.client('s3') s3list = s3.list_buckets() print (s3list) return str(s3list['Buckets']) except Exception as e: print(e) raise e    Click Save.\n  Click Test, accept the default event template, enter event name of test, then click Create.\n  Click Test again, and in a few seconds the function output should highlight green and you can expand the detail to see the response from the S3 API.\n  How could the example policies be improved?\n"},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_workload_efficiency/3_visualizations/","title":"Create the Visualizations","tags":[],"description":"","content":"We will now create visualizations of our workload effiency. We will add the new dataset, and then build different visualizations to see what exactly impacts efficiency and where to look to improve it.\nBasic Efficiency visualization We will create a visualization from the application log files.\n  Go into QuickSight\n  Click Manage data:   Click New data set:   Click Athena:   Enter the Data source name: Efficiency, and click Create data source\n  Select the costusage Database, and the efficiency Table and click Select\n  Select Spice and click Edit/Preview data\n  Make sure there is data in the bottom pane:\n  Ensure you have data and click Save \u0026amp; visualize:\n  Create a Clustered bar combo chart. Place datetime (aggregate hour) on the x-axis, cost (sum) in the bars column, add request (count) to the lines field\n  Label the chart Requests vs Cost:   We can see that it roughly follows the same pattern, however there are times when the trends change. Below you can see the requests increase, but the cost decreases. Also the requests remain the same, and the cost decreases:   Maybe it can be explained through something other than request count, lets add mbytes (sum) to the lines field well to see if there is correlation there:   Again, similar trends and anomalies. MBytes remains constant and cost decreases:   Lets now create our efficiency visualition. Add a calculated field named efficiency with the formula below. Our efficiency metric will be requests per dollar:\ncount(request) / sum(cost)    Add a visualization, select a Line chart. Place datetime (hour) in the x-axis, efficiency as the value,\n  We now have a chart showing our efficiency over time. Notice how the efficiency changes significantly at the end of the day:   You can now see increases and decreases in efficiency clearly, look when the output increases and cost remains the same, or the cost remains the same and the output decreases:   You now have a baseline efficiency metric. Use this to look for areas of low efficiency of your workload - this will provide areas to cost optimize.\nRequest visualization Lets look deeper into the types of requests to see if we can get better insight into what is driving our costs and efficiency.\n  Lets look at a sample of a successful log request in our application log files:\n /index.php?name=Isabella,user=sponsored,work=26    We can see there are the fields:\n Name User Work    Lets create a calculated field RequestType with the formula below. This will separate out the types of requests, health checks, image requests and other/errors from the request field:\n ifelse(locate(request,\u0026quot;index.html\u0026quot;) \u0026gt; 0,split(request,',',2),ifelse(locate(request,\u0026quot;health.html\u0026quot;) \u0026gt; 0,\u0026quot;HealthCheck\u0026quot;,ifelse(locate(request,\u0026quot;image_file\u0026quot;) \u0026gt; 0,\u0026quot;ImageFile\u0026quot;,\u0026quot;error\u0026quot;)))    Create a Cluster bar combo chart visualization. Place datetime (HOUR) in the x-axis, add Request (count) in the bars, add RequestType in the Group/Color for bars, add efficiency to lines:   We can see a correlation between the user type and the efficiency, when paid users increase the efficiency goes down, this indicates its more costly to service paid customers. Also look at the efficiency increase when there are large amounts of errors, you may which to filter and exclude low value outputs from the measures of efficiency. These insights can be used to categorize different types of requests in your workload and understand how to charge them back appropriately.   You can use this to increase your efficiency by removing unwanted requests, for example you may use CloudFront to more efficiency handle errors - instead of processing them on the web servers.\nCongratulations! You have now calculated the efficiency of a workload, you can see how your efficiency changes over time, and look into the types of requests to understand what contributes cost to your workload.\n  function prevStep(){ window.open(\"..\\/2_efficiency_data\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/4_tear_down\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/100_labs/100_inventory_patch_management/3_deploy_env_iaac/","title":"Deploy an Environment Using Infrastructure as Code","tags":[],"description":"","content":"Tagging We will make extensive use of tagging throughout the lab. The CloudFormation template for the lab includes the definition of multiple tags against a variety of resources.\nAWS enables you to assign metadata to your AWS resources in the form of tags . Each tag is a simple label consisting of a customer-defined key and an optional value that can make it easier to manage, search for, and filter resources. Although there are no inherent types of tags, commonly adopted categories of tags include technical tags (e.g., Environment, Workload, InstanceRole, and Name), tags for automation (e.g., Patch Group, and SSMManaged), business tags (e.g., Owner), and security tags (e.g., Confidentiality).\nApply the following best practices when using tags:\n Use a standardized, case-sensitive format for tags, and implement it consistently across all resource types Consider tag dimensions that support the following:  Managing resource access control with IAM  Cost tracking Automation AWS console organization   Implement automated tools to help manage resource tags. The Resource Groups Tagging API enables programmatic control of tags, making it easier to automatically manage, search, and filter tags and resources. Err on the side of using too many tags rather than too few tags. Develop a tagging strategy .   Note It is easy to modify tags to accommodate changing business requirements; however, consider the consequences of future changes, especially in relation to tag-based access control, automation, or upstream billing reports.\n  Important Patch Group is a reserved tag key used by Systems Manager Patch Manager that is case sensitive with a space between the two words.\n Management Tools: CloudFormation AWS CloudFormation is a service that helps you model and set up your Amazon Web Services resources so that you can spend less time managing those resources and more time focusing on your applications. You create a template that describes all the AWS resources that you want (like Amazon EC2 instances or Amazon RDS DB instances) and AWS CloudFormation provisions and configures those resources for you. AWS CloudFormation enables you to use a template file to create and delete a collection of resources as a single unit (a stack).\nThere is no additional charge for AWS CloudFormation . You pay for AWS resources (such as Amazon EC2 instances, Elastic Load Balancing load balancers, etc.) created using AWS CloudFormation in the same manner as if you created the resources manually. You only pay for what you use as you use it. There are no minimum fees and no required upfront commitments.\n3.1 Deploy the Lab Infrastructure To deploy the lab infrastructure:\n Download the CloudFormation script for this lab through this link . Use your administrator account to access the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Choose Create Stack. On the Select Template page, select Upload a template file and select the OE_Inventory_and_Patch_Mgmt.json file you just downloaded.  AWS CloudFormation Designer\nAWS CloudFormation Designer is a graphic tool for creating, viewing, and modifying AWS CloudFormation templates. With Designer you can diagram your template resources using a drag-and-drop interface. You can edit their details using the integrated JSON and YAML editor. AWS CloudFormation Designer can help you see the relationship between template resources.\nOn the Select Template page, in the lower-right corner, click the View in Designer button. Briefly review the graphical representation of the environment we are about to create, including the template in the JSON and YAML formats. You can use this feature to convert between JSON and YAML formats. Choose the Create Stack icon (a cloud with an arrow) to return to the Select Template page. On the Select Template page, choose Next.  A CloudFormation template is a JSON or YAML formatted text file that describes your AWS infrastructure containing both optional and required sections . In the next steps, we will provide a name for our stack and parameters that will be passed into the template to help define the resources that will be implemented.\nIn the Specify Details section, define a Stack name, such as OELabStack1. In the Parameters section:  Leave InstanceProfile blank as we have not yet defined an instance profile. Leave InstanceTypeApp and InstanceTypeWeb as the default free-tier-eligible t2.micro value. Select the EC2 KeyName you defined earlier from the list.   In a browser window, go to https://checkip.amazonaws.com/ to get your IP. Enter your IP address in SSHLocation in CIDR notation (i.e., ending in /32). Define the Workload Name as Test. Choose Next.   On the Options page under Tags, define a Key of Owner, with Value set to the username you choose for your administrator. You may define additional keys as needed. The CloudFormation template creates all the example tags given in the discussion on tagging above. Leave all other sections unmodified. Scroll to the bottom of the page and choose Next. On the Review page, review your choices and then choose Create. On the CloudFormation console page  Check the box next to your Stack Name to see its details. If your Stack Name is not displayed, click the refresh button (circular arrow) in the top right until it appears. If the details are not displayed, choose the refresh button until details appear.   Choose the Events tab for your selected workload to see the activity log from the creation of your CloudFormation stack.  When the Status of your stack displays CREATE_COMPLETE in the filter list, you have just created a representation of a typical lift and shift 2-tier application migrated to the cloud.\nNavigate to the EC2 console to view the deployed systems:  Choose Instances. Select a server and review the details under its Description and Tag tabs. (Optional) choose Security Groups and select the Security Group whose name begins with the name of your stack. Examine the inbound rules. (Optional) navigate to the VPC console and examine the configuration of the VPC you just created.    The impact of Infrastructure as Code With infrastructure as code, if you can deploy one environment, you can deploy any number of copies of that environment. In this example we have created a Test environment. Later, we will repeat these steps to deploy a Prod environment.\nThe ability to dynamically deploy temporary environments on-demand enables parallel experimentation, development, and testing efforts. It allows duplication of environments to recreate and analyze errors, as well as cut-over deployment of production systems using blue-green methodologies. These practices contribute to reduced risk, increased operations effectiveness, and efficiency.\n function prevStep(){ window.open(\"..\\/2_setup_env\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/4_inventory_mgmt\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_autonomous_patching_with_ec2_image_builder_and_systems_manager/3_deploy_the_ami_builder_pipeline/","title":"Deploy The AMI Builder Pipeline","tags":[],"description":"","content":"In this section we will be building our Amazon Machine Image Pipeline leveraging EC2 Image Builder service. EC2 Image Builder is a service that simplifies the creation, maintenance, validation, sharing, and deployment of Linux or Windows Server images for use with Amazon EC2 and on-premises. Using this service, eliminates the automation heavy lifting you have to build in order to streamline the build and management of your Amazon Machine Image.\nUpon completion of this section we will have an Image builder pipeline that will be responsible for taking a golden AMI Image, and produce a newly patched Amazon Machine Image, ready to be deployed to our application cluster, replacing the outdated one.\nIn this section you have the option to build the pipeline manually using AWS console, or if you are keen to complete the lab quickly, you can simply deploy from the cloudformation template.\n  Click here to build your pipeline using CloudFormation on the command line   3.1. Download The Cloudformation Template. Download the template here .\n3.2. Deploy Using The Command Line. Command Line: To deploy from the command line, ensure that you have installed and configured AWS CLI with the appropriate credentials:\naws cloudformation create-stack --stack-name pattern3-pipeline \\ --template-body file://pattern3-pipeline.yml \\ --parameters ParameterKey=MasterAMI,ParameterValue=ami-0f96495a064477ffb\t\\ ParameterKey=BaselineVpcStack,ParameterValue=pattern3-base \\ --capabilities CAPABILITY_IAM \\ --region ap-southeast-2 Note :  For simplicity, we have used Sydney \u0026lsquo;ap-southeast-2\u0026rsquo; as the default region for this lab. We have also pre-configured the MasterAMI parameter to be the AMI id of Amazon Linux 2 AMI (HVM) in Sydney region ami-0f96495a064477ffb. If you choose to to use a different region, please change the AMI Id accordingly for your region.      Click here to build your pipeline using CloudFormation through the console   Console: 3.1. Download The Cloudformation Template. Download the template here .\n3.2. Deploy Using The Console. If you need detailed instructions on how to deploy CloudFormation stacks from within the console, please follow this guide.  Use pattern3-pipeline as the Stack Name. Provide the name of the vpc cloudformation stack you create in section 1 ( we used pattern3-base as default ) as the BaselineVpcStack parameter value. Use the AMI Id of Amazon Linux 2 AMI (HVM) as the MasterAMI parameter value. ( In Sydney region ami-0f96495a064477ffb if you choose to to use a different region, please change the AMI Id accordingly for your region. )  3.3. Take note of the ARN. When the CloudFormation template deployment is completed, note the output produced by the stack.\nYou can do this by clicking on the stack name you just created, and select the \u0026lsquo;Outputs Tab\u0026rsquo; as shown in diagram below.\nPlease take note of the Pipeline ARN specified under Pattern3ImagePipeline output\n    Click here to build your pipeline interactively   In this section we will go through the process manually to get a better understanding of the how the pipeline is constructed in EC2 Image Builder service.\nTo build this pipeline there are several subtasks we need to do:\n Create an S3 bucket for logging purposes. Create an IAM role for use by the EC2 Image Builder. Create an Image Builder Component. Create an Image Builder Recipe. Create an Image Builder Pipeline.  3.1. Create an S3 Bucket. We are going to use an S3 bucket to store the the EC2 Image Build process, so lets create one.\n3.1.1. As S3 is a global namespace, for consistency please use the naming convention pattern3-logging with a unique UUID number at the end.\nYou can achieve this on a mac or UNIX terminal by setting a variable called $bucket as follows:\nbucket=pattern3-logging-`uuidgen | awk -F- '{print tolower($1$2$3)}'` echo $bucket 3.1.2. Hopefully you should have a bucket name returned to you which you can then use to create the bucket as follows:\naws s3 mb s3://$bucket --region ap-southeast-2  Alternatively you can use any randomized string at the end of the standard bucket name and create a bucket manually through the console. Please refer to this guide to create S3 bucket.   3.2 Create IAM role We will need to create an IAM role that will be used by the EC2 Image Builder service.This IAM role will be used as the instance profile role of the temporary EC2 instance the service will launch. The service will use this instance to run the necessary activity, in this case our patch update. Therefore the role will need to have the appropriate policies to do this activity.\nFollow below steps to create the IAM role:\n3.2.1. Navigate to IAM within the console and select \u0026lsquo;role\u0026rsquo; from the left hand panel and then select \u0026lsquo;create role\u0026rsquo; as shown:\n3.2.2. Select \u0026lsquo;AWS Service\u0026rsquo; from the types of trusted entities and then select \u0026lsquo;EC2\u0026rsquo;, and \u0026lsquo;next: Permissions\u0026rsquo; as shown:\n3.2.3. Using the filter, search \u0026amp; select the following policies: * EC2InstanceProfileForImageBuilder * AmazonSSMManagedInstanceCore\n3.2.4. Click \u0026lsquo;Next:Tags\u0026rsquo;.\n3.2.5. On the next screen click \u0026lsquo;Next:Review\u0026rsquo;.\n3.2.6. Enter pattern3-recipe-instance-role for the Role Name and add a description. The three policies listed above should be added as follows:\n3.2.7. In the IAM console, locate the role you just created.\n3.2.8. Click on the role and click + Add inline policy\n3.2.9. Select the JSON Tab and paste in below policy, replace the \u0026lt;s3 logging bucket\u0026gt; in the json snippet below with the bucket name you created in previous step.\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;s3:*\u0026quot;, \u0026quot;Resource\u0026quot;: [ \u0026quot;arn:aws:s3:::\u0026lt;s3 logging bucket\u0026gt;/*\u0026quot; ] } ] } 3.2.10. Click Review Policy\n3.2.11. Enter a name for the policy, and click \u0026lsquo;Create Policy\u0026rsquo;\n3.2.12. Once you are done with this, you should now see another entry in the Policies with the name you just specified, expanding on that you should see the policy specified as screen shot below.\n3.3 Create a Security Group. Our EC2 Image Build pipeline is also going to need a security group that will be assigned to the temporary EC2 instance it uses, so lets create one now so that we can include it later in the lab.\n3.3.1. Follow this guide to create a Security Group.\n3.3.2. For this purpose, we do not need to assign anything in the Inbound rule of the security group.\n3.3.3. We do need to ensure that the outbound rules allow traffic out to the internet.\nYour Security group rules should look like below, so edit your security group accordingly:\n3.3.4. Ensure that the security group is created in the VPC id you\u0026rsquo;ve taken note of in section 1.2.\nIf you don\u0026rsquo;t remember the VPC-id, please refer to the instruction on section 1.2 in this lab for clarification.\n3.3.5. Name the Security Group pattern3-pipeline-instance-security-group\n3.4. Create a Component. In this section we will create a construct in EC2 Image Builder called a Component. This construct essentially contains instructions on what you would like to build into the AMI. For more information about EC2 Image builder Component, please refer to this guide .\nTo do this, Please follow below following steps:\n3.4.1. Navigate to the EC2 Image Builder service from the console main page.\n3.4.2. From the EC2 Image Builder service, select Components from the left hand menu and then select Create Component as shown here:\n3.4.3. Add the following values to to the options, leaving the rest of the settings as default:\n Version: 1.0.0 Platform Linux Compatible OS versions: Amazon Linux 2 Component Name: pattern3-pipeline-ConfigureOSComponent Description: Component to update the OS with latest package versions.  3.4.4. Once that\u0026rsquo;s done, select \u0026lsquo;Define document content\u0026rsquo;\n3.4.5. Copy and paste in below definition document in the section under it.\nname: ConfigureOS schemaVersion: 1.0 phases: - name: build steps: - name: UpdateOS action: UpdateOS Please Note that this definition is specified in YAML, so please ensure indentation is correct.\nIn this scenario, we have a very simple definition in our component, which is to run an UpdateOS action which will the packages in our OS. There are many other action activity you can specify in the component. For more information about EC2 Image Builder component, please refer to this guide 3.4.6. When you have completed these inputs, select Create Component to complete the component setup.\n3.5. Create An Image Builder Recipe. Next, we will create an Image Builder Recipe, which specifies the components, and other configuration we are going to define for our pipeline.\nTo do this, please complete the following steps:\n3.5.1. Select Recipes from the left hand menu and then select Create Recipe.\n3.5.2. Enter the following as configuration details:\n Name: pattern3-pipeline-ConfigureOSRecipe Version: 1.0.0 Description: Pattern3 Configure OS Recipe  3.5.3. Select Enter custom AMI ID and enter: the AMI ID for Amazon Linux 2 AMI (HVM) in your region:\n( In Sydney region ami-0f96495a064477ffb, please change the AMI Id accordingly if you use other region.)\n3.5.4. Under Build components select Browse build components and then filter by Created by me to include the component which you created earlier ( pattern3-pipeline-ConfigureOSComponent )/\n3.5.5. Once you have entered all of the configuration details, select \u0026lsquo;Create Recipe\u0026rsquo; at the bottom of the screen.\n3.6. Create An Image Builder Pipeline Using the Recipe We will now create the Image Builder Pipeline to run our recipe.\nTo do this, please complete the following steps:\n3.6.1. Remain in the Image Builder Recipe screen and use the tick box to select the recipe which you just created.\n3.6.2. From the Actions menu, select Create pipeline from this recipe as shown here:\n3.6.3. Enter the following information to configure the pipeline:\n Name: pattern3-pipeline Description: Pattern 3 pipeline to update OS. Role: Specify the instance role which you created in step 3.2.2. Build Schedule: Manual Infrastructure Settings/Instance Type: Select an M4.large here if possible, although smaller instances can be used. Infrastructure Settings/VPC, subnet and security groups/Virtual Private Cloud: Select the VPC that have taken note in section 1.2 of the lab (the output components will list the VPC details). Infrastructure Settings/VPC, subnet and security groups/Subnet ID: Select the private Subnet ID from section 1.2 of the lab. Infrastructure Settings/VPC, subnet and security groups/Security Group Select the security group which you created before in step 3.2.3. Infrastructure Settings/Troubleshooting Settings/S3 location: Enter the S3 bucket that you specified in section 3.2.1.  Note:  For the instance types listed, an M4.large will take 20-30 minutes to build. If you want to save costs, please use a smaller instance but be prepared to wait for a bit longer for completion.  3.6.4. Once you have completed the above configuration, select Next at the bottom of the screen to go to the next configuation page.\n3.6.5. Leave the rest empty and click Review.\n3.6.6. Review the configuration is according to our specification above, and click Create Pipeline\n3.6.7. Take note of the pipeline ARN, as we will need this for the next section.\n3.7 Run Your Pipeline. Now that we have created all the construct, we can test the pipeline to ensure that it is working correctly. To do this select Run Pipeline from the Actions menu with the pipeline selected as shown here:\nOnce this is executed, you can observe the pipeline execution, and wait for the AMI to be built.\nNote: EC2 Image Builder pipeline will execute an SSM Automation Document in the background to orchestrate all the activities in building the AMI. If you go into your System Manager Automation document console, you should be able to see the execution running, and observe the activities in more detailed.\nPlease refer to this guide on how to view the Automation document execution details in your console.\nYou should be able to see an execution running under ImageBuilderBuildImageDocument document, which is the document used by EC2 Image builder to execute it\u0026rsquo;s activities.\n  Now that you have completed the deployment of the Image Builder Pipeline, move to section 4 of the lab whre we will use Systems Manager to build the automation stage of the architecture.\n END OF SECTION 3\n "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_aws_resource_optimization/3_prio_resource_opt/","title":"Download the Amazon EC2 Resource Optimization CSV File and sort it to find quick wins","tags":[],"description":"","content":"  Download the Amazon EC2 Resource Optimization report:   If you dont have any Amazon EC2 Resource Optimization recommendation use the file below as a reference. Sample Amazon EC2 Resource Optimization file (.csv)   First lets exclude instances that are too small or were only running for a few hours from the analysis. By doing so, we minimize the time required to perform rightsizing modifications that would otherwise result in minimal savings.   NOTE: Depending on how many cost allocation tags you have enabled on your account the columns may differ from the example, that said try to match the formulas using the screenshots below and the default column names.\nInsert a new column to the right of the Recommended Action column. The first row will be the label for the column TooSmall. In each row below the label, paste the following formula:\n=IF(Q2\u0026lt;25,1,0) Where Column Q = Recommended Instance Type 1 Estimated Savings\nThat formula will flag all EC2 instances with a 1 any instance that will fail to deliver more than $25/month in savings (or $300/year). Feel free to adjust the threshold for your organization own savings expectation. If you prefer to perform the analysis against instance families instead of potential savings you can use the following formula to exclude smaller instances from the recommendations as well.\n=IF(N2=\u0026quot;Modify\u0026quot;,IF(SUMPRODUCT(--(NOT(ISERR(SEARCH({\u0026quot;nano\u0026quot;,\u0026quot;micro\u0026quot;,\u0026quot;small\u0026quot;,\u0026quot;medium\u0026quot;},D2)))))\u0026gt;0,\u0026quot;1\u0026quot;,\u0026quot;0\u0026quot;),\u0026quot;0\u0026quot;) Where Column N = Recommended Action and Column D = Instance Type\nNext, lets flag EC2 instances that belong to previous generations (C4, M3, etc), if you are investing engineer time on rightsizing let\u0026rsquo;s make sure you are also leveraging the newest technology available. Newer EC2 generations have a superior performance increasing the changes of success for the rightsizing exercise, they also generally cost less than previous generations providing a higher cost vs benefit.  Insert a new column Old Gen to the right of the Instance Type field, and paste the following formula:\n=IF(SUMPRODUCT(--(NOT(ISERR(SEARCH({\u0026quot;c4\u0026quot;,\u0026quot;c3\u0026quot;,\u0026quot;c1\u0026quot;,\u0026quot;m4\u0026quot;,\u0026quot;m3\u0026quot;,\u0026quot;m2\u0026quot;,\u0026quot;m1\u0026quot;,\u0026quot;r3\u0026quot;,\u0026quot;r4\u0026quot;,\u0026quot;i2\u0026quot;,\u0026quot;cr1\u0026quot;,\u0026quot;hs1\u0026quot;,\u0026quot;g2\u0026quot;},D2)))))\u0026gt;0,\u0026quot;1\u0026quot;,\u0026quot;0\u0026quot;) Column D = Instance Type\nNow lets sort the recommendations by low complexity and higher savings:  Minimum Effort: Set the minimum savings required\nFirst we want to only focus on savings that are worth our effort, we will define this as $100. Apply a number filter on Recommended Instance Type 1 Estimated Savings that is Greater than 100\nGroup 1: Idle EC2 resources\nFilter the data on Recommended Action = \u0026ldquo;Terminate\u0026rdquo;\nSort the data by Recommended Instance Type 1 Estimated Savings = Largest to smallest\nStart filtering the idle resources or instances where CPU utilization \u0026lt;1%, it is likely these instances were launched and forgotten so the potential savings here may represent the entire On Demand cost.\nThe resulting filtered list should be where you start rightsizing discussions with application owners; perform an investigation to understand why these instance were launched and validate their usage with the resource owner. If possible, terminate them.\nIf you are using the Rightsizing CSV file provided in this lab exercise, you will notice that we filtered down from an original 2,534 recommendations to 16 and identified $3,458 per month in potential savings.\nGroup 2: Previous generation instances\nFilter the data on Recommended Actions = Modify AND OldGen = 1 AND TooSmall = 0\nFilter the data on Recommended Instance Type 1 Projected CPU \u0026lt; 40%\nSort the data by Recommended Instance Type 1 Estimated Savings = Largest to smallest\nThis will focus on the underutilized resources (\u0026lt;40% CPU) that belongs to previous generations and can either be downsized within the same family (column P below) or modernized to the newest generation.\nMoving to a modern generation may require additional testing hours compared to instances identified on Group 1, but depending on the case it can maximize savings and performance.\n    Linux vs new gen Windows vs new gen  Linux vs new gen Windows vs new gen     c3.large $0.105/hr up 19% $0.188/hr up 5% m3.large $0.133/hr up 27% $0.259/hr up 27%   c4.large $0.100/hr up 15% $0.192/hr up 7% m4.large $0.100/hr up 4% $0.192/hr up 2%   c5.large $0.985/hr 0% $0.177/hr 0% m5.large $0.096/hr 0% $0.188/hr 0%    prices are from US-Virginia (Nov 2019)\nIf you are using the Rightsizing CSV file provided in this lab exercise, you will notice that we filtered down from originally 2,534 recommendations to 22 with $6,362 per month in potential savings.\nGroup 3: Current generation instances\nFilter the data on Recommended Actions = Modify AND OldGen = 0 AND TooSmall = 0\nThe filter on Recommended Instance Type 1 Projected CPU \u0026lt; 40% should still be in place.\nSort the data by Recommended Instance Type 1 Estimated Savings = Largest to smallest\nThis will select underutilized resources from the current, most modern generation. We recommend sorting them by potential savings to make sure you are prioritizing the instances that will provide larger savings first.\nAlso, do not forget to check the other recommended instance types (columns U to AD); Amazon EC2 Resource Optimization will recommend up to 3 instances for each resource moving from a more conservative recommendation (the first recommendation) to a more aggressive and higher savings recommendation (second and third recommendations).\nIf you are using the Rightsizing CSV file provided in this lab exercise, you will notice that we filtered down from originally 2,534 recommendations to 22 with $4,879.56 per month in potential savings.\n function prevStep(){ window.open(\"..\\/2_resource_opt\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/4_act_resource_opt\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_4_cost_and_usage_analysis/3_cost_usage_download/","title":"Download your monthly cost and usage file","tags":[],"description":"","content":"It is possible to download a CSV version of your summary cost and usage information. This can be accessed by a spreadsheet application for ease of use. We will download your monthly usage file and view it.\n  Go to the billing dashboard:   Click on Bills from the left menu:   Select the Date you require from the drop down menu, by clicking on the menu item:   Click on Download CSV:   It will download a CSV version of the bill you can use in a spreadsheet application. It is recommended to NOT use this data source for calculations and analysis, instead you should use the Cost and Usage Report, which is covered in 200_4_Cost_and_Usage_Analysis .    function prevStep(){ window.open(\"..\\/2_cost_usage_detail\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/4_tear_down\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_100_quick_steps_to_security_success/3_additional_guardrails/","title":"Enable Additional Guardrails","tags":[],"description":"","content":"Control Tower guardrails Control Tower includes a number of guardrails to help improve your security posture. These guardrails are either preventative or detective. Preventative guardrails limit some actions and are implemented through AWS Organizations service control policies and are either enforced or not enabled. Detective guardrails detect resources in your landing zone which are in a noncompliant state. These are implemented via AWS Config ] and show resources that are either clear, in violation or not enabled.\nMake sure you review the mandatory guardrails and then review other guardrails you can enable . The strongly recommended guard rails follow the best practices for a Well-Architected environment. They are disabled by default but are strongly encouraged to be enabled. There are also additional elective guardrails to consider which may be suitable for your workload. If you want to add additional service control policies there is an AWS solution Customizations for AWS Control Tower to get started.\nService Control Policies AWS Organizations policies allow you to apply additional controls to accounts. In the examples given below these are attached to the root which will affect all accounts within the organization. You can also create specific service control policies for separate organizational units within your organization.\nWalk through for a non-control tower environment If you are not leveraging Control Tower it is strongly recommended that you implement the below service control policy to prevent AWS CloudTrail from being disabled.\n Navigate to AWS Organization and select the Policies tab Click Create policy Enter a policy name for your policy and paste the policy JSON below into the policy editor Click Create policy Select the policy you have just created and in the right-hand panel select *roots Press Attach to attach the policy to your organizations root  Policy to prevent users disabling CloudTrail Note: AWS Control Tower already includes a mandatory guard rail preventing this\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Deny\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;cloudtrail:StopLogging\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } #\n"},{"uri":"https://wellarchitectedlabs.com/reliability/100_labs/100_deploy_cloudformation/3_explore_webapp/","title":"Explore the Web Application","tags":[],"description":"","content":"  Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation.\n Wait until CloudFormationLab stack status is CREATE_COMPLETE before proceeding. This should take about four minutes Click on the CloudFormationLab stack Click on the Outputs tab For the Key WebsiteURL copy the value. This is the URL of your test web service  Hint: it will start with http://healt-alb and end in \u0026lt;aws region\u0026gt;.elb.amazonaws.com      Click the URL and it will bring up the website:\n Troubleshooting: if you see an error such as 502 Bad Gateway, then wait 60 seconds and try again. It takes some time for the servers to initialize.     The website simulates a recommendation engine making personalized suggestions for classic television shows. You should note the following features:\n Area A shows the personalized recommendation  It shows first name of the user and the show that was recommended The workshop simulation is simple. On every request it chooses a user at random, and shows a recommendation statically mapped to that user. The user names, television show names, and this mapping are in a DynamoDB table, which is simulating the RecommendationService   Area B shows metadata which is useful to you during the lab  The instance_id and availability_zone enable you to see which EC2 server and Availability Zone were used for each request      Use the following architectural diagram as you explore the site  A - There is one EC2 instance deployed per Availability Zone B - Refresh the website several times, note that the EC2 instance and Availability Zone change from among the three available C - Elastic Load Balancing (ELB) is used here. An Application Load Balancer receives each request and distributes it among the available EC2 server instances across Availability Zones.  The requests are stateless, and therefore can be routed to any of the available EC2 instances   D - The EC2 instances are in an Amazon EC2 Auto Scaling Group . This Auto Scaling Group was configured to maintain three instances, therefore if one instance is detected as unhealthy it will be replaced to maintain three healthy instances.  AWS Auto Scaling can also be configured to scale up/down dynamically in response to workload consitions such as CPU utilization or request count.         Well-Architected for Reliability: Best practices     Use highly available network connectivity for your workload public endpoints: These endpoints and the routing to them must be highly available. You used Elastic Load Balancing which provides load balancing across Availability Zones, performs Layer 4 (TCP) or Layer 7 (http/https) routing, integrates with AWS WAF, and integrates with AWS Auto Scaling to help create a self-healing infrastructure and absorb increases in traffic while releasing resources when traffic decreases.   Implement loosely coupled dependencies: Dependencies such as\u0026hellip; load balancers are loosely coupled. Loose coupling helps isolate behavior of a component from other components that depend on it, increasing resiliency and agility.   Deploy the workload to multiple locations: Distribute workload data and resources across multiple Availability Zones or, where necessary, across AWS Regions. These locations can be as diverse as required.   Automate healing on all layers: Upon detection of a failure, use automated capabilities to perform actions to remediate it    You have deployed the cloud infrastructure architecture that can support a high reliability workload\n  This an example architecture of the cloud infrastructure necessary for reliable workloads\n  Addition of dynamic auto scaling would further improve reliability\n  Reliability also depends on software architecture, network configuration, operational excellence, and testing (especially Chaos Engineering which tests resilience), which are outside the scope of this lab.\n Without best practices for all of these, which can be found in the Reliability pillar of the AWS Well-Architected Framework , the workload will not achieve high reliability goals.   function prevStep(){ window.open(\"..\\/2_deploy_webapp\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/4_explore_cloudformation\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step       "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_health_checks_and_dependencies/3_deep_healthcheck/","title":"Implement deep health checks","tags":[],"description":"","content":"3.1 Re-enable the dependency service For the next part of the lab restore access to the getRecommendation API on the RecommendationService\n Return to the AWS Systems Manager \u0026gt; Parameter Store on the AWS Management Console Set the value of RecommendationServiceEnabled back to true and Save changes Confirm the web service is now returning \u0026ldquo;personalized\u0026rdquo; recommendations again  3.2 Inject fault on a single server Previously you simulated a failure of the service dependency. Now you will simulate a failure on a single server (of the three servers running). You will simulate a fault on this server that prevents only it from calling the otherwise healthy service dependency.\n  Navigate to the EC2 Instances console  There should be three EC2 instances with Instance State running, one in each Availability Zone (they will have Name WebApp1) Click the gear icon in the upper-right and select IAM Instance Profile Name (in addition to what is already selected)    Select only the EC2 instance in Availability Zone us-east-2c\n  Click Action \u0026gt; Instance Settings \u0026gt; Attach/Replace IAM Role\n  From IAM role, click WebApp1-EC2-noDDB-Role-HealthCheckLab\n  Click Apply\n  Click Close\n  This will return you to the EC2 Instances console. Observe under IAM Instance Profile Name (it is one of the displayed columns) which IAM roles each EC2 instance has attached\n  The IAM role attached to an EC2 instance determines what permissions it has to access AWS resources. You changed the role of the us-east-2c instance to one that is almost the same as the other two, except it does not have access to DynamoDB. Since DynamoDB is used to mock our service dependency, the us-east-2c server no longer has access to the service dependency (RecommendationService). Stale credentials is an actual fault that servers might experience. Your actions above simulate stale (invalid) credentials on the us-east-2c server.\n3.4 Observe application behavior and determine how to fix it   Observe the website behavior now\n Refresh the website multiple times noting which Availability Zone the serving the request The servers in us-east-2a and us-east-2b continue to function normally The server in us-east-2c still succeeds, but it uses the static response. Why is this?    The service dependency RecommendationServiceEnabled is still healthy\n  It is the server in us-east-2c that is unhealthy - it has stale credentials\n Return to the Target Groups and under the Targets tab observe the results of the ELB health checks They are all Status healthy, and are therefore all receiving traffic. Why does the server in us-east-2c show healthy for this check?    The service would deliver a better experience if it:\n Identified the us-east-2c server as unhealthy and did not route traffic to it Replaced this server with a healthy one     Well-Architected for Reliability: Best practices     Make services stateless where possible: Services should either not require state, or should offload state such that between different client requests, there is no dependence on locally stored data on disk or in memory. This enables servers to be replaced at will without causing an availability impact. Amazon ElastiCache or Amazon DynamoDB are good destinations for offloaded state.   Automate healing on all layers: Upon detection of a failure, use automated capabilities to perform actions to remediate. Ability to restart is an important tool to remediate failures. As discussed previously for distributed systems, a best practice is to make services stateless where possible. This prevents loss of data or availability on restart. In the cloud, you can (and generally should) replace the entire resource (for example, EC2 instance, or Lambda function) as part of the restart. The restart itself is a simple and reliable way to recover from failure. Many different types of failures occur in workloads. Failures can occur in hardware, software, communications, and operations. Rather than constructing novel mechanisms to trap, identify, and correct each of the different types of failures, map many different categories of failures to the same recovery strategy. An instance might fail due to hardware failure, an operating system bug, memory leak, or other causes. Rather than building custom remediation for each situation, treat any of them as an instance failure. Terminate the instance, and allow AWS Auto Scaling to replace it. Later, carry out the analysis on the failed resource out of band.      From the Target Groups console click on the the Health checks tab\n The ELB health check is configured to return healthy when it receives an http 200 response on the /healthcheck path Since the healthcheck code simply always returns http 200, the bad server still returns http 200 and is seen as healthy.    3.4 Create a deep healthcheck to identify bad servers  Update server code to add a deep health check response  You will create and configure a new health check that will include a check on whether the server can access its dependency This is a deep health check \u0026ndash; it checks the actual function of the server including the ability to call service dependencies   This will be implemented by updating the server code on the /healthcheck path  Choose one of the options below (Option 1 - Expert or Option 2 - Assisted) to improve the code and add the deep health check.\n3.4.1 Option 1 - Expert option: make and deploy your changes to the code You may choose this option, or skip to Option 2 - Assisted option\nThis option requires you have access to place a file in a location accessible via https/https via a URL. For example a public readable S3 bucket, gist (use the raw option to get the URL), or your private webserver.\n Start the existing server code that you added error handling to, or alternatively download the lab sample code from here: server_errorhandling.py  Calls to /healthcheck should in turn make a test call to RecommendationService using User ID 0  If the RecommendationService returns the string test for both Result and UserName then it is healthy If it is healthy then return http code 200 (OK) If it is not healthy then return http code 503 (Service Unavailable) Also return the same EC2 meta-data that is returned on the call to the / path   Put your updated server code in a location where it can be downloaded via its URL using wget In the AWS Console go the HealthCheckLab CloudFormation stack and Update it:  Leave Use current template selected and click Next Find the ServerCodeUrl parameter and enter the URL for your new code When stack status is CREATE_COMPLETE (about four minutes) then continue    If you completed the Option 1 - Expert option, then skip the Option 2 - Assisted option section and continue with 3.4.3 Health check code\n3.4.2 Option 2 - Assisted option: deploy workshop provided code  The new server code including error handling can be viewed here  Search for Healthcheck request in the comments. What will this code do now if called on this health check URL?  Deploy the new health check code   Navigate to the AWS CloudFormation console\n  Click on the HealthCheckLab stack\n  Click Update\n  Leave Use current template selected and click Next\n  Find the ServerCodeUrl parameter and enter the following:\n https://aws-well-architected-labs-ohio.s3.us-east-2.amazonaws.com/Healthcheck/Code/server_healthcheck.py    Click Next until the last page\n  At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names\n  Click Update stack\n  Click on Events, and click the refresh icon to observe the stack progress\n New EC2 instances running the error handling code are being deployed When stack status is CREATE_COMPLETE (about four minutes) then continue    3.4.3 Health check code This is the health check code from server_healthcheck.py . The Option 2 - Assisted option uses this code. If you used the Option 1 - Expert option, you can consult this code as a guide.\nCode:   Click here to see the code:   # Healthcheck request - will be used by the Elastic Load Balancer elif self.path == \u0026#39;/healthcheck\u0026#39;: is_healthy = False error_msg = \u0026#39;\u0026#39; TEST = \u0026#39;test\u0026#39; # Make a request to RecommendationService using a predefined # test call as part of health assessment for this server try: # call RecommendationService using the test user user_id = str(0) response = call_getRecommendation(self.region, user_id) # Parses value of recommendation from DynamoDB JSON return value tv_show = response[\u0026#39;Item\u0026#39;][\u0026#39;Result\u0026#39;][\u0026#39;S\u0026#39;] user_name = response[\u0026#39;Item\u0026#39;][\u0026#39;UserName\u0026#39;][\u0026#39;S\u0026#39;] # Server is healthy of RecommendationService returned the expected response is_healthy = (tv_show == TEST) and (user_name == TEST) # If the service dependency fails, capture diagnostic info except Exception as e: error_msg += str(traceback.format_exception_only(e.__class__, e)) # Based on the health assessment # If it succeeded return a healthy code # If it failed return a server failure code message = \u0026#34;\u0026#34; if (is_healthy): self.send_response(200) self.send_header(\u0026#39;Content-type\u0026#39;, \u0026#39;text/html\u0026#39;) self.end_headers() message += \u0026#34;\u0026lt;h1\u0026gt;Success\u0026lt;/h1\u0026gt;\u0026#34; # Add metadata message += get_metadata() else: self.send_response(503) self.send_header(\u0026#39;Content-type\u0026#39;, \u0026#39;text/html\u0026#39;) self.end_headers() message += \u0026#34;\u0026lt;h1\u0026gt;Fail\u0026lt;/h1\u0026gt;\u0026#34; message += \u0026#34;\u0026lt;h3\u0026gt;Error message:\u0026lt;/h3\u0026gt;\u0026#34; message += error_msg # Add metadata message += get_metadata() self.wfile.write( bytes( html.format(Title=\u0026#34;healthcheck\u0026#34;, Content=message), \u0026#34;utf-8\u0026#34; ) )   \n3.4.4 Verify Elastic Load Balancer (ELB) is configured to use the new deep health check  From the Target Groups console click on the the Health checks tab For Path verify the value is /healthcheck Click the Targets tab so you can monitor health check status  3.4.5 Observe behavior of web service with added deep health check  Continue the lab after the HealthCheckLab CloudFormation stack is complete.  The CloudFormation stack update reset the EC2 instance IAM roles, so the system is back to its original no-fault state. You will re-introduce the single-server fault and observe the new behavior.\n  Refresh the web service multiple times and note all three servers are functioning without error\n  Copy the URL of the web service to a new tab and append /healthcheck to the end of the URL\n  The new URL should look like:\nhttp://healt-alb1l-\u0026lt;...\u0026gt;.elb.amazonaws.com/healthcheck    Refresh several times and observe the health check on the three servers\n  Note the check is successful - the check now includes a call to the RecommendationService (the DynamoDB table)\n  Go to the Target Groups console click on the Targets tab and note the health status as per the ELB health checks.\n    To re-introduce the stale credentials fault, again change the IAM role for the EC2 instance in us-east-2c to WebApp1-EC2-noDDB-Role-HealthCheckLab\n See 3.2 Inject fault on one of the servers if you need a reminder of how to do this.    Go to the Target Groups console click on the Targets tab and note the health status as per the ELB health checks (remember to refresh)\n  Note that the server in us-east-2c is now failing the health check with a http code 503 Service Not Available\n With an Interval of 15 seconds, and a Healthy threshold of 2, it can take up to 30 seconds to see the status update.    The ELB has identified the us-east-2c server as unhealthy and will not route traffic to it\n  This is known as fail-closed behavior\n    Refresh the web service multiple times and note it is however still functioning without error\n And unlike before it is no longer returning a static response - it only returns personalized recommendations Note that only the servers in us-east-2a and us-east-2b are serving requests     Well-Architected for Reliability: Best practices     Monitor all components of the workload to detect failures: Continuously monitor the health of your workload so that you and your automated systems are aware of degradation or complete failure as soon as they occur.   Failover to healthy resources: Ensure that if a resource failure occurs, that healthy resources can continue to serve requests.       Well-Architected for Reliability: Health Checks     The load balancer will only route traffic to healthy application instances. The health check needs to be at the data plane/application layer indicating the capability of the application on the instance. This check should not be against the control plane. A health check URL for the web application will be present and configured for use by the load balancer      Repair the server  Navigate to the EC2 Instances console and select only the instance in us-east-2c Click Action \u0026gt; Instance State \u0026gt; Terminate Click Yes, Terminate  The EC2 instance will shut down Amazon EC2 Auto Scaling will recognize there are less then the three Desired Capacity and will start up a new EC2 instance The new instance replaces the one with the stale credentials fault, and loads fresh credentials   From the Target Groups console Targets tab note the health check status of the new server in us-east-2c  The new instance in us-east-2c will first show Description Target registration is in progress Then Description is This target is currently passing target group\u0026rsquo;s health checks, then you may continue the workshop (The Description may show Health checks failed with these codes: [502], before getting to a healthy state. This is expected as the server initializes) From the time you terminate the EC2 instance, it will take four to five minutes to get the new EC2 instance up and in a healthy state   Refresh the web service multiple times and note that personalized recommendations are once again being served from all three servers   function prevStep(){ window.open(\"..\\/2_handle_dependency\", \"_self\") } function nextStep(){ window.open(\"..\\/4_fail_open\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/100_labs/100_walkthrough_of_the_well-architected_tool/3_perform_review/","title":"Performing a review","tags":[],"description":"","content":"  From the detail page for the workload, click the Start reviewing button, then select the AWS Well-Architected Framework to review:   In this walkthrough, we are only going to complete the Reliability Pillar questions. Collapse the Operational Excellence questions by selecting the collapsing icon on the left of the words Operation Excellence on the left:   Expand the Reliability Questions by selecting the expanding icon to the left of the word Reliability:   Select the first question: REL 1. How do you manage service quotas and constraints?   Answer the REL 1 to REL 13 questions as you understand your current ability. You can use the Info links to help you understand what the answers mean and to get more context on the questions.   As you complete the question, select the Next Button at the bottom of the answers:   When you get to the last Reliability question, or the first Performance Pillar question, select Save and Exit:    function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/4_save_milestone\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/3_failure_injection_prep/","title":"Preparation for Failure Injection","tags":[],"description":"","content":"Failure injection (also known as chaos testing) is an effective and essential method to validate and understand the resiliency of your workload and is a recommended practice of the AWS Well-Architected Reliability Pillar . Here you will initiate various failure scenarios and assess how your system reacts.\nPreparation Before testing, please prepare the following:\n  Region must be Ohio\n  We will be using the AWS Console to assess the impact of our testing\n  Throughout this lab, make sure you are in the Ohio region\n    Get VPC ID\n A VPC (Amazon Virtual Private Cloud) is a logically isolated section of the AWS Cloud where you have deployed the resources for your service For these tests you will need to know the VPC ID of the VPC you created as part of deploying the service Navigate to the VPC management console: https://console.aws.amazon.com/vpc In the left pane, click Your VPCs 1 - Tick the checkbox next to ResiliencyVPC 2 - Copy the VPC ID   Save the VPC ID - you will use later whenever \u0026lt;vpc-id\u0026gt; is indicated in a command    Get familiar with the service website\n Point a web browser at the URL you saved from earlier. (If you do not recall this, then see these instructions ) Note the availability_zone and instance_id Refresh the website several times watching these values Note the values change. You have deployed one web server per each of three Availability Zones.  The AWS Elastic Load Balancer (ELB) sends your request to any of these three healthy instances. Refer to the diagram at the start of this Lab Guide to review your deployed system architecture.            Availability Zones (AZs) are isolated sets of resources within a region, each with redundant power, networking, and connectivity, housed in separate facilities. Each Availability Zone is isolated, but the Availability Zones in a Region are connected through low-latency links. AWS provides you with the flexibility to place instances and store data across multiple Availability Zones within each AWS Region for high resiliency.   Learn more: After the lab see this whitepaper on regions and availability zones       function prevStep(){ window.open(\"..\\/2_configure_env\", \"_self\") } function nextStep(){ window.open(\"..\\/4_failure_injection_ec2\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_pricing_model_analysis/3_quicksight_setup/","title":"Setup QuickSight Dashboard","tags":[],"description":"","content":"We will now setup the QuickSight dashboard to visualize your usage by cost, and setup the analysis to provide Savings Plan recommendations.\n  Go to the QuickSight service:   Click on Manage data:   Click on New data set:   Click Athena:   Enter a Data source name of SP_Usage and click Create data source:   Select the costmaster database, and the sp_usage table, click Select:   Ensure SPICE is selected, click Visualize:   Click on QuickSight to go to the home page:   Click on Manage data:   Select the sp_usage Dataset:   Click Schedule refresh:   Click Create:   Enter a schedule, it needs to be refreshed daily, and click Create:   Click Cancel to exit:   Click the x in the top corner:   You now have your data set setup ready to create a visualization.\n Advanced Setup This section is optional and replaces the next two steps by creating the dashboard from a template. You will require access and knowledge of the AWS CLI, and Enterprise edition of QuickSight. If you do not have the access, go to the next step and manually create the dashboard as per the lab.\n  Go to this web page to request access to the template. Enter you AWS AccountID and click Submit: Template Access   Edit the following command, replacing AccountID and region, then using the CLI list the QuickSight datasets and copy the Arn for the sp_usage dataset:\n aws quicksight list-data-sets --aws-account-id (AccountID) --region (region)     Edit the following command, replacing AccountID and region, then using the CLI list your QuickSight users ARNs:\n aws quicksight list-users --aws-account-id (AccountID) --namespace default --region (region)     Create a local file create-dashboard.json with the text below, replace the values (Account ID), (User ARN), (Dataset ARN):\n { \u0026quot;AwsAccountId\u0026quot;: \u0026quot;(Account ID)\u0026quot;, \u0026quot;DashboardId\u0026quot;: \u0026quot;SP_usage_analysis\u0026quot;, \u0026quot;Name\u0026quot;: \u0026quot;sp_usage analysis\u0026quot;, \u0026quot;Permissions\u0026quot;: [ { \u0026quot;Principal\u0026quot;: \u0026quot;(User ARN)\u0026quot;, \u0026quot;Actions\u0026quot;: [ \u0026quot;quicksight:DescribeDashboard\u0026quot;, \u0026quot;quicksight:ListDashboardVersions\u0026quot;, \u0026quot;quicksight:UpdateDashboardPermissions\u0026quot;, \u0026quot;quicksight:QueryDashboard\u0026quot;, \u0026quot;quicksight:UpdateDashboard\u0026quot;, \u0026quot;quicksight:DeleteDashboard\u0026quot;, \u0026quot;quicksight:DescribeDashboardPermissions\u0026quot;, \u0026quot;quicksight:UpdateDashboardPublishedVersion\u0026quot; ] } ], \u0026quot;SourceEntity\u0026quot;: { \u0026quot;SourceTemplate\u0026quot;: { \u0026quot;DataSetReferences\u0026quot;: [ { \u0026quot;DataSetPlaceholder\u0026quot;: \u0026quot;sp_usage\u0026quot;, \u0026quot;DataSetArn\u0026quot;: \u0026quot;(Dataset ARN)\u0026quot; } ], \u0026quot;Arn\u0026quot;: \u0026quot;arn:aws:quicksight:us-east-1:869004330191:template/SP-Analysis-template\u0026quot; } }, \u0026quot;VersionDescription\u0026quot;: \u0026quot;1\u0026quot; }    Edit the following command, replacing (region), Run the command to create the dashboard and you should get a 200 response:\n aws quicksight create-dashboard --cli-input-json file://create-dashboard.json --region (region)     After a few minutes the dashboard will become available in QuickSight, click on the Dashboard name:   Click Share, click Share dashboard,   Add the required users, or share with all users, ensure you check Save as for each user:   Click Save as:   Enter an Analysis name and click Create:   You will now have the analysis created automatically from the template:   You have successfully created the analysis from a template, this lab is complete.\n  function prevStep(){ window.open(\"..\\/2_usage_source\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/4_recommendation_dashboard\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_5_cost_visualization/3_share_analysis/","title":"Share your Analysis and Dashboard","tags":[],"description":"","content":"Now that your QuickSight Analysis is complete, it is time to share the Analysis or publish a Dashboard. An Analysis is a read and write copy of the Visuals and Data Set that you created. A dashboard is a read-only version, allowing the user to apply filters but not make any changes to the Visuals or Data Set.\nShare an analysis   To share an analysis, click on Share on the top right, then select Share analysis:   Share with Authors and Admins in your QuickSight account by searching by email address. Once you have added all the users, click Share:   The users will then receive an email similar to the one below. When they click on Click to View theyll be taken straight to the analysis, and they will have full access to modify the analysis as we have been doing in this workshop.:   Publish a dashboard   To publish a dashboard click on Share in the upper right, and select Publish dashboard:   Enter a name for the dashboard, and click Publish dashboard:   Share with users in your QuickSight account by searching by email address. Once you have added all the users, select their permission levels and click Share. For the permissions, Viewer: can view, filter and sort the dashboard data, they can also use controls. Co-owner: can edit and share the dashboard.   Click the x button in the top right to close the Manage dashboard sharing dialog:   You will then have the dashboard on screen:   All users will receive an email:    function prevStep(){ window.open(\"..\\/2_create_visualizations\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/4_tear_down\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/200_labs/200_automating_operations_with_playbooks_and_runbooks/3_simulate_application_issue/","title":"Simulate Application Issue","tags":[],"description":"","content":"In this section we will simulate a performance issue to trigger an alarm notifying our the operations team about the founded issue. In our scenario today we will assume a hypothetical API response SLA of under 6 seconds, if API response takes longer than 6 seconds broader user experience is severely impacted, and our Operations team needs to response to this.\nTo detect this response time, we have deployed a canary monitoring using CloudWatch Synthetics to continuously check the application response time and trigger a CloudWatch alarm when response time is gone above the SLA.\nThe resources needed for this section is already deployed along with the cloudformation template of your application in previous section. to proceed with this section, please ensure you have completed Section 2 of this lab.\n3.1 Sending traffic to Application In this part of the lab we will be sending multiple concurrent requests to the application to simulate large incoming traffic. This is intended to overwhelm the service, and as time goes, response time to the application will become slower thus, setting of the CloudWatch Alarm that will notify our Operations team via email.\nFrom the cloud9 do the following\nConfirm that you have the test.json in your working folder that contains this text.\n{\u0026quot;Name\u0026quot;:\u0026quot;Test User\u0026quot;,\u0026quot;Text\u0026quot;:\u0026quot;This Message is a Test!\u0026quot;} Execute below command.\nALBURL=\u0026quot;\u0026lt; Application Endpoint URL captured from section 2\u0026gt;\u0026quot; ab -p test.json -T application/json -c 3000 -n 60000000 -v 4 http://$ALBURL/encrypt This command uses Apache Benchmark to send 60000000 requests at 3000 concurrently.\nOnce you executed this command, you will see the command output gradually from successful 200 response\nTo several 504 time out response, this is intended, as we are essentially flooding the API. The requests we are generating is overwhelming our application triggering the occasional Timeout generated by our Load Balancer.\nKeep the command running in the background and proceed to the next step.\n3.2 Observing the alarm being triggered. After approximately 1-3 minutes you should now see an alarm being triggered. You can check this by going to the Cloudwatch console, and click on the Alarms section on the left menu. Click on the Alarms called mysecretword-canary-duation-alarm that should currently be in an Alarm State.\nClicking on the Alarm will display the CloudWatch metrics that the alarm is triggering from. Which is the Duration metric emitted by CloudWatchSynthetics with CanaryName mysecretword-canary This metric essentially measures how it takes for the canary requests until a response is received by the application. The alarm is triggered whenever the value of the Duration metric is above 6 seconds within 1 minute duration.\n3.3 Observing the Canary. Working backwards from the alarm, let\u0026rsquo;s now take a look at the Canary monitoring the API emitting those metrics.\nMonitoring application health, can be done from multiple angles, system / applications logs, metrics operating system, and services the application is running on. None of these however will be able to replicate accurately the a point of view from the consumer of your application.\nThis is why in this scenario a canary is implemented to regularly call and check the health of the API from it\u0026rsquo;s external endpoint using CloudWatch synthetics To look at the Canary configuration, go to CloudWatch Console, and click on Synthetics and Canaries on the left menu. Locate and click the Canary called mysecret-canary\nUnder Configuration tab you can see the Canary configuration a snippet of the Canary Script. scrolling down to the requestOptionStep1 variable of the script, you can see that the script is calling the /encrypt action of the API via the load balancer endpoint, passing a dummy payload.\nAnd under Monitoring tab you will be able to see a visualization of the Duration metric that is being used to trigger our alarm.\nNow that we have successfully triggered the Alarm, you should also see that an email arriving in your mailbox notifying about the Alarm.\nThis concludes Section 3 of this lab. With this section concluded we have simulated a large incoming traffic to our application API overwhelming the application, causing the API to response slowly and occasionally timed out.\nIn a real operational environment, this scenario would trigger an incident notification, alerting the operations team to take action.\nSo now,let\u0026rsquo;s proceed to the next Section of this lab, where we will be building the Automated playbook to investigate issue in the application.\n function prevStep(){ window.open(\"..\\/2_configure_ecs_repository_and_deploy_application_stack\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/4_build_execute_investigative_playbook\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step      END OF SECTION 3\n "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_3_pricing_models/3_filter_csv/","title":"Sort and filter the RI CSV files","tags":[],"description":"","content":"RI purchases should be done frequently (bi-weekly or monthly), so for each cycle we want: low risk and high return purchases, and purchase the top 50-75% of those high quality recommendations. This will ensure you have sufficiently high coverage, while minimizing the risk of unused RIs.\nFilter out low risk, and high return RIs   To get the lowest risk, we sort by Fully Paid Day smallest to largest, as these will be fully paid off in the shortest amount of time. You can see that some of the RI\u0026rsquo;s below are fully paid off in around 7months, so if they are used for 7 months - they have paid themselves off completely.   We will separate the very low, low, and medium risk recommendations. Add in some empty lines before a Fully Paid Day of 8, and a fully paid day of 10, also copy the header line across:   We have categorized the risk, so we will now look for the highest return recommendations in each category. Sort each of the three groups by Estimated Monthly Savings, largest to smallest:   Depending on your usage and business, chose a minimum estimated monthly savings - a typical value for customers is in the range of $50-100. A recommendation with a saving lower than this may not save enough. Aim for the top 50-70% of recommendations. We have chosen $100, in each of the three groups grey out anything less than this:   Filter out usage patterns It would be a large amount of effort to view the daily usage patterns over the month for every recommendation - checking for declining usage or erratic usage, but we can do this programatically. By looking at the columns, we can assess the underlying usage pattern.\n  If the Max hourly usage is close to Min hourly usage, within 75-100% - then the usage would be relatively flat, with low variance. Go through and highlight these cells green. You could do this with a formula, but a very fast judgment is ok:   If the Average hourly usage is close to the Max hourly usage, then the minimum was only a small duration, so highlight anything green where the Average is roughly within 75-100% of the Max:   Now we look for a declining usage pattern. If the recommendation for the last 7 days is less than the 30 days, usage is declining - and you should consult your business to determine if usage will continue to fall. If the 7day Recommended Instance Quantity is equal or more than the 30day Recommended Instance Quantity then highlight the cell green:   Now we will see if the recommendation is close to the average, if its not then usage is varying. If the recommendation is NOT above, equal or close to the average (within 10%) then remove the highlighting from the recommendation column:   The processed sample files are available here:\n Combined_EC2_RI_Rec.xls   Making recommendations We now go through the spreadsheet and apply business rules to make the best low risk \u0026amp; high return purchases that are right for the business. We look at each of the risk categories as follows:\n Low risk and very low risk - this is the first group of recommendations (fully paid below 8)   For any recommendations that are highlighted in the 7Day column, recommend the lowest of the 30Day or 7Day Columns. For any recommendations that were not highlighted in the 7Day column but are highlighted in the Average hourly usage, select a percentage of either the 30Day or 7Day column (which ever is lower).  Medium risk - this is the second group of recommendations (fully paid below 10)   From the recommendations highlighted in the 7Day column, select a portion of these on a case by case basis based on business knowledge  Other suggestions for recommendations that do not fall into the categories above, and are not greyed out:\n Re-evaluate in another 7-14 days to observe the usage trend Purchase a lower percentage of the average hourly Purchase a higher percentage of the minimum hourly  You have successfully processed all the recommendations. You now have the right low risk and high return recommendations, based on your usage patterns. Take the recommendations, and purchase the quantity required in the correct accounts.\n  function prevStep(){ window.open(\"..\\/2_prepare_csv\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/4_tear_down\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_remote_configuration_installation_and_viewing_cloudwatch_logs/3_create_cw_config/","title":"Store the CloudWatch Config File in Parameter Store","tags":[],"description":"","content":"3. Create the CloudWatch Config File with Parameter Store You will use Parameter Store, a tool in Systems Manager, to store the CloudWatch agent configuration. Parameter store allows you to securely store configuration data and secrets for reusability. You can re-use configuration data that is well controlled and consistent. In this case, you need to store the configuration file for CloudWatch Agent on your EC2 instance. The CloudWatch agent configuration data specifies which logs and metrics will be sent to CloudWatch as well as the source of this data.\n Open the Systems Manager console . Choose Parameter Store from the left side menu under Application Management. Choose Create parameter from that screen.  Enter the parameter name AmazonCloudWatch-securitylab-cw-config. You may use a different name, but it must begin with AmazonCloudWatch``- in order to be recognized by CloudWatch as a valid configuration file. Give your parameter a description, such as This is a CloudWatch Agent config file for use in the Well Architected security lab. Set Tier to Standard. Set Type to String. Set Data type to text. In the Value field, copy and paste the contents of the config.json file found in the lab assets. This config file specifies which metrics and logs to collect.  The agent section specifies which user to run the logs agent as, and how frequently to collect logs. The logs section specifies which log files to monitor and which log group and stream to place those logs in. This information can be seen in collect_list. For this lab, you are collecting SSH logs, Apache Web Server logs, and logs for the CloudWatch Agent itself. We will examine these logs more closely in a later step The metrics section specifies which metrics are collected (in metrics_collected), the frequency of collection, measurement, and other details. To learn more about creating config files, see this link .   Click Create parameter.  Recap: In this portion of the lab, you created a re-usable, centrally stored configuration file stored in Amazon Systems Manager Parameter Store. You can re-use configuration data stored in Parameter Store while ensuring that it is consistent and correct across uses. This becomes especially helpful when scaling, as you can re-use configuration files across fleets of instances. This highlights the Well-Architected best practice of configuring services and resources centrally by maintaining configuration files centrally in Parameter Store.\n function prevStep(){ window.open(\"..\\/2_install_cw_agent\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/4_start_cw_agent\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_automated_deployment_of_detective_controls/3_cleanup/","title":"Tear down","tags":[],"description":"","content":"The following instructions will remove the resources that have a cost for running them.\nNote: If you are planning on doing the lab 300_Incident_Response_with_AWS_Console_and_CLI we recommend you only tear down this stack after completing that lab as their is a dependency on AWS CloudTrail being enabled for the other lab.\nDelete the stack:\n Sign in to the AWS Management Console, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Select the DetectiveControls stack. Click the Actions button then click Delete Stack. Confirm the stack and then click the Yes, Delete button.  Empty and delete the S3 buckets:\n Sign in to the AWS Management Console, and open the S3 console at https://console.aws.amazon.com/s3/. Select the CloudTrail bucket name you previously created without clicking the name.  Click Empty bucket and enter the bucket name in the confirmation box.  Click Confirm and the bucket will be emptied when the bottom task bar has 0 operations in progress.  With the bucket now empty, click Delete bucket.  Enter the bucket name in the confirmation box and click Confirm.  Repeat steps 2 to 6 for the Config bucket you created.   References \u0026amp; useful resources AWS CloudTrail User Guide AWS CloudFormation User Guide Amazon GuardDuty User Guide AWS Config User Guide "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_automated_deployment_of_iam_groups_and_roles/3_cleanup/","title":"Tear down","tags":[],"description":"","content":"The following instructions will remove the resources that have a cost for running them. Please note that the changes you made to the root login, users, groups, and policies have no charges associated with them.\nDelete the IAM stack:\n Sign in to the AWS Management Console, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Select the baseline-iam stack. Click the Actions button then click Delete Stack. Confirm the stack and then click the Yes, Delete button.   References \u0026amp; useful resources AWS Identity and Access Management User Guide IAM Best Practices and Use Cases AWS CloudFormation User Guide "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_automated_deployment_of_web_application_firewall/3_cleanup/","title":"Tear down","tags":[],"description":"","content":"The following instructions will remove the resources that have a cost for running them. Please note that Security Groups and SSH key will exist. You may remove these also or leave for future use.\nDelete the CloudFront distribution:\n Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. From the console dashboard, select the distribution you created earlier and click the Disable button. To confirm, click the Yes, Disable button. After approximately 15 minutes when the status is Deployed, select the distribution and click the Delete button, and then to confirm click the Yes, Delete button.  Delete the AWS WAF stack:\n Sign in to the AWS Management Console, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Select the waf-cloudfront stack. Click the Actions button, and then click Delete Stack. Confirm the stack, and then click the Yes, Delete button.   References \u0026amp; useful resources Amazon Elastic Compute Cloud User Guide for Linux Instances Amazon CloudFront Developer Guide Tutorial: Configure Apache Web Server on Amazon Linux 2 to Use SSL/TLS AWS WAF, AWS Firewall Manager, and AWS Shield Advanced Developer Guide "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_automated_cur_updates_and_ingestion/3_cleanup/","title":"Teardown","tags":[],"description":"","content":"  Delete the Glue database, select the database name, click Action and click Delete database:   Delete the CloudFormation stack, select the stack, click Actions and click Delete stack:    function prevStep(){ window.open(\"..\\/2_multiple_curs\\/\", \"_self\") } function nextStep(){ window.open(\"\\/cost\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with COST 3 - \u0026ldquo;How do you monitor usage and cost?\u0026quot;\nClick here to access the Well-Architected Tool   Previous Step Complete this lab     "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_bidirectional_replication_for_s3/3_test_replication/","title":"Test bi-directional cross-region replication (CRR)","tags":[],"description":"","content":"To test bi-directional replication using the two rules your created, you will upload another object into each of the east and west S3 buckets and observe it is replicated across to the other bucket. For this step you will need two more test objects:\n These are files that you will upload into each S3 bucket. They should not be too big, as this will increase the time to upload it from your computer. If you do not have files to use, you can download file #1 and download file #2      File #1 File #2          3.1 Upload objects to their respective Amazon S3 buckets 3.1.1 Upload object #1 to the east S3 bucket  Go to the Amazon S3 console , or if you are already there click on Amazon S3 in the upper left corner Click on the name of the east bucket  if you used Ohio the name will be \u0026lt;your_naming_prefix\u0026gt;-crrlab-us-east-2   Click on  Upload Upload the file you will use as object #1  Drag and drop the file or click Add files Click Upload (note there is a Next button, but you do not need to click it)    3.1.2 Upload object #2 to the west S3 bucket  Click on Amazon S3 in the upper left corner of the Amazon S3 console Click on the name of the west bucket  if you used Oregon the name will be \u0026lt;your_naming_prefix\u0026gt;-crrlab-us-west-2   Click on  Upload Upload the file you will use as object #2  Drag and drop the file or click Add files Click Upload (note there is a Next button, but you do not need to click it)    3.2 Verify bi-directional replication  You are already looking at the objects in the west bucket  Verify that object #1, that you uploaded to the east bucket is present here also Note the Replication status is REPLICA   Click on Amazon S3 in the upper left corner Click on the name of the east bucket  Verify that object #2, that you uploaded to the west bucket is present here also Note the Replication status is REPLICA    3.3 Explore which Amazon S3 events trigger replication and which do not 3.3.1 Use CloudWatch Logs Insights to query the CloudTrail logs AWS CloudTrail is a service that provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services. You will use AWS CloudTrail to explore which Amazon S3 events trigger replication to occur.\n  Change back to the east AWS region\n If you used the directions in this lab, then this is Ohio (us-east-2)    The CloudFormation template you deployed configured CloudTrail to deliver a trail to CloudWatch Logs. Therefore:\n Go to the CloudWatch console  Click on Insights (under Logs) on the left    Where it says Select log group(s) select the one named CloudTrail/logs/\u0026lt;your_prefix_name\u0026gt;\n  Right below that is where you can enter a query\n  Delete the query that is there\n  and enter the following query. It returns all PutObject requests on S3 buckets\n fields @timestamp, requestParameters.key AS key, | requestParameters.bucketName AS bucket, | userIdentity.invokedBy AS invokedBy, | userIdentity.arn AS arn, | userIdentity.sessionContext.sessionIssuer.userName AS UserName | filter eventName ='PutObject' | sort @timestamp desc | limit 20      Click Run query\n  Look at the results at the bottom of the screen\n  3.3.2 Difference between uploaded and replicated objects in S3 bucket You are looking for three results, one for each of the test objects you uploaded. Use the key field to see the test object names.\n  Troubleshooting: If your query returned less or more than three results then consult this guide to tuning your Insights query   For these events look at the tabular attributes returned by the query at the bottom of the page\n However, if you want to see all the attributes, you can click to the left of each event    The three events correspond to each of the objects you put into the S3 buckets\n The object you put into the east bucket testing rule #1 The object you put into the east bucket testing bi-directional replication The object you put into the west bucket testing bi-directional replication  Look at the bucket for this event. This event is for the east bucket This is actually the replication event for the object you put into the west bucket        What is different between events where you uploaded the object into the bucket and events where the object was put into the bucket by replication?   Replicated objects have a userIdentity.invokedBy value of \u0026ldquo;AWS Internal\u0026rdquo;\nThe userIdentity is different - see the arn and username\n    The CloudWatch Logs Insights page should look like this:\nThe result is:\n For an object uploaded by you  Amazon S3 triggers the rule you configured to replicate it to another bucket And sets Replication status to COMPLETED   For an object replicated from another bucket  Amazon S3 knows not to re-replicate the object And sets Replication status to REPLICA    3.4 Additional exercises These are optional. They help you to explore and understand bi-direction cross-region replication on Amazon S3.\n  Look at the Permissions on the \u0026lt;your-naming-prefix\u0026gt;-S3-Replication-Role-\u0026hellip; IAM Roles\n Why do they have the policies that they do?    What happens when you rename an object in one of the buckets?\n Hint: if you cannot figure it out consider that versioning is enabled (and must be enabled for replication to work)    Switch to the west AWS region and run the same CloudWatch Insights Query there.\n What do you expect?    3.5 Summary You created two S3 buckets in two different AWS regions. You then setup bi-directional cross-region replication (CRR) between the two Amazon S3 buckets. Putting an object in either bucket resulted in the object asynchronously being backed up to the other bucket. Objects encrypted in their original bucket are also encrypted in their replication bucket. Objects are replicated once \u0026ndash; replication \u0026ldquo;looping\u0026rdquo; is prevented.\n function prevStep(){ window.open(\"..\\/2_configure_replication\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/4_cleanup\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_testing_for_resiliency_of_ec2/3_failure_injection/","title":"Test Resiliency Using Failure Injection","tags":[],"description":"","content":"Failure injection (also known as chaos testing) is an effective and essential method to validate and understand the resiliency of your workload and is a recommended practice of the AWS Well-Architected Reliability Pillar . Here you will initiate various failure scenarios and assess how your system reacts.\nPreparation Before testing, please prepare the following:\n  Region must be the one you selected when you deployed your WebApp\n  We will be using the AWS Console to assess the impact of our testing\n  Throughout this lab, make sure you are in the correct region. For example the following screen shot shows the desired region assuming your WebApp was deployed to Ohio region\n    Get VPC ID\n A VPC (Amazon Virtual Private Cloud) is a logically isolated section of the AWS Cloud where you have deployed the resources for your service For these tests you will need to know the VPC ID of the VPC you created as part of deploying the service Navigate to the VPC management console: https://console.aws.amazon.com/vpc In the left pane, click Your VPCs 1 - Tick the checkbox next to WebApp1-VPC 2 - Copy the VPC ID   Save the VPC ID - you will use later whenever \u0026lt;vpc-id\u0026gt; is indicated in a command    Get familiar with the service website\n Point a web browser at the URL you saved from earlier  If you do not recall this, then in the WebApp1-Static stack click the Outputs tab, and open the WebsiteURL value in your web browser, this is how to access what you just created)   Note the instance_id (begins with i-) - this is the EC2 instance serving this request Refresh the website several times watching these values Note the values change. You have deployed two web servers per each of three Availability Zones.  The AWS Elastic Load Balancer (ELB) sends your request to any of these three healthy instances.      3.1 EC2 failure injection This failure injection will simulate a critical problem with one of the three web servers used by your service.\n  Navigate to the EC2 console at http://console.aws.amazon.com/ec2 and click Instances in the left pane.\n  There are three EC2 instances with a name beginning with WebApp1. For these EC2 instances note:\n Each has a unique Instance ID There is two instances per each Availability Zone All instances are healthy    Open up two more console in separate tabs/windows. From the left pane, open Target Groups and Auto Scaling Groups in separate tabs. You now have three console views open\n  To fail one of the EC2 instances, use the VPC ID as the command line argument replacing \u0026lt;vpc-id\u0026gt; in one (and only one) of the scripts/programs below. (choose the language that you setup your environment for)\n   Language Command     Bash ./fail_instance.sh \u0026lt;vpc-id\u0026gt;   Python python fail_instance.py \u0026lt;vpc-id\u0026gt;   Java java -jar app-resiliency-1.0.jar EC2 \u0026lt;vpc-id\u0026gt;   C# .\\AppResiliency EC2 \u0026lt;vpc-id\u0026gt;   PowerShell .\\fail_instance.ps1 \u0026lt;vpc-id\u0026gt;      The specific output will vary based on the command used, but will include a reference to the ID of the EC2 instance and an indicator of success. Here is the output for the Bash command. Note the CurrentState is shutting-down\n $ ./fail_instance.sh vpc-04f8541d10ed81c80 Terminating i-0710435abc631eab3 { \u0026quot;TerminatingInstances\u0026quot;: [ { \u0026quot;CurrentState\u0026quot;: { \u0026quot;Code\u0026quot;: 32, \u0026quot;Name\u0026quot;: \u0026quot;shutting-down\u0026quot; }, \u0026quot;InstanceId\u0026quot;: \u0026quot;i-0710435abc631eab3\u0026quot;, \u0026quot;PreviousState\u0026quot;: { \u0026quot;Code\u0026quot;: 16, \u0026quot;Name\u0026quot;: \u0026quot;running\u0026quot; } } ] }    Go to the EC2 Instances console which you already have open (or click here to open a new one )\n  Refresh it. (Note: it is usually more efficient to use the refresh button in the console, than to refresh the browser)   Observe the status of the instance reported by the script. In the screen cap below it is shutting down as reported by the script and will ultimately transition to terminated.\n    3.2 System response to EC2 instance failure Watch how the service responds. Note how AWS systems help maintain service availability. Test if there is any non-availability, and if so then how long.\n3.2.1 System availability Refresh the service website several times. Note the following:\n Website remains available The remaining two EC2 instances are handling all the requests (as per the displayed instance_id)  3.2.2 Load balancing Load balancing ensures service requests are not routed to unhealthy resources, such as the failed EC2 instance.\n  Go to the Target Groups console you already have open (or click here to open a new one )\n If there is more than one target group, select the one with whose name begins with WebAp    Click on the Targets tab and observe:\n  Status of the instances in the group. The load balancer will only send traffic to healthy instances.\n  When the auto scaling launches a new instance, it is automatically added to the load balancer target group.\n  In the screen cap below the unhealthy instance is the newly added one. The load balancer will not send traffic to it until it is completed initializing. It will ultimately transition to healthy and then start receiving traffic.\n  Note the new instance was started in the same Availability Zone as the failed one. Amazon EC2 Auto Scaling automatically maintains balance across all of the Availability Zones that you specify.\n    From the same console, now click on the Monitoring tab and view metrics such as Unhealthy hosts and Healthy hosts\n  3.2.3 Auto scaling Autos scaling ensures we have the capacity necessary to meet customer demand. The auto scaling for this service is a simple configuration that ensures at least three EC2 instances are running. More complex configurations in response to CPU or network load are also possible using AWS.\n  Go to the Auto Scaling Groups console you already have open (or click here to open a new one )\n If there is more than one auto scaling group, select the one with the name that starts with WebApp1    Click on the Activity History tab and observe:\n  The screen cap below shows that instances were successfully started at 17:25\n  At 19:29 the instance targeted by the script was put in draining state and a new instance ending in \u0026hellip;62640 was started, but was still initializing. The new instance will ultimately transition to Successful status\n    Draining allows existing, in-flight requests made to an instance to complete, but it will not send any new requests to the instance. Learn more: After the lab see this blog post for more information on draining.\nLearn more: After the lab see Auto Scaling Groups to learn more how auto scaling groups are setup and how they distribute instances, and Dynamic Scaling for Amazon EC2 Auto Scaling for more details on setting up auto scaling that responds to demand\n3.2.4 EC2 failure injection - conclusion Deploying multiple servers and Elastic Load Balancing enables a service suffer the loss of a server with no availability disruptions as user traffic is automatically routed to the healthy servers. Amazon Auto Scaling ensures unhealthy hosts are removed and replaced with healthy ones to maintain high availability.\n        Availability Zones (AZs) are isolated sets of resources within a region, each with redundant power, networking, and connectivity, housed in separate facilities. Each Availability Zone is isolated, but the Availability Zones in a Region are connected through low-latency links. AWS provides you with the flexibility to place instances and store data across multiple Availability Zones within each AWS Region for high resiliency.   Learn more: After the lab see this whitepaper on regions and availability zones     function prevStep(){ window.open(\"..\\/2_configure_env\", \"_self\") } function nextStep(){ window.open(\"..\\/4_cleanup\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_iam_tag_based_access_control_for_ec2/3_test_role/","title":"Test Role","tags":[],"description":"","content":"3.1 Assume ec2-admin-team-alpha Role Now you will use an existing IAM user with MFA enabled to assume the new ec2-admin-team-alpha role.\n Sign in to the AWS Management Console as an IAM user with MFA enabled. https://console.aws.amazon.com . In the console, click your user name on the navigation bar in the upper right. It typically looks like this: username@account_ID_number_or_alias then click Switch Role. Alternatively you can paste the link in your browser that you recorded earlier. On the Switch Role page, type you account ID number in the Account field, and the name of the role ec2-admin-team-alpha that you created in the previous step in the Role field. (Optional) Type text that you want to appear on the navigation bar in place of your user name when this role is active. A name is suggested, based on the account and role information, but you can change it to whatever has meaning for you. You can also select a color to highlight the display name. Click Switch Role. If this is the first time choosing this option, a page appears with more information. After reading it, click Switch Role. If you clear your browser cookies, this page can appear again.   The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you replacing the permission that you had as the IAM user.\nTip\nThe last several roles that you used appear on the menu. The next time you need to switch to one of those roles, you can simply click the role you want. You only need to type the account and role information manually if the role is not displayed on the Identity menu.\n  3.2 Launch Instance With \u0026amp; Without Tags  Navigate to the EC2 Management Console in the us-east-2 (Ohio) region https://us-east-2.console.aws.amazon.com/ec2/v2/home?region=us-east-2 . The EC2 Dashboard should display a list of errors including You are not authorized. This is the first test passed, as us-east-2 region is not allowed.  Navigate to the EC2 Management Console in the us-east-1 (North Virginia) region https://us-east-1.console.aws.amazon.com/ec2/v2/home?region=us-east-1 . The EC2 Dashboard should display a summary list of resources with the only error being Error retrieving resource count from Elastic Load Balancing as that requires additional permissions.  Click Launch Instance button to start the wizard. Click Select next to the first Amazon Linux 2 Amazon Machine Image to launch.  Accept the default instance size by clicking Next: Configure Instance Details.  Accept default details by clicking Next: Add Storage.  Accept default storage options by clicking Next: Add Tags.  Lets add an incorrect tag now that will fail to launch. Click Add Tag enter Key of Name and Value of Example. Repeat to add Key of Team and Value of Beta. Note: Keys and values are case sensitive! Click Next: Configure Security Group.  Click Select an existing security group, click the check box next to security group with name default, then click Review and Launch.  Click Launch then click the option to Proceed without a key pair. Tick the I acknowledge box then click Launch Instances.  The launch should fail, if it succeeded verify the role you are using and the managed roles you have attached as per previous steps.  Click Back to Review Screen then click Edit tags to modify the tags. Change the Team key to a value of Alpha which matches the IAM policy previously created then click Review and Launch.  On the review launch page once again click Launch then click the option to Proceed without a key pair. Tick the I acknowledge box then click Launch Instances. You should see a message that the instance is now launching. Click View Instances and do not terminate it just yet.  3.3 Modify Tags On Instances  Continuing from 3.2 in the EC2 Management Console instances view, click the check box next to the instance named Example then the Tags tab.  Click Add/Edit Tags, try changing the Team key to a value of Test then click Save. An error message should appear. Change the Team key back to Alpha, and edit the Name key to a value of Test and click Save. The request should succeed.  3.4 Manage Instances  Continuing from 3.3 in the EC2 Management Console instances view, click the check box next to the instance named Test. Click Actions button then expand out Instance State then Terminate. Check the instance is the one you wish to terminate by it\u0026rsquo;s name and click Yes, Terminate. The instance should now terminate.  Congratulations! You have now learnt about IAM tag based permissions for EC2!  "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_5_cost_visualization/3_sp_coverage/","title":"View your Savings Plan coverage","tags":[],"description":"","content":"You can view your Savings Plan coverage to look for anomalies or changes in coverage.\n  In Cost Explorer, click on Saved reports on the left:   Click on Coverage report:   You can see the coverage is 0%:   Scroll down to the table, click on the arrow next to On-demand spend to sort from the largest spend to the lowest. This helps show your opportunity for cost savings:    function prevStep(){ window.open(\"..\\/2_cost_usage_account\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/4_elasticity\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_3_pricing_models/","title":"Level 100: Pricing Models","tags":[],"description":"","content":"Last Updated May 2020\nYour browser doesnt support video, or if you're on GitHub head to https://wellarchitectedlabs.com to watch the video.  Authors  Nathan Besh, Cost Lead Well-Architected  Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com\nIntroduction This hands-on lab will guide you through the steps to perform an analysis of Savings Plans for your AWS cost and usage. The skills you learn will help you implement the correct pricing models for your workloads, in alignment with the AWS Well-Architected Framework.\nNOTE: There is also a 200 level lab on building a Savings Plan analysis dashbord. Goals  Perform basic analysis of your recommended Savings Plans  Prerequisites  AWS Account Setup has been completed Consistent usage in your account for more than 1 month  Permissions required  Log in as the Cost Optimization team, created in AWS Account Setup   Costs  https://aws.amazon.com/aws-cost-management/pricing/ There are no costs for this lab  Time to complete  The lab should take approximately 15 minutes to complete  Steps:  View your Savings Plan recommendations   Understand your usage trend   Analyze your Savings Plan recommendations   Visualize your Savings Plan recommendations   Tear down    function prevStep(){ window.open(\"\", \"_self\") } function nextStep(){ window.open(\".\\/1_view_recommendations\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool  Start Lab  "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_3_pricing_models/","title":"Level 200: Pricing Models","tags":[],"description":"","content":"Last Updated May 2020\nAuthors  Nathan Besh, Cost Lead, Well-Architected Spencer Marley, Commercial Architect Paul Lambden, Principal Technical Account Manager  Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com\nIntroduction Commitment discounts offer lower prices when you make a commitment to a minimum amount of usage. Reserved Instances (RI\u0026rsquo;s) are available for RDS, RedShift, DynamoDB, Elasticache, and Elasticsearch (For EC2 - Savings Plans should be purchased). This hands-on lab will guide you through the steps to perform a Reserved Instance analysis, and make low risk, high return RI purchases at scale. The skills you learn will help you ensure your workloads utilize different pricing models in alignment with the AWS Well-Architected Framework.\nGoals  Perform a Reserved Instance analysis Make low risk, high return RI recommendations  Prerequisites  AWS Account Setup has been completed Consistent usage in your account for more than 1 month to trigger a recommendation  Permissions required  Log in as the Cost Optimization team, created in AWS Account Setup   Costs  There are no associated costs for this lab  Steps:  View an RI report   Download and prepare the RI CSV files   Sort and filter the RI CSV files   Tear down    function prevStep(){ window.open(\"\", \"_self\") } function nextStep(){ window.open(\".\\/1_ri_report\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool  Start Lab  "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_incident_response_playbook_with_jupyter-aws_iam/","title":"Level 300: Incident Response Playbook with Jupyter - AWS IAM","tags":[],"description":"","content":"Authors  Ben Potter, Security Lead, Well-Architected Byron Pogson, Solutions Architect, AWS  Introduction This hands-on lab will guide you through running a basic incident response playbook using Jupyter. It is a best practice to be prepared for an incident, and practice your investigation and response tools and processes. You can find more best practices by reading the Security Pillar of the AWS Well-Architected Framework .\nThe skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nGoals  Identify tooling for incident response Automate containment for incident response Pre-deploy tools for incident response  Prerequisites  An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . CloudTrail must already be enabled in your account and logging to CloudWatch Logs, follow the Automated Deployment of Detective Controls lab to enable.  Steps:  Install Python \u0026amp; AWS CLI   Playbook Run   References \u0026amp; useful resources  AWS CLI Command Reference  AWS Identity and Access Management User Guide  CloudWatch Logs Insights Query Syntax  Jupyter   "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_200_incident_response_day/","title":"Quest: AWS Incident Response Day","tags":[],"description":"This quest is the guide for incident response workshop often ran at AWS led events.","content":"About this Guide This quest is the guide for an AWS led event including incident response day. Using an AWS supplied, or your own AWS account, you will learn through hands-on labs in the AWS Well-Architected area of Incident Response . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nPrerequisites  An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .   Lab 1 - Automated Deployment of Detective Controls This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail, AWS Config, and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of each service.\nStart now!  Lab 2 - Protecting workloads on AWS from the instance to the edge In this workshop, you will build an environment consisting of two Amazon Linux web servers behind an application load balancer. The web servers will be running a PHP web site that contains several vulnerabilities. You will then use AWS Web Application Firewall (WAF), Amazon Inspector and AWS Systems Manager to identify the vulnerabilities and remediate them.\nStart now!  Lab 3 - Incident Response with AWS Console and CLI This hands-on lab will guide you through a number of examples of how you could use the AWS Console and Command Line Interface (CLI) for responding to a security incident. It is a best practice to be prepared for an incident, and have appropriate detective controls enabled.\nStart now!  Lab 4 - Getting Hands on with Amazon GuardDuty Walks you through a scenario covering threat detection and automated remediation using Amazon GuardDuty; a managed threat detection service. The scenario simulates an attack that spans a few threat vectors, representing just a small sample of the threats that GuardDuty is able to detect.\nStart now!  Lab 5 - Open Source AWS Memory Forensics This lab consists of using an open source python module for orchestrating memory acquisitions and analysis using AWS Systems Manager . It analyzes the memory dump using Rekall with the most common plugins: psaux, pstree, netstat, ifconfig, pidhashtable.\nStart now!   Further Learning AWS Security Incident Response Guide Find further information on the AWS website around AWS Cloud Security and in particular what your responsibilities are under the shared security model  Authors  Ben Potter, Security Lead, Well-Architected  "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_300_reinvent2020_automate_with_weinvest/","title":"Quest: re:Invent 2020 - Automate The Well-Architected Way With WeInvest","tags":[],"description":"This quest is a collection of lab patterns which are covered in the upcoming session at re:Invent 2020: Automate The Well-Architected Way with WeInvest","content":"About this Guide This quest is a collection of featured lab patterns with are covered in the re:Invent 2020 session: Automate The Well-Architected Way with WeInvest.\nUsing this collection of labs, the user will be able to walk through the featured patterns from the session which WeInvest have worked with AWS to implement within their business to build an improved and effective security posture.\nUsing either an AWS supplied, or your own AWS account, you will learn through hands-on labs in the AWS Well-Architected area of Incident Response . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nPrerequisites  An AWS account that you are able to use for testing, that is not used for production or other purposes.  NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\n Lab 1 - Autonomous Montoring Of Cryptographic Activity With KMS. In this lab we will walk you through an example scenario of monitoring our KMS service for encryption and decryption activity. We will autonomously detect abormal activity beyond a predefined threshold and respond accordingly, using the following services:\n AWS CloudTrail - Used for capturing API events within the environment. Amazon CloudWatch Log Groups - Used to log our CloudTrail API events. Amazon CloudWatch Metric Filter to create apply filter so we can measure the only the events that matters for us. Amazon CloudWatch Alarms to allow our system to react against pre created events Amazon Simple Notification Service to allow us to send email notification when an event occurs.  Start now!  Lab 2 - Autonomous Patching With EC2 Image Builder and Systems Manager. In this lab we will walk you through a blue/green deployment methodology to build an entirely new Amazon Machine Image (AMI) that contains the latest operating system patch, which can be deployed into an application cluster. We will use the following services to complete the workload deployment:\n EC2 Image Builder to automate creation of the AMI  Systems Manager Automated Document to orchestrate the execution. CloudFormation with AutoScalingReplacingUpdate update policy, to gracefully deploy the newly created AMI into the workload with minimal interruption to the application availability.  Start now!   Further Learning AWS Security Incident Response Guide Find further information on the AWS website around AWS Cloud Security and in particular what your responsibilities are under the shared security model  Authors  Tim Robinson - Well-Architected Geo Solution Architect (Asia)  "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/","title":"300 Labs","tags":[],"description":"","content":"List of labs available  Level 300: Implementing Health Checks and Managing Dependencies to improve Reliability  Improve reliability of a service by decoupling service dependencies, using health checks, and demonstrating when to use fail-open and fail-closed behaviors\n Level 300: Testing for Resiliency of EC2, RDS, and AZ  Use code to inject faults simulating EC2, RDS, and Availability Zone failures. These are used as part of Chaos Engineering to test workload resiliency\n Level 300: Fault Isolation with Shuffle Sharding  Implement shuffle sharding to minimize scope of impact of failures\n "},{"uri":"https://wellarchitectedlabs.com/cost/costeffectiveresources/","title":"Cost Effective Resources","tags":[],"description":"","content":"About cost effective resources Using the appropriate services, instances and resources for your workload is key to cost savings. A well-architected workload uses the most cost-effective resources, which can have a significant and positive economic impact. You also have the opportunity to use managed services to reduce costs. AWS offers a variety of flexible and cost-effective pricing options to acquire instances from EC2 and other services in a way that best fits your needs.\n Step 1 - Pricing Models By using the right pricing model for your workload resources, you pay the lowest price for that resource.\n           100 Level Lab: This lab will introduce you to working with Savings Plans (SP\u0026rsquo;s), utilizing AWS Cost Explorer to make low risk, high return SP purchases for your business.     200 Level Lab: This lab will will create a Savings Plan analysis dashboard. Using this you can completely analyze your usage and select the best commitment for your business.     200 Level Lab: This lab will introduce you to working with Reserved Instances (RI\u0026rsquo;s), utilizing AWS Cost Explorer to make low risk, high return RI purchases for your business.    Step 2 - Right Sizing            This lab will show you how to use Cost Explorer to perform right sizing recommendations on your resources.     This lab will show you how to install the memory agent.    "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_goals_and_targets/3_monitor_cost_usage/","title":"Monitor Cost and Usage","tags":[],"description":"","content":"Monitor Cost and Usage Allocate costs based on workload metrics Measure efficiency  Goal: Measure workload efficiency Target: Within 6 months all Tier1 workloads must have efficiency dashboards, within 12months all Tier2 workloads must have efficiency dashboards. Best Practice: Allocate costs based on workload metrics  Measures: % of workloads with dashboards Good/Bad: Good Why? When does it work well or not?: Ensures the organization is focusing on the correct metric (efficiency), and not the bill. Contact/Contributor: natbesh@amazon.com   Costs   Goal: Reduction in bill Target: Reduce the bill by x% in the next billing cycle Best Practice: Decommission Resources  Measures: Reduction in bill Good/Bad: Bad Why? When does it work well or not?: Does not drive improvement or capability building, can stifle innovation and positive growth. Does not factor in outcomes - ignores efficiency. Can have false positives - goal can be achieved by doing nothing if next months usage decreases, or impossible to achieve if next months usage is significantly higher Contact/Contributor: natbesh@amazon.com   Assign business value to costs \u0026amp; usage Tagging  Goal: Add tags to all of our bill (where possible) Target: No more than $100 a month of taggable spend, is to be un-tagged Best Practice: Assign organization meaning to cost and usage  Measures: % of tag-able bill untagged Good/Bad: Good Why? When does it work well or not?: Helps add meaning to cost and usage, allows for adjustment between effort/reward with the % of bill approach, instead of using resource count. Contact/Contributor: natbesh@amazon.com   function prevStep(){ window.open(\"..\\/2_govern_usage\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/4_decommission_resources\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_testing_backup_and_restore_of_data/4_test_restore/","title":"Test Restore","tags":[],"description":"","content":"A backup of a data source is useful only if data can be restored from it. If backups aren\u0026rsquo;t tested, you might find yourself in a situation where your workload has been impacted by an event and you need to recover data from your backups, but the backups are faulty and restoring data is not possible, or exceeds your RTO. To avoid such situations, backups taken should always be tested to ensure they can be used to recover data.\nIn this lab, you will leverage AWS Lambda to automatically test all backups created to ensure recovery is successful, and clean up any resources that were created during the restore test process to save on cost. This will ensure you are aware of any faulty backups that might be unusable to recover data from. Automating this process with notifications enabled will ensure there is minimal operational overhead and that the Operations teams are aware of backup and restore statuses.\nWhen testing recovery, it is important to define the criteria for successful data recovery from the restored resource. This will depend on a variety of factors such as the data source, the type of data, the margin for error, etc. Organizations and workload owners are responsible for defining this success criteria.\nThe EC2 Instance that was created as part of this lab is running a simple web application. For this use-case, I have determined that data recovery is successful if the application is running on the restored resource as well. If the restored resource is missing any application critical files, the healthchecks made against the restored resource will fail, indicating an issue with the backup.\nTesting Recovery For the purpose of this lab, we will simulate the action performed by AWS Backup when creating backups of data sources by creating an on-demand backup to see if the backup is successful. Once the backup is completed, you will receive a notification stating that the backup job has completed and the lambda function will get invoked. The Lambda function will make API calls to start restoring data from the backup that was created. This will help ascertain that the backup is good. Once the restore process has been completed, you will receive another notification confirming this, and the lambda function will get invoked again to clean up new resources that were created as part of the restore. Once the cleanup has been completed, you will receive one last notification confirming cleanup.\n  Use your administrator account to access the AWS Backup console - https://us-east-1.console.aws.amazon.com/backup/home?region=us-east-1#home\n  Click on CREATE AN ON-DEMAND BACKUP in the middle of the screen.\n  Under RESOURCE TYPE, select EC2. Paste in the Instance ID obtained from the Output of the CloudFormation Stack.\n  Under BACKUP WINDOW, ensure that the CREATE BACKUP NOW option is selected.\n  Under EXPIRE, select the option DAYS AFTER CREATION and enter 1 for the value for this lab. This will ensure that the backup is deleted after 1 day.\n  Under Backup Vault, select the BACKUP-LAB-VAULT.\n  Leave the default IAM role selected.\n  Click CREATE ON-DEMAND BACKUP.\n  Click on JOBS from the menu on the left and select BACKUP JOBS. You should see a new backup job started with the status of RUNNING. Click on the RESTORE JOBS tab, there shouldn\u0026rsquo;t be any restore jobs running.\n  Periodically refresh the console until the STATUS changes to COMPLETED. It should take about 5-10 minutes to complete.\n  After the job is completed, click on the JOB ID and view the DETAILS. You should see the Recovery Point ARN that was created, the RESOURCE ID for which the backup was created, and the RESOURCE TYPE for which the backup was created.\n  Monitor your email to see if you receive a Notification from AWS Backup. Compare details in the email to what you see on the AWS Console, they should match. It takes about 10 mins for the email to show up once the backup job has completed.\n  The SNS Topic that was created as part of the CloudFormation stack has been configured as a trigger for the Lambda function. When AWS Backup publishes a new message when a backup or restore job is completed, the Lambda function gets invoked.\n  Let\u0026rsquo;s take a look at the relevant section of the Lambda function code to understand what will happen once the backup job is completed and a notification has been received. You can view the full Lambda function code here .\nThe Lambda function first obtains the recovery point restore metadata for the recovery point that was created when the on-demand backup job was initiated.\nmetadata = backup.get_recovery_point_restore_metadata( BackupVaultName=backup_vault_name, RecoveryPointArn=recovery_point_arn ) Once the recovery point restore metadata has been retrieved, the function will then use this to make an API call to AWS Backup to start a restore job.\nrestore_request = backup.start_restore_job( RecoveryPointArn=recovery_point_arn, IamRoleArn=iam_role_arn, Metadata=metadata['RestoreMetadata'] )   Go back to JOBS and switch to the RESTORE JOBS tab. You should see a RESTORE JOB running. The lambda function that was created as part of this lab has requested a restore job from AWS Backup. This is to ensure data recovery from the backup is successful.\n  Periodically refresh the console until the STATUS changes to COMPLETED. It should take about 5-10 minutes to complete.\n  After the job is completed, click on the JOB ID and view the DETAILS. You should see the Recovery Point ARN from which the restore was tested, the RESOURCE ID of the newly created EC2 Instance, and the RESOURCE TYPE for which the restore was created.\n  Note down the RESOURCE ID of the newly created EC2 Instance and verify that it exists from the EC2 Console - https://console.aws.amazon.com/ec2/v2/home?region=us-east-1#Instances:sort=instanceState. Note down the public IP of the new EC2 Instance.\n  Monitor your email to see if you have received a Notification from AWS Backup confirming the restore job was successful. Compare details in the email to what you see on the AWS Console, they should match. It takes about 10 mins for the email to show up once the restore job has completed.\n  While waiting for the notification, let\u0026rsquo;s take a look at the relevant sections of the Lambda function code to see what happens when a restore job is completed. You can view the full Lambda function code here .\nAfter receiving confirmation from AWS Backup that the restore job has completed successfully, the Lambda function will verify data recovery. To do this, an API call is made to retrieve the public IP address of the new EC2 Instance. It makes an HTTP GET request to the new EC2 Instance to check if the application is running. If a valid response (200 in this case) is received, it is ascertained that data recovery was successful as per the recovery success criteria that was established earlier in this section. The Lambda function will then make an API call to EC2 to terminate the new EC2 Instance to save on cost. You can manually verify this as well by visiting the following URL:\nhttp://\u0026lt;PUBLIC_IP_OF_THE_NEW_INSTANCE\u0026gt;/\ninstance_details = ec2.describe_instances( InstanceIds=[ instance_id ] ) public_ip = instance_details['Reservations'][0]['Instances'][0]['PublicIpAddress'] http = urllib3.PoolManager() url = public_ip resp = http.request('GET', url) print(resp.status) if resp.status == 200: print('Valid response received. Data recovery validated. Proceeding with deletion.') print('Deleting: ' + instance_id) delete_request = ec2.terminate_instances( InstanceIds=[ instance_id ] ) else: print('Invalid response. Data recovery is questionable.')   Monitor your email to see if you have received a Restore Test Status notification confirming the deletion of the newly created resource. Check the EC2 Console to verify that the new EC2 Instance has been terminated. - https://console.aws.amazon.com/ec2/v2/home?region=us-east-1#Volumes:sort=size\n  Use your administrator account to access the AWS CloudWatch console - https://console.aws.amazon.com/cloudwatch/home?region=us-east-1\n  Click on LOGS from the menu on the left side.\n  For filter, paste the following string after replacing the value for the name of the CloudFormation stack that was created as part of this lab.\n/aws/lambda/RestoreTestFunction-\u0026lt;YOUR CLOUDFORMATION STACK NAME\u0026gt;\n  Click on the LOG STREAM and view the output of the Lambda function\u0026rsquo;s execution to understand the different steps performed by the function to automate this process.\n  Review of Best Practices Implemented Identify all data that needs to be backed up and perform backups or reproduce the data from sources: Back up important data using Amazon S3, Amazon EBS snapshots, or third-party software. Alternatively, if the data can be reproduced from sources to meet RPO, you may not require a backup.\nPerform data backup automatically or reproduce the data from sources automatically: Automate backups or the reproduction from sources using AWS features (for example, snapshots of Amazon RDS and Amazon EBS, versions on Amazon S3, etc.), AWS Marketplace solutions, or third-party solutions.\nPerform periodic recovery of the data to verify backup integrity and processes: Validate that your backup process implementation meets Recovery Time Objective and Recovery Point Objective through a recovery test.\n function prevStep(){ window.open(\"..\\/3_enable_notifications\", \"_self\") } function nextStep(){ window.open(\"..\\/5_cleanup\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_using_awscli_to_manage_wa_reviews/4_save_milestone/","title":"Saving a milestone","tags":[],"description":"","content":"Overview A milestone records the state of a workload at a particular point in time.\nSave a milestone after you initially complete all the questions associated with a workload. As you change your workload based on items in your improvement plan, you can save additional milestones to measure progress.\nA best practice is to save a milestone when you first do a new W-A review, or every time you make improvements to a workload.\n1. Create a Milestone  Using the create-milestone API we will save our current progress as our first milestone aws wellarchitected create-milestone --workload-id \u0026quot;\u0026lt;WorkloadId\u0026gt;\u0026quot; --milestone-name Rev1  The return value will be the WorkloadId and the Milestone number assigned.   2. List all milestones  If we want to see all milestones associated with a workload Using the list-milestones API you can see all milestones associated with a workload aws wellarchitected list-milestones --workload-id \u0026quot;\u0026lt;WorkloadId\u0026gt;\u0026quot; --max-results 50  This will return a summary of the milestones you have created for the workload.   3. Retrieve the results from a milestone  Using the get-milestone API , you can see all of the metadata for a specific milestone: aws wellarchitected get-milestone --workload-id \u0026quot;\u0026lt;WorkloadId\u0026gt;\u0026quot; --milestone-number 1  This will return the complete workload information for the workload milestone (such as when it was created and what lenses were included)   4. List all question answers based from a milestone  If you want to see all of the best practices for when the milestone was created, you can use Using the list-answers API , you can see all of the best practices for when the milestone was created: aws wellarchitected list-answers --workload-id \u0026quot;\u0026lt;WorkloadId\u0026gt;\u0026quot; --lens-alias \u0026quot;wellarchitected\u0026quot; --milestone-number 1  This will return a large json structure with all of the questions and best practices for each pillar.    function prevStep(){ window.open(\"..\\/3_perform_review\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/5_view_report\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_fault_isolation_with_shuffle_sharding/4_impact_of_failures_sharding/","title":"Impact of failures with sharding","tags":[],"description":"","content":"Break the application We will now introduce the poison pill into the workload by including the bug query-string with our requests and see how the updated the workload architectures handles it. As in our previous case, imagine that customer Alpha triggered the bug in the application again.\n  Include the query-string bug with a value of true and make a request as customer Alpha. The modified URL should look like this - http://shuffle-alb-1p2xbmzo541rr-1602891463.us-east-1.elb.amazonaws.com/?name=Alpha\u0026amp;bug=true\n  This should result in an Internal Server Error response on the browser indicating that the application has stopped working as expected on the instance that processed this request\n  At this point, there is one healthy instance still available on the shard so other customers mapped to that shard are not impacted. You can verify this by opening another browser tab and specifying the URL with customer name Bravo or Charlie and without the bug query string such as:\n http://shuffle-alb-1p2xbmzo541rr-1602891463.us-east-1.elb.amazonaws.com/?name=Bravo http://shuffle-alb-1p2xbmzo541rr-1602891463.us-east-1.elb.amazonaws.com/?name=Charlie    Just like before, customer Alpha, not aware of this bug in the application, will retry the request. Refresh the page to simulate this as you did before. This request is routed to the other healthy instance in the shard. The bug is triggered again and the other instance goes down as well. The entire shard is now affected.\n  All requests to this shard will now fail because there are no healthy instances in the shard. You can verify this by sending requests for customers Alpha, Bravo, or Charlie. No matter how many times the page is refreshed, you will see a 502 Bad Gateway.\n  Since customers can only make requests to the shard they are assigned to, customers Delta, Echo, and Foxtrot are not affected by customer Alphas actions. You can verify this by making the following requests:\n http://shuffle-alb-8vonmf2ywl5z-682850122.us-east-1.elb.amazonaws.com/?name=Delta http://shuffle-alb-8vonmf2ywl5z-682850122.us-east-1.elb.amazonaws.com/?name=Echo http://shuffle-alb-8vonmf2ywl5z-682850122.us-east-1.elb.amazonaws.com/?name=Foxtrot    The impact is localized to a specific shard, shard 1 in this case, and only customers Alpha, Bravo, and Charlie are affected. The scope of impact has now been reduced so that only 50% of customers are affected by the failure induced by the poison pill.\n   With this sharded architecture, the scope of impact is reduced by the number of shards. Here with two shards, if a customer experiences a problem, then the shard hosting them might be impacted, as well as all of the other customers on that shard. However, that shard represents only one half of the overall service. Since this is just a lab we kept it simple with only two shards, but with more shards, the scope of impact decreases further. Adding more shards requires adding more capacity (more workers). But in the next steps of this lab you will learn how with shuffle sharding, we can do exponentially better again without adding capacity.\n Fix the application As in the previous section, Systems Manager will be used to fix the application and return functionality to the users that are affected - Alpha, Bravo, and Charlie.\n  Go to the Outputs section of the CloudFormation stack and open the link for SSMDocument. This will take you to the Systems Manager console.\n  Click on Run command which will open a new tab on the browser\n  Scroll down to the Targets section and select Choose instances manually\n  Check the box next to the EC2 instances with the names Worker-1 and Worker-2.\n  Scroll down to the Output options section and uncheck the box next to Enable an S3 bucket. This will prevent Systems Manager from writing log files based on the command execution to S3.\n  Click on Run\n  You should see the command execution succeed in a few seconds\n  Once the command has finished execution, you can go back to the application and test it to verify it is working as expected. Make sure that the query-string bug is not included in the request. For example, http://shuffle-alb-1p2xbmzo541rr-1602891463.us-east-1.elb.amazonaws.com/?name=Alpha should return a valid response. Refresh the page a few times to make sure responses are being received from 2 different EC2 instances. Repeat this process for the other customers that were affected - Bravo and Charlie.\n   function prevStep(){ window.open(\"..\\/3_implement_sharding\", \"_self\") } function nextStep(){ window.open(\"..\\/5_implement_shuffle_sharding\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_monitoring_linux_ec2_cloudwatch/4_adding_metrics_to_dashboard/","title":"Add metrics to Dashboard","tags":["Linux","Amazon Linux","EC2","CloudWatch","CloudWatch Dashboard"],"description":"","content":"Now that we have a dashboard for our Linux EC2 instance, we can add an additional metric for available memory.\n Let\u0026rsquo;s add a new widget to our CloudWatch Dashboard. Click on the \u0026ldquo;Add Widget\u0026rdquo; button in the upper right corner.  Click Line, then next Click Metrics and then Configure Select \u0026ldquo;PerfLab\u0026rdquo; under Custom Namespaces and then select the second metric group ImageId, InstanceId, InstanceType Make sure to search only for the InstanceId if you have multiple machines reporting metrics Find the mem_used and \u0026ldquo;mem_total\u0026rdquo; and click the check box next to it  Click on Graphed Metrics (2) and then select 5 seconds as the period Click on Create widget  You should now see two widgets on your Dashboard  You can drag the widgets around the screen and re-size them if you wish  You can also change the time period, select 1h to show just the most recent metrics  You can also set the auto-refresh rate for the Dashboard by using this pull-down. Select 10s  Click the Save Dashboard button before proceeding to the next step.   function prevStep(){ window.open(\"..\\/3_creating_cloudwatch_dashboard\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/5_generating_load\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_monitoring_linux_ec2_cloudwatch/","title":"Level 100: Monitoring an Amazon Linux EC2 instance with CloudWatch Dashboards","tags":["Linux","Amazon Linux","EC2","CloudWatch","CloudWatch Dashboard"],"description":"How to configure an Amazon CloudWatch Dashboard to get aggregated views of the health and performance of a Amazon Linux EC2 instance.","content":"Authors  Eric Pullen, Performance Efficiency Lead Well-Architected  Introduction This hands-on lab will guide you through creating an Amazon EC2 instance running Amazon Linux and then configuring a Amazon CloudWatch Dashboard to get aggregated views of the health and performance information for that instance. This lab should enables you to quickly get started with CloudWatch monitoring and explore account and resource-based views of metrics. You can find more best practices by reading the Performance Efficiency Pillar of the AWS Well-Architected Framework . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nGoals  Monitor a Amazon Linux EC2 machine to identify CPU and memory bottlenecks  Prerequisites  An AWS account that you are able to use for testing, that is not used for production or other purposes An IAM role in your AWS account  Costs  https://aws.amazon.com/cloudwatch/pricing/  You can create 3 dashboards for up to 50 metrics per month on the free tier and then it is $3.00 per dashboard per month This lab creates one dashboard, so the maximum cost would be $3.00 per month if you have already consumed the free tier.   The default lab uses a t3.large EC2 instance which will consume approximately $3.00 for every day the lab is running The VPC that is created for this lab will build a Nat Gateway, and will consume $5.50 per day when deployed. Using defaults, the total cost of the lab would be at least $8.50 per day  NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\n  function prevStep(){ window.open(\"\", \"_self\") } function nextStep(){ window.open(\".\\/1_deploy_vpc\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool  Start Lab  Steps:  Deploying the infrastructure   Deploying an instance   Create CloudWatch Dashboard   Add metrics to Dashboard   Generate CPU and Memory load   Teardown   "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_monitoring_windows_ec2_cloudwatch/4_adding_metrics_to_dashboard/","title":"Add metrics to Dashboard","tags":["Windows Server","Windows","EC2","CloudWatch","CloudWatch Dashboard"],"description":"","content":"Now that we have a dashboard for our Windows EC2 instance, we can add an additional metric for available memory.\n Let\u0026rsquo;s add a new widget to our CloudWatch Dashboard. Click on the \u0026ldquo;Add Widget\u0026rdquo; button in the upper right corner.  Click Line, then next Click Metrics and then Configure Select CWAgent and then select the second metric group \u0026ldquo;ImageId, InstanceId, InstanceType, objectname\u0026rdquo; Make sure to search only for the InstanceId if you have multiple machines reporting metrics Find the \u0026ldquo;Memory available Mbytes\u0026rdquo; and click the check box next to it.  Click on \u0026ldquo;Graphed Metrics (1)\u0026rdquo; and then select \u0026ldquo;5 seconds\u0026rdquo; as the period Click on Create widget  You should now see two widgets on your Dashboard.  You can drag the widgets around the screen and re-size them if you wish.  You can also change the time period, select 1h to show just the most recent metrics  You can also set the auto-refresh rate for the Dashboard by using this pull-down. Select 10s  Click the \u0026ldquo;Save Dashboard\u0026rdquo; button before proceeding to the next step.   function prevStep(){ window.open(\"..\\/3_creating_cloudwatch_dashboard\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/5_generating_load\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/100_labs/100_enable_security_hub/","title":"Enable Security Hub","tags":[],"description":"","content":"Last Updated: September 2020\nAuthor: Pierre Liddle, Principal Security Architect\nAWS Security Hub gives you a comprehensive view of your high-priority security alerts and compliance status across AWS accounts. There are a range of powerful security tools at your disposal, from firewalls and endpoint protection to vulnerability and compliance scanners. But oftentimes this leaves your team switching back-and-forth between these tools to deal with hundreds, and sometimes thousands, of security alerts every day. With Security Hub, you now have a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services, such as Amazon GuardDuty, Amazon Inspector, and Amazon Macie, as well as from AWS Partner solutions. Your findings are visually summarized on integrated dashboards with actionable graphs and tables. You can also continuously monitor your environment using automated compliance checks based on the AWS best practices and industry standards your organization follows. Get started with AWS Security Hub in just a few clicks in the Management Console and once enabled, Security Hub will begin aggregating and prioritizing findings.\nPrerequisites  An AWS account that you are able to use for testing, that is not used for production or other purposes.  Costs  Typically less than $1 per month if the account is only used for personal testing or training, and the tear down is not performed. AWS Security Hub pricing  AWS Pricing   Steps:  Enable AWS Security Hub via AWS Console   References \u0026amp; useful resources AWS Security Hub "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/100_labs/100_dependency_monitoring/4_test_fail_condition/","title":"Test Fail Condition","tags":[],"description":"","content":"Now that an alarm has been created to alert and send out notifications when the external service is experiencing an outage, it is time to test it. To do this, an outage can be simulated so that the external service is no longer able to write data into S3. This can be achieved in a few different ways. These are also a few different failure modes for the dependent service, which could cause the Lambda function to not get invoked. While the alarm will provide visibility into the outage itself, it will not identify the root cause.\n External service no longer has permission to write to S3 (role used by the service has been removed/modified) S3 bucket policy has been changed so that the external service no longer has access to write to it S3 bucket configuration has been modified and S3 notifications removed so that notifications are no longer sent to Lambda to invoke the function External service is experiencing network connectivity issues preventing it from writing to S3  In this lab, the last failure mode - Loss of connectivity will be simulated. To do this, the default route for the subnet can be removed so that the external service running on EC2 will no longer have a path to reach the internet.\n  Go to the VPC console at https://console.aws.amazon.com/vpc and click on Route Tables\n  Search for the route table WA-Lab-RouteTable\n  Click on the Routes tab and then click Edit routes\n  On the Edit routes page, find the route with the destination of 0.0.0.0/0 and click on the X at the end of that row\n  Click on Save routes\n  The external service running on EC2 no longer has a path to reach the internet, which means it cannot write data to S3. Now that an \u0026ldquo;outage\u0026rdquo; has occurred, it is time to see if the alarm is able to identify this and send out notifications.\n  Go to the Amazon CloudWatch console at https://console.aws.amazon.com/cloudwatch and click on Alarms\n  Search for the alarm WA-Lab-Dependency-Alarm and click on it\n  Monitor the alarm state to see if it changes (NOTE: It will take a few minutes for the alarm state to change since CloudWatch attempts to retrieve a higher number of data points than specified by Evaluation Periods when evaluating a metric with missing data)\n  Once the alarm state changes from OK to In alarm, CloudWatch will execute the action specified, in this case, sends a message to the SNS Topic specified\n  Monitor the email address that was specified during the CloudFormation Stack creation process in section 1 Deploy the Infrastructure\n  SNS will send a notification providing details of the alarm, the change in state, reason for change, and additional data\n  Once the notification has been received, the team responsible for the workload can start investigating to identify the cause of failure. This will ensure a timely response to dependent service outages, and allow for improved business continuity.\nThe alarm will go back to an OK state once the metric is no longer breaching the threshold defined, in this case, at least 1 Lambda invocation every minute. This can be achieved by adding the default route back to the route table so that the external service running on EC2 is able to reach the internet again.\n  Go to the VPC console at https://console.aws.amazon.com/vpc and click on Route Tables\n  Search for the route table WA-Lab-RouteTable\n  Click on the Routes tab and then click Edit routes\n  On the Edit routes page, click on Add route and enter the following:\n Destination - 0.0.0.0/0 Target - Click on the dropdown, select Internet Gateway, and click on WA-Lab-InternetGateway    Click on Save routes\n  Now that internet connectivity has been re-established, the external service should start writing data to S3 again, which will in turn invoke the Lambda function. Since Lambda functions will now start getting invoked periodically again, the WA-Lab-Dependency-Alarm should go back to an OK state.\n Go to the Amazon CloudWatch console at https://console.aws.amazon.com/cloudwatch and click on Alarms Search for the alarm WA-Lab-Dependency-Alarm and click on it The alarm should be in an OK state confirming that the fix worked and the external service is functioning normally again  You have now configured an alarm and tested it to ensure dependency monitoring has been established and notifications will be sent out in the event of an outage. While it is important to be notified of events affecting dependent resources, responses to these events are just as important. Once a notification has been received, the event has to be effectively tracked to ensure the right resources are assigned to it and to avoid duplication of effort. The Bonus Content in the next section talks about how this can be achieved and how it can be automated. With this approach responses to events will be manual. If there is a known and codified procedure (runbook) for the event, SNS can be used to trigger the execution of the runbook in response to the event.\n function prevStep(){ window.open(\"..\\/3_create_alarm\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/5_bonus_content\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_cur_queries/queries/compute/","title":"Compute","tags":[],"description":"","content":"These are queries for AWS Services under the Compute product family .\nUse the clipboard in the top right of the text boxes below to copy all of the text to be pasted.\n You may need to change variables used as placeholders in your query. ${table_Name} is a common variable which needs to be replaced. Example: cur_db.cur_table\n Table of Contents   EC2 Total Spend   Query Description This query will display the top costs for all spend with the product code of \u0026lsquo;AmazonEC2\u0026rsquo;. This will include all pricing categories (i.e. OnDemand, Reserved etc..) as well as charges for storage on EC2 (i.e. gp2). The query will output the product code as well as the product description to provide context. It is ordered by largest to smallest spend.\nPricing Please refer to the EC2 pricing page .\nCost Explorer Links These links are provided as an example to compare CUR report output to Cost Explorer output.\nUnblended Cost Link Sample Output Download SQL File Link to Code Copy Query SELECT line_item_product_code, line_item_line_item_description, round(sum(line_item_unblended_cost),2) as sum_line_item_unblended_cost FROM ${table_name} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND line_item_product_code like '%AmazonEC2%' AND line_item_line_item_type NOT IN ('Tax','Refund') AND line_item_product_code like '%AmazonEC2%' GROUP BY line_item_product_code, line_item_line_item_description ORDER BY sum_line_item_unblended_cost desc    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         EC2 Hours a Day   Query Description This query will provide the EC2 usage quantity measured in hours for each purchase option and each instance type. The output will include detailed information about the instance type, amortized cost, purchase option, and usage quantity. The output will be ordered by usage quantity in descending order.\nPricing Page Please refer to the EC2 pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT year, month, bill_billing_period_start_date, product_instance_type, date_trunc('hour', line_item_usage_start_date) as hour_line_item_usage_start_date, bill_payer_account_id, line_item_usage_account_id, (CASE WHEN (savings_plan_savings_plan_a_r_n \u0026lt;\u0026gt; '') THEN 'SavingsPlan' WHEN (reservation_reservation_a_r_n \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (line_item_usage_type LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END) as purchase_option, sum(CASE WHEN line_item_line_item_type = 'SavingsPlanCoveredUsage' THEN savings_plan_savings_plan_effective_cost WHEN line_item_line_item_type = 'DiscountedUsage' THEN reservation_effective_cost WHEN line_item_line_item_type = 'Usage' THEN line_item_unblended_cost ELSE 0 END) as amortized_cost, round(sum(line_item_usage_amount), 2) usage_quantity FROM ${table_name} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND ( (line_item_product_code = 'AmazonEC2') AND (product_servicecode \u0026lt;\u0026gt; 'AWSDataTransfer') AND (line_item_operation LIKE '%RunInstances%') AND (line_item_usage_type NOT LIKE '%DataXfer%') ) AND ( (line_item_line_item_type = 'Usage') OR (line_item_line_item_type = 'SavingsPlanCoveredUsage') OR (line_item_line_item_type = 'DiscountedUsage') ) GROUP BY year, month, bill_billing_period_start_date, product_instance_type, date_trunc('hour', line_item_usage_start_date), bill_payer_account_id, 7, 8 ORDER BY usage_quantity DESC    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         EC2 Effective Savings Plans   Query Description This query will provide EC2 consumption of Savings Plans across Compute resources by linked accounts. It also provides you with the savings received from these Savings Plans and which Savings Plans its connected to. The output is ordered by date.\nPricing Please refer to the EC2 pricing page .\nSample Output Download SQL File Link to Code Copy Query  SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((\u0026quot;line_item_usage_start_date\u0026quot;),'%Y-%m-%d') AS day_line_item_usage_start_date, SPLIT_PART(savings_plan_savings_plan_a_r_n, '/', 2) AS savings_plan_savings_plan_a_r_n, CASE savings_plan_offering_type WHEN 'EC2InstanceSavingsPlans' THEN 'EC2 Instance Savings Plans' WHEN 'ComputeSavingsPlans' THEN 'Compute Savings Plans' ELSE savings_plan_offering_type END AS \u0026quot;Type\u0026quot;, savings_plan_region, CASE WHEN product_product_name = 'Amazon EC2 Container Service' THEN 'Fargate' WHEN product_product_name = 'AWS Lambda' THEN 'Lambda' ELSE product_instance_type_family END AS \u0026quot;Instance Type Family\u0026quot;, SUM (TRY_CAST(line_item_unblended_cost as decimal(16, 8))) as \u0026quot;On Demand Cost\u0026quot;, SUM(TRY_CAST(savings_plan_savings_plan_effective_cost AS decimal(16, 8))) as \u0026quot;Effective Cost\u0026quot;, SUM(CAST(line_item_unblended_cost AS decimal(16,8))) AS sum_line_item_unblended_cost, savings_plan_end_time FROM ${table_name} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND savings_plan_savings_plan_a_r_n \u0026lt;\u0026gt; '' AND line_item_line_item_type = 'SavingsPlanCoveredUsage' GROUP by bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((\u0026quot;line_item_usage_start_date\u0026quot;),'%Y-%m-%d'), savings_plan_savings_plan_a_r_n, savings_plan_offering_type, savings_plan_region, product_instance_type_family, product_product_name, savings_plan_end_time ORDER BY day_line_item_usage_start_date;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         Compute with Savings Plans   Query Description This query will provide details about Compute usage that is covered by Savings Plans. The output will include detailed information about the usage type, usage amount, Savings Plans ARN, line item description, and Savings Plans effective savings as compared to On-Demand pricing. The public pricing on-demand cost will be summed and in descending order.\nPricing Please refer to the Savings Plans pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, bill_billing_period_start_date, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,'%Y-%m') AS month_line_item_usage_start_date, savings_plan_savings_plan_a_r_n, line_item_product_code, line_item_usage_type, sum(line_item_usage_amount) sum_line_item_usage_amount, line_item_line_item_description, pricing_public_on_demand_rate, sum(pricing_public_on_demand_cost) AS sum_pricing_public_on_demand_cost, savings_plan_savings_plan_rate, sum(savings_plan_savings_plan_effective_cost) AS sum_savings_plan_savings_plan_effective_cost FROM ${table_name} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND line_item_line_item_type LIKE 'SavingsPlanCoveredUsage' GROUP BY bill_payer_account_id, bill_billing_period_start_date, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,'%Y-%m'), savings_plan_savings_plan_a_r_n, line_item_product_code, line_item_usage_type, line_item_unblended_rate, line_item_line_item_description, pricing_public_on_demand_rate, savings_plan_savings_plan_rate ORDER BY sum_pricing_public_on_demand_cost DESC    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         Account Spend of Shared Savings Plan   Query Description This query focuses on surfacing accounts which have utilized AWS Savings Plans for which they are not a buyer.\nPricing Please refer to the Savings Plans pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT year, month, bill_payer_account_id, line_item_usage_account_id, savings_plan_offering_type, line_item_resource_id, SUM(CAST(line_item_unblended_cost AS decimal(16, 8))) AS sum_line_item_unblended_cost, SUM(CAST(savings_plan_savings_plan_effective_cost AS decimal(16, 8))) AS sum_savings_plan_savings_plan_effective_cost FROM ${table_name} WHERE year = '2020' AND (month BETWEEN '9' AND '12' OR month BETWEEN '09' AND '12') AND (bill_payer_account_id = '111122223333' AND line_item_usage_account_id = '444455556666' AND line_item_line_item_type = 'SavingsPlanCoveredUsage' AND savings_plan_savings_plan_a_r_n NOT LIKE '%444455556666%') GROUP BY year, month, line_item_resource_id, line_item_usage_account_id, bill_payer_account_id, savings_plan_offering_type ORDER BY sum_savings_plan_savings_plan_effective_cost DESC;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         Lambda   Query Description This query focuses on Lambda and the breakdown of its costs by different usage element. Split by Resource IDs you can view the usage, unblended costs and amortized cost broken down by different pricing plans. These results will be ordered by date and costs.\nPricing Please refer to the Lambda pricing page .\nSample Output Download SQL File Link to Code Copy Query  SELECT * FROM ( ( SELECT bill_payer_account_id, line_item_usage_account_id, line_item_line_item_type, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d') AS day_line_item_usage_start_date, product_region, CASE WHEN line_item_usage_type LIKE '%%Lambda-Edge-GB-Second%%' THEN 'Lambda EDGE GB x Sec.' WHEN line_item_usage_type LIKE '%%Lambda-Edge-Request%%' THEN 'Lambda EDGE Requests' WHEN line_item_usage_type LIKE '%%Lambda-GB-Second%%' THEN 'Lambda GB x Sec.' WHEN line_item_usage_type LIKE '%%Request%%' THEN 'Lambda Requests' WHEN line_item_usage_type LIKE '%%In-Bytes%%' THEN 'Data Transfer (IN)' WHEN line_item_usage_type LIKE '%%Out-Bytes%%' THEN 'Data Transfer (Out)' WHEN line_item_usage_type LIKE '%%Regional-Bytes%%' THEN 'Data Transfer (Regional)' ELSE 'Other' END as UsageType, line_item_resource_id, pricing_term, SUM(CAST(line_item_usage_amount AS double)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS decimal(16,8))) AS sum_line_item_unblended_cost, sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0 WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0 WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;reservation_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (\u0026quot;reservation_unused_amortized_upfront_fee_for_billing_period\u0026quot; + \u0026quot;reservation_unused_recurring_fee\u0026quot;) WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN 0 ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) \u0026quot;amortized_cost\u0026quot; FROM ${table_name} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND product_product_name = 'AWS Lambda' AND line_item_line_item_type like '%%Usage%%' AND product_product_family IN ('Data Transfer', 'Serverless') AND line_item_line_item_type NOT IN ('Tax','Credit','Refund','EdpDiscount','Fee','RIFee') GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d'), product_region, line_item_usage_type, line_item_resource_id, pricing_term, line_item_line_item_type ORDER BY day_line_item_usage_start_date, sum_line_item_usage_amount, sum_line_item_unblended_cost ) UNION ( SELECT bill_payer_account_id, line_item_usage_account_id, line_item_line_item_type, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d') AS day_line_item_usage_start_date, product_region AS Region, CASE WHEN line_item_usage_type LIKE '%%Lambda-Edge-GB-Second%%' THEN 'Lambda EDGE GB x Sec.' WHEN line_item_usage_type LIKE '%%Lambda-Edge-Request%%' THEN 'Lambda EDGE Requests' WHEN line_item_usage_type LIKE '%%Lambda-GB-Second%%' THEN 'Lambda GB x Sec.' WHEN line_item_usage_type LIKE '%%Request%%' THEN 'Lambda Requests' WHEN line_item_usage_type LIKE '%%In-Bytes%%' THEN 'Data Transfer (IN)' WHEN line_item_usage_type LIKE '%%Out-Bytes%%' THEN 'Data Transfer (Out)' WHEN line_item_usage_type LIKE '%%Regional-Bytes%%' THEN 'Data Transfer (Regional)' ELSE 'Other' END as UsageType, line_item_resource_id, CASE savings_plan_offering_type WHEN 'ComputeSavingsPlans' THEN 'Compute Savings Plans' ELSE savings_plan_offering_type END AS ChargeType, SUM(CAST(line_item_usage_amount AS double)) AS sum_line_item_usage_amount, SUM(CAST(savings_plan_savings_plan_effective_cost AS decimal(16,8))) AS sum_savings_plan_savings_plan_effective_cost, sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0 WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0 WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;reservation_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (\u0026quot;reservation_unused_amortized_upfront_fee_for_billing_period\u0026quot; + \u0026quot;reservation_unused_recurring_fee\u0026quot;) WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN 0 ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) \u0026quot;amortized_cost\u0026quot; FROM ${table_name} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND product_product_name = 'AWS Lambda' AND product_product_family IN ('Data Transfer', 'Serverless') AND line_item_line_item_type NOT IN ('Tax','Credit','Refund','EdpDiscount','Fee','RIFee') GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d'), product_region, line_item_usage_type, line_item_resource_id, savings_plan_offering_type, line_item_line_item_type ORDER BY day_line_item_usage_start_date ASC, sum_line_item_usage_amount DESC ) ) AS aggregatedTable ORDER BY day_line_item_usage_start_date, sum_line_item_usage_amount, sum_line_item_unblended_cost;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         Elastic Load Balancing   Query Description This query will display cost and usage of Elastic Load Balancers which didn\u0026rsquo;t receive any traffic last month and ran for more than 336 hours (14 days). Resources returned by this query could be considered for deletion.\nPricing Please refer to the Elastic Load Balancing pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, SPLIT_PART(line_item_resource_id, ':', 6) split_line_item_resource_id, product_region, pricing_unit, sum_line_item_usage_amount, CAST(cost_per_resource AS decimal(16, 8)) AS \u0026quot;sum_line_item_unblended_cost\u0026quot; FROM ( SELECT line_item_resource_id, product_region, pricing_unit, line_item_usage_account_id, bill_payer_account_id, SUM(line_item_usage_amount) AS sum_line_item_usage_amount, SUM(SUM(line_item_unblended_cost)) OVER (PARTITION BY line_item_resource_id) AS cost_per_resource, SUM(SUM(line_item_usage_amount)) OVER (PARTITION BY line_item_resource_id, pricing_unit) AS usage_per_resource_and_pricing_unit, COUNT(pricing_unit) OVER (PARTITION BY line_item_resource_id) AS pricing_unit_per_resource FROM ${table_name} WHERE line_item_product_code = 'AWSELB' -- get previous month AND month = cast(month(current_timestamp + -1 * interval '1' MONTH) AS VARCHAR) -- get year for previous month AND year = cast(year(current_timestamp + -1 * interval '1' MONTH) AS VARCHAR) AND line_item_line_item_type = 'Usage' GROUP BY line_item_resource_id, product_region, pricing_unit, line_item_usage_account_id, bill_payer_account_id ) WHERE -- filter only resources which ran more than half month (336 hrs) usage_per_resource_and_pricing_unit \u0026gt; 336 AND pricing_unit_per_resource = 1 ORDER BY cost_per_resource DESC    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback       CUR queries are provided as is. We recommend validating your data by comparing it against your monthly bill and Cost Explorer prior to making any financial decisions. If you wish to provide feedback on these queries, there is an error, or you want to make a suggestion, please email: curquery@amazon.com\n "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_1_aws_account_setup/4_configure_sso/","title":"Enable Single Sign On (SSO)","tags":[],"description":"","content":"You will create an AWS Organization, and join two or more accounts to the management account. An organization will allow you to centrally manage multiple AWS accounts efficiently and consistently. It is recommended to have a management account that is used for security and administration, with access provided for limited billing tasks. A dedicated member account will be created for the Cost Optimization team or function, and another (or multiple) member account/s created to contain workload resources.\nYou will need organizations:CreateOrganization access, and 2 or more AWS accounts. When you join a member account to a management account, it will contain all billing information for that member account. Member accounts will no longer have any billing information, including historical billing information. Ensure you backup or export any reports or data before joining accounts to a management account.\nConfigure SSO You will create an AWS Organization with the management account.\n  Login to the AWS console as an IAM user with the required permissions, start typing SSO into the Find Services box and click on AWS Single Sign-On:   Click Enable AWS SSO:   Select Groups:   Click Create group:   Enter a Group name of Cost_Optimization and a description, click Create:   Click Users:   Click Add user:   Enter the following details:\n   Username Password Email address First name Last name Display name Configure the optional fields as required click Next: Groups:    Select the Cost_Optimization group and click Add user:   The user will receive an email, with a link to Accept invitation, the Portal URL and their Username:   When the user goes to the portal, they will enter in a Password and click Update user:   The user will then Click Continue:   Users will not have permissions until you complete the rest of this step.\n  Click on AWS accounts, select Permission sets, and click Create permission set:   Select Create a custom permission set, enter a name of management_CostOptimization, enter a Description, set the Session duration, select Create a custom permissions policy. Use the policy below as a starting point, modify it to your requirements and paste it in the policy field, click Create.\n  You MUST work with your security team/specialist to ensure you create the policies inline with least privileges for your organization.\n   Click here for Custom permissions policy   {\r\u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;,\r\u0026quot;Statement\u0026quot;: [\r{\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Action\u0026quot;: [\r\u0026quot;budgets:*\u0026quot;,\r\u0026quot;ce:*\u0026quot;,\r\u0026quot;aws-portal:*Usage\u0026quot;,\r\u0026quot;aws-portal:*PaymentMethods\u0026quot;,\r\u0026quot;aws-portal:*Billing\u0026quot;,\r\u0026quot;cur:DescribeReportDefinitions\u0026quot;,\r\u0026quot;cur:PutReportDefinition\u0026quot;,\r\u0026quot;cur:DeleteReportDefinition\u0026quot;,\r\u0026quot;cur:ModifyReportDefinition\u0026quot;,\r\u0026quot;pricing:DescribeServices\u0026quot;,\r\u0026quot;wellarchitected:*\u0026quot;,\r\u0026quot;savingsplans:*\u0026quot;\r],\r\u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;\r}\r]\r}\r    Click Create permission set\n  Select Create a custom permission set, enter a name of Member_CostOptimization, enter a Description, set the Session duration, select Create a custom permissions policy. Use the policy below as a starting point, modify it to your requirements, replace (management CUR bucket) and (Cost Optimization Member Account ID) and paste it in the policy field, click Create.\n  You MUST work with your security team/specialist to ensure you create the policies inline with least privileges for your organization.\n   Click here for Custom permissions policy   {\r\u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;,\r\u0026quot;Statement\u0026quot;: [\r{\r\u0026quot;Sid\u0026quot;: \u0026quot;CostServices\u0026quot;,\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Action\u0026quot;: [\r\u0026quot;ce:*\u0026quot;,\r\u0026quot;budgets:*\u0026quot;,\r\u0026quot;aws-portal:*Usage\u0026quot;,\r\u0026quot;aws-portal:*PaymentMethods\u0026quot;,\r\u0026quot;aws-portal:*Billing\u0026quot;,\r\u0026quot;pricing:DescribeServices\u0026quot;,\r\u0026quot;wellarchitected:*\u0026quot;,\r\u0026quot;savingsplans:*\u0026quot;\r],\r\u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;\r},\r{\r\u0026quot;Sid\u0026quot;: \u0026quot;S3ManagementCUR\u0026quot;,\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Action\u0026quot;: [\r\u0026quot;s3:GetObject\u0026quot;,\r\u0026quot;s3:ListBucket\u0026quot;\r],\r\u0026quot;Resource\u0026quot;: [\r\u0026quot;arn:aws:s3:::(management CUR bucket)\u0026quot;\r]\r},\r{\r\u0026quot;Sid\u0026quot;: \u0026quot;AthenaGlueAndServiceReadAccess\u0026quot;,\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Action\u0026quot;: [\r\u0026quot;athena:*\u0026quot;,\r\u0026quot;glue:*\u0026quot;,\r\u0026quot;iam:ListRoles\u0026quot;,\r\u0026quot;iam:ListPolicies\u0026quot;,\r\u0026quot;s3:GetBucketLocation\u0026quot;,\r\u0026quot;s3:ListAllMyBuckets\u0026quot;\r],\r\u0026quot;Resource\u0026quot;: [\r\u0026quot;*\u0026quot;\r]\r},\r{\r\u0026quot;Sid\u0026quot;: \u0026quot;QuickSightAccess\u0026quot;,\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Action\u0026quot;: [\r\u0026quot;quicksight:CreateUser\u0026quot;,\r\u0026quot;quicksight:Subscribe\u0026quot;\r],\r\u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;\r},\r{\r\u0026quot;Sid\u0026quot;: \u0026quot;IAMAccessForGlue\u0026quot;,\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Action\u0026quot;: \u0026quot;iam:*\u0026quot;,\r\u0026quot;Resource\u0026quot;: [\r\u0026quot;arn:aws:iam::(Cost Optimization Member Account ID):role/service-role/AWSGlueServiceRole-Cost*\u0026quot;,\r\u0026quot;arn:aws:iam::(Cost Optimization Member Account ID):policy/service-role/AWSGlueServiceRole-Cost*\u0026quot;\r]\r},\r{\r\u0026quot;Sid\u0026quot;: \u0026quot;S3AccessForAthena\u0026quot;,\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Action\u0026quot;: [\r\u0026quot;s3:GetBucketLocation\u0026quot;,\r\u0026quot;s3:GetObject\u0026quot;,\r\u0026quot;s3:ListBucket\u0026quot;,\r\u0026quot;s3:ListBucketMultipartUploads\u0026quot;,\r\u0026quot;s3:ListMultipartUploadParts\u0026quot;,\r\u0026quot;s3:AbortMultipartUpload\u0026quot;,\r\u0026quot;s3:CreateBucket\u0026quot;,\r\u0026quot;s3:PutObject\u0026quot;\r],\r\u0026quot;Resource\u0026quot;: [\r\u0026quot;arn:aws:s3:::aws-athena-query-results-*\u0026quot;\r]\r},\r{\r\u0026quot;Sid\u0026quot;: \u0026quot;FullS3AccessForBucketsWithSpecificPrefix\u0026quot;,\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Action\u0026quot;: \u0026quot;s3:*\u0026quot;,\r\u0026quot;Resource\u0026quot;: [\r\u0026quot;arn:aws:s3:::cost*\u0026quot;\r]\r}\r]\r}\r    Click AWS organization, select the management account, click Assign users:   Select Groups, select the Cost_Optimization group, click Next: Permission sets:   Select the management_CostOptimization Permission set, click Finish:   Click Proceed to AWS accounts:   setup the Cost Optimization member account, select the Memeber account, click Assign users\n  Select Groups, select the Cost_Optimization group, click Next: Permission sets:   Select the Member_CostOptimization Permission set, click Finish\n  Click Proceed to AWS accounts\n  You have now setup your Cost Optimization users, group and their permissions.\n  function prevStep(){ window.open(\"..\\/3_cur\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/5_account_settings\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_enterprise_dashboards/4_distribute_dashboards/","title":"Distribute Dashboards","tags":[],"description":"","content":"Authors  Nathan Besh, Cost Lead Well-Architected (AWS)  You now have a set of dashboards to provide insight and assist with analysis. The most effective way to allow users to access and work with the dashboards is to create users in QuickSight, and provide access to the dashboard. This will provide full access to all the most recent data, enable the use of features such as filters to dive deep into the data and perform analysis work..\nThis step will look at ways to distribute the content to users, which can be an effective reminder - for example, weekly updates to ensure people are checking their dashboards and tracking progress towards goals.\nConfigure email reports We will configure a weekly email report of the Cost Intelligence dashboard. This will ensure that all relevant parties within your organization have the access and visibility to the information they need.\nAn email report sends the first page of a dashboard only. You can change the front page of an analysis, re-save it as a dashboard, and then create the report to send out different reports via email.\n   Login to QuickSight\n  Select All dashboards and click on the Cost Intelligence dashboard\n  Click on Share, then Share dashboard   Ensure all the required users have access to the dashboard:   Click on Share, then Email report:   Create the Schedule, the Text and report preferences, add recipients and click Save report:   If required, modify the analysis and re-save it as a dashboard, and create additional email reports.\n  You have successfully created an email report.\n  function prevStep(){ window.open(\"..\\/3_create_data_transfer_cost_analysis\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/5_tear_down\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_organization_data_cur_connection/4_visualize_organization_data_in_quicksight/","title":"Visualize Organization Data in QuickSight","tags":[],"description":"","content":"Join your AWS Organizations data with the AWS Cost \u0026amp; Usage Report in Amazon Quicksight In Amazon Quicksight we will add to the existing AWS Cost \u0026amp; Usage Report from the Cost Visualization lab.\n Log on to the console via SSO, go to the QuickSight service, Enter your email address and click Continue:  Click Manage data in the top right:  In your data set click on the Cost \u0026amp; Usage dataset you have already created. Select Edit Data Set  On the Top Left of the screen click Add Data  In the pop up select your organisation_data and click Select  Now your data has been added we can setup the join. Click on the two circles connecting the two data sets. Below you will see two down downs where we can select the joins.  The left box should be your Cost and Usage Report. Under that select the drop down bock and choose line_item_usage_account_id. In the right box under the Organizations_data select account_number. On the right select Full Join. Then click Apply.  You can save this as a new data set by changing the name at the top to Cost_and_Usage_Data_with_Org. You then click on Save.  Let\u0026rsquo;s setup a Schedule refresh for the data. Click on the QuickSight Icon in the top left to bring you back to the home page. Then click on Datasets on the left and find your dataset you just created. Click on the name then select Schedule refresh the click Create.  Choose your Time zone and select Weekly. Selecting the Starting day as tomorrow so you match the 7 days set in the Amazon CloudWatch Event. Click Create and then the Exit button.   Click on your dataset again and click Create analysis.   Now you will be taken to a new dashboard. On the left you can see some of the column names we have addedd such as account_name.\n  On the top of the screen click on Field wells and pull in the following:\n   Drag in account_name into Y axis line_itemunblended_cost into Value (ensure the Aggregate is Sum).  The visuale below will show you your spend by account name.\nCongratulations - QuickSight is now setup for your users to see the account names and other details in your dashbaords.\n  function prevStep(){ window.open(\"..\\/3_utilize_organization_data_source\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/5_join_cost_intelligence_dashboard\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_automated_deployment_of_vpc/","title":"Level 200: Automated Deployment of VPC","tags":[],"description":"","content":"Last Updated: July 2020\nAuthors: Ben Potter, Security Lead, Well-Architected\nIntroduction This hands-on lab will use AWS CloudFormation to create an Amazon VPC to outline some of the AWS security features available. Using CloudFormation to automate the deployment provides a repeatable way to create and update, and you can re-use the template after this lab.\nThe example template will deploy a completely new VPC incorporating a number of AWS security best practices which include:\nNetworking subnets created in 3 availability zones for the following network tiers:\n Application Load Balancer - named ALB1 Application instances - named App1 Shared services - named Shared1 Database - named DB1  VPC Architecture:  VPC endpoints are created for private connectivity to AWS services. Additional endpoints can be enabled for the application tier using the App1SubnetsPrivateLinkEndpoints CloudFormation parameter. NAT Gateways are created to allow subnets in the VPC to connect to the internet, without any direct ingress access as defined by the Route Table . Network ACLs control access at each subnet tier. VPC Flow Logs captures information about IP traffic and stores it in Amazon CloudWatch Logs.  Requirements  An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account with access to CloudFormation, EC2, VPC, IAM. Basic understanding of AWS CloudFormation , visit the Getting Started section of the user guide.  NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . It is recommended to delete the CloudFormation stack when you have completed the lab.\n Steps:  Create VPC Stack   Tear Down   References \u0026amp; useful resources  Well-Architected: Protecting Networks  AWS CloudFormation User Guide  Amazon VPC User Guide  Security in Amazon Virtual Private Cloud   "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_automated_deployment_of_vpc/2_cleanup/","title":"Tear Down","tags":[],"description":"","content":"The following instructions will remove the resources that you have created in this lab.\nNote: If you are planning on completing the lab 200_Automated_Deployment_of_EC2_Web_Application we recommend you only tear down this lab after completing both, as there is a dependency on this VPC.\nDelete the VPC CloudFormation stack:\n Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Click the radio button on the left of the WebApp1-VPC stack. Click the Actions button then click Delete stack. Confirm the stack and then click Delete button.  Delete the CloudWatch Logs:\n Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudwatch/ . Click Logs in the left navigation. Click the radio button on the left of the WebApp1-VPC-VPCFlowLogGroup-\u0026lt;some unique ID\u0026gt;. Click the Actions Button then click Delete Log Group. Verify the log group name then click Yes, Delete.   "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_clock_source_performance/4_cleanup/","title":"Teardown","tags":[],"description":"","content":"In this lab, you created two different EC2 instances and tested gettime system calls to each on to test the performance for each clocksource type. You were able to set a new clocksource for a Xen based instance type and see a dramatic improvement in the time it takes for these kinds of system calls.\nRemove all the resources via CloudFormation In order to remove the lab, go into the CloudFormation console, select the deployed template, click the drop down next to Create Stack and then click Delete Stack. This should remove all components created for this Lab.\nReferences \u0026amp; useful resources  AWS Systems Manager  Nitro  How to manage the clock source for EC2 instances running Linux    function prevStep(){ window.open(\"..\\/3_change_clock\\/\", \"_self\") } function nextStep(){ window.open(\"\\/cost\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with PERF2 - \u0026ldquo;Understand the available compute configuration options.\u0026quot;\nClick here to access the Well-Architected Tool   Previous Step Complete this lab     "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_aws_resource_optimization/4_act_resource_opt/","title":"Action the recommendations","tags":[],"description":"","content":"During this lab exercise, we learned how to prioritize the rightsizing recommendations with the goal of identifying low complexity and high savings recommendations. We initially started with 2,534 recommendations with a potential saving of $86,627 but we managed to identify the top 60 cases with lowest complexity that together add up to $14,699.56 of the overall potential saving.\nGroup 1 (Idle) and Group 2 (Previous Generation) are the less complex cases where you may want to start the rightsizing exercises for your organization. As you gain more confidence and learn how to develop a regular process for rightsizing, your organization will be able to rapidly act on Group 3 (Current/modern generation) and other cases.\n function prevStep(){ window.open(\"..\\/3_prio_resource_opt\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/5_ec2_rs_best_practices\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_deploy_and_update_cloudformation/4_add_s3/","title":"Add an Amazon S3 Bucket to the Stack","tags":[],"description":"","content":"In this task, you will gain experience in editing a CloudFormation template and updating your CloudFormation stack\n Your objective is to deploy a new Amazon S3 bucket  3.1 Edit the CloudFormation template file  From the Amazon S3 Template Snippets documentation page, copy the YAML example for Creating an Amazon S3 Bucket with Defaults Edit the simple_stack.yaml file you downloaded earlier to include an Amazon S3 bucket  Under the Resources section add the snippet you copied You do not require any Properties for this new S3 bucket resource Indents are important in YAML \u0026ndash; use two spaces for each indent. Look at the other resources for guidance The correct solution only needs two lines \u0026ndash; one for the Logical ID and one for the Type Save the template    Once you have edited the template, continue with the following steps to update the stack.\n3.2 Update the CloudFormation stack - specify updated template  Go to the AWS CloudFormation console  Click on Stacks Click on the CloudFormationLab stack Click Update Now click Replace current template selected. You are replacing the template again. Click Upload a template file Click Choose file  Select simple_stack.yaml, your edited CloudFormation template file   Click Next   At this point you may see an error where you remain on the Update stack screen and a red banner across the top of the page displays an error message If you see Template format error then:  Check the indentation and punctuation in your simple_stack.yaml file Once you have corrected the error, click Choose file again to reload you new corrected file    If you did not see an error you may proceed\n3.3 Update the CloudFormation stack - complete the deployment  On the Specify stack details click Next Click Next again, until you arrive at the Review CloudFormationLab screen  Scroll down to Change set preview and note your S3 bucket is the only resource being added At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Create stack   When stack status is CREATE_COMPLETE for your update (about one minute) then continue Click the Resources tab  Note your new S3 bucket is listed among the resources deployed Click on the Physical ID of the S3 bucket to view the bucket on the S3 console Note the name is cloudformationlab-mys3bucket-\u0026lt;some_random_string\u0026gt;.    The name for the S3 bucket was auto-generated by CloudFormation based on your CloudFormation stack name (converted to lowercase), plus the string \u0026ldquo;mys3bucket\u0026rdquo;, plus a randomly generated string.\n The name for an S3 bucket must be unique across all S3 buckets in AWS Your bucket was assigned an auto-generated name because you did not specify a name in the S3 bucket properties in your CloudFormation template In the next exercise you will add a bucket name property for your S3 bucket and update the deployment  3.4 Assign name property for the S3 bucket For this task you are going to specify a Parameter where you can set the bucket name. To do this you will add a property on the S3 bucket resource that uses this parameter.\n  Under the Parameters section of your simple_stack.yaml template look at the S3BucketName parameter\n  It is not currently used in the template\n # S3 Bucket S3BucketName: Type: String Description: The name for the S3 bucket - must be unique across all of AWS (3-63 lowercase letters or numbers) Default: replaceme AllowedPattern: '^[a-z0-9]{5,40}$' ConstraintDescription: 3-63 characters; must contain only lowercase letters or numbers    It is a string for which we have configured certain constraints\n  The AllowedPattern is a regular expression specifying only lowercase letters or numbers and a string length between 3-63 characters\n  This satisfies the constraints on what is allowed in an S3 bucket name\n  It is actually more constrictive than what is allowed. See Rules for Bucket Naming under Bucket Restrictions and Limitations for more details.\n    Add two more lines to your S3 bucket under in the Resources section of your template so it looks like this\n  Be caution to maintin the two-space indents where indicated\n MyS3Bucket: Type: AWS::S3::Bucket Properties: BucketName: !Ref S3BucketName    The Properties label defines that the items that follow (indented underneath) are properties of the S3 bucket\n  For the BucketName property you are specifying a reference to another value in the template. Specifically you are indicating that the string entered as the S3BucketName parameter should be used as the name of the bucket\n    Go to the AWS CloudFormation console   Click on Stacks\n  Click on the CloudFormationLab stack\n  Click Update\n  Now click Replace current template selected. This is different from what you did for the last update.\n  Click Upload a template file\n  Click Choose file\n Select simple_stack.yaml, your edited CloudFormation template file    Click Next \u0026ndash; Look for any errors reported\n  On the Specify stack details look at the Parameters\n You must enter a value for S3BucketName (you must replace the deafult value) Remember it must be a name that no other bucket in all of AWS is already using    Click Next again, until you arrive at the Review CloudFormationLab screen\n Scroll down to Change set preview and note your S3 bucket will be modified Note where it says Replacement is True. This means it will actually delete the current bucket and replace it with a new one with the newly specified name At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Create stack    When stack status is CREATE_COMPLETE for your update (about one minute) then continue\n   Under the resources see your newly named S3 bucket  Troubleshooting\n If when trying to upload your new template you see Invalid template resource property  Check that the properties you specified for the resource you added match the properties in the documentation. Once you have corrected the error, click Choose file again to reload you new corrected file   If your CloudFormation stack fails, then click on the Events tab and scroll down to find the source of the error  If you see a message like \u0026lt;your_chosen_bucket_name\u0026gt; already exists then re-do the CloudFormation update steps, but specify a more unique bucket name     function prevStep(){ window.open(\"..\\/3_cfn_params\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/5_add_ec2\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_100_loft_introduction_to_security/4_automated_detective_controls/","title":"Automated Deployment of Detective Controls","tags":[],"description":"","content":"This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of AWS CloudTrail.\nWalkthrough Only complete step 2, GuardDuty from: Automated Deployment of Detective Controls "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/200_labs/200_automating_operations_with_playbooks_and_runbooks/4_build_execute_investigative_playbook/","title":"Build &amp; Execute Investigative Playbook","tags":[],"description":"","content":"In the previous section we have built a sample vpc environment with a deployed our application API running in it. We then sent through large incoming traffic to the API to simulating an increase with the Application latency driven by the application being overwhelmed. For a seasoned systems administrator / engineer, who knows the ins and outs of the application architecture and it\u0026rsquo;s general behaviour, investigating latency issue might be a relatively simple task to do. He / She would already know which service or components are involved in the application, and which metrics and data matters and which one don\u0026rsquo;t. Over time these administrator / engineer would build a certain muscle memory around this and they would instinctively be able to perform these tasks when issue occurred.\nThe problem with this is that all those information and knowledge is contained within the individual admin / engineer, passing on the knowlegde and methodology invovled can be very hard to do. This is where a playbooks comes into place. Playbooks are essentially predefined steps to perform to identify an issue. The results from any process step are used to determine the next steps to take until the issue is identified or escalated.\nThis could be just as simple as a wiki page with instructions or guides that a non seasoned admin / engineer can follow to investigate the issue. But to be able to increase scalability, reliability and time taken to execute this playbook, you\u0026rsquo;d want to look into automating this playbook as much as possible. There are various different tools you can use to build automated playbook, but in AWS we have AWS Systems Manager Automation Document that you could utilize to build this. The service allows you to build a series of executable steps to orchestrate your investigation into the issue, you can execute python / nodejs script, call the api of AWS service directly, or execute a remote command into the operating system where your application is running (if your workload runs on EC2).\nSo, in this section we will focus on how we can troubleshoot the issue with our sample application using an automated playbook we build in AWS Systems Manager Automation Document.\n4.0 Prepare playbook IAM Role \u0026amp; SNS Topic Instructions to be added\n4.1 Playbook Build - Gathering resources in sample application. As an administrator / engineer, before we even investigate anything in the application, we need to know what are the services / components involved. When we receive the alarm from cloudwatch in the inbox, the information presented does not contain straight away these components / services involved. So the first thing we need to do is to build a playbook step to acquire all the related resources using information tha tis contained in the alarm. please follow below steps to continue.\nNote: For the following step to build and run playbook. You can follow a step by step guide via AWS console or you can deploy a cloudformation template to build the playbook.\n   Click here for Console step by step   To build our playbook, go ahead and go to the AWS Systems Manager console, from there click on documents to get into the page as per screen shot.\nOnce you are there, click on Create Automation\nNext, enter in Playbook-GatherAppResources-Canary-CloudWatchAlarm in the Name and past in below notes in the Description box. This is to provide descriptions on what this playbook does.\n # What is does this playbook do? This playbook will query the CloudWatch Synthetics Canary, and look for all resources related to the application based on it's Application Tag. This playbook takes an input of the CloudWatch Alarm ARN triggered by the canary Note : Application resources must be deployed using CloudFormation and properly tagged accordingly. ## Steps taken in the code ### Step 1 1. Describe CloudWatch Alarm ARN, and identify the Canary resource. 2. Describe the Canary resource to gather the value of 'Application' tag 3. Gather Cloudformation Stack with the same value of 'Application' tag. 4. List all resources in Cloudformation Stack. 5. Parse list of resources into String Output. Systems Manager supports putting in nots as markdown, so feel free to format it as needed.\nUnder Assume role field, enter in the ARN of the IAM role we created in the previous step.\nUnder Input Parameters field, enter AlarmARN as the Parameter name, set the type as String and Required as Yes. This will essentially define a Parameter into our playbook, so that we can pass on the value of the CloudWatch Alarm to the main step that will do the action.\nRight under Add Step section enter Gather_Resources_For_Alarm under the Step name, select aws::executeScript as the Action type.\nUnder Inputs set Python3.6 as the Runtime, and specify script_handler as the Handler.\nPaste in below python codes into the Script section.\nimport json import re from datetime import datetime import boto3 import os def arn_deconstruct(arn): arnlist = arn.split(\u0026quot;:\u0026quot;) service=arnlist[2] region=arnlist[3] accountid=arnlist[4] servicetype=arnlist[5] name=arnlist[6] return { \u0026quot;Service\u0026quot;: service, \u0026quot;Region\u0026quot;: region, \u0026quot;AccountId\u0026quot;: accountid, \u0026quot;Type\u0026quot;: servicetype, \u0026quot;Name\u0026quot;: name } def locate_alarm_source(alarm): cwclient = boto3.client('cloudwatch', region_name = alarm['Region'] ) alarm_source = {} alarm_detail = cwclient.describe_alarms(AlarmNames=[alarm['Name']]) if len(alarm_detail['MetricAlarms']) \u0026gt; 0: metric_alarm = alarm_detail['MetricAlarms'][0] namespace = metric_alarm['Namespace'] # Condition if NameSpace is CloudWatch Syntetics if namespace == 'CloudWatchSynthetics': if 'Dimensions' in metric_alarm: dimensions = metric_alarm['Dimensions'] for i in dimensions: if i['Name'] == 'CanaryName': source_name = i['Value'] alarm_source['Type'] = namespace alarm_source['Name'] = source_name alarm_source['Region'] = alarm['Region'] alarm_source['AccountId'] = alarm['AccountId'] result = alarm_source return result def locate_canary_endpoint(canaryname,region): result = None synclient = boto3.client('synthetics', region_name = region ) res = synclient.get_canary(Name=canaryname) canary = res['Canary'] if 'Tags' in canary: if 'TargetEndpoint' in canary['Tags']: target_endpoint = canary['Tags']['TargetEndpoint'] result = target_endpoint return result def locate_app_tag_value(resource): result = None if resource['Type'] == 'CloudWatchSynthetics': synclient = boto3.client('synthetics', region_name = resource['Region'] ) res = synclient.get_canary(Name=resource['Name']) canary = res['Canary'] if 'Tags' in canary: if 'Application' in canary['Tags']: apptag_val = canary['Tags']['Application'] result = apptag_val return result def locate_app_resources_by_tag(tag,region): result = None # Search CloufFormation Stacks for tag cfnclient = boto3.client('cloudformation', region_name = region ) list = cfnclient.list_stacks(StackStatusFilter=['CREATE_COMPLETE','ROLLBACK_COMPLETE','UPDATE_COMPLETE','UPDATE_ROLLBACK_COMPLETE','IMPORT_COMPLETE','IMPORT_ROLLBACK_COMPLETE'] ) for stack in list['StackSummaries']: app_resources_list = [] stack_name = stack['StackName'] stack_details = cfnclient.describe_stacks(StackName=stack_name) stack_info = stack_details['Stacks'][0] if 'Tags' in stack_info: for t in stack_info['Tags']: if t['Key'] == 'Application' and t['Value'] == tag: app_stack_name = stack_info['StackName'] app_resources = cfnclient.describe_stack_resources(StackName=app_stack_name) for resource in app_resources['StackResources']: app_resources_list.append( { 'PhysicalResourceId' : resource['PhysicalResourceId'], 'Type': resource['ResourceType'] } ) result = app_resources_list return result def script_handler(event, context): result = {} arn = event['CloudWatchAlarmARN'] alarm = arn_deconstruct(arn) # Locate tag from CloudWatch Alarm alarm_source = locate_alarm_source(alarm) # Identify Alarm Source tag_value = locate_app_tag_value(alarm_source) #Identify tag from source if alarm_source['Type'] == 'CloudWatchSynthetics': endpoint = locate_canary_endpoint(alarm_source['Name'],alarm_source['Region']) result['CanaryEndpoint'] = endpoint # Locate cloudformation with tag resources = locate_app_resources_by_tag(tag_value,alarm['Region']) result['ApplicationStackResources'] = json.dumps(resources) return result Under Additional inputs specify input value to the step passing in the parameter we created before. to do this, select Input Payload under Input name and specify CloudWatchAlarmARN: '{{AlarmARN}}' as the Input Value. the \u0026lsquo;{{AlarmARN}}\u0026rsquo; section essentially references the parameter value we created before. Within the same section, specify the outputs of the step. Resources as the Name $.Payload.ApplicationStackResources as the Selector and String as the Type.\nFor more information about Automation Document syntax, please refer here Once your setting looks as per screen shot below, click on Create Automation\n    Click here for CloudFormation Console deployment step   Download the template here. If you decide to deploy the stack from the console, ensure that you follow below requirements \u0026amp; step:\n Please follow this guide for information on how to deploy the cloudformation template. Use waopslab-playbook-gather-resources as the Stack Name, as this is referenced by other stacks later in the lab.      Click here for CloudFormation CLI deployment step   Download the template here. To deploy from the command line, ensure that you have installed and configured AWS CLI with the appropriate credentials.\naws cloudformation create-stack --stack-name waopslab-playbook-gather-resources \\ --parameters ParameterKey=PlaybookIAMRole,ParameterValue=\u0026lt;ARN of Playbook IAM role (defined in previous step)\u0026gt; \\ --template-body file://playbook_gather-resources.yml Example:\naws cloudformation create-stack --stack-name waopslab-playbook-gather-resources \\ --parameters ParameterKey=arn:aws:iam::000000000000:role/xxxx-playbook-role \\ --template-body file://playbook_gather-resources.yml Note: Please adjust your command-line if you are using profiles within your aws command line as required.\nConfirm that the stack has installed correctly. You can do this by running the describe-stacks command as follows:\naws cloudformation describe-stacks --stack-name waopslab-playbook-gather-resources Locate the StackStatus and confirm it is set to CREATE_COMPLETE\n  Once the document is using any of the above method created, you can find the newly created document under the Owned by me tab of the Document resource, click on the playbook called Playbook-GatherAppResources-Canary-CloudWatchAlarm and click on Execute Automation to execute our playbook\nPaste in the Cloudwatch Alarm ARN (you can refer to the email you received from the simulation we did on section 3), and click on Execute\nYou can observe the playbook executing, and once it is completed, you can click on the Step Id to see the final message. output of the step. And in our case, the step had executed the python script to find the resources related to the application, using CloudWatch Alarm ARN. and once it is successful, you should be able to see this output listing all the resources of the application\nCopy the Resources list output from the section in the screenshot above ( marked in red box ) Paste the output into a temporary location, as we will need this value for our next step.\n4.2 Playbook Build - Gathering Information from relevant Application resources. Now that have have defined a playbook that captures all related AWS resources in the application, the next thing we want to do is to investigate relevant resources and capture the most recent statistics, logs to look for insights and better understand the cause of the issue.\nIn practice there can be various permutations on what resources you will look at depending on the context of the issue. But in this scenario, we will be looking specifically at the Elastic Load Balancer, the Elastic Compute Service, and Relational Database Statistics and logs.\nWe will then highlight the metrics that is considered outside the threshold.\nPlease follow below instructions to build this playbook.\nNote: For the majority, the tasks to be done this step will be identical with our previous step, we will just be passing in different scripts in each of the step to query the related services and gather it\u0026rsquo;s data. Therefore to reduce the repetition in the steps we will deploy this playbook via cloudformation template Please follow the steps below to deploy via cloudformation template via CLI / or Console.\n   Click here for CloudFormation Console deployment step   Download the template here. If you decide to deploy the stack from the console, ensure that you follow below requirements \u0026amp; step:\n Please follow this guide for information on how to deploy the cloudformation template. Use waopslab-playbook-investigate-resources as the Stack Name, as this is referenced by other stacks later in the lab.      Click here for CloudFormation CLI deployment step   Download the template here. To deploy from the command line, ensure that you have installed and configured AWS CLI with the appropriate credentials.\naws cloudformation create-stack --stack-name waopslab-playbook-investigate-resources \\ --parameters ParameterKey=PlaybookIAMRole,ParameterValue=\u0026lt;ARN of Playbook IAM role (defined in previous step)\u0026gt; \\ --template-body file://playbook_investigate_application_resources.yml Example:\naws cloudformation create-stack --stack-name waopslab-playbook-investigate-resources \\ --parameters ParameterKey=arn:aws:iam::000000000000:role/xxxx-playbook-role \\ --template-body file://playbook_investigate_application_resources.yml Note: Please adjust your command-line if you are using profiles within your aws command line as required.\nConfirm that the stack has installed correctly. You can do this by running the describe-stacks command as follows:\naws cloudformation describe-stacks --stack-name waopslab-playbook-investigate-resources Locate the StackStatus and confirm it is set to CREATE_COMPLETE\n  Once the document is using any of the above method created, you can find the newly created document under the Owned by me tab of the Document resource, click on the playbook called Playbook-InvestigateAppResources-ELB-ECS-RDS and click on Execute Automation to execute our playbook\nPaste in the Resources List you took note from the output of the previous playbook (refer to previous step) under Resources and click on Execute\nWait until all steps are completed successfully.\nAt this point because this playbook are meant to capture the most recent data points of the application, for the output to make sense, you the application simulation in the previous section needs to be running. To have a meaningful data captured by this playbook, Please ensure to have the simulation command running when you execute this playbook.\n4.3 Playbook Build - Chaining 2 Playbooks into 1. So far we have 2 separate playbooks that does 2 different things, one is to gather the list of resources of the application, and the other is to go through relevant resources and investigate it\u0026rsquo;s logs and statistics.\nEach of this playbook can be kept as a repeatable artefact that is re-usable for multiple different purposes. That said, executing these 2 separate steps and manually passing the output as an input to the other playbook is a tedious task, and it requires human intervention.\nTherefore in this step we will automate our playbook further by creating a parent playbook that orchestrates the 2 Investigative playbook subsequently, additionally we will also inject another step to send notification to our Developers and System Owners to notify of this issue.\nFollow below instructions to build the Playbook.\n   Click here for CloudFormation CLI deployment step       Click here for CloudFormation Console deployment step       Click here for Console step by step   To build our playbook, go ahead and go to the AWS Systems Manager console, from there click on documents to get into the page as per screen shot.\nOnce you are there, click on Create Automation\nNext, enter in Playbook-InvestigateApplication-From-CanaryCloudWatchAlarm in the Name and past in below notes in the Description box. This is to provide descriptions on what this playbook does.\n# What is does this playbook do? This playbook will execute **Playbook-GatherAppResources-Canary-CloudWatchAlarm** to gather Application resources monitored by Canary. Then subsequently execute **Playbook-InvestigateAppResources-ELB-ECS-RDS** to Investigate the resources for issues. Outputs of the investigation will be sent to SNS Topic Subscriber Systems Manager supports putting in nots as markdown, so feel free to format it as needed.\nUnder Assume role field, enter in the ARN of the IAM role we created in the previous step.\nUnder Input Parameters field, enter AlarmARN as the Parameter name, set the type as String and Required as Yes. This will define a Parameter into our playbook, so that we can pass on the value of the CloudWatch Alarm to the main step that will do the action.\nAdd another parameter by clicking on the Add a parameter link. Enter SNSTopicARN as the Parameter name, set the type as String and Required as Yes. This will define another Parameter into our playbook, so that we can send notification to the Owner and Developer.\nClick Add Step and create the first step of aws:executeAutomation Action type with StepName PlaybookGatherAppResourcesCanaryCloudWatchAlarm Specify Playbook-GatherAppResources-Canary-CloudWatchAlarm as the Document name under Inputs, and under Additional inputs specify RuntimeParameters with AlarmARN:'{{AlarmARN}}' as it\u0026rsquo;s value ( refer to screenshot below )\nThis essentially defines that in this step we will be executing the first playbook we created which takes input of the CloudWatch AlarmARN and returns the list of related resources.\nOnce this step is defined, add another step by clicking on Add Step at the bottom of the section.\nFor this second step, specify the Step name as PlaybookInvestigateAppResourcesELBECSRDS abd action type aws:executeAutomation. Specify Playbook-InvestigateAppResources-ELB-ECS-RDS as the Document name, and RuntimeParameters as Resources: '{{PlaybookGatherAppResourcesCanaryCloudWatchAlarm.Output}}'\nThis second step will take the output of the first step and pass that to the second Playbook to execute the investigation of relevant resources.\nNow for the last step, we want to take the output investigation from the second step and send that to the SNS topic where our owner, developers, and admin are subscribed.\nspecify the Step name as AWSPublishSNSNotification abd action type aws:executeAutomation. Specify AWS-PublishSNSNotification as the Document name, and RuntimeParameters as below\nTopicArn: '{{SNSTopicARN}}' Message: '{{ PlaybookInvestigateAppResourcesELBECSRDS.Output }}' This last step will take the output of the second step which contains summary data of the investigation and AWS-PublishSNSNotification which will send an email to the SNS we specified in the parameters.\n \n4.4 Playbook Test. Now that we have built our Playbook to Investigate this issue, lets test this in a simulated scenario.\nGo back to your Cloud9 terminal you created in Section 2, execute the command to send traffic to the application.\nALBURL=\u0026quot;\u0026lt; Application Endpoint URL captured from section 2\u0026gt;\u0026quot; ab -p test.json -T application/json -c 3000 -n 60000000 -v 4 http://$ALBURL/encrypt Leave it running for about 2-3 minutes, and wait until the notification email from the alarm arrives.\nOnce the alarm notification email arrived, capture the CloudWatch Alarm ARN\nThen go to the Systems Manager Automation document we just created in the previous step, and execute the playbook passing the ARN as the AlarmARN input value, along with the SNSTopicArn\nWait until the playbook is successfully executed\nOne it is done, you should see an email coming through to your email. This email will contain summary of the investigation done by our playbook.\nCopy and paste the message section, and use a json linter tool such as jsonlint.com to give the json better structure for visibility.\nNow let\u0026rsquo;s see what our playbook has found.\n END OF SECTION 4\n "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_aws_resource_optimization/4_memory_plugin/","title":"Cloudwatch Agent Manual Install","tags":[],"description":"","content":"  We are now going to manually install the CloudWatch agent to start collecting memory data, to start let\u0026rsquo;s go back to the Amazon EC2 Dashboard.   On the left bar, click on Instances and select the EC2 Instance with the CloudWatchAgentServerRole IAM role.   Connect into the EC2 Instance using the browser-based SSH connection tool.   Download the Amazon Cloudwatch agent package, the instructions below are for Amazon Linux, for other OS please check here   wget https://s3.amazonaws.com/amazoncloudwatch-agent/linux/amd64/latest/AmazonCloudWatchAgent.zip Unzip and Install the package  unzip AmazonCloudWatchAgent.zip sudo ./install.sh Configure the AmazonCloudWatchAgent profile  Before running the CloudWatch agent on any servers, you must create a CloudWatch agent configuration file, which is a JSON file that specifies the metrics and logs that the agent is to collect, including custom metrics. You can create it by using the wizard or by writting it yourself from scratch. Any time you change the agent configuration file, you must then restart the agent to have the changes take effect.\nThe wizard can autodetect the credentials and AWS Region to use if you have the AWS credentials and configuration files in place. For more information about these files, see Configuration and Credential Files in the AWS Systems Manager User Guide and the AWS documentation page .\nFor now, let\u0026rsquo;s start the CloudWatch agent configuration file wizard executing the command below at the selected EC2 instance.\nsudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-config-wizard For this lab we want to keep the following structure:\n   CloudWatch Agent Configutation File Wizard Parameter     On which OS are you planning to use the agent? 1. Linux   Are you using EC2 or On-Premises hosts? 1. EC2   Which user are you planning to run the agent? 2. cwagent   Do you want to turn on StatsD daemon? 2. No   Do you want to monitor metrics from CollectD? 2. No   Do you want to monitor any host metrics? 1. Yes   Do you want to monitor cpu metrics per core? 2. No   Do you want to add ec2 dimensions? 1. Yes   Would you like to collect your metrics at high resolution? 4. 60s   Which default metrics config do you want? 1. Basic   Are you satisfied with the above config? 1. Yes   Do you have any existing CloudWatch Log Agent? 2. No   Do you want to monitor any log files? 2. No   Do you want to store the config in the SSM parameter store? 2. No    The CloudWatch Agent config file should look like the following:\n{ \u0026quot;agent\u0026quot;: { \u0026quot;metrics_collection_interval\u0026quot;: 60, \u0026quot;run_as_user\u0026quot;: \u0026quot;cwagent\u0026quot; }, \u0026quot;metrics\u0026quot;: { \u0026quot;append_dimensions\u0026quot;: { \u0026quot;AutoScalingGroupName\u0026quot;: \u0026quot;${aws:AutoScalingGroupName}\u0026quot;, \u0026quot;ImageId\u0026quot;: \u0026quot;${aws:ImageId}\u0026quot;, \u0026quot;InstanceId\u0026quot;: \u0026quot;${aws:InstanceId}\u0026quot;, \u0026quot;InstanceType\u0026quot;: \u0026quot;${aws:InstanceType}\u0026quot; }, \u0026quot;metrics_collected\u0026quot;: { \u0026quot;disk\u0026quot;: { \u0026quot;measurement\u0026quot;: [ \u0026quot;used_percent\u0026quot; ], \u0026quot;metrics_collection_interval\u0026quot;: 60, \u0026quot;resources\u0026quot;: [ \u0026quot;*\u0026quot; ] }, \u0026quot;mem\u0026quot;: { \u0026quot;measurement\u0026quot;: [ \u0026quot;mem_used_percent\u0026quot; ], \u0026quot;metrics_collection_interval\u0026quot;: 60 } } } } Start the CloudWatch Agent  sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -c file:/opt/aws/amazon-cloudwatch-agent/bin/config.json -s It may take up to 5 minutes for the metrics to become available, go back to the Amazon CloudWatch console page, under the Metrics session to validate that you are getting Memory information.\nClick CWAgent: Click ImageID,InstanceID,InstanceType: Select the Instance from the list below: You have now completed the CloudWatch agent installation and will be able to monitor on Amazon CloudWatch the memory utilization of that instance.\n[BONUS] The next step is not mandatory to complete this lab.\nIf you have a lot of instances manually installing the CloudWatch agent in each of them is not a scalable option, instead consider using a pre-configured AWS CloudFormation template to automatically install the CloudWatch agent by default on all your stack. As an example on how to do that check the following steps:\n  Right-click and save link as: here to download the AWS Cloudformation template\n  Go to the AWS CloudFormation console\n  Click to Create Stack and select Upload a template file and point to the downloaded file\n  Enter a Stack Name and select a KeyName\n  Enter the tag Key: Event | Value: myStackforWACostLab\n  Click Next and Create stack\n   function prevStep(){ window.open(\"..\\/3_attach_iamrole\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/5_ec2_updated_rec\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_automated_cur_query_and_email_delivery/4_configure_function_parameters/","title":"Configure parameters of function code and upload code to S3","tags":[],"description":"","content":"This step is used to edit parameters (CUR database name and table, SES sender and recipient etc) in the Lambda function code, which is then uploaded to S3 for Lambda execution.\n  Download function code https://d3h9zoi3eqyz7s.cloudfront.net/Cost/AutoCURDelivery.zip to your local disk. This zip file includes: - auto_cur_delivery.py - Lambda function code\n config.yml - Configuration file package/ - All dependencies, libraries, including pandas, numpy, Xlrd, Openpyxl, Xlsxwriter, pyyaml    Unzip config.yml from within AutoCURDelivery.zip, and open it into a text editor.\n  Configure the following parameters in config.yml:\n CUR_Output_Location: Your S3 bucket created previously, i.e. S3://my-cur-bucket/out-put/ CUR_DB: CUR database and table name defined in Athena, i.e. athenacurcfn_my_athena_report.myathenareport CUR_Report_Name: Report filename that is sent with SES as an attachment, i.e. cost_utilization_report.xlsx Region: The region where SES service is called, i.e. us-east-1 Subject: SES mail subject, i.e. Cost and Utilization Report Sender: Your sender e-mail address, i.e. john@example.com Recipient: Your recipient e-mail addresses. If there are multiple recipients, separate them by comma, i.e. john@example.com,alice@example.com    Keep other configuration unchanged and save config.yml.\n  Add the updated config.yml back to AutoCURDelivery.zip.\n  Upload AutoCURDelivery.zip to your S3 bucket. Make sure this S3 path is in the same region as Lambda function created in next step. NOTE this is a large 30+MB file, so it may take a little time.\n   function prevStep(){ window.open(\"..\\/3_iam_policy_and_role\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/5_create_lambda_function\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_autonomous_monitoring_of_cryptographic_activity_with_kms/4_configure_the_workload_logging_and_alarm/","title":"Configure The Workload Logging and Alarm","tags":[],"description":"","content":"4.1. We are now going to create a filter within our CloudWatch Log Group. This filter will generate a CloudWatch metric which we will use as to create our alarm.\nTo create your filter, complete the following configuration steps:\n4.1.1. Navigate to CloudWatch in your console and click on Log Groups on the side menu.\n4.1.2. Locate the pattern1-logging-loggroup you created in the previous section and click on the the log group as shown:\n4.1.3. Select the tick box beside the log groups, click on Actions and then Create metric filter as shown:\n4.1.4. Enter below filter under Filter pattern\n{ $.errorCode = \u0026quot;*\u0026quot; \u0026amp;\u0026amp; $.eventSource= \u0026quot;kms.amazonaws.com\u0026quot; \u0026amp;\u0026amp; $.userIdentity.sessionContext.sessionIssuer.arn = \u0026quot;\u0026lt;ECS Task Role ARN\u0026gt;\u0026quot; } Note: Replace \u0026lt; ECS Task Role ARN \u0026gt; with the value of OutputPattern1ECSTaskRole. This value was provided in the Output section in the pattern1-app. If you need a reminder, you can refer to section 2.3.\nWhen you have completed this, you can click Next.\n4.1.5. It is important at this stage to understand the importance of filtering using this rule. The filter which we created in the previous step will look for all error codes which come from an eventSource of kms.amazonaws.com where the identity of the request matches the ECS Task role ARN.\nThis means that When KMS triggers an event by our application, the event registered within CloudTrail will look like this:\n { \u0026quot;eventVersion\u0026quot;: \u0026quot;1.05\u0026quot;, \u0026quot;userIdentity\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;AssumedRole\u0026quot;, ... \u0026quot;sessionContext\u0026quot;: { \u0026quot;sessionIssuer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;Role\u0026quot;, \u0026quot;principalId\u0026quot;: \u0026quot;AROAQKTRYBJEYHGY4HLFO\u0026quot;, \u0026quot;arn\u0026quot;: \u0026quot;arn:aws:iam::xxxxxxxxxxx:role/pattern1-application-Pattern1ECSTaskRole\u0026quot;, \u0026quot;accountId\u0026quot;: \u0026quot;xxxxxxxxxxx\u0026quot;, \u0026quot;userName\u0026quot;: \u0026quot;pattern1-application-Pattern1ECSTaskRole\u0026quot; }, ... } }, \u0026quot;eventTime\u0026quot;: \u0026quot;2020-11-16T22:25:39Z\u0026quot;, \u0026quot;eventSource\u0026quot;: \u0026quot;kms.amazonaws.com\u0026quot;, \u0026quot;eventName\u0026quot;: \u0026quot;Decrypt\u0026quot;, \u0026quot;awsRegion\u0026quot;: \u0026quot;ap-southeast-2\u0026quot;, \u0026quot;errorCode\u0026quot;: \u0026quot;IncorrectKeyException\u0026quot;, \u0026quot;errorMessage\u0026quot;: \u0026quot;The key ID in the request does not identify a CMK that can perform this operation.\u0026quot;, ..... \u0026quot;responseElements\u0026quot;: null, \u0026quot;requestID\u0026quot;: \u0026quot;11748bbd-ddcd-4ee2-9f42-9cec69f414b1\u0026quot;, \u0026quot;eventID\u0026quot;: \u0026quot;1f620618-46e5-4f78-93cc-0b7bccfff5d2\u0026quot;, \u0026quot;readOnly\u0026quot;: true, \u0026quot;eventType\u0026quot;: \u0026quot;AwsApiCall\u0026quot;, \u0026quot;recipientAccountId\u0026quot;: \u0026quot;xxxxxxxxxxx\u0026quot; } Our configured filter rule will perform filtering based on the JSON keys which are presented by the event as follows:\n $.eventSource : Describes the EventSource of \u0026ldquo;kms.amazon.com\u0026rdquo; signifying that it is a KMS event. $.errorCode : Describes any value with key \u0026ldquo;ErrorCode\u0026rdquo; signifying that an error event is being generated. $.userIdentity.sessionContext.sessionIssuer.arn: filters for the the userIdentity that executes the event. This is the assumed role that is used by ECS, which indicates that this call was made from our application running in the container.  Now that we have explained the details of how our filter operates, we can complete the configuration.\n4.1.5. In the Assign Metric form, enter the following configuration detail:\n Enter pattern1-logging-metricfilter as the Filter name. Enter Pattern1Application/KMSSecurity as the Metric namespace. Enter KMSSecurityError as the Metric name. Enter 1 as the Metric Value.  Your completed configuration should match the following screenshot:\nWhen you have verified your configuration, click Next and Create metric filter\n4.2 Create The Metric Alarm. Once your Metric filter has been created, you should be able to view it under the Metric filters tab of your LogGroups. We will now create the Metric Alarm from this filter.\nComplete the following steps:\n4.2.1. Select the Metric filter you just created, then click on CreateAlarm as shown:\n4.2.2. Change the name of the metric to KMSsecurityError and set the Period to 10 seconds as shown:\n4.2.3. Within the conditions dialog box, configure the following:\n Set the Threshold type as Static Set the condition as Greater \u0026gt; threshold Set the threshold value as 1 Under Additional Configuration set Missing data treatment as Treat missing data as Ignore(maintain the alarm state)  Your configuration should match the following screenshot:\nWhen your configuration is complete, click Next\n4.2.4. In the Notification dialog box, configure the following:\n Select In alarm as alarm trigger state. Select Create new topic and enter pattern1-logging-topic as the topic name. Enter an email address where you would like to receive notification.  When your configuration is complete, click Create topic then click Next\n4.2.5. Complete the following configuration to complete the alarm setup:\n Enter pattern1-logging-alarm as the Alarm name and click Next Review the setting and click Create Alarm Wait for an email to arrive in your mailbox, and confirm subscription to you the topic once it arrives as shown here:  This completes the creation of the filter and alarm for the lab. Proceed to Section 5 to test functionality.\n END OF SECTION 4\n "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_2_cost_and_usage_governance/4_budget_report/","title":"Create and implement an AWS Budget Report","tags":[],"description":"","content":"AWS Budgets Reports allow you to create and send daily, weekly, or monthly reports to monitor the performance of your AWS Budgets.\n  From the Budgets dashboard, Click on Budgets Reports:   Click Create budget report:   Create a report with the following details:\n Report name: WeeklyBudgets Select all budgets    Click Configure delivery settings \u0026gt;:   Configure the delivery settings:\n Report frequency: Weekly Day of week: Monday Email recipients:     Click Confirm budget report \u0026gt;:   Review the configuration, click Create:   Your budget report should now be complete:   You should receive an email similar to the one below:   You have created a budget report. Use reports to regularly track your progress against defined budgets.\n  function prevStep(){ window.open(\"..\\/3_budget_spcoverage\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/5_tear_down\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_splitting_sharing_cur_access/4_lambda_function/","title":"Create Lambda function to run the Saved Queries","tags":[],"description":"","content":"This Lambda function ties everything together, it will remove all objects in the current months S3 folders, find the Athena queries to run, and then execute the saved Athena queries. First we will create the role with permissions for Lambda to use, then the Lambda function itself.\n1 - Go to the IAM service dashboard\n2 - Create a policy named LambdaSubAcctSplit\n3 - Edit the following policy inline with security best practices, and add it to the policy:\n./Code/SubAcctSplit_Role.md 4 - Create a Role for Lambda to call services\n5 - Attach the LambdaSubAcctSplit policy\n6 - Name the role LambdaSubAcctSplit\n7 - Go into the Lambda service dashboard\n8 - Create a function named SubAcctSplit, Author from scratch using the Python 3.7 Runtime and role LambdaSubAcctSplit:\n9 - Copy the code into the editor from here: ./Code/Sub_Account_Split.md 10 - Edit the code as per the instructions at the top.\n11 - Under Basic settings set the Timeout to 30seconds, and review this after the test at the end\n12 - Change the Execution role to LambdaSubAcctSplit\n13 - Save the function\n14 - Test that the function by clicking on the Test button at the top, and make sure that it executes correctly: 15 - Go into the S3 Service dashboard, view the output folder and verify that there are files for the current month. Check the Last modified time stamp to ensure they were created at the time of the test.\nYou have now setup the Lambda function which executes the queries. The final step is to trigger this Lambda function every time a new CUR file is delivered.\n function prevStep(){ window.open(\"..\\/3_athena_queries\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/5_trigger_lambda\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_lambda_cross_account_bucket_policy/4_create_lambda/","title":"Create Lambda in account 1","tags":[],"description":"","content":"  Open the Lambda console https://console.aws.amazon.com/lambda\n  Click Create a function\n  Accept the default Author from scratch\n  Enter function name as Lambda-List-S3\n  Select Python 3.7 runtime\n  Expand Permissions, click Use an existing role, then select the Lambda-List-S3-Role\n  Click Create function\n  Replace the example function code with the following\n  Replace bucketname with the S3 bucket name from account 2\nimport json import boto3 import os import uuid def lambda_handler(event, context): try: # Create an S3 client s3 = boto3.client('s3') # Call S3 to list current buckets objlist = s3.list_objects( Bucket='bucketname', MaxKeys = 10) print (objlist['Contents']) return str(objlist['Contents']) except Exception as e: print(e) raise e      Click Save.\n  Click Test, accept the default event template, enter an event name for the test, then click Create\n  Click Test again, and in a few seconds the function output should highlight green and you can expand the detail to see the response from the S3 API\n  "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_pricing_model_analysis/4_recommendation_dashboard/","title":"Create the Recommendation Dashboard","tags":[],"description":"","content":"  Go to the QuickSight service homepage:   Go to the sp_usage analysis:   Create a line chart, add line_item_usage_start_date to the X axis, aggregate day. Add spprice to the Value and set the aggregate to min. Drag the product_instance_type to Colour field well. Change the title to Usage in Savings Plan Rates:   Click Parameters, and click Create one:   Parameter name OperatingSystem, Data type String, click Set a dynamic default:   Select your dataset, and select product_operating_system for the columns, click Apply:   Click Create:   Click Control:   Enter OperatingSystem as the display name, style Single select drop down, values Link to a data set field, dataset your data set, column product_operating_system, click Add:   Using the process above, Add the parameter Region:\n Name: Region Data type: String Values: Single value Dyanmic default Dataset: your dataset, product_location, product_location Add as: Control Control Display Name: Region Style: Single select drop down Values: link to data set field Data set: your data set Column: product_location    Using the process above, Add the parameter Tenancy:\n Name: Tenancy Data type: String Values: Single value Dyanmic default Dataset: your dataset, product_tenancy, product_tenancy Add as: Control Control Display Name: Tenancy Style: Single select drop down Values: link to data set field Data set: your data set Column: product_tenancy    Create an InstanceType parameter, datatype String, Single value, Static default value of . (a full stop):   Click Control,   Display name InstanceType, style Text box, click Add:   Click Filter and click Create one, select product_instance_type:   Edit the filter, Filter type:\n All visuals Custom filter Contains Use Parameters InstanceType click Apply:     Create a Parameter DaysToDisplay:\n Name: DaysToDisplay Data type: Integer Values: Single value Static default value: 90 Click Create:     Click Control:   Enter a Display name DaysToDisplay, Style Text box and click Add:   Click on Filter, click +, and select line_item_usage_start_date:   Click on the new filter:   Select a filter type of:\n All visuals Relative dates Days Last N days select Use parameters, and accept to change the scope of the filter select the parameter DaysToDisplay click Apply:     Create a filter for product_operating_system:\n All visuals Type: Custom filter equals Use parameters, change the scope of this filter: yes Parameter: OperatingSystem     Create a filter for product_location:\n All visuals Type: Custom filter equals Use parameters, change the scope of this filter: yes Parameter: Region     Create a filter for product_tenancy:\n All visuals Type: Custom filter equals Use parameters, change the scope of this filter: yes Parameter: Tenancy     Click on Visualize, click Add, select Add calculated field:   Field name HoursDisplayed, add the formula below and click Create:\ndistinct_count({line_item_usage_start_date})     Create a calculated field HoursRun, formula:\nHoursDisplayed / (${DaysToDisplay} * 24)    Create a calculated field PayOffMonth, formula:\nifelse(((((sum(spprice) / HoursDisplayed) * 730 * 12) / ((sum(odprice) / (${DaysToDisplay} * 24)) * 730))) \u0026lt; 12,((((sum(spprice) / HoursDisplayed) * 730 * 12) / ((sum(odprice) / (${DaysToDisplay} * 24)) * 730))),12)    Create a calculated field SavingsPlanReco, formula:\nifelse(PayOffMonth \u0026lt; 12,percentile(spprice,10),0.00)    Create a calculated field StartSPPrice, formula:\nlag(min(spprice),[{line_item_usage_start_date} ASC],${DaysToDisplay} - 2,[{product_instance_type}])    Create a calculated field Trend, formula:\n(min(spprice) - {StartSPPrice}) / min(spprice)    Create a calculated field First3QtrAvg, formula:\nwindowAvg(avg(spprice),[{line_item_usage_start_date} ASC],${DaysToDisplay},${DaysToDisplay} / 4,[{product_instance_type}])    Create a calculated field LastQtrAvg, formula:\nwindowAvg(avg(spprice),[{line_item_usage_start_date} ASC],${DaysToDisplay} / 4,1,[{product_instance_type}])    Create a calculated field TrendAvg, formula:\n(LastQtrAvg- First3QtrAvg) / First3QtrAvg    Add a Visual, click Add, select Add visual:   Select a Table visualization, Group by product_instance_type, Add the values:\n SavingsPlanReco PayOffMonth discountrate, aggregate: average HoursRun, show as percent Label it Recommendations     Add a Pivot Table visual, Rows: product_instance_type and line_item_usage_start_date aggreate: day, Add the values:\n instancecount aggregate: average Trend TrendAvg (show as percent) Label it Trends,     Add a filter to this visual only:\n Filter on: StartSPPrice Type: Custom filter Operation: Greater than Value: -1 Nulls: Exclude nulls    Decrease the width of the date column as much as possible, its not needed\n   function prevStep(){ window.open(\"..\\/3_quicksight_setup\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/5_format_dashboard\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_autonomous_patching_with_ec2_image_builder_and_systems_manager/4_deploy_the_build_automation_with_ssm/","title":"Deploy The Build Automation With SSM","tags":[],"description":"","content":"Now that our AMI Builder Pipeline is built, we can now work on the final automation stage with Systems Manager.\nIn this section we will orchestrate the build of a newly patched AMI and its associated deployment into our application cluster.\nTo automate this activities we will leverage AWS Systems Manager Automation Document .\nUsing our SSM Automation document we will execute the following activities:\n Automate the execution of the EC2 Image Builder Pipeline. Wait for the pipeline to complete the build, and capture the newly created AMI with updated OS patch. Then it will Update the CloudFormation application stack with the new patched Amazon Machine Image. This AMI update to the stack will in turn trigger the CloudFormation AutoScalingReplacingUpdate policy to perform a simple equivalent of a blue/green deployment of the new Autoscaling group.  Note: Using this approach, we streamline the creation of our AMI, and at the same time minimize interruption to applications within the environment.\nAdditionally, by leveraging the automation built in Cloudformation through autoscaling update policy, we reduce the complexity associated with building out a blue/green deployment structure manually. Lets look at how this works in detail:\n Firstly, CloudFormation detects the need to update the LaunchConfiguration with a new Amazon Machine Image. Then, CloudFormation will launch a new AutoScalingGroup, along with it\u0026rsquo;s compute resource (EC2 Instance) with the newly patched AMI. CloudFormation will then wait until all instances are detected healthy by the Load balancer, before terminating the old AutoScaling Group, ultimately achieving a blue/green model of deployment. Should the new compute resource failed to deploy, cloudformation will rollback and keep the old compute resource running.  For details about how this is implemented in the CloudFormation template, please review the pattern3-application.yml template deployed in section 2.\nOnce we complete this section our architecture will reflect the following diagram:\nIn this section you have the option to build the resources manually using AWS console. If however you are keen to complete the lab quickly, you can simply deploy from the CloudFormation template and take a look at the deployed architecture. Select the appropriate section:\n  Build with a CloudFormation template on the command-line   4.1. Get The Template The template for Section 4 which can be found here .\n4.2. Deploy From The Command Line To deploy from the command line, ensure that you have installed and configured AWS CLI with the appropriate credentials. When you are ready, execute the following command:\naws cloudformation create-stack --stack-name pattern3-automate \\ --template-body file://pattern3-automate.yml \\ --parameters ParameterKey=ApplicationStack,ParameterValue=pattern3-app \\ ParameterKey=ImageBuilderPipelineARN,ParameterValue=\u0026lt;enter image builder pipeline arn\u0026gt; 4.3. Record The CloudFormation Output. Once the template is finished execution, note the Automation Document Name from the Cloudformation output specified under Pattern3CreateImageOutput.\n    Build with a CloudFormation template in the console   4.1. Get The Template The template for Section 4 which can be found here .\n4.2. Deploy From The Console To deploy the template from console please follow this guide for information on how to deploy the cloudformation template.\n Use pattern3-automate as the Stack Name. Provide the ARN of the pipeline you created in section 3.2.6 as ImageBuilderPipelineARN parameter value. Provide the cloudfromation stack name you created in section 2.1 as ApplicationStack parameter value.  4.3. Record The CloudFormation Output. Once the template is finished execution, note the Automation Document Name from the Cloudformation output specified under Pattern3CreateImageOutput.\n    Build Automation Document Manually   4.1. Access Systems Manager From The Console. From the AWS console, select \u0026lsquo;Systems Manager\u0026rsquo;.\nWhen you get to the front page of the service, use the left hand panel and go down to the bottom of the menu to select Documents from the Shared Resources as follows:\n.\n4.2. Create Automation Document In this section we will go through steps to create the automation document, explaining the automation document configuration detail interactively as we walk through. To ensure that you get the formatting correct when you insert the automation document we have provided a full copy for you to download here .\n4.2.1. Firstly access Systems Manager from the AWS Console.\n4.2.2. When you get to the front page of the service, use the left hand panel and go down to the bottom of the menu to select Documents from the Shared Resources as follows:\n.\n4.2.3. From the main page, select the Create Automation button to build an automation document.\n4.2.4. Enter the name of the automation document and select the Editor option to enter a the document directly into the console.\n4.2.5. Next we need to add the document specification below into the editor. Add the document which you downloaded at the start of section 4.2.. The following steps will explain the document configuration in stages.\n4.2.6. Firstly, we need to specify the schemaVersion and parameters which our document will take as an Input.\nIn this case we will take the ImageBuilderPipeline ARN as well as the name of the Application Stack (default: pattern3-app)\ndescription: CreateImage schemaVersion: '0.3' parameters: ImageBuilderPipelineARN: description: (Required) Corresponding EC2 Image Builder Pipeline to execute. type: String ApplicationStack: description: (Required) Corresponding Application Stack to Deploy the Image to. type: String 4.2.7. Next we will specify the first step which is to execute image builder pipeline we created in previous section. Passing the parameter inputs we specified before. This execution is achieved by calling the AWS service API directly leveraging aws:executeAwsApi action type in SSM Automation Document.\nmainSteps: - name: ExecuteImageCreation action: aws:executeAwsApi maxAttempts: 10 timeoutSeconds: 3600 onFailure: Abort inputs: Service: imagebuilder Api: StartImagePipelineExecution imagePipelineArn: '{{ ImageBuilderPipelineARN }}' outputs: - Name: imageBuildVersionArn Selector: $.imageBuildVersionArn Type: String 4.2.8. In the next we will specify aws:waitForAwsResourceProperty action wait for the Image to complete building.\n - name: WaitImageComplete action: aws:waitForAwsResourceProperty maxAttempts: 10 timeoutSeconds: 3600 onFailure: Abort inputs: Service: imagebuilder Api: GetImage imageBuildVersionArn: '{{ ExecuteImageCreation.imageBuildVersionArn }}' PropertySelector: image.state.status DesiredValues: - AVAILABLE 4.2.9. Once the wait is complete, and the Image is ready, we will then call another aws:executeAwsApi to capture the AMI Id and pass the value into the next step.\n - name: GetBuiltImage action: aws:executeAwsApi maxAttempts: 10 timeoutSeconds: 3600 onFailure: Abort inputs: Service: imagebuilder Api: GetImage imageBuildVersionArn: '{{ ExecuteImageCreation.imageBuildVersionArn }}' outputs: - Name: image Selector: $.image.outputResources.amis[0].image Type: String 4.2.10. With the AMI id we received in previous step, we will then pass the id to our Application CloudFormation Stack and trigger an update using aws:executeAwsApi action.\n - name: UpdateCluster action: aws:executeAwsApi maxAttempts: 10 timeoutSeconds: 3600 onFailure: Abort inputs: Service: cloudformation Api: UpdateStack StackName: '{{ ApplicationStack }}' UsePreviousTemplate: true Parameters: - ParameterKey: BaselineVpcStack UsePreviousValue: true - ParameterKey: AmazonMachineImage ParameterValue: '{{ GetBuiltImage.image }}' Capabilities: - CAPABILITY_IAM 4.2.11. Once the update executes we will once again wait for the Cloudformation update to complete, and return with the UPDATE_COMPLETE status.\n - name: WaitDeploymentComplete action: aws:waitForAwsResourceProperty maxAttempts: 10 timeoutSeconds: 3600 onFailure: Abort inputs: Service: cloudformation Api: DescribeStacks StackName: '{{ ApplicationStack }}' PropertySelector: Stacks[0].StackStatus DesiredValues: - UPDATE_COMPLETE 4.2.12. We have provided commentary above, to give you a picture of what is being executed in this automation document. As a whole your Automation Document should look as below. Please copy and paste below, and make that the indentation is correct as this document is specified in YAML format. Alternatively you can download the file here description: CreateImage schemaVersion: '0.3' parameters: ImageBuilderPipelineARN: description: (Required) Corresponding EC2 Image Builder Pipeline to execute. type: String ApplicationStack: description: (Required) Corresponding Application Stack to Deploy the Image to. type: String mainSteps: - name: ExecuteImageCreation action: aws:executeAwsApi maxAttempts: 10 timeoutSeconds: 3600 onFailure: Abort inputs: Service: imagebuilder Api: StartImagePipelineExecution imagePipelineArn: '{{ ImageBuilderPipelineARN }}' outputs: - Name: imageBuildVersionArn Selector: $.imageBuildVersionArn Type: String - name: WaitImageComplete action: aws:waitForAwsResourceProperty maxAttempts: 10 timeoutSeconds: 3600 onFailure: Abort inputs: Service: imagebuilder Api: GetImage imageBuildVersionArn: '{{ ExecuteImageCreation.imageBuildVersionArn }}' PropertySelector: image.state.status DesiredValues: - AVAILABLE - name: GetBuiltImage action: aws:executeAwsApi maxAttempts: 10 timeoutSeconds: 3600 onFailure: Abort inputs: Service: imagebuilder Api: GetImage imageBuildVersionArn: '{{ ExecuteImageCreation.imageBuildVersionArn }}' outputs: - Name: image Selector: $.image.outputResources.amis[0].image Type: String - name: UpdateCluster action: aws:executeAwsApi maxAttempts: 10 timeoutSeconds: 3600 onFailure: Abort inputs: Service: cloudformation Api: UpdateStack StackName: '{{ ApplicationStack }}' UsePreviousTemplate: true Parameters: - ParameterKey: BaselineVpcStack UsePreviousValue: true - ParameterKey: AmazonMachineImage ParameterValue: '{{ GetBuiltImage.image }}' Capabilities: - CAPABILITY_IAM - name: WaitDeploymentComplete action: aws:waitForAwsResourceProperty maxAttempts: 10 timeoutSeconds: 3600 onFailure: Abort inputs: Service: cloudformation Api: DescribeStacks StackName: '{{ ApplicationStack }}' PropertySelector: Stacks[0].StackStatus DesiredValues: - UPDATE_COMPLETE 4.2.13. Once that\u0026rsquo;s done click Create Automation\nNow that we have created the Automation Document, let\u0026rsquo;s go ahead and execute it.\n4.3. Start The Monitor Script. Before we execute the document, we have provided a simple script for you to continuously query the Application Load Balancer http address during the document execution. This is to show that the load balancer remains available throughout the deployment.\n4.3.1. Firstly, download the monitor script here .\n4.3.2. Now change permissions of the script if required and execute passing in the application load balancer DNS address. Note that the DNS address is provided in the output of the application CloudFormation stack in section 2.2 under OutputPattern3ALBDNSName.\nExecute the script as follows:\n./watchscript.sh http://\u0026lt;enter DNS address for the Application Load Balancer\u0026gt; As mentioned above, the script will run a continuous poll of the ALB throughout the next few steps to demonstrate that there is no interruption to traffic during the patch process.\nFor clarity, you might want to run this in a separate dedicated terminal as it will continue to poll the ALB in a loop.\nYou can leave this script running, and monitor to see if there is any failed response to the application. Your output should look similar to this:\n4.4 Start the Automation Document. Once your monitor script is running a continous poll of the ALB, you can execute the SSM automation document.\nTo Execute the automation document, you can run the following command:\naws ssm start-automation-execution \\ --document-name \u0026quot;\u0026lt;enter_document_name\u0026gt;\u0026quot; \\ --parameters \u0026quot;ApplicationStack=\u0026lt;enter_application_stack_name\u0026gt;,imageBuilderPipeline=\u0026lt;enter_image_builder_pipeline_arn\u0026gt;\u0026quot; Note:  The value of \u0026lt;enter_document_name\u0026gt; is provided as output to the CloudFormation template which you noted in section 4.1.1, or in section 4.2 if you are building it manually. The value of \u0026lt;enter_application_stack_name\u0026gt; is the name that you provided to the application stack in Section 2 (default is pattern3-app). The value of \u0026lt;enter_image_builder_pipeline_arn\u0026gt; is the ARN of the Image Builder Pipeline. You can get this from the output to the pipeline stack from Section 3.1.2 or 3.2.6 if you are building it manually via the console..  When you have successfully executed the command you will be provided with an AutomationExecutionID.\nTo check the status of the currently running automation executions, you can use the following command:\naws ssm describe-automation-executions Note that you can pass a filter to the command with the AutomationExecutionID which you were provided from the automation execution as follows:\naws ssm describe-automation-executions --filter \u0026quot;Key=ExecutionId,Values=\u0026lt;enter_execution_id\u0026gt;\u0026quot; 4.5. Confirm that the AMI has been Updated Via the Load Balancer DNS Name. When the automation execution is completed, use your web browser to access your application load balancer DNS name, together with the \u0026lsquo;details.php\u0026rsquo; script added to the end of the address. You will now find that the AMI-ID has been updated with a new one, indicating that your original autoscaling group has been replaced with an updated group which is configured to use the patched AMI. as follows:\nThis concludes our lab.\n   END OF SECTION 4 END OF LAB\n "},{"uri":"https://wellarchitectedlabs.com/reliability/100_labs/100_deploy_cloudformation/4_explore_cloudformation/","title":"Explore the CloudFormation Template","tags":[],"description":"","content":"In this section you will explore the CloudFormation template and learn how you were able to deploy the web application infrastructure using it\n  Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation\n Click on the CloudFormation stack that you deployed Click on the Template tab     Alternate: You previously downloaded the CloudFormation Template staticwebapp.yaml. You can also view it in the text editor of your choice\n  The template is written in a format called YAML , which is commonly used for configuration files. CloudFormation templates can also be written in JSON.\nLook through the template. You will notice several sections:\n  The Parameters section is used to prompt for inputs that can be used elsewhere in the template. The template is asking for several inputs, but also provides default values for each one.\n Look through these and start to reason about what some are used for. For example InstanceType is a parameter where the user can choose that Amazon EC2 instance type to deploy for the servers used in this Web App. Search the file for !Ref InstanceType. !Ref! is a built-in function that refrences the value of a parameter. Here you can see it is used to provide a value to the Auto Scaling Launch Configuration, which is used to laucnh new EC2 instances.    The Conditions section is where you can setup if/then-like control of what happens during template deployment. It defines the circumstances under which entities are created or configured.\n  The Resources section is the \u0026ldquo;heart\u0026rdquo; of the template. It is where you define the infrastructure to be deployed. Look at the first resource defined.\n It is the Amazon DynamoDB table used as the mock for the RecommendationService It has a logical ID which in this case is DynamoDBServiceMockTable. This logical ID is how we refer to the DynamoDB table resource within the CloudFormation template. It has a Type which tells CloudFormation which type of resource to create. In this case a AWS::DynamoDB::Table And it has Properties that define the values used to create the VPC    The Outputs section is used to display selective information about resources in the stack.\n In this case it uses the built-in function !GetAtt to get the DNS Name for the Application Load Balancer. This URL is what you used to access the WebApp    The Metadata section here is used to group and order how the CloudFormation parameters are displayed when you deploy the template using the AWS Console\n function prevStep(){ window.open(\"..\\/3_explore_webapp\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/5_cleanup\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step       "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_2_cost_and_usage_governance/4_ec2_restrict_size/","title":"Extend an IAM Policy to restrict EC2 usage by instance size","tags":[],"description":"","content":"We can also restrict the size of instance that can be launched. This can be used to ensure only low cost instances can be created within an account. This is ideal for testing and development, where high capacity instances may not be required. We will extend the EC2 family policy above, and add restrictions by adding the sizes of instances allowed.\nExtend the EC2Family_Restrict IAM Policy   Log on to the console as your regular user with the required permissions, go to the IAM service page:   Click on Policies on the left menu:   Click on Filter policies, then select Customer managed:   Click on EC2_FamilyRestrict to modify it:   Click on Edit policy:   Click on the JSON tab:   Modify the policy by adding in the sizes, add in nano, medium, large, be careful not to change the syntax and not remove the quote characters. Click on Review policy:   Click on Save changes:   Log out from the console\n  You have successfully modified the policy to restrict usage by instance size.\n Verify the policy is in effect   Logon to the console as the TestUser1 user, click on Services and go to the EC2 dashboard:   Try to launch an instance by clicking Launch Instance, select Launch Instance:   Click on Select next to the Amazon Linux 2 AMI:   We will attempt to launch a t3.micro which was successful before. Click on Review and Launch:   Review the configuration and take note of the security group created, click Launch:   Select Proceed without a key pair, and click I acknowledge that i will not be able to\u0026hellip;, then click Launch Instances:   You will get a failure, as it wasn\u0026rsquo;t a size we allowed in the policy. Click Back to Review Screen:   Click Edit instance type:   We will now select a t3.nano which will succeed. Click Review and Launch:   Select Yes, I want to continue with this instance type (t3.nano), and click Next:   Review the configuration and click Launch:   Select Proceed without a key pair, and click I acknowledge that i will not be able to\u0026hellip;, then click Launch Instances:   It will succeed. Click on the Instance ID and terminate the instance as above:   Log out of the console as TestUser1.\n  You have successfully implemented an IAM policy that restricts all EC2 instance operations by family and size.\n  function prevStep(){ window.open(\"..\\/3_ec2_restrict_family\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/5_ec2_volume_type\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_health_checks_and_dependencies/4_fail_open/","title":"Fail open when appropriate","tags":[],"description":"","content":"4.1 Disable RecommendationService again  Confirm the service is healthy  Refresh the web service multiple times and note that personalized recommendations are being served from all three servers   You will now simulate another complete failure of the RecommendationService. Every request will fail for every request on every server  Return to the AWS Systems Manager \u0026gt; Parameter Store on the AWS Management Console Set the value of RecommendationServiceEnabled once again to false and Save changes    What is the expected behavior? The previous time you simulated a complete failure of the RecommendationService\n The web service failed with a http 502 error Then you implemented error handling and the following were observed  The service returned a static response (as per the error handling code) Since the healthcheck code at that time was configured to only return http 200, it reported healthy status for all servers    Now, with the new deep health check in place\u0026hellip;\n What status do you expect the elastic load balancer to report for the servers? How will the AWS Elastic Load Balancer handle traffic routing to the servers?  4.2 Observe fail-open behavior   Refresh the web service multiple times\n Look at which servers (and Availability Zones) are serving requests Note that the service does not fail But as expected (without access to RecommendationServiceEnabled) it always serves static responses    Refresh the health check URL multiple times\n The deep health detects that RecommendationServiceEnabled is not available and returns a failure code for all servers    From the Target Groups console Targets tab note the health check status of all the servers (you may need ot refresh)\n  They all report unhealthy with http code 503. This is the code the deep health check is configured to return when the dependency is not available\n  Note the message at the top of the tab (if you do not see a message, try refreshing the entire page using the browser refresh function)\n   The Amazon Builders' Library: Implementing health checks     When an individual server fails a health check, the load balancer stops sending it traffic. But when all servers fail health checks at the same time, the load balancer fails open, allowing traffic to all servers.   When we rely on fail-open behavior, we make sure to test the failure modes of the dependency heath check.      A system set to fail-open does not shut down when failure conditions are present. Instead, the system remains open and operations continue. The AWS Application Load Balancer here exhibits this fail-open behavior and the service continues to serve requests sent to it by the load balancer.\n   Reset the value of RecommendationServiceEnabled to true and observe that the service resumes serving personalized recommendations.  The RecommendationServiceEnabled parameter was initially intended to simulate the failure of RecommendationService for this lab But now that we have implemented fail-open behavior and graceful degradation we could use the RecommendationServiceEnabled parameter as an emergency lever to cuto-off traffic to RecommendationService if there was a serious problem with it.        Well-Architected for Reliability: Best practice     Implement emergency levers: These are rapid processes that may mitigate availability impact on your workload. They can be operated in the absence of a root cause. An ideal emergency lever reduces the cognitive burden on the resolvers to zero by providing fully deterministic activation and deactivation criteria. Example levers include blocking all robot traffic or serving a static response. Levers are often manual, but they can also be automated.     function prevStep(){ window.open(\"..\\/3_deep_healthcheck\", \"_self\") } function nextStep(){ window.open(\"..\\/5_cleanup\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/100_labs/100_inventory_patch_management/4_inventory_mgmt/","title":"Inventory Management using Operations as Code","tags":[],"description":"","content":"Management Tools: Systems Manager AWS Systems Manager is a collection of features that enable IT Operations that we will explore throughout this lab.\nThere are set up tasks and pre-requisites that must be satisfied prior to using Systems Manager to manage your EC2 instances or on-premises systems in hybrid environments .\n You must use a supported operating system  Supported operating systems include versions of Windows, Amazon Linux, Ubuntu Server, RHEL, and CentOS   The SSM Agent must be installed  The SSM Agent for Windows also requires PowerShell 3.0 or later to run some SSM documents    Your EC2 instances must have outbound internet access You must access Systems Manager in a supported region Systems Manager requires IAM roles  for instances that will process commands for users executing commands    SSM Agent is installed by default on:\n Amazon Linux base AMIs dated 2017.09 and later Windows Server 2016 instances Instances created from Windows Server 2003-2012 R2 AMIs published in November 2016 or later  There is no additional charge for AWS Systems Manager . You only pay for your underlying AWS resources managed or created by AWS Systems Manager (e.g., Amazon EC2 instances or Amazon CloudWatch metrics). You only pay for what you use as you use it. There are no minimum fees and no upfront commitments.\n4.1 Setting up Systems Manager  Use your administrator account to access the Systems Manager console at https://console.aws.amazon.com/systems-manager/. Choose Managed Instances from the navigation bar. If you have not satisfied the pre-requisites for Systems Manager, you will arrive at the AWS Systems Manager Managed Instances page.  As a user with AdministratorAccess permissions, you already have User Access to Systems Manager . The Amazon Linux AMIs used to create the instances in your environment are dated 2017.09. They are supported operating systems and have the SSM Agent installed by default. If you are in a supported region the remaining step is to configure the IAM role for instances that will process commands.   Create an Instance Profile for Systems Manager managed instances:  Navigate to the IAM console  In the navigation pane, choose Roles. Then choose Create role. In the Select type of trusted entity section, verify that the default AWS service is selected. In the Choose the service that will use this role section, scroll past the first reference to EC2 (EC2 Allows EC2 instances to call AWS services on your behalf) and choose EC2 from within the field of services. This will open the Select your use case section further down the page. In the Select your use case section, choose EC2 Role for Simple Systems Manager to select it. Then choose Next: Permissions.   Under Attached permissions policy, verify that AmazonEC2RoleforSSM is listed, and then choose Next: Review. In the Review section:  Enter a Role name, such as ManagedInstancesRole. Accept the default in the Role description. Choose Create role.   Apply this role to the instances you wish to manage with Systems Manager:  Navigate to the EC2 Console and choose Instances. Select the first instance and then choose Actions, Security, and Modify IAM Role. Under Modify IAM Role, select ManagedInstancesRole from the drop down list and choose Save. Repeat this process, assigning ManagedInstancesRole to each of the 3 remaining instances.   Return to the Systems Manager console and choose Managed Instances from the navigation bar. Periodically choose Managed Instances until your instances begin to appear in the list. Over the next couple of minutes your instances will populate into the list as managed instances.   Note If desired, you can use a more restrictive permission set to grant access to Systems Manager.\n 4.2 Create a Second CloudFormation Stack  Create a second CloudFormation stack using the procedure in 3.1 with the following changes:  In the Specify Details section, define a Stack name, such as OELabStack2. Specify the InstanceProfile using the ManagedInstancesRole you defined. Define the Workload Name as Prod.    Systems Manager: Inventory You can use AWS Systems Manager Inventory to collect operating system (OS), application, and instance metadata from your Amazon EC2 instances and your on-premises servers or virtual machines (VMs) in your hybrid environment. You can query the metadata to quickly understand which instances are running the software and configurations required by your software policy, and which instances need to be updated.\n4.3 Using Systems Manager Inventory to Track Your Instances  Under Instances \u0026amp; Nodes in the AWS Systems Manager navigation bar, choose Inventory.  Scroll down in the window to the Corresponding managed instances section. Inventory currently contains only the instance data available from the EC2 Choose the InstanceID of one of your systems. Examine each of the available tabs of data under the Instance ID heading.   Inventory collection must be specifically configured and the data types to be collected must be specified  Choose Inventory in the navigation bar. Choose Setup Inventory in the top right corner of the window   In the Setup Inventory screen, define targets for inventory:  Under Specify targets by, select Specifying a tag Under Tags specify Environment for the key and OELabIPM for the value     Note You can select all managed instances in this account, ensuring that all managed instances will be inventoried. You can constrain inventoried instances to those with specific tags, such as Environment or Workload. Or you can manually select specific instances for inventory.\n Schedule the frequency with which inventory is collected. The default and minimum period is 30 minutes  For Collect inventory data every, accept the default 30 Minute(s)   Under parameters, specify what information to collect with the inventory process  Review the options and select the defaults   (Optional) If desired, you may specify an S3 bucket to receive the inventory execution logs (you will need to create a destination bucket for the logs prior to proceeding):  Check the box next to Sync inventory execution logs to an S3 bucket under the Advanced options. Provide an S3 bucket name. (Optional) Provide an S3 bucket prefix.   Choose Setup Inventory at the bottom of the page (it can take up to 10 minutes to deploy a new inventory policy to an instance). To create a new inventory policy, from Inventory, choose Setup inventory. To edit an existing policy, from State Manager in the left navigation menu, select the association and choose Edit.   Note You can create multiple Inventory specifications. They will each be stored as associations within Systems Manager State Manager.\n Systems Manager: State Manager In State Manager, an association is the result of binding configuration information that defines the state you want your instances to be in to the instances themselves. This information specifies when and how you want instance-related operations to run that ensure your Amazon EC2 and hybrid infrastructure is in an intended or consistent state.\nAn association defines the state you want to apply to a set of targets. An association includes three components and one optional set of components:\n A document that defines the state Target(s) A schedule (Optional) Runtime parameters.  When you performed the Setup Inventory actions, you created an association in State Manager.\n4.4 Review Association Status  Under Actions in the navigation bar, select State Manager. At this point, the Status may show that the inventory activity has not yet completed.  Choose the single Association id that is the result of your Setup Inventory action. Examine each of the available tabs of data under the Association ID heading. Choose Edit. Enter a name under Name - optional to provide a more user friendly label to the association, such as InventoryAllInstances (white space is not permitted in an Association Name).    Inventory is accomplished through the following:\n The activities defined in the AWS-GatherSoftwareInventory command document. The parameters provided in the Parameters section are passed to the document at execution. The targets are defined in the Targets section.   Important In this example there is a single target, the wildcard. The wildcard matches all instances making them all targets.\n  The schedule for this activity is defined under Specify schedule and Specify with to use a CRON/Rate expression on a 30 minute interval. There is the option to specify Output options.   Note If you change the command document, the Parameters section will change to be appropriate to the new command document.\n Navigate to Managed Instances under Instances and Nodes in the navigation bar. An Association Status has been established for the inventoried instances under management. Choose one of the Instance ID links to go to the inventory of the instance. The Inventory tab is now populated and you can track associations and their last activity under the Associations tab. Navigate to Compliance under Instances \u0026amp; Nodes in the navigation bar. Here you can view the overall compliance status of your managed instances in the Compliance Summary and the individual compliance status of systems in the Corresponding managed instances section below.   Note The inventory activity can take up to 10 minutes to complete. While waiting for the inventory activity to complete, you can proceed with the next section.\n Systems Manager: Compliance You can use AWS Systems Manager Configuration Compliance to scan your fleet of managed instances for patch compliance and configuration inconsistencies. You can collect and aggregate data from multiple AWS accounts and Regions, and then drill down into specific resources that arent compliant.\nBy default, Configuration Compliance displays compliance data about Systems Manager Patch Manager patching and Systems Manager State Manager associations. You can also customize the service and create your own compliance types based on your IT or business requirements. You can also port data to Amazon Athena and Amazon QuickSight to generate fleet-wide reports.\n function prevStep(){ window.open(\"..\\/3_deploy_env_iaac\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/5_patch_mgmt\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_iam_permission_boundaries_delegating_role_creation/4_knowledge_check/","title":"Knowledge Check","tags":[],"description":"","content":" The security best practices followed in this lab are:\n Manage credentials and authentication Use of MFA for access to provide additional access control. Grant access through roles or federation: Roles with associated policies have been used to define appropriate permission boundaries. Grant least privileges: The roles are scoped with minimum privileges to accomplish the task.  "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_iam_tag_based_access_control_for_ec2/4_knowledge_check/","title":"Knowledge Check","tags":[],"description":"","content":" The security best practices followed in this lab are:\n Grant least privileges: The roles are scoped with minimum privileges to accomplish the task.  "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_100_quick_steps_to_security_success/4_monitoring_and_alerting/","title":"Monitoring and Alerting","tags":[],"description":"","content":"Lastly, we will setup your foundations for monitoring the security status of your AWS environment and look at how we can build some basic alerting to security incidents. AWS Security Hub gives you a comprehensive view of the security of your account including compliance checks against best practices such as the Centre for Information Security AWS Foundational Benchmark . We will also enable Amazon GuardDuty - a threat detection service which leverages machine learning to detect anomalies across your AWS CloudTrail, Amazon VPC Flow Logs, and DNS logs.\nBoth Security Hub and Guard Duty have a concept of a \u0026ldquo;management\u0026rdquo; and \u0026ldquo;member\u0026rdquo; account. The management account will receive data for all member accounts that are enrolled in it. A best practice is to enable your security audit account to be the management where your security team has read-only access to it. In addition to enabling these tools, setup notifications to ensure that you receive alerts as they occur. Develop a process per alert to handle incident response and over time automate your responses.\nWalk through Note that these steps need to be repeated in each region you wish to monitor.\n Amazon GuardDuty has the ability to delegate administration. Follow the instructions in the documentation to designate a Delegated Administrator and add Member Accounts . Use the audit account as your delegated administrator account. This will enable GuardDuty for all accounts within your organization and make the findings accessible from the Audit account. Enable AWS Security Hub . Follow the steps outlined in the blog post Automating AWS Security Hub Alerts with AWS Control Tower lifecycle events If you are not using Control Tower you can leverage the AWS Security Hub Multi-account Scripts to enable it across accounts. In your audit account, send AWS Security Findings to Email to ensure that your security team is alerted as findings are triggered.  "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/100_labs/100_walkthrough_of_the_well-architected_tool/4_save_milestone/","title":"Saving a milestone","tags":[],"description":"","content":"  From the detail page for the workload, click the Save milestone button:   Enter a name for the milestone as AWS Workshop Milestone and click the Save button:   Click on the Milestones tab:   This will display the milestone and data about it:    function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/5_view_report\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_remote_configuration_installation_and_viewing_cloudwatch_logs/4_start_cw_agent/","title":"Start the CloudWatch Agent","tags":[],"description":"","content":"Now that your CloudWatch agent is installed on your EC2 Instance, we need to load the configuration file and restart the CloudWatch agent in order to begin collecting logs. This can be done remotely from the Systems Manager console using Run Command.\n Open the Systems Manager console . Choose Run command from the left side menu under Instances \u0026amp; Nodes. Click Run Command on the page that opens up. In the Command document box, click in the search bar. Select Document name prefix, then equals, and enter AmazonCloudWatch-ManageAgent. Select the command that appears in the results. This command sends commands directly to the CloudWatch agent on your instances by remotely running scripts on the instance. You will be sending a configure command with the created parameter from Parameter Store to instruct the CloudWatch agent installed on the EC2 instance to use this configuration and start collecting logs.  Under Command parameters:  Set Action to Configure. Set Mode to ec2. Set Optional Configuration Source to ssm. Set Optional Configuration Location to the name of the parameter you created in Parameter Store. If you used the name provided above, it should be called AmazonCloudWatch-securitylab-cw-config. Set Optional Restart to yes.    Under Targets:  Select Choose instances manually. You should see a list of running instances. Select the instance that was launched by the CloudFormation template you deployed for this lab. This will be named Security-CW-Lab-Instance.   Under Output Options, deselect Enable writing to an S3 bucket. Choose Run. Optionally, in the Targets and outputs areas, select the button next to an instance name and choose View output. Systems Manager should show that the agent was successfully installed in a few seconds.  Recap: In this section, you started the CloudWatch Agent on your EC2 instance using Systems Manager Run Command. The command ran a shell script on the EC2 instance. This script instructs the CloudWatch agent to use the configuration file stored in Parameter Store, which gives the agent information on where to collect logs from, how often to collect them, and how to store them in CloudWatch. The script instructs the agent to reboot and begin collecting logs. This enables people to perform actions at a distance by not directly accessing the instance.\n function prevStep(){ window.open(\"..\\/3_create_cw_config.md\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/5_generate_logs\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_4_cost_and_usage_analysis/4_tear_down/","title":"Tear down","tags":[],"description":"","content":"There is no configuration performed within this lab, so no teardown is required.\n function prevStep(){ window.open(\"..\\/3_cost_usage_download\\/\", \"_self\") } function nextStep(){ window.open(\"\\/cost\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with COST1 - \u0026ldquo;How do you implement cloud financial management?\u0026quot;\nClick here to access the Well-Architected Tool   Previous Step Complete this lab     "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_3_pricing_models/4_tear_down/","title":"Tear down","tags":[],"description":"","content":"There are no resources or configuration items that are created during this workshop.\n function prevStep(){ window.open(\"..\\/3_filter_csv\\/\", \"_self\") } function nextStep(){ window.open(\"\\/cost\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with COST7 - \u0026ldquo;How do you use pricing models to reduce cost?\u0026quot;\nClick here to access the Well-Architected Tool   Previous Step Complete this lab     "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_4_cost_and_usage_analysis/4_tear_down/","title":"Tear down","tags":[],"description":"","content":"Amazon Athena only charges when it is being used, i.e. data is being scanned - so if it is not being actively queried, there are no charges. There may be some charges from AWS Glue if it is above the free tier limit.\nAs it is best practice to regularly analyze your usage and cost, so there is no teardown for this lab.\n function prevStep(){ window.open(\"..\\/3_cur_analysis\\/\", \"_self\") } function nextStep(){ window.open(\"\\/cost\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with COST3 - \u0026ldquo;How do you monitor usage and cost?\u0026quot;\nClick here to access the Well-Architected Tool   Previous Step Complete this lab     "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_5_cost_visualization/4_tear_down/","title":"Tear down","tags":[],"description":"","content":"It is best practice to regularly analyze your usage and cost, so you should not tear down this lab unless you have an alternative visualization solution.\nDelete your Dashboard   Go to the QuickSight homepage, and select All dashboards:   Click the 3 dots next to the dashboard you created:   Click Delete:   Click Delete:   Delete your Analysis   Click on All analyses:   Click the 3 dots next to the analysis you created:   Click Delete:   Click Delete to confirm:   Delete the dataset   Click on Manage data:   Select the dataset:   Click Delete data set:   Confirm by clicking Delete:    function prevStep(){ window.open(\"..\\/3_share_analysis\\/\", \"_self\") } function nextStep(){ window.open(\"\\/cost\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with COST3 - \u0026ldquo;How do you monitor usage and cost?\u0026quot;\nClick here to access the Well-Architected Tool   Previous Step Complete this lab     "},{"uri":"https://wellarchitectedlabs.com/security/100_labs/100_cloudfront_with_s3_bucket_origin/4_cleanup/","title":"Tear down","tags":[],"description":"","content":"The following instructions will remove the CloudFront distribution and S3 bucket created in this lab.\nDelete the CloudFront distribution:\n Open the Amazon CloudFront console at (https://console.aws.amazon.com/cloudfront/home). From the console dashboard, select the distribution you created earlier and click the Disable button. To confirm, click the Yes, Disable button. After approximately 15 minutes when the status is Disabled, select the distribution and click the Delete. button, and then to confirm click the Yes, Delete button.  Delete the S3 bucket:\n Open the Amazon S3 console at https://console.aws.amazon.com/s3/ . Check the box next to the bucket you created previously, then click Empty from the menu. Confirm the bucket you are emptying. Once the bucket is empty check the box next to the bucket, then click Delete from the menu. Confirm the bucket you are deleting.   "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_automated_iam_user_cleanup/2_cleanup/","title":"Tear down","tags":[],"description":"","content":"It is recommended to keep this lab in place to continuously audit your environment. To remove this stack execute:\naws cloudformation delete-stack --stack-name IAM-User-Cleanup\n"},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_basic_ec2_with_waf_protection/4_cleanup/","title":"Tear down","tags":[],"description":"","content":"The following instructions will remove the resources that have a cost for running them. Please note that Security Groups and SSH key will exist. You may remove these also or leave for future use.\nTerminate the instance:\n Sign in to the AWS Management Console, and open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. From the left console instance menu, select Instances. Select the instance you created to terminate. From the Actions button (or right click) select Instance State \u0026gt; Terminate.  Verify this is the instance you want terminated, then click the Yes, Terminate button.  Delete the Application Load Balancer:\n Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. From the console dashboard, choose Load Balancers from the Load Balancing section. Choose the load balancer you created previously such as lab-alb and click Actions, then Delete. Confirm by clicking Yes, Delete. From the console dashboard, choose Target Groups from the Load Balancing section. Choose the target group you created previously such as lab-alb and click Actions, then Delete.  Delete the AWS WAF stack:\n Open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Select the lab-waf-regional stack. Click the Actions button, and then click Delete Stack. Confirm the stack, and then click the Yes, Delete button.  "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_lambda_cross_account_iam_role_assumption/4_cleanup/","title":"Tear down","tags":[],"description":"","content":"Remove the lambda function, then roles.\n "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_bidirectional_replication_for_s3/4_cleanup/","title":"Tear down this lab","tags":[],"description":"","content":"If you are attending an in-person workshop and were provided with an AWS account by the instructor:\n There is no need to tear down the lab. Feel free to continue exploring. Log out of your AWS account when done.  If you are using your own AWS account:\n You may leave these resources deployed for as long as you want. When you are ready to delete these resources, see the following instructions  Empty the S3 buckets You cannot delete an Amazon S3 bucket unless it is empty, so you need to empty the buckets you created. There are a total of four buckets:\n Replication bucket in east region: \u0026lt;your_naming_prefix\u0026gt;-crrlab-us-east-2 Replication bucket in west region: \u0026lt;your_naming_prefix\u0026gt;-crrlab-us-west-2 Logging bucket in east region: logging-\u0026lt;your_naming_prefix\u0026gt;-us-east-2 Logging bucket in west region: logging-\u0026lt;your_naming_prefix\u0026gt;-us-west-2  Go to the Amazon S3 console , or if you are already there click on Amazon S3 in the upper left corner\nFor each of he four buckets do the following:\n Select the radio button next to the bucket Click Empty Type the bucket name in the confirmation box Click Empty After you see the message Successfully emptied bucket then click Exit For the logging buckets it is also recommended your delete the bucket now to prevent the logs from writing more data there after you empty it  Follow the same steps as above, but click the Delete button (instead of Empty)    Remove AWS CloudFormation provisioned resources If you are using an AWS supplied to you as part of an in-person AWS workshop\nHow to delete an AWS CloudFormation stack If you are already familiar with how to delete an AWS CloudFormation stack, then skip to the next section: Delete workshop CloudFormation stacks\n Go to the AWS CloudFormation console: https://console.aws.amazon.com/cloudformation Select the CloudFormation stack to delete and click Delete In the confirmation dialog, click Delete stack The Status changes to DELETE_IN_PROGRESS Click the refresh button to update and status will ultimately progress to DELETE_COMPLETE When complete, the stack will no longer be displayed. To see deleted stacks use the drop down next to the Filter text box. To see progress during stack deletion  Click the stack name Select the Events column Refresh to see new events    Delete workshop CloudFormation stacks  First delete the S3-CRR-lab-east CloudFormation stack in Ohio (us-east-2) Then delete the S3-CRR-lab-west CloudFormation stack in Oregon (us-west-2)  Troubleshooting: if your CloudFormation stack deletion fails with status DELETE_FAILED and error (from the Events tab) Cannot delete entity, must detach all policies first then see these additional instructions  function prevStep(){ window.open(\"..\\/3_test_replication\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/5_resources\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_testing_for_resiliency_of_ec2/4_cleanup/","title":"Tear down this lab","tags":[],"description":"","content":"The following instructions will remove the resources that you have created in this lab.\nIf you deployed the CloudFormation stacks as part of the prerequisites for this lab, then delete these stacks to remove all the AWS resources. If you need help with how to delete CloudFormation stacks then follow these instructions to tear down those resources:\n Delete the WebApp resources  Wait for this stack deletion to complete Delete the VPC resources   Otherwise, there were no additional new resources added as part of this lab.\n References \u0026amp; useful resources AWS CloudFormation User Guide Amazon EC2 User Guide for Linux Instances  function prevStep(){ window.open(\"..\\/3_failure_injection\\/\", \"_self\") } function nextStep(){ window.open(\"\\/cost\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with REL 12 How do you test reliability?\nClick here to access the Well-Architected Tool   Previous Step Complete this lab     "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_automated_deployment_of_ec2_web_application/2_cleanup/","title":"Tear down this lab","tags":[],"description":"","content":"The following instructions will remove the resources that you have created in this lab.\nDelete the WordPress or Static Web Application CloudFormation stack:\n Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Click the radio button on the left of the WebApp1-WordPress or WebApp1-Static stack. Click the Actions button then click Delete stack. Confirm the stack and then click Delete button. Access the Key Management Service (KMS) console https://console.aws.amazon.com/cloudformation/    References \u0026amp; useful resources AWS CloudFormation User Guide Amazon EC2 User Guide for Linux Instances "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_cloudfront_with_waf_protection/4_cleanup/","title":"Tear down this lab","tags":[],"description":"","content":"The following instructions will remove the resources that have a cost for running them. Please note that Security Groups and SSH key will exist. You may remove these also or leave for future use.\nDelete the CloudFront distribution:\n Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. From the console dashboard, select the distribution you created earlier and click the Disable button. To confirm, click the Yes, Disable button. After approximately 15 minutes when the status is Deployed, select the distribution and click the Delete button, and then to confirm click the Yes, Delete button.  Delete the AWS WAF stack:\n Sign in to the AWS Management Console, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Select the waf-cloudfront stack. Click the Actions button, and then click Delete Stack. Confirm the stack, and then click the Yes, Delete button.   References \u0026amp; useful resources Amazon Elastic Compute Cloud User Guide for Linux Instances Amazon CloudFront Developer Guide Tutorial: Configure Apache Web Server on Amazon Linux 2 to Use SSL/TLS AWS WAF, AWS Firewall Manager, and AWS Shield Advanced Developer Guide "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_workload_efficiency/4_tear_down/","title":"Teardown","tags":[],"description":"","content":"The follwoing resources were created during this lab and can be deleted:\n S3 bucket, name starting with costefficiency Glue Classifier WebLogs Glue Crawler ApplicationLogs IAM Role \u0026amp; Policy AWSGlueServiceRole-CostWebLogs Glue Database webserverlogs Crawler CostUsage IAM Role \u0026amp; Policy AWSGlueServiceRole-Costusage Glue Database CostUsage Athena table costusagefiles_workshop.hourlycost Athena table costusagefiles_workshop.efficiency QuickSight dataset efficiency QuickSight Analysis efficiency analysis   function prevStep(){ window.open(\"..\\/3_visualizations\\/\", \"_self\") } function nextStep(){ window.open(\"\\/cost\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with COST 3 - \u0026ldquo;How do you monitor usage and cost? \u0026ldquo;\nClick here to access the Well-Architected Tool   Previous Step Complete this lab     "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/4_failure_injection_ec2/","title":"Test Resiliency Using EC2 Failure Injection","tags":[],"description":"","content":"4.1 EC2 failure injection This failure injection will simulate a critical problem with one of the three web servers used by your service.\n  Before starting, view the deployment machine in the AWS Step Functions console to verify the deployment has reached the stage where you can start testing:\n single region: WaitForWebApp shows completed (green) multi region: WaitForWebApp1 shows completed (green)    Navigate to the EC2 console at http://console.aws.amazon.com/ec2 and click Instances in the left pane.\n  There are three EC2 instances with a name beginning with WebServerforResiliency. For these EC2 instances note:\n Each has a unique Instance ID There is one instance per each Availability Zone All instances are healthy    Open up two more console in separate tabs/windows. From the left pane, open Target Groups and Auto Scaling Groups in separate tabs. You now have three console views open\n  To fail one of the EC2 instances, use the VPC ID as the command line argument replacing \u0026lt;vpc-id\u0026gt; in one (and only one) of the scripts/programs below. (choose the language that you setup your environment for)\n   Language Command     Bash ./fail_instance.sh \u0026lt;vpc-id\u0026gt;   Python python fail_instance.py \u0026lt;vpc-id\u0026gt;   Java java -jar app-resiliency-1.0.jar EC2 \u0026lt;vpc-id\u0026gt;   C# .\\AppResiliency EC2 \u0026lt;vpc-id\u0026gt;   PowerShell .\\fail_instance.ps1 \u0026lt;vpc-id\u0026gt;      The specific output will vary based on the command used, but will include a reference to the ID of the EC2 instance and an indicator of success. Here is the output for the Bash command. Note the CurrentState is shutting-down\n $ ./fail_instance.sh vpc-04f8541d10ed81c80 Terminating i-0710435abc631eab3 { \u0026quot;TerminatingInstances\u0026quot;: [ { \u0026quot;CurrentState\u0026quot;: { \u0026quot;Code\u0026quot;: 32, \u0026quot;Name\u0026quot;: \u0026quot;shutting-down\u0026quot; }, \u0026quot;InstanceId\u0026quot;: \u0026quot;i-0710435abc631eab3\u0026quot;, \u0026quot;PreviousState\u0026quot;: { \u0026quot;Code\u0026quot;: 16, \u0026quot;Name\u0026quot;: \u0026quot;running\u0026quot; } } ] }    Go to the EC2 Instances console which you already have open (or click here to open a new one )\n  Refresh it. (Note: it is usually more efficient to use the refresh button in the console, than to refresh the browser)\n  Observe the status of the instance reported by the script. In the screen cap below it is shutting down as reported by the script and will ultimately transition to terminated.\n    4.2 System response to EC2 instance failure Watch how the service responds. Note how AWS systems help maintain service availability. Test if there is any non-availability, and if so then how long.\n4.2.1 System availability Refresh the service website several times. Note the following:\n Website remains available The remaining two EC2 instances are handling all the requests (as per the displayed instance_id)  4.2.2 Load balancing Load balancing ensures service requests are not routed to unhealthy resources, such as the failed EC2 instance.\n  Go to the Target Groups console you already have open (or click here to open a new one )\n If there is more than one target group, select the one with the Load Balancer named ResiliencyTestLoadBalancer    Click on the Targets tab and observe:\n  Status of the instances in the group. The load balancer will only send traffic to healthy instances.\n  When the auto scaling launches a new instance, it is automatically added to the load balancer target group.\n  In the screen cap below the unhealthy instance is the newly added one. The load balancer will not send traffic to it until it is completed initializing. It will ultimately transition to healthy and then start receiving traffic.\n  Note the new instance was started in the same Availability Zone as the failed one. Amazon EC2 Auto Scaling automatically maintains balance across all of the Availability Zones that you specify.\n    From the same console, now click on the Monitoring tab and view metrics such as Unhealthy hosts and Healthy hosts\n  4.2.3 Auto scaling Autos scaling ensures we have the capacity necessary to meet customer demand. The auto scaling for this service is a simple configuration that ensures at least three EC2 instances are running. More complex configurations in response to CPU or network load are also possible using AWS.\n  Go to the Auto Scaling Groups console you already have open (or click here to open a new one )\n If there is more than one auto scaling group, select the one with the name that starts with WebServersforResiliencyTesting    Click on the Activity History tab and observe:\n  The screen cap below shows that all three instances were successfully started at 17:25\n  At 19:29 the instance targeted by the script was put in draining state and a new instance ending in \u0026hellip;62640 was started, but was still initializing. The new instance will ultimately transition to Successful status\n    Draining allows existing, in-flight requests made to an instance to complete, but it will not send any new requests to the instance. Learn more: After the lab see this blog post for more information on draining.\nLearn more: After the lab see Auto Scaling Groups to learn more how auto scaling groups are setup and how they distribute instances, and Dynamic Scaling for Amazon EC2 Auto Scaling for more details on setting up auto scaling that responds to demand\n4.2.4 EC2 failure injection - conclusion Deploying multiple servers and Elastic Load Balancing enables a service suffer the loss of a server with no availability disruptions as user traffic is automatically routed to the healthy servers. Amazon Auto Scaling ensures unhealthy hosts are removed and replaced with healthy ones to maintain high availability.\n function prevStep(){ window.open(\"..\\/3_failure_injection_prep\", \"_self\") } function nextStep(){ window.open(\"..\\/5_failure_injection_rds\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_5_cost_visualization/4_elasticity/","title":"View your Elasticity","tags":[],"description":"","content":"NOTE: This exercise requires you have enabled hourly granularity within Cost Explorer, this can be done by following the instructions here - AWS Account Setup , Step 6 - Enable Cost Explorer. There are additional costs to enable this granularity.\nA key part of cost optimization is ensuring that your systems scale with your usage. This visualization will show how your systems operate over time.\n  Click on Cost Explorer to go back to the default view:   Click the down arrow to change the period, select 14D and click Apply:   Click on Monthly and change the granularity to Hourly:   Click on Bar, then select Line:   You will now have in depth insight to how your environment is operating. You can see in this example the EC2 Instances scaling every day, you can see a period of large ELB usage, and EC2-Other, which includes charges related to EC2 such as data transfer.    function prevStep(){ window.open(\"..\\/3_sp_coverage\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/5_ri_coverage\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_3_pricing_models/4_visualize_recommendations/","title":"Visualize your Savings Plan recommendations","tags":[],"description":"","content":"A visualization of your recommendation can be used as a quick double check, and also assist to demonstrate the savings and risks to other job functions.\nWe will use the Cost Explorer hourly granularity feature to visualize Savings Plan recommendations. You need to have this enabled to view hourly usage, and there are associated costs.\n  In the console go to the Billing Dashboard:   Click on Savings Plans:   Click on Recommendations, and select EC2 Instance, 1-year, All upfront and 7 days:   Scroll down and pick a specific instance type:   Scroll up and click Cost Explorer:   Click the time period Last 7 days in this example, then click 7D and click Apply:   Click the granularity Monthly in this example, and select Hourly:   Click the Service filter, select EC2-Instances (Elastic Compute Cloud - Compute) and click Apply filters:   Click Instance Type in the Group by menu:   Click on Stack and select Line:   Apply a filter on the region if you use multiple regions:   If you have multiple instance types for a single family, you can select them by using a filter and choosing a stack graph. This will show the total costs for that family in the required region.\n  Click More filters:   Click Purchase Option, select On Demand and click Apply:   Hover over the recommendation that matches the family you chose:   Cost Explorer gives $0.51 hourly usage of C5.large which is the only instance in the C5 family. Our recommended commitment was $0.30 per hour for C5 all upfront. Go to the pricing page: https://aws.amazon.com/savingsplans/pricing/\n  Select the correct parameters that match the recommendation and view the rates:   The savings is 41%. We were using $0.51, so multiplied by 1-41% (0.59) = $0.30.\n  We can see the recommendation is accurate and valid, and see the usage pattern associated with the recommendation.\n function prevStep(){ window.open(\"..\\/3_analyze_recommendations\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/5_tear_down\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_4_cost_and_usage_analysis/","title":"Level 100: Cost and Usage Analysis","tags":[],"description":"","content":"Last Updated May 2020\nAuthors  Nathan Besh, Cost Lead Well-Architected  Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com\nIntroduction This hands-on lab will guide you through the steps to perform analysis of your AWS cost and usage. The skills you learn will help you monitor your cost and usage, in alignment with the AWS Well-Architected Framework.\nGoals  Perform basic analysis of your cost and usage  Prerequisites  AWS Account Setup has been completed An AWS account with usage for more than 1 month  Permissions required  Log in as the Cost Optimization team, created in AWS Account Setup  NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required.  Costs  There are no costs for this lab  Time to complete  The lab should take approximately 10 minutes to complete  Steps:  View your AWS Invoices   View your cost and usage in detail   Download your monthly cost and usage file   Tear down    function prevStep(){ window.open(\"\", \"_self\") } function nextStep(){ window.open(\".\\/1_view_invoice\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool  Start Lab  "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_4_cost_and_usage_analysis/","title":"Level 200: Cost and Usage Analysis","tags":[],"description":"","content":"Last Updated May 2020\nAuthors  Nathan Besh, Cost Lead, Well-Architected Spencer Marley, Commercial Architect  Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com\nIntroduction This hands-on lab will guide you through the steps to setup a platform to analyze your cost and usage reports. The skills you learn will help you perform analysis on your cost and usage, in alignment with the AWS Well-Architected Framework.\nGoals  Setup an analysis platform for your cost and usage data Perform basic analysis of your cost and usage  Prerequisites  AWS Account Setup has been completed Have usage that is tagged (preferred, not mandatory)  Permissions required  Log in as the Cost Optimization team, created in AWS Account Setup  NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required.  Costs  Variable depending on bill size and analysis performed Approximately less than $5 for the supplied files and small accounts  Time to complete  The lab should take approximately 20 minutes to complete  Steps:  Verify your CUR files are being delivered   Use AWS Glue to enable access to CUR files via Amazon Athena   Cost and Usage analysis   Tear down    function prevStep(){ window.open(\"\", \"_self\") } function nextStep(){ window.open(\".\\/1_verify_cur\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool  Start Lab  "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_testing_for_resiliency_of_ec2/","title":"Level 200: Testing for Resiliency of EC2 instances","tags":["test_resiliency"],"description":"Use code to inject faults simulating EC2 failures. These are used as part of Chaos Engineering to test workload resiliency","content":"Authors  Rodney Lester, Senior Solutions Architect Manager, AWS Well-Architected Seth Eliot, Principal Reliability Solutions Architect, AWS Well-Architected  Introduction The purpose if this lab is to teach you the fundamentals of using tests to ensure your implementation is resilient to failure by injecting failure modes into your application. This may be a familiar concept to companies that practice Failure Mode Engineering Analysis (FMEA). It is also a key component of Chaos Engineering, which uses such failure injection to test hypotheses about workload resiliency. One primary capability that AWS provides is the ability to test your systems at a production scale, under load.\nIt is not sufficient to only design for failure, you must also test to ensure that you understand how the failure will cause your systems to behave. The act of conducting these tests will also give you the ability to create playbooks how to investigate failures. You will also be able to create playbooks for identifying root causes. If you conduct these tests regularly, then you will identify changes to your application that are not resilient to failure and also create the skills to react to unexpected failures in a calm and predictable manner.\nIn this lab, you will deploy a 2-tier resource, with a reverse proxy (Application Load Balancer), and Web Application on Amazon Elastic Compute Cloud (EC2).\nThe skills you learn will help you build resilient workloads in alignment with the AWS Well-Architected Framework Goals:  Reduce fear of implementing resiliency testing by providing examples in common development and scripting languages Resilience testing of EC2 instances Learn how to implement resiliency using those tests Learn how to think about what a failure will cause within your infrastructure Learn how common AWS services can reduce mean time to recovery (MTTR)  Prerequisites:  An AWS Account that you are able to use for testing, that is not used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to create Amazon Virtual Private Cloud(s) (VPCs), including subnets, security groups, internet gateways, NAT Gateways, Elastic IP Addresses, and route tables. The credentials must also be able to create the database subnet group needed for a Multi-AZ RDS instance. The credential will need permissions to create IAM Role, instance profiles, AWS Auto Scaling launch configurations, application load balancers, auto scaling group, and EC2 instances. An IAM user or federated credentials into that account that has permissions to deploy the deployment automation, which consists of IAM service linked roles, AWS Lambda functions, and an AWS Step Functions state machine to execute the deployment.  Note: This 200 level lab covers only EC2 failure injection. If you would prefer a more feature-rich 300 level lab that demonstrates EC2 failure, RDS failure, and AZ failure then see Level 300: Testing for Resiliency of EC2, RDS, and AZ . That 300 level lab includes everything in this 200 level lab, plus additional fault simulations.\nNOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\n  function prevStep(){ window.open(\"\", \"_self\") } function nextStep(){ window.open(\".\\/1_prerequisite\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool  Start Lab  Steps:  Deploy the Infrastructure and Application   Configure Execution Environment   Test Resiliency Using Failure Injection   Tear down this lab   "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_incident_response_with_aws_console_and_cli/","title":"Level 300: Incident Response with AWS Console and CLI","tags":[],"description":"","content":"Authors  Ben Potter, Security Lead, Well-Architected  Introduction This hands-on lab will guide you through a number of examples of how you could use the AWS Console and Command Line Interface (CLI) for responding to a security incident. It is a best practice to be prepared for an incident, and have appropriate detective controls enabled. You can find more best practices by reading the Security Pillar of the AWS Well-Architected Framework .\nThe skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nGoals  Identify tooling for incident response Automate containment for incident response Pre-deploy tools for incident response  Prerequisites  An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . CloudTrail must already be enabled in your account and logging to CloudWatch Logs, follow the Automated Deployment of Detective Controls lab to enable.  Steps:  Getting Started   Identity \u0026amp; Access Management   Amazon VPC   "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_splitting_sharing_cur_access/","title":"Level 300: Splitting the CUR and Sharing Access","tags":[],"description":"","content":"Authors  Nathan Besh, Cost Lead, Well-Architected  Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com\nIntroduction This hands-on lab will guide you on how to automatically extract part of your CUR file, and then deliver it to another S3 bucket and folder to allow another account to access it. This is useful to allow sub accounts or business units to access their data, but not see the rest of the original CUR file. You can also exclude specific columns such as pricing - only allowing a sub account to view their usage information.\nCommon use cases are:\n Separate linked account data, so each linked account can see only their data Providing sub accounts their data without pricing Separate out specific usage, by tag or service  The lab has been designed to configure a system that can expand easily, for any new requirement:\n Create a new folder in S3 with the required bucket policy Do the one-off back fill for previous months (if required) Create the saved queries in Athena Specify the permissions in the Lambda script  Goals  Automatically extract a portion of the CUR file each time it is delivered Deliver this to a location that is accessible to another account  Prerequisites  Multiple AWS Accounts (At least two) Billing reports auto update configured as per 300_Automated_CUR_Updates_and_Ingestion   Permissions required  Create IAM policies and roles Create and modify S3 Buckets, including policies and events Create and modify Lambda functions Modify CloudFormation templates Create, save and execute Athena queries Create and run a Glue crawler  Steps:  Setup Output S3 Bucket   Perform one off Fill of Member/Linked Data   Create Athena Saved Queries to Write new Data   Create Lambda function to run the Saved Queries   Trigger the Lambda When a CUR is Delivered   Sub Account Crawler Setup   Tear Down    function prevStep(){ window.open(\"\", \"_self\") } function nextStep(){ window.open(\".\\/1_setup_s3_output\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool  Start Lab  "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_300_security_best_practices_workshop_ec2/","title":"Quest: AWS Security Best Practices Workshop","tags":[],"description":"This quest is the guide for security best practices workshop often ran at AWS led events including AWS Summits.","content":"Authors  Ben Potter, Security Lead, Well-Architected Pierre Liddle, Principal Solutions Architect  About this Guide This quest is the guide for an AWS led event including AWS Summits security best practices workshop. Using your own AWS account you will learn through hands-on labs in securing an Amazon EC2-based web application covering identity \u0026amp; access management, detective controls, infrastructure protection, data protection and incident response. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nPrerequisites  An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .   Lab 1 - Identity \u0026amp; Access Management For Lab 1 choose one of labs to run based on your interest or experience:\nLab 1a - IAM Permission Boundaries Delegating Role Creation This hands-on lab will guide you through the steps to configure an example AWS Identity and Access Management (IAM) permission boundary. AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature in which you use a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by the policy.\nIn this lab you will create a series of policies attached to a role that can be assumed by an individual such as a developer, the developer can then use this role to create additional user roles that are restricted to specific services and regions. This allows you to delegate access to create IAM roles and policies, without them exceeding the permissions in the permission boundary. We will also use a naming standard with a prefix, making it easier to control and organize policies and roles that your developers create.\nIAM Permission Boundaries Delegating Role Creation  Lab 1b - IAM Tag Based Access Control for EC2 This hands-on lab will guide you through the steps to configure example AWS Identity and Access Management (IAM) policies, and a AWS IAM role with associated permissions to use EC2 resource tags for access control. Using tags is powerful as it helps you scale your permission management, however you need to be careful about the management of the tags which you will learn in this lab.In this lab you will create a series of policies attached to a role that can be assumed by an individual such as an EC2 administrator. This allows the EC2 administrator to create tags when creating resources only if they match the requirements, and control which existing resources and values they can tag.\nIAM Tag Based Access Control for EC2  Lab 2 - Automated Deployment of VPC This hands-on lab will guide you through the steps to configure an Amazon VPC and outline some of the AWS security features. AWS CloudFormation will be used to automate the deployment and provide a repeatable way to re-use the template after this lab. The example CloudFormation template will deploy a completely new VPC incorporating a number of AWS security best practices which are:\nNetworking subnets created in multiple availability zones for the following network tiers:\n Application Load Balancer - named ALB1 Application instances - named App1 Shared services - named Shared1 Databases - named DB1  VPC endpoints are created for private connectivity to AWS services.NAT Gateways are created to allow different subnets in the VPC to connect to the internet, without any direct ingress access being possible due to Route Table configurations.Network ACLs control access at each subnet layer.While VPC Flow Logs captures information about IP traffic and stores it in Amazon CloudWatch Logs. Do not follow tear down instructions until you have completed this quest, as the EC2 lab requires this VPC.\nAutomated Deployment of VPC  Lab 3 - Automated Deployment of EC2 Web Application This hands-on lab will guide you through the steps to configure a web application in Amazon EC2 with a defense in depth approach.The WordPress example CloudFormation template will deploy a basic WordPress content management system, incorporating a number of AWS security best practices. This example is not intended to be a comprehensive WordPress system, please consult Build a WordPress Website for more information.\nAutomated Deployment of EC2 Web Application  Lab 4 - Automated Deployment of Detective Controls This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail, AWS Config, and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of each service.\nAutomated Deployment of Detective Controls  Lab 5 - Enable Security Hub AWS Security Hub gives you a comprehensive view of your high-priority security alerts and compliance status across AWS accounts. There are a range of powerful security tools at your disposal, from firewalls and endpoint protection to vulnerability and compliance scanners. But oftentimes this leaves your team switching back-and-forth between these tools to deal with hundreds, and sometimes thousands, of security alerts every day. With Security Hub, you now have a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services, such as Amazon GuardDuty, Amazon Inspector, and Amazon Macie, as well as from AWS Partner solutions. Your findings are visually summarized on integrated dashboards with actionable graphs and tables. You can also continuously monitor your environment using automated compliance checks based on the AWS best practices and industry standards your organization follows. Get started with AWS Security Hub in just a few clicks in the Management Console and once enabled, Security Hub will begin aggregating and prioritizing findings.\nEnable Security Hub  Lab 6 - Incident Response with AWS Console and CLI This hands-on lab will guide you through a number of examples of how you could use the AWS Console and Command Line Interface (CLI) for responding to a security incident. It is a best practice to be prepared for an incident, and have appropriate detective controls enabled. You can find more best practices by reading the Security Pillar of the AWS Well-Architected Framework .\nIncident Response with AWS Console and CLI  "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/","title":"100 Labs","tags":[],"description":"","content":"List of labs available  Level 100: AWS Account Setup: Lab Guide   Level 100: Cost and Usage Governance   Level 100: Pricing Models   Level 100: Cost and Usage Analysis   Level 100: Cost Visualization   Level 100: EC2 Rightsizing   Level 100: Goals and Targets   "},{"uri":"https://wellarchitectedlabs.com/security/quests/","title":"Quests","tags":[],"description":"","content":"Quests are designed to collate a group of relevant labs and other resources together into a common theme for you to follow and learn.\nThe following quests are aligned to the security best practice questions in AWS Well-Architected.\nList of quests available  Quest: Loft - Introduction to Security  Introduction to AWS security basics, used as the workshop in AWS loft events.\n Quest: Quick Steps to Security Success  In just one day (or an hour a day for a week!) implement some foundational security controls to immediately improve your security posture.\n Quest: AWS Incident Response Day  This quest is the guide for incident response workshop often ran at AWS led events.\n Quest: re:Invent 2020 - Automate The Well-Architected Way With WeInvest  This quest is a collection of lab patterns which are covered in the upcoming session at re:Invent 2020: Automate The Well-Architected Way with WeInvest\n Quest: AWS Security Best Practices Workshop  This quest is the guide for security best practices workshop often ran at AWS led events including AWS Summits.\n Quest: AWS Security Best Practices Day  This quest is the guide for an AWS led event including security best practices day. Includes identity \u0026amp; access management, detective controls, infrastructure protection, data protection and incident response.\n Quest: Managing Credentials \u0026amp; Authentication    Quest: Control Human Access    Quest: Control Programmatic Access    Quest: Detect \u0026amp; Investigate Events    Quest: Defend Against New Threats    Quest: Protect Networks    Quest: Protect Compute    Quest: Classify Data    Quest: Protect Data at Rest    Quest: Protect Data in Transit    Quest: Incident Response    "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_goals_and_targets/4_decommission_resources/","title":"Decommission Resources","tags":[],"description":"","content":"Decommission resources Decommission resources automatically   Goal: Reduce decommission costs of workloads Target: All workloads are to have automatic decommission of non-ephemeral storage resources. Best Practice: Decommission resources automatically  Measures: % of workloads with automatic decommission, cost of non-decommissioned resources Good/Bad: Good Why? When does it work well or not?: Reduce cost of manual decommission work Contact/Contributor: natbesh@amazon.com     Goal: Reduce waste Target: Reduce waste by x% Best Practice: Decommission resources  Measures: amount of waste removed Good/Bad: Bad Why? When does it work well or not?: Does not work long term as it requires waste to always exist, but it could be possibly used as a short term goal. Instead, set a positive goal of minimizing waste creation - such as having waste not exceed a set $ amount. Contact/Contributor: natbesh@amazon.com   function prevStep(){ window.open(\"..\\/3_monitor_cost_usage\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/5_select_services\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_testing_backup_and_restore_of_data/5_cleanup/","title":"Teardown","tags":[],"description":"","content":"The following instructions will remove the resources that you have created in this lab.\nCleaning up AWS Backup Resources  Sign in to the AWS Management Console and navigate to the AWS Backup console - https://us-east-1.console.aws.amazon.com/backup/home?region=us-east-1#home Click on BACKUP VAULTS from the menu on the left side, and select BACKUP-LAB-VAULT. Under the section BACKUPS, delete all the RECOVERY POINTS. Once all the RECOVERY POINTS have been deleted, delete the Backup Vault by clicking on DELETE on the top right hand corner. Click on BACKUP PLANS from the menu on the left side, and select BACKUP-LAB. Scroll down to the section RESOURCE ASSIGNMENTS, and delete the resource assignment. Delete the BACKUP PLAN by clicking on DELETE on the upper right corner of the screen.  Cleaning up the CloudFormation Stack  Sign in to the AWS Management Console and navigate to the AWS CloudFormation console - https://console.aws.amazon.com/cloudformation/ Select the stack WA-Backup-Lab, and delete the stack.  Cleaning up the CloudWatch Logs  Sign in to the AWS Management Console, and open the CloudWatch console at https://console.aws.amazon.com/cloudwatch/ . Click Logs in the left navigation. Click the radio button on the left of the /aws/lambda/RestoreTestFunction. Click the Actions Button then click Delete Log Group. Verify the log group name then click Yes, Delete.  Thank you for using this lab  function prevStep(){ window.open(\"..\\/4_test_restore\\/\", \"_self\") } function nextStep(){ window.open(\"\\/cost\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with REL 9 How do you back up data?\nClick here to access the Well-Architected Tool   Previous Step Complete this lab     "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_cur_queries/queries/container/","title":"Container","tags":[],"description":"","content":"These are queries for AWS Services under the Container product family .\nUse the clipboard in the top right of the text boxes below to copy all of the text to be pasted.\n You may need to change variables used as placeholders in your query. ${table_Name} is a common variable which needs to be replaced. Example: cur_db.cur_table\n Table of Contents   Amazon Elastic Container Services   Query Description This query will output the daily cost and usage per resource, by operation and service, for Elastic Consainer Services, ECS and EKS, both unblended and amortized costs are shown. To provide you with a complete picture of the data, to match totals in cost explorer, if you are using Savings Plans you will likely see results with blank resource IDs, these represent the Savings Plans Negation values for compute cost already covered by Savings Plans.\nPricing Please refer to the Amazon ECS pricing page and the Amazon EKS pricing page .\nSample Output Sample output includes a subset of query columns Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, SPLIT_PART(line_item_resource_id,':',6) as split_line_item_resource_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d') AS day_line_item_usage_start_date, line_item_operation, line_item_product_code, SUM(CAST(line_item_usage_amount AS double)) AS sum_line_item_usage_amount, SUM(line_item_unblended_cost) \u0026quot;sum_line_item_unblended_cost\u0026quot; , sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0 WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0 WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;reservation_effective_cost\u0026quot; ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) \u0026quot;amortized_cost\u0026quot; FROM ${table_name} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') and line_item_product_code in ('AmazonECS','AmazonEKS') AND line_item_line_item_type NOT IN ('Tax','Credit','Refund','EdpDiscount') GROUP BY bill_payer_account_id, line_item_usage_account_id, 3, 4, line_item_operation, line_item_product_code ORDER BY day_line_item_usage_start_date, sum_line_item_unblended_cost, sum_line_item_usage_amount, line_item_operation;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         Amazon ECS - Daily Usage Hours and Cost by Usage Type and Purchase Option   Query Description This query will output the daily ECS cost and usage per resource, by usage type and purchase option, both unblended and amortized costs are shown. To provide you with a complete picture of the data, to match totals in cost explorer, if you are using Savings Plans you will likely see results with blank resource IDs, these represent the Savings Plans Negation values for compute cost already covered by Savings Plans.\nPricing Please refer to the Amazon ECS pricing page .\nSample Output Sample output includes a subset of query columns Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d') AS day_line_item_usage_start_date, SPLIT_PART(SPLIT_PART(line_item_resource_id,':',6),'/',2) AS split_line_item_resource_id, CASE WHEN line_item_usage_type LIKE '%%Fargate-GB%%' THEN 'GB per hour' WHEN line_item_usage_type LIKE '%%Fargate-vCPU%%' THEN 'vCPU per hour' END AS case_line_item_usage_type, CASE line_item_line_item_type WHEN 'SavingsPlanCoveredUsage' THEN CASE savings_plan_offering_type WHEN 'ComputeSavingsPlans' THEN 'Compute Savings Plans' ELSE savings_plan_offering_type END WHEN 'SavingsPlanNegation' THEN CASE savings_plan_offering_type WHEN 'ComputeSavingsPlans' THEN 'Compute Savings Plans' ELSE savings_plan_offering_type END ELSE CASE pricing_term WHEN 'OnDemand' THEN 'OnDemand' WHEN '' THEN 'Spot Instance' ELSE pricing_term END END AS case_pricing_term, SUM(CAST(line_item_usage_amount AS double)) AS sum_line_item_usage_amount, SUM(CASE pricing_term WHEN 'OnDemand' THEN line_item_unblended_cost WHEN '' THEN line_item_unblended_cost END) AS sum_line_item_unblended_cost, SUM(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0 WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0 WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;reservation_effective_cost\u0026quot; ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) \u0026quot;amortized_cost\u0026quot; FROM ${table_name} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND line_item_product_code in ('AmazonECS') AND line_item_operation != 'ECSTask-EC2' AND product_product_family != 'Data Transfer' AND line_item_line_item_type NOT IN ('Tax','Credit','Refund','EdpDiscount') GROUP BY bill_payer_account_id, line_item_usage_account_id, 3, 4, 5, 6 ORDER BY day_line_item_usage_start_date ASC, case_pricing_term, sum_line_item_usage_amount DESC;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback       CUR queries are provided as is. We recommend validating your data by comparing it against your monthly bill and Cost Explorer prior to making any financial decisions. If you wish to provide feedback on these queries, there is an error, or you want to make a suggestion, please email: curquery@amazon.com\n "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_using_awscli_to_manage_wa_reviews/5_view_report/","title":"Viewing and downloading the report","tags":[],"description":"","content":"Overview You can generate a workload report for a lens. The report contains your responses to the workload questions, your notes, and the current number of high and medium risks identified.\n1. Gather pillar and risk data for a workload  Using the get-lens-review API , you can retrieve the pillar review summaries for the workload: aws wellarchitected get-lens-review --workload-id \u0026quot;\u0026lt;WorkloadId\u0026gt;\u0026quot; --lens-alias \u0026quot;wellarchitected\u0026quot;  This will return a summary of the workload review summaries for each pillar.\n  1. Generate and download workload PDF  Using the get-lens-review-report , you can retrieve the pillar review report in PDF format: aws wellarchitected get-lens-review-report --workload-id \u0026quot;\u0026lt;WorkloadId\u0026gt;\u0026quot; --lens-alias \u0026quot;wellarchitected\u0026quot; --query 'LensReviewReport.Base64String' --output text | base64 --decode \u0026gt; WAReviewOutput.pdf  This will export the object into a PDF report file.\n   function prevStep(){ window.open(\"..\\/4_save_milestone\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/6_programmatic\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_fault_isolation_with_shuffle_sharding/5_implement_shuffle_sharding/","title":"Implement shuffle sharding","tags":[],"description":"","content":"In this section we will update the architectural design of the workload and implement shuffle sharding. Shuffle sharding is a combinatorial implementation of a sharded architecture. With shuffle sharding we create virtual shards with a subset of the capacity of the workload ensuring that the virtual shards are mapped to a unique subset of customers with no overlap. By minimizing the number of Workers a single customer is able to interact with within the workload, and spreading resources in a combinatorial way, we will be able to further reduce the impact of a potential posion pill. In a shuffle sharded system, the scope of impact of failures can be calculated using the following formula:\nThe formula can be expanded to calculate the number of unique combinations that can exist given the number of workers and the number of workers per shard, also referred to as shard size. The calculation is performed using factorials.\nFor example if there were 100 workers, and we assign a unique combination of 5 workers to a shard, then the failure of any 1 shard will only impact 0.0000013% of customers.\nUpdate the workload architecture   Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and select the stack that was created as part of this lab - Shuffle-sharding-lab\n  Click on Update\n  Under Prerequisite - Prepare template, select Replace current template\n For Template source select Amazon S3 URL In the text box under Amazon S3 URL specify https://aws-well-architected-labs-virginia.s3.amazonaws.com/Reliability/300_Fault_Isolation_with_Shuffle_Sharding/shuffle-sharding.yaml    Click Next\n  No changes are required for Parameters. Click Next\n  For Configure stack options click Next\n  On the Review page:\n Scroll to the end of the page and select I acknowledge that AWS CloudFormation might create IAM resources with custom names. This ensures CloudFormation has permission to create resources related to IAM. Additional information can be found here .  Note: The template creates an IAM role and Instance Profile for EC2. These are the minimum permissions necessary for the instances to be managed by AWS Systems Manager. These permissions can be reviewed in the CloudFormation template under the \u0026ldquo;Resources\u0026rdquo; section - InstanceRole.\n Click Update stack    This will take you to the CloudFormation stack status page, showing the stack update in progress. The stack takes about 1 minute to go through all the updates. Periodically refresh the page until you see that the Stack Status is in UPDATE_COMPLETE.\nWith this stack update, the architecture of the workload has been updated by introducing 6 Application Load Balancer listener rules and Target Groups. These listener rules have been configured to inspect the incoming request for a query-string name. Depending on the value provided, the request is routed to one of six target groups where each target group consists of 2 EC2 instances.\nTest the shuffle sharded application Now that the application has been deployed, it is time to test it to understand how it works. The sample application used in this lab is the same as before, a simple web application that returns a message with the Worker that responded to the request. Customers pass in a query string with the request to identify themselves. The query string used here is name.\n  Copy the URL provided in the Outputs section of the CloudFormation stack created in the previous string. Append the query string /?name=Alpha to the URL and paste it into a web browser. The full string should look similar to this - http://shuffle-alb-1p2xbmzo541rr-1602891463.us-east-1.elb.amazonaws.com/?name=Alpha\n  Refresh the web browser a few times to see that responses are being returned from different EC2 instances on the back-end. Notice that after implementing shuffle sharding, you are seeing responses being returned from only 2 instances for customer Alpha\u0026rsquo;s requests. No matter how many times you refresh the page or try a different browser, customer Alpha will only receive responses from 2 EC2 instances. This is because we have created Application Load Balancer listener rules that divert traffic to a specific subset of the overall capacity of the workload, also known as a shard. Each customer has a unique combination of EC2 instances that will respond to requests with no 2 customers having the same combination. The following diagram provides a breakdown of how customers are mapped to EC2 instances.\n  Update the value for the query string to one of the other customers, the possible values are - Alpha, Bravo, Charlie, Delta, Echo, and Foxtrot\n http://shuffle-alb-8vonmf2ywl5z-682850122.us-east-1.elb.amazonaws.com/?name=Alpha http://shuffle-alb-8vonmf2ywl5z-682850122.us-east-1.elb.amazonaws.com/?name=Bravo http://shuffle-alb-8vonmf2ywl5z-682850122.us-east-1.elb.amazonaws.com/?name=Charlie http://shuffle-alb-8vonmf2ywl5z-682850122.us-east-1.elb.amazonaws.com/?name=Delta http://shuffle-alb-8vonmf2ywl5z-682850122.us-east-1.elb.amazonaws.com/?name=Echo http://shuffle-alb-8vonmf2ywl5z-682850122.us-east-1.elb.amazonaws.com/?name=Foxtrot    Refresh the web browser multiple times to verify that customers are receiving responses only from EC2 instances in the shard they are mapped to\n   Customer Name Workers     Alpha Worker-1 and Worker-2   Bravo Worker-1 and Worker-3   Charlie Worker-1 and Worker-4   Delta Worker-2 and Worker-3   Echo Worker-2 and Worker-4   Foxtrot Worker-3 and Worker-4       function prevStep(){ window.open(\"..\\/4_impact_of_failures_sharding\", \"_self\") } function nextStep(){ window.open(\"..\\/6_impact_of_failures_shuffle_sharding\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_monitoring_linux_ec2_cloudwatch/5_generating_load/","title":"Generate CPU and Memory load","tags":["Linux","Amazon Linux","EC2","CloudWatch","CloudWatch Dashboard"],"description":"","content":"We have a CloudWatch dashboard to show us CPU and Memory statistics for the deployed EC2 instance. In order to showcase the dashboards, lets add a synthetic load to the machine.\nStress For this lab, the EC2 instance will install a utility called stress. This tool is designed to subject your system to a configurable measure of CPU, memory, I/O and disk stress. This command we will use:\nsudo stress --cpu 8 --vm-bytes $(awk '/MemAvailable/{printf \u0026quot;%d\\n\u0026quot;, $2 * 0.9;}' \u0026lt; /proc/meminfo)k --vm-keep -m 1  \u0026ndash;cpu  This will spawn 8 CPU workers spinning on a square root task (sqrt(x))   \u0026ndash;vm-bytes  This will use 90% of the available memory from /proc/meminfo   \u0026ndash;vm-keep  This will re-dirty memory instead of freeing and reallocating.   -m 1  This will spawn 1 worker spinning on malloc()/free()    Generate Load  Open a new tab for the AWS console with this link: https://console.aws.amazon.com/ec2/v2/home?r#Instances:instanceState=running;tag:Name=LinuxMachineDeploy You should see the EC2 instance we have deployed.  Troubleshooting: If you do not see the instance, and you changed the CloudFormation stack name when deploying, then delete the Name: LinuxMachineDeploy filter and search for the instance with the same name as you used for your stack    Click the checkbox next to the machine, and then click \u0026ldquo;Connect\u0026rdquo;  Select \u0026ldquo;Session Manager\u0026rdquo; and then click Connect. This will open a new Linux bash shell to run commands on the EC2 instance.  Type sudo stress --cpu 8 --vm-bytes $(awk '/MemAvailable/{printf \u0026quot;%d\\n\u0026quot;, $2 * 0.9;}' \u0026lt; /proc/meminfo)k --vm-keep -m 1 This will start to consume all of the available memory as well as all CPU\u0026rsquo;s within the instance  Go back to your browser tab that contains the CloudWatch Dashboard. You should see the CPU and Memory graphs change within 10-15 seconds.  cpu_usage_user goes up as the test script consumes CPU mem_used goes up as the script consumes all of it except for a small reserve and should level off right below mem_total    As time goes on, it will continue to update the graph. In order to remove the load, press CTRL-C to stop the stress script. Go back to your browser tab that contains the CloudWatch Dashboard to watch as the CPU load goes down and the amount of free memory increases.    function prevStep(){ window.open(\"..\\/4_adding_metrics_to_dashboard\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/6_cleanup\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_monitoring_windows_ec2_cloudwatch/5_generating_load/","title":"Generate CPU and Memory load","tags":["Windows Server","Windows","EC2","CloudWatch","CloudWatch Dashboard"],"description":"","content":"We have a CloudWatch dashboard to show us CPU and Memory statistics for the deployed EC2 instance. In order to showcase the dashboards, lets add a synthetic load to the machine. We have 2 PowerShell scripts that have already been deployed to the instance to facilitate this.\ncpu_stress.ps1 This script will start multiple threads (one per CPU in the machine) to keep the processor busy doing a simple math computation. We set the thread priority to \u0026ldquo;Lowest\u0026rdquo; so it should still allow system processes to continue.\n  View cpu_stress.ps1 Code   \u0026lt;# .EXAMPLE .\\cpu_stress.ps1 This will execute the script against all cores .DESCRIPTION #\u0026gt; # CPUs in the machine $cpus=$env:NUMBER_OF_PROCESSORS # Lower the thread so it won't overwhelm the system for other things [System.Threading.Thread]::CurrentThread.Priority = 'Lowest' ##################### # perfmon counters for CPU $Global:psPerfCPU = new-object System.Diagnostics.PerformanceCounter(\u0026quot;Processor\u0026quot;,\u0026quot;% Processor Time\u0026quot;,\u0026quot;_Total\u0026quot;) $psPerfCPU.NextValue() | Out-Null $StartDate = Get-Date Write-Output \u0026quot;=-=-=-=-=-=-=-=-=-= Stress Machine Started: $StartDate =-=-=-=-=-=-=-=-=-=\u0026quot; Write-Warning \u0026quot;This script will saturate all available CPUs in the machine\u0026quot; Write-Warning \u0026quot;To cancel execution of all jobs, close the PowerShell Host Window (or terminate the remote session)\u0026quot; Write-Output \u0026quot;=-=-=-=-=-=-=-=-=-= CPUs in box: $cpus =-=-=-=-=-=-=-=-=-= \u0026quot; # This will stress the CPU foreach ($loopnumber in 1..$cpus){ Start-Job -ScriptBlock{ $result = 1 foreach ($number in 1..0x7FFFFFFF){ $result = $result * $number }# end foreach }# end Start-Job }# end foreach Write-Output \u0026quot;Created sub-jobs to consume the CPU\u0026quot; # Ask the user if they want to clear out RAM, if so we will continue Read-Host -Prompt \u0026quot;Press any key to stop the JOBs CTRL+C to quit\u0026quot; Write-Output \u0026quot;Clearing CPU Jobs\u0026quot; Receive-Job * Stop-Job * Remove-Job * $EndDate = Get-Date Write-Output \u0026quot;=-=-=-=-=-=-=-=-=-= Stress Machine Complete: $EndDate =-=-=-=-=-=-=-=-=-=\u0026quot;    mem_stress.ps1 This script will create an ever expanding array in RAM to attempt to consume as much as possible. We do reserve 512Mb of ram for the OS to continue to operate.\n  View mem_stress.ps1 Code   \u0026lt;# .EXAMPLE .\\mem_stress.ps1 This will execute the script to consume all of the memory (less 512 for the OS to survive) .DESCRIPTION #\u0026gt; # RAM in box $box=get-WMIobject Win32_ComputerSystem $Global:physMB=$box.TotalPhysicalMemory / 1024 /1024 # Create object to get current memory available $Global:psPerfMEM = new-object System.Diagnostics.PerformanceCounter(\u0026quot;Memory\u0026quot;,\u0026quot;Available Mbytes\u0026quot;) $psPerfMEM.NextValue() | Out-Null # leave 512Mb for the OS to survive. $HEADROOM=512 $ram = $physMB - $psPerfMEM.NextValue() $maxRAM=$physMB - $HEADROOM $progress = ($ram / $maxRAM) * 100 $completed = [int]$progress $StartDate = Get-Date Write-Output \u0026quot;=-=-=-=-=-=-=-=-=-= Memory Stress Started: $StartDate =-=-=-=-=-=-=-=-=-=\u0026quot; Write-Output \u0026quot;mem_stress - This script will consume all but 512MB of RAM available on the machine\u0026quot; Write-Output \u0026quot;Starting consumed RAM: $ram out of $maxRAM ($completed% Full)\u0026quot; Write-Output \u0026quot;=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\u0026quot; # If you increase the size of the array the GC seems to do quicker cleanups # Not sure why, but 200MB seems to be the suite spot $a = \u0026quot;a\u0026quot; * 200MB # These are the arrays we will create to consume all of the RAM $growArray = @() $growArray += $a $bigArray = @() $k=0 $lastCompleted = 900 # This loop will continue until we have consumed all of the RAM minus the headroom while ($ram -lt $maxRAM) { $bigArray += ,@($k,$growArray) $k += 1 $growArray += $a # Find out how much RAM we are now consuming $ram = $physMB - $psPerfMEM.NextValue() $progress = ($ram / $maxRAM) * 100 $completed = [int]$progress $status_string = -join([int]$ram,\u0026quot; of \u0026quot;,[int]$maxRAM, \u0026quot;MB ($completed% Complete)\u0026quot;) # Only show the message when we have a change in percentage if ($completed -ne $lastCompleted) { Write-Output \u0026quot;$status_string\u0026quot; $lastCompleted = $completed } } Write-Output \u0026quot;=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\u0026quot; # Do a final check of RAM after consuming it all $ram = $physMB - $psPerfMEM.NextValue() Write-Output \u0026quot;FINAL $ram / $maxRAM\u0026quot; # Ask the user if they want to clear out RAM, if so we will continue Read-Host -Prompt \u0026quot;Press any key to clear out RAM or CTRL+C to quit\u0026quot; Write-Output \u0026quot;Clearing RAM\u0026quot; ##################### # and now release it all. $bigArray.clear() #remove-variable bigArray $growArray.clear() #remove-variable growArray [System.GC]::Collect() ##################### $ram = $physMB - $psPerfMEM.NextValue() Write-Output \u0026quot;RAM HAS BEEN CLEARED: $ram / $maxRAM\u0026quot;    Generate Load   Open a new tab for the AWS console with this link: https://console.aws.amazon.com/ec2/v2/home?r#Instances:instanceState=running;tag:Name=WindowsMachineDeploy\n  You should see the EC2 instance we have deployed.\n Troubleshooting: If you do not see the instance, and you changed the CloudFormation stack name when deploying, then delete the Name: WindowsMachineDeploy filter and search for the instance with the same name as you used for your stack     Click the checkbox next to the machine, and then click \u0026ldquo;Connect\u0026rdquo;   Select \u0026ldquo;Session Manager\u0026rdquo; and then click Connect. This will open a new tab with a PowerShell console for the instance.   Type C:\\mem_stress.ps1 at the console and it will start to consume memory resources   Go back to the previous broswer tab that has the EC2 console connect screen and click Connect again. This will open another PowerShell console.   Type C:\\cpu_stress.ps1 at the console and it will start to consume CPU resources   Go back to your browser tab that contains the CloudWatch Dashboard. You should see the CPU and Memory graphs change within 10-15 seconds.\n Processor % User Time goes up as the test script consumes CPU Memory Available goes down, as the script consumes all of it except for a small reserve     As time goes on, it will continue to update the graph. In order to remove the load, go back to each of the console windows and simply press any key. This will cause the script to reclaim all resources it has consumed.\n  Go back to your browser tab that contains the CloudWatch Dashboard to watch as the CPU load goes down and the amount of free RAM increases.    function prevStep(){ window.open(\"..\\/4_adding_metrics_to_dashboard\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/6_cleanup\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/100_labs/100_create_a_data_bunker/","title":"Create a Data Bunker Account","tags":[],"description":"","content":"Last Updated: September 2020\nAuthor: Byron Pogson, Solution Architect\nIntroduction In this lab we will create a secure data bunker. A data bunker is a secure account which will hold important security data in a secure location. Ensure that only members of your security team have access to this account. In this lab we will create a new security account, create a secure S3 bucket in that account and then turn on CloudTrail for our organisation to send these logs to the bucket in the secure data account. You may want to also think about what other data you need in there such as secure backups.\nIf you are using AWS Control Tower the steps in this lab cover what has already been configured for the Control Tower Log Archive Account .\n Prerequisites  A multi-account structure with AWS Organizations  You have access to a role with administrative access to the management account for your AWS Organization  Costs  Typically less than $1 per month if the account is only used for personal testing or training, and the tear down is not performed Amazon S3 pricing Amazon CloudFront Pricing  AWS CloudTrail pricing  AWS Pricing   Steps:  Creating data bunker account in console   "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/100_labs/100_dependency_monitoring/5_bonus_content/","title":"Bonus Content","tags":[],"description":"","content":"Now that dependency monitoring has been established by leveraging CloudWatch Metrics and CloudWatch alarms, the last piece of the \u0026ldquo;puzzle\u0026rdquo; is to ensure that events related to the external service are tracked effectively so that relevant stakeholders are aware of the status of resolution. Alarms and notifications are good to alert teams of potential issues, however, tracking an event such as this will ensure co-ordination of efforts towards resolution. AWS Systems Manager OpsCenter can be used to achieve this. An OpsItem can be created to track events and quickly understand the current status of an event and can help answer questions such as - what level of severity is the event? what resources are affected? what is the status of the event? are there other events similar to this?\nAutomating creation of an OpsItem, coupled with alarms and notifications will allow teams to quickly triage events and lead to faster, more organized resolution.\nThis process can be automated by using a Lambda function to create an OpsItem every time the dependency alarm goes into an In alarm state.\n  Go to the Amazon SNS console at https://console.aws.amazon.com/sns/v3 and click on Topics\n  Click on the SNS Topic that was created as part of this lab - WA-Lab-Dependency-Notification\n  Scroll down to the Subscriptions section and click on Create subscription\n  On the Create subscription page, make the following changes:\n Topic ARN - leave the default value that is already on there Protocol - select AWS Lambda from the dropdown Endpoint - paste the ARN of the OpsItemFunction copied from the Outputs section of the CloudFormation stack from section 1 Deploy the Infrastructure    Click on Create subscription\n  To test this, follow the instructions in the previous section on testing a fail condition by deleting the default route. This time, when the alarm goes into an In alarm state, an OpsItem will be created in OpsCenter, in addition to the notification being sent to the email address specified.\n  Go to the AWS Systems Manager console at https://console.aws.amazon.com/systems-manager and click on OpsCenter\n  Click on the OpsItems tab, search by Title, select contains, and enter the value as S3 Data Writes\n  Click on the OpsItem that has been created with the title S3 Data Writes failing\n  Expand the OpsItem details section by clicking on the triangle next to it, and view the information available there such as severity, category, etc.\n  Scroll down to the see the Related resources, in this case, the S3 bucket to which the writes are failing\n  The event can now be efficiently tracked using the OpsItem, and remediation work can be better co-ordinated. Additionally, you can choose to execute pre-created Runbooks which are listed under the Runbooks section and automate the remediation. You can create custom runbooks depending on the type of event.\n function prevStep(){ window.open(\"..\\/4_test_fail_condition\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/6_cleanup\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_cur_queries/queries/customer_engagement/","title":"Customer Engagement","tags":[],"description":"","content":"These are queries for AWS Services under the Customer Engagement product family .\nUse the clipboard in the top right of the text boxes below to copy all of the text to be pasted.\n You may need to change variables used as placeholders in your query. ${table_Name} is a common variable which needs to be replaced. Example: cur_db.cur_table\n Table of Contents   Amazon Connect   Query Description This query will provide daily unblended cost and usage information per linked account for Amazon Connect. The output will include specific details about the usage type, usage description, and product usage region. The cost will be summed and in descending order.\nPricing Please refer to the Connect pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT * FROM ( ( SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d') AS day_line_item_usage_start_date, product_region, CASE WHEN line_item_usage_type LIKE '%%end-customer-mins' THEN 'End customer minutes' WHEN line_item_usage_type LIKE '%%chat-message' THEN 'Chat messages' ELSE 'Others' END AS UsageType, line_item_line_item_description, SUM(CAST(line_item_usage_amount AS double)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS decimal(16,8))) AS sum_line_item_unblended_cost FROM ${table_name} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND line_item_product_code = 'AmazonConnect' AND line_item_line_item_type = 'Usage' GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d'), product_region, line_item_usage_type, line_item_line_item_description ) UNION ALL ( SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d') AS day_line_item_usage_start_date, product_region, CASE WHEN line_item_usage_type LIKE '%%did-numbers' THEN 'DID days of use' WHEN line_item_usage_type LIKE '%%tollfree-numbers' THEN 'Toll free days of use' WHEN line_item_usage_type LIKE '%%did-inbound-mins' THEN 'Inbound DID minutes' WHEN line_item_usage_type LIKE '%%outbound-mins' THEN 'Outbound minutes' WHEN line_item_usage_type LIKE '%%tollfree-inbound-mins' THEN 'Inbound Toll Free minutes' ELSE 'Other' END AS UsageType, line_item_line_item_description, SUM(CAST(line_item_usage_amount AS double)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS decimal(16,8))) AS sum_line_item_unblended_cost FROM ${table_name} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND line_item_product_code = 'ContactCenterTelecomm' AND line_item_line_item_type = 'Usage' GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d'), product_region, line_item_usage_type, line_item_line_item_description ) ) AS aggregatedTable ORDER BY day_line_item_usage_start_date ASC, sum_line_item_unblended_cost DESC;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback       CUR queries are provided as is. We recommend validating your data by comparing it against your monthly bill and Cost Explorer prior to making any financial decisions. If you wish to provide feedback on these queries, there is an error, or you want to make a suggestion, please email: curquery@amazon.com\n "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_1_aws_account_setup/5_account_settings/","title":"Configure account settings","tags":[],"description":"","content":"It is important to ensure your account contacts are up to date and correct. This allows AWS to be able to contact the correct people in your organization if required. It is recommended to use a mailing list or shared email that is accessible by multiple team members for redudancy. Ensure the email accounts are actively monitored.\n  Log in to your Management account with administrative privileges, Click on the account name in the top right, and click on My Account from the menu:   Scroll down to Alternate Contacts and click on Edit:   Enter information into each of the fields for Billing, Operations and Security, and click Update:    function prevStep(){ window.open(\"..\\/4_configure_sso\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/6_quicksight\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_organization_data_cur_connection/5_join_cost_intelligence_dashboard/","title":"Join with the Enterprise Cost Intelligence Dashboard","tags":[],"description":"","content":"Join with the Enterprise Cost Intelligence Dashboard This section is optional and shows how you can add your AWS Organization Datas to your Enterprise Dashboards - 200_Enterprise_Dashboards .\nThis example will show you how to map your Enterprise Dashboard linked_account_id to your Organizations account_number to add account information that is meaningful to your organization. This is to replace this step: https://wellarchitectedlabs.com/cost/200_labs/200_enterprise_dashboards/2_modify_cost_intelligence/.\n  Go to the Amazon QuickSight service homepage\n  In QuickSight, select the summary_view Data Set\n  Select Edit data set\n  Select Add data:   Select your Amazon Athena organization_data table and click Select   Select the two circles to open the join configuration then select Left to change your join type:   Create following join clause :\n linked_account_id = account_number Click Apply     Scroll down in the field list, and confirm the Account ID must be Int:   Select Save\n  Repeat steps 2-9, creating mapping joins for your remaining QuickSight data sets:\n s3_view ec2_running_cost compute_savings_plan_eligible_spend    You now have new fields that can be used on the visuals in the Cost Intelligence Dashboard - we will now use them\n  Go to the Cost Intelligence Analysis\n  Edit the calculated field Account:   Change the formula from toString({linked_account_id}) to {account_name}   You can now select a visual, select the Account field, and you will see the account names in your visuals, instead of the Account number:   You now have successfully utilized Organization mapping data on your Cost Intelligence Dashboard\n  function prevStep(){ window.open(\"..\\/4_visualize_organization_data_in_quicksight\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/6_bonus_org_tags\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_enterprise_dashboards/5_tear_down/","title":"Tear down","tags":[],"description":"","content":"To perform a teardown for this lab, perform the following steps:\n  Remove QuickSight email reports\n Go into QuickSight Select All dashboards Click on the dashboard name Click Reports Select Unsubscribe Click Update    Remove any created QuickSight dashboards\n Go into QuickSight Select All dashboards Click the 3 dots next to the dashboard name Click Delete Click Delete    Remove any QuickSight analyses\n Go into QuickSight Select All analyses Click the 3 dots next to the analysis name Click Delete Click Delete    Remove QuickSight Datasets\n Go into QuickSight Click Manage data Click on the dataset, we created  summary_view s3_view compute_savings_plan_eligible_spend ec2_running_cost data_transfer_view   Click Delete data set Click Delete    Remove the Athena views\n Go into Athena Execute the following commands to remove the Cost Intelligence views:  drop view costmaster.compute_savings_plan_eligible_spend drop view costmaster.ec2_running_cost drop view costmaster.ri_sp_mapping drop view costmaster.s3_view drop view costmaster.summary_view drop view costmaster.data_transfer_view       function prevStep(){ window.open(\"..\\/4_distribute_dashboards\\/\", \"_self\") } function nextStep(){ window.open(\"\\/cost\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with COST3 - \u0026ldquo;How do you monitor usage and cost?\u0026quot;\nClick here to access the Well-Architected Tool   Previous Step Complete this lab     "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_deploy_and_update_cloudformation/5_add_ec2/","title":"Add an Amazon EC2 Instance to the Stack","tags":[],"description":"","content":"In this task, your objective is to add an Amazon EC2 instance to the template, then update the stack with the revised template.\nWhereas the bucket definition was rather simple (just two to four lines), defining an Amazon EC2 instance is more complex because it needs to use associated resources, such as an AMI, security group and subnet.\nFor this exercise we wil assume you now know how to edit your CloudFormation template and update your CloudFormation stack with the updated template\n4.1 Get the latest AMI to use for your EC2 instance In the Parameters section of your template, look at the LatestAmiId parameter.\nLatestAmiId: Description: Gets the latest AMI from Systems Manager Parameter store Type: 'AWS::SSM::Parameter::Value\u0026lt;AWS::EC2::Image::Id\u0026gt;' Default: '/aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2'  This is a special parameter. This parameter uses the AWS Systems Manager Parameter Store to retrieve the latest AMI (specified in the Default parameter, which in this case is Amazon Linux 2) for the stack\u0026rsquo;s region. This makes it easy to deploy stacks in different regions without having to manually specify an AMI ID for every region.\n Go to the AWS CloudFormation console  Click on Stacks Click on the CloudFormationLab stack Click on the Parameters tab   Look at the Value and Resolved value for LatestAmiId  You see here how it resolves to an AMI ID    For more details of this method, see: AWS Compute Blog: Query for the latest Amazon Linux AMI IDs using AWS Systems Manager Parameter Store 4.2 Add the EC2 instance resource to your CloudFormation template and deploy it   Edit the CloudFormation Template, adding a new resource for an EC2 instance\nUse this documentation page for assistance: AWS::EC2::Instance  Use the YAML format For Logical ID (the line above Type) use MyEC2Instance  You only need to specify these six properties:\n  IamInstanceProfile: Refer to Web1InstanceInstanceProfile, which is defined elsewhere in the template\n  ImageId: Refer to LatestAmiId, which is the parameter discussed previously\n  InstanceType: Refer to InstanceType, another parameter\n  SecurityGroupIds: Refer to PublicSecurityGroup, which is defined elsewhere in the template\n  SubnetId: Refer to PublicSubnet1, which is defined elsewhere in the template\n  Tags: Use this YAML block:\n Tags: - Key: Name Value: Simple Server    Remember\n  When referring to other resources in the same template, use !Ref. See the BucketName example you already implemented\n  When referring to SecurityGroupIds, the template is actually expecting a list of security groups. You therefore need to list the security group like this:\n SecurityGroupIds: - !Ref PublicSecurityGroup    Not sure what to do???\n To download a sample solution, right-click and download this link: simple_stack_plus_s3_ec2.yaml  Or click below to see exactly what to add to your CloudFormation template.      Click here to see YAML for adding your EC2 instance:    MyEC2Instance: Type: AWS::EC2::Instance Properties: IamInstanceProfile: !Ref Web1InstanceInstanceProfile ImageId: !Ref LatestAmiId InstanceType: !Ref InstanceType SecurityGroupIds: - !Ref PublicSecurityGroup SubnetId: !Ref PublicSubnet1 Tags: - Key: Name Value: Simple Server      Once you have edited the template, update the stack deployment with your revised template file.\n  On the Parameters screen of the CloudFormation update switch EC2SecurityEnabledParam to true\n   Important     Change EC2SecurityEnabledParam to true   This will tell the template to create resources your EC2 instance will need such as the Security Group and IAM Role      This deployment of the CloudFormation stack will take about three minutes\n  The instance will now be displayed in the Resources tab.\n    Go to the EC2 console to see the Simple Server that was created. Explore the properties of this EC2 instance.\n  The final deployment is now represented by this architecture diagram:\n4.3 [Optional bonus task] Add a web server to the EC2 instance In this task you will update your CloudFormation template to modify the deployed EC2 instance so that it runs a simple web server\n  Modify the EC2 resource in the template\n  Delete the following properties form the EC2 resource\n SecurityGroupIds SubnetId    Add the following properties using the YAML below\n NetworkInterfaces: adds an external IP address (and DNS name) for the EC2 instance UserData: a simple bash script to install and run an Apache web server. This runs on EC2 instance creation only.    Visually the diff for this looks like:\n  The final EC2 instance resource should look like this:\n MyEC2Instance: Type: AWS::EC2::Instance Properties: IamInstanceProfile: !Ref Web1InstanceInstanceProfile ImageId: !Ref LatestAmiId InstanceType: !Ref InstanceType Tags: - Key: Name Value: Simple Server NetworkInterfaces: - AssociatePublicIpAddress: \u0026quot;true\u0026quot; DeviceIndex: \u0026quot;0\u0026quot; GroupSet: - Ref: PublicSecurityGroup SubnetId: Ref: PublicSubnet1 UserData: Fn::Base64: | #!/bin/bash yum -y update sudo yum install -y httpd sudo systemctl start httpd      Add an output value so you can easily find the public DNS of the EC2 instance\n  Insert the following YAML under the Outputs section of your CloudFormation template\n PublicServerDNS: Value: !GetAtt MyEC2Instance.PublicDnsName    Use the other entry under Outputs to ensure your new entry has the right indentation\n  The !GetAtt function can return various attributes of the resource. In this case the public DNS name of the EC2 instance.\n  NOTE: if you used a Logical ID other than MyEC2Instance when you added your EC2 resource, then you should use that name here\n  To download a sample solution, right-click and download this link: simple_stack_plus_s3_ec2_server.yaml     Update the CloudFormation stack using the modified template\n  After deployment is complete, click on the Outputs tab for the CloudFormation stack\n Click on the public DNS name    You should see the Apache HTTP server Test Page, indicating your EC2 instance is running the web server and is accessible from the Internet.\n function prevStep(){ window.open(\"..\\/4_add_s3\", \"_self\") } function nextStep(){ window.open(\"..\\/6_cleanup\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_aws_resource_optimization/5_ec2_rs_best_practices/","title":"Amazon EC2 Rightsizing Best Practices","tags":[],"description":"","content":"  Start simple: idle resources, non-critical development/QA and previous generation instances will require less testing hours and provide quick wins (The Amazon EC2 Launch time statistics can be used to identify instances that have been running longer than others and is a good statistic to sort your Amazon EC2 instances by).\n  Rightsize before performing a migration: If you skip rightsizing to save time, your migration speed might increase, but you will end up with higher cloud infrastructure spend for a potentially longer period of time. Instead, leverage the test and QA cycles during a migration exercise to test several instance types and families. Also, take that opportunity to test different sizes and burstable instances like the t family.\n  The best rightsizing starts on day 1: As you perform rightsizing analysis, and ultimately rightsize resources, ensure any learnings are being shared across your organization and influencing the design of new workloads and upcoming migrations.\n  Measure Twice, Cut Once: Test, then test some more: The last thing you want is for a new resource type to be uncapable of handling load, or functioning incorrectly.\n  Test once and perform multiple rightsizing: Aggregate instances per autoscaling group and tags to scale rightsizing activities.\n  Combine Reserved Instance or Savings Plans strategy with Rightsizing to maximize savings: For Standard RIs and EC2 Instance SP: Perform your pricing model purchases after rightsizing and for Convertible RIs, exchange them after rightsizing. Compute Savings plan will automatically adjust the commitment for the new environment.\n  Ignore burstable instance families (T types): These families are designed to typically run at low CPU percentages for significant periods of time and shouldnt be part of the instance types being analyzed for rightsizing.\n   function prevStep(){ window.open(\"..\\/4_act_resource_opt\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/6_tear_down\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/200_labs/200_automating_operations_with_playbooks_and_runbooks/5_build_execute_remediation_runbook/","title":"Buiild &amp; Execute Remediation Playbook","tags":[],"description":"","content":"Following the completion of section 4, we can complete the lab by testing the workload. We will achieve this by running a decrypt API call to our application. This will trigger a failed decrypt event which should result in our alarm being triggered and an SNS notification sent to the email address which you specified as an endpoint in the previous section.\nComplete the following steps to test the system functionality:\n5.1. Initiate a successful decryption operation Run the command shown below within your Cloud9 IDE, replacing the \u0026lt; encrypt key \u0026gt; with the key value that you took note of in section 2.4 as well as the \u0026lt; Application endpoint URL \u0026gt; with the OutputPattern1ApplicationEndpoint url you took note on section 2.3.3\nALBURL=\u0026quot;http://\u0026lt; Application endpoint URL \u0026gt;\u0026quot; curl --header \u0026quot;Content-Type: application/json\u0026quot; --request GET --data '{\u0026quot;Name\u0026quot;:\u0026quot;Andy Jassey\u0026quot;,\u0026quot;Key\u0026quot;:\u0026quot;\u0026lt;encrypt key\u0026gt;\u0026quot;}' $ALBURL/decrypt Once that is successful, you should see out put like below:\n{\u0026quot;Text\u0026quot;:\u0026quot;Welcome to ReInvent 2020!\u0026quot;} 5.2. Initiate an unsuccessful decryption operation Now that we have confirmed that the decrypt API is operational, let\u0026rsquo;s trigger a deliberate decryption failure to invoke our alerting.\nRun below command once again, but this time, pass on a wrong key for the encrypt key (you can just use whatever value).\nALBURL=\u0026quot;http://\u0026lt; Application endpoint URL \u0026gt;\u0026quot; curl --header \u0026quot;Content-Type: application/json\u0026quot; --request GET --data '{\u0026quot;Name\u0026quot;:\u0026quot;Andy Jassey\u0026quot;,\u0026quot;Key\u0026quot;:\u0026quot;some-random-false-key\u0026quot;}' $ALBURL/decrypt Once it is triggered, you should see output like below signifying that the decrypt procedure has failed, and in the background a failed KMS API has been called. :\n{\u0026quot;Message\u0026quot;:\u0026quot;Data decryption failed, make sure you have the correct key\u0026quot;} Make sure that you repeat this several times in a row, to ensure you we are triggering the alarm. This will result in email notification to the endpoint you defined earlier in the lab.\nNote:  CloudTrail can typically take up to 15 mins to pick up the API event and trigger your alarm. For more information about this, please visit Cloudtrail FAQ page  5.3. Observing the alarm. If all the components are configured correctly, you should receive an email notification triggered by the CloudWatch alarm similar to this:\n5.3.1. Click on the URL included in the email that will take you to the CloudWatch Alarm resource in AWS console.\n5.3.2. Observe the state changes under the History section, and notice each activity change as follows:\nCongratulations! you have completed the Pattern1 lab.\n END OF SECTION 5\n "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_automated_cur_query_and_email_delivery/5_create_lambda_function/","title":"Create a Lambda function","tags":[],"description":"","content":"We will now create a Lambda function which will run the code and produce the reports. NOTE: this Lambda function must be created in the same region as S3 bucket for CUR query results created earlier.\n  Go to the Lambda console, click Create function.\n  Select Author from scratch, configure the following parameters:\n Function name: Auto_CUR_Delivery Runtime: Python 3.7 Execution role: Use an existing role Existing role: Lambda_Auto_CUR_Delivery_Role click Create function.     In the top right-hand corner of Lambda configuration page, click Select a test event drop-down box and choose Configure test events.   Use the default event template Hello world, because this function does not need any input event parameters, set a event name AutoCURDeliveryTest, and click Create.   In Function code section, configure the following:\n Code entry type: Upload a file from Amazon S3 Amazon S3 link URL: https://s3.amazonaws.com/bucket name/AutoCURDelivery.zip Handler: auto_cur_delivery.lambda_handler     Scroll down to Basic settings section, set Memory to 512 MB, and timeout to 5 min.   Keep other configurations as default, scroll to the very top and click Save. Click the Actions drop-down box and choose Publish new version.   Set the Version description to v1, and click Publish.   We have finished the configuration and we will now test it. Make sure AutoCURDeliveryTest event is selected, click Test.   It takes a few seconds to execute Lambda function, and you\u0026rsquo;ll see all logs after execution.   Check your e-mail recipients, they should receive a mail for cost \u0026amp; utilization report with an excel file attached, similarly as below:   By default, the cost \u0026amp; utilization report contains:\n Cost_By_Service - Cost in the recent three months split by service (e.g. current month is Jul, the recent three months are Jul, Jun and May, same as below) Data_Cost_By_Service - Data cost in the recent three months split by service MoM_Inter_AZ_DT(with graph) - Month over months inter-AZ data transfer usage and change in the recent three months MTD_S3_By_Bucket - Month to date S3 cost and usage type split by bucket name MTD_ELB_By_Name - Month to date ELB cost split by ELB name and region MTD_CF_By_Distribution - Month to date Cloudfront cost and usage split by distribution id    Now you have completed this auto CUR delivery solution with default CUR query. In the next step we will add an additional query, and a CloudWatch scheduled event to trigger Lambda function as required.\n function prevStep(){ window.open(\"..\\/4_configure_function_parameters\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/6_cloudwatch_event\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_2_cost_and_usage_governance/5_ec2_volume_type/","title":"Create an IAM policy to restrict EBS Volume creation by volume type","tags":[],"description":"","content":"Extending cost optimization governance beyond compute instances will ensure overall higher levels of cost optimization. Similar to EC2 instances, there are different storage types. Governing the type of storage that can be created in an account can be effective to minimize cost.\nWe will create an IAM policy that denies operations that contain provisioned IOPS (io1) EBS volume types. This will not only restrict creating a volume, but all other actions that attempt to use this volume type.\nNOTE: it is best practice to provide only the minimum access required, the policy used here is for brevity and simplicity, and should only be implemented as a demonstration before being removed.\n Create the EBS type restrictive IAM Policy   Log on to the console as your regular user with the required permissions, go to the IAM service page:   Click on Policies on the left menu:   Click Create policy:   Click on the JSON tab:   Copy and paste the policy into the console:   IAM Policy   { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;VisualEditor0\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Deny\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;ec2:*\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;Condition\u0026quot;: { \u0026quot;StringEquals\u0026quot;: { \u0026quot;ec2:VolumeType\u0026quot;: \u0026quot;io1\u0026quot; } } } ] }   \n  Click on Review Policy:   Configure the following details:\n Name: EC2EBS_Restrict Description: Dont allow EBS io1 volumes Click Create policy:     You have successfully created an IAM policy to restrict EBS actions by volume type.\n Apply the policy to your test group   Click on Groups from the left menu:   Click on the CostTest group:   Click on Attach Policy:   Click on Policy Type, then click Customer Managed:   Select the checkbox next to EC2EBS_Restrict, and click Attach Policy:   You have successfully attached the policy to the CostTest group.\n Log out from the console\n Verify the policy is in effect   Logon to the console as the TestUser1 user, click on Services then click EC2:   Try to launch an instance by clicking Launch Instance, select Launch Instance:   Click Select next to Amazon Linux 2\u0026hellip;:   Select t3.nano (which is allowed as per our already applied policy, which we tested in the last exercise), click Next: Configure Instance Details:   Click Next Add Storage:   Click on Add New Volume, click on the dropdown, then select Provisioned IOPS SSD (io1):   Click Review and Launch:   Take note of the security group created, and click Launch:   Select Proceed without a key pair, and click I acknowledge that i will not be able to\u0026hellip;, then click Launch Instances:   The launch will fail, as it contained an io1 volume. Click Back to Review Screen:   Scroll down and click Edit storage:   Click the dropdown and change it to General Purpose SSD(gp2), click Review and Launch:   Click Launch:   Select Proceed without a key pair, and click I acknowledge that i will not be able to\u0026hellip;, then click Launch Instances:   It will now succeed, as it doesn\u0026rsquo;t contain an io1 volume type. Click on the instance ID and terminate the instance as above:   Log out of the console as TestUser1.\n  You have successfully implemented an IAM policy that denies operations if there is an EBS volume of type io1.\n  function prevStep(){ window.open(\"..\\/4_ec2_restrict_size\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/6_tear_down\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_pricing_model_analysis/5_format_dashboard/","title":"Format the Recommendation Dashboard","tags":[],"description":"","content":"We will format the recommendation dashboard, this will improve its appearance, and also includes some business rules.\n  Click on Themes, then click on Midnight:   Select the Recommendations table, click the three dots, click Conditional formatting:   Column: PayOffMonth, Add background color:   Enter the formatting:\n Condition: Greater than Value: 9 Color: red Click Apply, click Close:     Using the same process, add formatting for the column discountrate:\n Type: Background color Condition: Less than Value: 10 NOTE adjust this for your business rules, speak with your finance teams Color: red Click Apply, click Close    Under the discountrate formatting, Click Add text color:\n Condition: Greater than Value: 20 Color: Green Click Apply, click Close    Using the same process, add formatting for the column HoursRun:\n Type: Add text color Condition: Less than Value: 0.6 Color: Red Click Add condition Condition#2: Less than Value: 0.85 Color: Orange Click Apply, click Close    Add formatting for the column SavingsPlanReco:\n Type: Add background color Format field based on: PayOffMonth Condition: Greater than Value: 9 Color: Red Click Apply, click Close Click Add text color Format field based on: discountrate Aggregation: Average Condition: greater than Value: 20 Color: Green Click Apply, click Close    Click SavingsPlanReco, Sort by Descending:\n  Select the Trends table, select conditional formatting, Column instancecount:\n Type: Add background color Condition: Less than Value: 5, speak with your team to set this at the appropriate level Color: red Click Add condition Condition #2: Less than Value: 10 Color: Orange Click Apply, click Close    Using the process above on the Trends table, Select the Trend column:\n Type: Add background color Condition: Less than Value 0 Color: Red Click Apply, click Close Click Add text color Condition: Greater than Value: 0 Color: Green Click Apply, Click Close    Add the same formatting to the TrendAvg column as the Trend column.\n  Congratulations - you now have an analytics dashboard for Savings Plan recommendations!\n  function prevStep(){ window.open(\"..\\/4_recommendation_dashboard\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/6_tear_down\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_remote_configuration_installation_and_viewing_cloudwatch_logs/5_generate_logs/","title":"Generate Logs","tags":[],"description":"","content":"In order to populate the logs you are collecting, you need to interact with the deployed website. The Apache web server service being used to host your website generates access logs. In the following steps, you will visit the website to generate these access logs.\n Go to the CloudFormation console . Select the stack you deployed for this lab, called security-cw-lab. Click on Outputs, then click on WebsiteURL. Refresh the page a few time to generate some activity on your website. Repeat steps 1-4, but add /example to the end of the website url. This will generate a 404 error, which is expected.  Generating these access logs will allow you to explore the ways in which you can inspect and view these logs, as shown in the following sections of this lab.\n function prevStep(){ window.open(\"..\\/4_start_cw_agent\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/6_view_cw_logs\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_100_quick_steps_to_security_success/5_operating/","title":"Operating","tags":[],"description":"","content":"Although there are instructions for Decommissioning a Landing Zone in the AWS Control Tower documentation it is strongly recommended that you keep them this quest in place unless you are decommissioning your AWS account structure. The steps in this quest are intended to improve your security posture and tearing down this quest will remove the audit logs and guard rails put in place.\nThere is no additional cost for AWS Control Tower, you only pay for the services used by Control Tower which scales with you. See the AWS Control Tower pricing page for detailed examples. As a baseline, the setup of just Control Tower has a US$5/month fee for a product in Service Catalogue in additional to a once off cost of US$0.011 for AWS Config to record it\u0026rsquo;s initial state. Security Hub and Guard Duty both have a 30 day free trial to give you an indicative cost. Security Hub has a cost associated with number of security checks and findings ingested which will scale with your AWS usage, see AWS Secuity Hub pricing for examples. Guard Duty pricing is based on the total amount of logs consumed by the service, this will also scale cost effectively with your AWS usage, see AWS Guard Duty pricing for examples. Note that you may also additionally incur tax based on your location.\nAdditional Resources and next steps  Find further information on the AWS website around AWS Cloud Security and in particular what your responsibilities are under the shared security model  Read more on how permission boundaries and service control policies allow you to delegate access across your organization Understand the Well-Architected Framework and how applying it can improve your security posture  "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/100_labs/100_inventory_patch_management/5_patch_mgmt/","title":"Patch Management","tags":[],"description":"","content":"Systems Manager: Patch Manager AWS Systems Manager Patch Manager automates the process of patching managed instances with security related updates.\n Note For Linux-based instances, you can also install patches for non-security updates.\n You can patch fleets of Amazon EC2 instances or your on-premises servers and virtual machines (VMs) by operating system type. This includes supported versions of Windows, Ubuntu Server, Red Hat Enterprise Linux (RHEL), SUSE Linux Enterprise Server (SLES), and Amazon Linux. You can scan instances to see only a report of missing patches, or you can scan and automatically install all missing patches. You can target instances individually or in large groups by using Amazon EC2 tags.\n Warning\n AWS does not test patches for Windows or Linux before making them available in Patch Manager . If any updates are installed by Patch Manager the patched instance is rebooted. Always test patches thoroughly before deploying to production environments.   Patch Baselines Patch Manager uses patch baselines, which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches. Later in this lab we will schedule patching to occur on a regular basis using a Systems Manager Maintenance Window task. Patch Manager integrates with AWS Identity and Access Management (IAM), AWS CloudTrail, and Amazon CloudWatch Events to provide a secure patching experience that includes event notifications and the ability to audit usage.\n Warning The operating systems supported by Patch Manager may vary from those supported by the SSM Agent.\n 5.1 Create a Patch Baseline  Under Instances and Nodes in the AWS Systems Manager navigation bar, choose Patch Manager. Click the View predefined patch baselines link under the Configure patching button on the upper right. Choose Create patch baseline. On the Create patch baseline page in the Provide patch baseline details section:  Enter a Name for your custom patch baseline, such as AmazonLinuxSecAndNonSecBaseline. Optionally enter a description, such as Amazon Linux patch baseline including security and non-security patches. Select Amazon Linux from the list.   In the Approval rules section:  Examine the options in the lists and ensure that Product, Classification, and Severity have values of All. Leave the Auto approval delay at its default of 0 days. Change the value of Compliance reporting - optional to Critical. Choose Add another rule. In the new rule, change the value of Compliance reporting - optional to Medium. Check the box under Include non-security updates to include all Amazon Linux updates when patching.    If an approved patch is reported as missing, the option you choose in Compliance reporting, such as Critical or Medium, determines the severity of the compliance violation reported in System Manager Compliance.\nIn the Patch exceptions section in the Rejected patches - optional text box, enter system-release.* This will reject patches to new Amazon Linux releases that may advance you beyond the Patch Manager supported operating systems prior to your testing new releases. For Linux operating systems, you can optionally define an alternative patch source repository . Choose the X in the Patch sources area to remove the empty patch source definition. Choose Create patch baseline and you will go to the Patch Baselines page where the AWS provided default patch baselines, and your custom baseline, are displayed.  Patch Groups A patch group is an optional method to organize instances for patching. For example, you can create patch groups for different operating systems (Linux or Windows), different environments (Development, Test, and Production), or different server functions (web servers, file servers, databases). Patch groups can help you avoid deploying patches to the wrong set of instances. They can also help you avoid deploying patches before they have been adequately tested.\nYou create a patch group by using Amazon EC2 tags. Unlike other tagging scenarios across Systems Manager, a patch group must be defined with the tag key: Patch Group (tag keys are case sensitive). You can specify any value (for example, web servers) but the key must be Patch Group.\n Note An instance can only be in one patch group.\n After you create a patch group and tag instances, you can register the patch group with a patch baseline. By registering the patch group with a patch baseline, you ensure that the correct patches are installed during the patching execution. When the system applies a patch baseline to an instance, the service checks if a patch group is defined for the instance.\n If the instance is assigned to a patch group, the system checks to see which patch baseline is registered to that group. If a patch baseline is found for that group, the system applies that patch baseline. If an instance isn\u0026rsquo;t assigned to a patch group, the system automatically uses the currently configured default patch baseline.  5.2 Assign a Patch Group  Choose the Baseline ID of your newly created baseline to enter the details screen. Choose Actions in the top right of the window and select Modify patch groups. In the Modify patch groups window under Patch groups, enter Critical, choose Add, and then choose Close to be returned to the Patch Baseline details screen.  AWS-RunPatchBaseline AWS-RunPatchBaseline is a command document that enables you to control patch approvals using patch baselines. It reports patch compliance information that you can view using the Systems Manager Compliance tools. For example,you can view which instances are missing patches and what those patches are.\nFor Linux operating systems, compliance information is provided for patches from both the default source repository configured on an instance and from any alternative source repositories you specify in a custom patch baseline. AWS-RunPatchBaseline supports both Windows and Linux operating systems.\nAWS Systems Manager: Document An AWS Systems Manager document defines the actions that Systems Manager performs on your managed instances. Systems Manager includes many pre-configured documents that you can use by specifying parameters at runtime, including \u0026lsquo;AWS-RunPatchBaseline\u0026rsquo;. These documents use JavaScript Object Notation (JSON) or YAML (a recursive acronym for \u0026ldquo;YAML Ain\u0026rsquo;t Markup Language\u0026rdquo;), and they include steps and parameters that you specify.\nAll AWS provided Automation and Run Command documents can be viewed in AWS Systems Manager Documents. You can create your own documents or launch existing scripts using provided documents to implement custom operations as code activities.\n5.3 Examine AWS-RunPatchBaseline in Documents To examine AWS-RunPatchBaseline in Documents:\n In the AWS Systems Manager navigation bar under Shared Resources, choose Documents. Click in the search box, select Document name prefix, and then Equal. Type AWS-Run into the text field and press Enter on your keyboard to start the search. Select AWS-RunPatchBaseline and choose View details. Review the content of each tab in the details page of the document.  AWS Systems Manager: Run Command AWS Systems Manager Run Command lets you remotely and securely manage the configuration of your managed instances. Run Command enables you to automate common administrative tasks and perform ad hoc configuration changes at scale. You can use Run Command from the AWS Management Console, the AWS Command Line Interface, AWS Tools for Windows PowerShell, or the AWS SDKs.\n5.4 Scan Your Instances with AWS-RunPatchBaseline via Run Command  Under Instances and Nodes in the AWS Systems Manager navigation bar, choose Run Command. In the Run Command dashboard, you will see previously executed commands including the execution of AWS-RefreshAssociation, which was performed when you set up inventory. (Optional) choose a Command ID from the list and examine the record of the command execution. Choose Run Command in the top right of the window. In the Run a command window, under Command document:  Choose the search icon and select Platform types, and then choose Linux to display all the available commands that can be applied to Linux instances. Choose AWS-RunPatchBaseline in the list.   In the Command parameters section, leave the Operation value as the default Scan. In the Targets section:  Under Specify targets by, choose Specifying a tag to reveal the Tags sub-section. Under Enter a tag key, enter Workload, and under Enter a tag value, enter Test and click Add.    The remaining Run Command features enable you to:\n Specify Rate control, limiting Concurrency to a specific number of targets or a calculated percentage of systems, or to specify an Error threshold by count or percentage of systems after which the command execution will end. Specify Output options to record the entire output to a preconfigured S3 bucket and optional S3 key prefix.   Note Only the last 2500 characters of a command document\u0026rsquo;s output are displayed in the console.\n  Specify SNS notifications to a specified SNS Topic on all events or on a specific event type for either the entire command or on a per-instance basis. This requires Amazon SNS to be preconfigured. View the command as it would appear if executed within the AWS Command Line Interface.   Choose Run to execute the command and return to its details page. Scroll down to Targets and outputs to view the status of the individual targets that were selected through your tag key and value pair. Refresh your page to update the status. Choose an Instance ID from the targets list to view the Output from command execution on that instance. Choose Step 1 - Output to view the first 2500 characters of the command output from Step 1 of the command, and choose Step 1 - Output again to conceal it. Choose Step 2 - Output to view the first 2500 characters of the command output from Step 2 of the command. The execution step for PatchWindows was skipped as it did not apply to your Amazon Linux instance. Choose Step 1 - Output again to conceal it.  5.5 Review Initial Patch Compliance  Under Instances \u0026amp; Nodes in the the AWS Systems Manager navigation bar, choose Compliance. On the Compliance page in the Compliance resources summary, you will now see that there are 4 systems that have critical severity compliance issues. In the Resources list, you will see the individual compliance status and details.  5.6 Patch Your Instances with AWS-RunPatchBaseline via Run Command  Under Instances and Nodes in the AWS Systems Manager navigation bar, choose Run Command. Choose Run Command in the top right of the window. In the Run a command window, under Command document:  Choose the search icon, select Platform types, and then choose Linux to display all the available commands that can be applied to Linux instances. Choose AWS-RunPatchBaseline in the list.   In the Targets section:  Under Specify targets by, choose Specifying a tag to reveal the Tags sub-section. Under Enter a tag key, enter Workload and under Enter a tag value enter Test.   In the Command parameters section, change the Operation value to Install. In the Targets section, choose Specify a tag using Workload and Test.   Note You could have choosen Manually selecting instances and used the check box at the top of the list to select all instances displayed, or selected them individually.\n  Note There are multiple pages of instances. If manually selecting instances, individual selections must be made on each page.\n  In the Rate control section:  For Concurrency, ensure that targets is selected and specify the value as 1.   Tip Limiting concurrency will stagger the application of patches and the reboot cycle, however, to ensure that your instances are not rebooting at the same time, create separate tags to define target groups and schedule the application of patches at separate times.\n For Error threshold, ensure that error is selected and specify the value as 1.   Choose Run to execute the command and to go to its details page. Refresh the page to view updated status and proceed when the execution is successful.   Warning Remember, if any updates are installed by Patch Manager, the patched instance is rebooted.\n 5.7 Review Patch Compliance After Patching  Under Instances \u0026amp; Nodes in the the AWS Systems Manager navigation bar, choose Compliance. The Compliance resources summary will now show that there are 4 systems that have satisfied critical severity patch compliance.  In the optional Scheduling Automated Operations Activities section of this lab you can set up Systems Manager Maintenance Windows and schedule the automated application of patches.\nThe Impact of Operations as Code In a traditional environment, you would have had to set up the systems and software to perform these activities. You would require a server to execute your scripts. You would need to manage authentication credentials across all of your systems.\nOperations as code reduces the resources, time, risk, and complexity of performing operations tasks and ensures consistent execution. You can take operations as code and automate operations activities by using scheduling and event triggers. Through integration at the infrastructure level you avoid \u0026ldquo;swivel chair\u0026rdquo; processes that require multiple interfaces and systems to complete a single operations activity.\n function prevStep(){ window.open(\"..\\/4_inventory_mgmt\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/6_maintenance_windows\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_bidirectional_replication_for_s3/5_resources/","title":"References &amp; useful resources","tags":[],"description":"","content":" What Is AWS Backup? - For backing up AWS resources other than S3 AWS re:Invent 2018: Architecture Patterns for Multi-Region Active-Active Applications (ARC209-R2)  AWS re:Invent 2019: Backup-and-restore and disaster-recovery solutions with AWS (STG208)  S3: Cross-Region Replication  Amazon S3 Replication Time Control for predictable replication time, backed by an SLA  Well-Architected Framework  Well-Architected best practices for reliability  Our Friend Rufus   Additional information on multi-region strategies for disaster recovery (DR) Recovery Time Objective (RTO) and Recovery Point Objective (RPO) These terms are most often associated with Disaster Recovery (DR), which are a set of objectives and strategies to recover workload availability in the case of a disaster\n Recovery time objective (RTO) is the overall length of time that a workloads components can be in the recovery phase, and therefore not available, before negatively impacting the organizations mission or mission/business processes. Recovery point objective (RPO) is the overall length of time that a workloads data can be unavailable, before negatively impacting the organizations mission or mission/business processes.  Use defined recovery strategies to meet defined recovery objectives If necessary, when architecting a multi-region strategy for your workload, you should choose one of the following strategies. They are listed in increasing order of complexity, and decreasing order of RTO and RPO. DR Region refers to an AWS Region other than the one used for your workload (or any AWS Region if your workload is on premises).\n Backup and restore (RPO in hours, RTO in 24 hours or less): Back up your data and applications into the DR Region. Restore this data when necessary to recover from a disaster. Pilot light (RPO in minutes, RTO in hours): Maintain a minimal version of an environment always running the most critical core elements of your system in the DR Region. When the time comes for recovery, you can rapidly provision a full-scale production environment around the critical core. Warm standby (RPO in seconds, RTO in minutes): Maintain a scaled-down version of a fully functional environment always running in the DR Region. Business-critical systems are fully duplicated and are always on, but with a scaled down fleet. When the time comes for recovery, the system is scaled up quickly to handle the production load. Multi-region active-active (RPO is none or possibly seconds, RTO in seconds): Your workload is deployed to, and actively serving traffic from, multiple AWS Regions. This strategy requires you to synchronize users and data across the Regions that you are using. When the time comes for recovery, use services like Amazon Route 53 or AWS Global Accelerator to route your user traffic to where your workload is healthy.  The bi-directional cross-region replication that you created in this lab is helpful for Pilot light, Warm standby, and Multi-region active-active strategies.\n function prevStep(){ window.open(\"..\\/4_cleanup\\/\", \"_self\") } function nextStep(){ window.open(\"\\/cost\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with REL 13 How do you plan for disaster recovery (DR)?\nClick here to access the Well-Architected Tool   Previous Step Complete this lab     "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_2_cost_and_usage_governance/5_tear_down/","title":"Tear down","tags":[],"description":"","content":"Delete a budget report We will delete the bugdet report.\n  From the Budgets Reports dashboard, click on the three dots next to the Weekly Budgets budget report, and click Delete:   You can see there are no budget reports:   Delete a budget We will delete all three budgets that we configured.\n  From the Budgets dashboard, click the three dots next to the budget CostBudget1, click Delete:   Do the same for the budgets EC2_actual and SP_Coverage\n   function prevStep(){ window.open(\"..\\/4_budget_report\\/\", \"_self\") } function nextStep(){ window.open(\"\\/cost\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with COST2 - \u0026ldquo;How do you govern usage?\u0026quot;\nClick here to access the Well-Architected Tool   Previous Step Complete this lab     "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_3_pricing_models/5_tear_down/","title":"Tear down","tags":[],"description":"","content":"There is no tear down required for this lab.\n function prevStep(){ window.open(\"..\\/4_visualize_recommendations\\/\", \"_self\") } function nextStep(){ window.open(\"\\/cost\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with COST7 - \u0026ldquo;How do you use pricing models to reduce cost?\u0026quot;\nClick here to access the Well-Architected Tool   Previous Step Complete this lab     "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_iam_permission_boundaries_delegating_role_creation/5_cleanup/","title":"Tear down","tags":[],"description":"","content":"Please note that the changes you made to the users, groups, and roles have no charges associated with them.\n Using the original IAM user, for each of the roles you created select them in the IAM console at https://console.aws.amazon.com/iam/ and click Delete role. The roles created are: app1-user-region-restricted-services developer-restricted-iam For each of the policies you created, one at a time select the radio button then Policy actions drop down menu then Delete. The policies created are: restrict-region-boundary createrole-restrict-region-boundary iam-restricted-list-read   References \u0026amp; useful resources Permissions Boundaries for IAM Entities AWS Identity and Access Management User Guide IAM Best Practices and Use Cases Become an IAM Policy Master in 60 Minutes or Less Actions, Resources, and Condition Keys for Identity And Access Management "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_iam_tag_based_access_control_for_ec2/5_cleanup/","title":"Tear down","tags":[],"description":"","content":"Please note that the changes you made to the policies and roles have no charges associated with them.\n Using the original IAM user, select the ec2-admin-team-alpha role in the IAM console at https://console.aws.amazon.com/iam/ and click Delete role. For each of the policies you created, one at a time select the radio button then Policy actions drop down menu then Delete. The policies created are: ec2-create-tags ec2-create-tags-existing ec2-list-read ec2-manage-instances ec2-run-instances   "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_lambda_cross_account_bucket_policy/5_cleanup/","title":"Tear down","tags":[],"description":"","content":" Remove the lambda function, then roles If you created a new S3 bucket, then you may remove it   "},{"uri":"https://wellarchitectedlabs.com/reliability/100_labs/100_deploy_cloudformation/5_cleanup/","title":"Tear down this lab","tags":[],"description":"","content":"If you are attending an in-person workshop and were provided with an AWS account by the instructor:\n There is no need to tear down the lab. Feel free to continue exploring. Log out of your AWS account when done.  If you are using your own AWS account:\n You may leave these resources deployed for as long as you want. When you are ready to delete these resources, see the following instructions  How to delete an AWS CloudFormation stack If you are already familiar with how to delete an AWS CloudFormation stack, then skip to the next section: Delete workshop CloudFormation stacks\n Go to the AWS CloudFormation console: https://console.aws.amazon.com/cloudformation Select the CloudFormation stack to delete and click Delete In the confirmation dialog, click Delete stack The Status changes to DELETE_IN_PROGRESS Click the refresh button to update and status will ultimately progress to DELETE_COMPLETE When complete, the stack will no longer be displayed. To see deleted stacks use the drop down next to the Filter text box. To see progress during stack deletion  Click the stack name Select the Events column Refresh to see new events    Delete workshop CloudFormation stacks  First delete the CloudFormationLab CloudFormation stack Wait for the CloudFormationLab CloudFormation stack to complete (it will no longer be shown on the list of actice stacks) Then delete the WebApp1-VPC CloudFormation stack   References \u0026amp; useful resources AWS CloudFormation\n What is AWS CloudFormation?  CloudFormation AWS Resource and Property Types Reference   AWS Resources that enable reliable architectures:\n What Is Amazon EC2 Auto Scaling?  Elastic Load Balancing: What Is an Application Load Balancer?  Availability Zones: AWS Global Infrastructure    function prevStep(){ window.open(\"..\\/4_explore_cloudformation\\/\", \"_self\") } function nextStep(){ window.open(\"\\/cost\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with REL 8 How do you implement change?\nClick here to access the Well-Architected Tool   Previous Step Complete this lab     "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_health_checks_and_dependencies/5_cleanup/","title":"Tear down this lab","tags":[],"description":"","content":"If you are attending an in-person workshop and were provided with an AWS account by the instructor:\n There is no need to tear down the lab. Feel free to continue exploring. Log out of your AWS account when done.  If you are using your own AWS account:\n You may leave these resources deployed for as long as you want. When you are ready to delete these resources, see the following instructions  Remove AWS CloudFormation provisioned resources How to delete an AWS CloudFormation stack If you are already familiar with how to delete an AWS CloudFormation stack, then skip to the next section: Delete workshop CloudFormation stacks\n Go to the AWS CloudFormation console: https://console.aws.amazon.com/cloudformation Select the CloudFormation stack to delete and click Delete In the confirmation dialog, click Delete stack The Status changes to DELETE_IN_PROGRESS Click the refresh button to update and status will ultimately progress to DELETE_COMPLETE When complete, the stack will no longer be displayed. To see deleted stacks use the drop down next to the Filter text box. To see progress during stack deletion  Click the stack name Select the Events column Refresh to see new events    Delete workshop CloudFormation stacks  First delete the HealthCheckLab CloudFormation stack Wait for the HealthCheckLab CloudFormation stack to complete (it will no longer be shown on the list of actice stacks) Then delete the WebApp1-VPC CloudFormation stack  Remove CloudWatch logs After deletion of the WebApp1-VPC CloudFormation stack is complete then delete the CloudWatch Logs:\n Open the CloudFormation console at https://console.aws.amazon.com/cloudwatch/ . Click Logs in the left navigation. Click the radio button on the left of the WebApp1-VPC-VPCFlowLogGroup-\u0026lt;some unique ID\u0026gt;. Click the Actions Button then click Delete Log Group. Verify the log group name then click Yes, Delete.   References \u0026amp; useful resources  Patterns for Resilient Architecture  Part 3  Amazon Builders' Library: Implementing health checks  Well-Architected Framework (see the Reliability pillar) Well-Architected best practices for reliability  Health Checks for Your Target Groups (for your Application Load Balancer)   function prevStep(){ window.open(\"..\\/4_fail_open\\/\", \"_self\") } function nextStep(){ window.open(\"\\/cost\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Congratulations! With completion of this lab you have learned several best practices. Consider how you can implement these, and update the Well-Architected Review for your workloads: REL 5 How do you design interactions in a distributed system to mitigate or withstand failures? REL 11 How do you design your workload to withstand component failures? .\nClick here to access the Well-Architected Tool   Previous Step Complete this lab     "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_autonomous_patching_with_ec2_image_builder_and_systems_manager/5_teardown/","title":"Teardown","tags":[],"description":"","content":"The following steps will remove the services which are deployed in the lab.\n  Teardown of CloudFormation Deployment   5.1. Remove the Automation Stack From the CloudFormation console, select the stack named pattern3-automate from the list and select Delete and confirm the deletion in the next dialog box.\n5.2. Remove the Pipeline Stack 5.2.1. Note: The stack will fail to remove unless the S3 bucket is empty. As a pre requisite, remove the contents of the bucket before continuing.\nFrom the CloudFormation console, click on the pattern3-pipeline stack name and examine the resources.\nFind the resource called Pattern3LoggingBucket and note the bucket name.\nProceed to the S3 console and remove the contents of the bucket, confirming the delete action.\n5.2.2. Now, from the CloudFormation console, select the stack named pattern3-pipeline from the list and select Delete and confirm the deletion in the next dialog box.\n5.3. Remove the Application Stack From the CloudFormation console, select the stack named pattern3-app from the list and select Delete and confirm the deletion in the next dialog box.\n5.4. Remove the Base Infrastructure Stack From the CloudFormation console, select the stack named pattern3-base from the list and select Delete and confirm the deletion in the next dialog box.\n  "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/5_failure_injection_rds/","title":"Test Resiliency Using RDS Failure Injection","tags":[],"description":"","content":"5.1 RDS failure injection This failure injection will simulate a critical failure of the Amazon RDS DB instance.\n  Before starting, view the deployment machine in the AWS Step Functions console to verify the deployment has reached the stage where you can start testing:\n single region: WaitForMultiAZDB shows completed (green) multi region: both WaitForRDSRRStack1 and CheckRDSRRStatus1 show completed (green)    Before you initiate the failure simulation, refresh the service website several times. Every time the image is loaded, the website writes a record to the Amazon RDS database\n  Click on click here to go to other page and it will show the latest ten entries in the Amazon RDS DB\n The DB table shows \u0026ldquo;hits\u0026rdquo; on our image page Website URL access requests are shown here for traffic against the image page. These include IPs of browser traffic as well as IPs of load balancer health checks For each region the AWS Elastic Load Balancer makes these health checks, so you will see three IP addresses from these Click on click here to go to other page again to return to the image page    Go to the RDS Dashboard in the AWS Console at http://console.aws.amazon.com/rds\n  From the RDS dashboard\n Click on \u0026ldquo;DB Instances (n/40)\u0026rdquo; Click on the DB identifier for your database (if you have more than one database, refer to the VPC ID to find the one for this workshop) If running the multi-region deployment, select the DB instance with Role=Master Select the Configuration tab    Look at the configured values. Note the following:\n Value of the Info field is Available RDS DB is configured to be Multi-AZ. The primary DB instance is in AZ us-east-2a and the standby DB instance is in AZ us-east-2b     To failover of the RDS instance, use the VPC ID as the command line argument replacing \u0026lt;vpc-id\u0026gt; in one (and only one) of the scripts/programs below. (choose the language that you setup your environment for)\n   Language Command     Bash ./failover_rds.sh \u0026lt;vpc-id\u0026gt;   Python python fail_rds.py \u0026lt;vpc-id\u0026gt;   Java java -jar app-resiliency-1.0.jar RDS \u0026lt;vpc-id\u0026gt;   C# .\\AppResiliency RDS \u0026lt;vpc-id\u0026gt;   PowerShell .\\failover_rds.ps1 \u0026lt;vpc-id\u0026gt;      The specific output will vary based on the command used, but will include some indication that the your Amazon RDS Database is being failedover: Failing over mdk29lg78789zt\n  5.2 System response to RDS instance failure Watch how the service responds. Note how AWS systems help maintain service availability. Test if there is any non-availability, and if so then how long.\n5.2.1 System availability   The website is not available. Some errors you might see reported:\n No Response / Timeout: Request was successfully sent to EC2 server, but server no longer has connection to an active database 504 Gateway Time-out: Amazon Elastic Load Balancer did not get a response from the server. This can happen when it has removed the servers that are unable to respond and added new ones, but the new ones have not yet finished initialization, and there are no healthy hosts to receive the request 502 Bad Gateway: The Amazon Elastic Load Balancer got a bad request from the server An error you will not see is This site cant be reached. This is because the Elastic Load Balancer has a node in each of the three Availability Zones and is always available to serve requests.    Continue on to the next steps, periodically returning to attempt to refresh the website.\n  5.2.2 Failover to standby  On the database console Configuration tab   Refresh and note the values of the Info field. It will ultimately return to Available when the failover is complete.\n  Note the AZs for the primary and standby instances. They have swapped as the standby has no taken over primary responsibility, and the former primary has been restarted. (After RDS failover it can take several minutes for the console to update as shown below. The failover has however completed)\n  From the AWS RDS console, click on the Logs \u0026amp; events tab and scroll down to Recent events. You should see entries like those below. In this case failover took less than a minute.\n Mon, 14 Oct 2019 19:53:37 GMT - Multi-AZ instance failover started. Mon, 14 Oct 2019 19:53:45 GMT - DB instance restarted Mon, 14 Oct 2019 19:54:21 GMT - Multi-AZ instance failover completed      5.2.3 EC2 server replacement   From the AWS RDS console, click on the Monitoring tab and look at DB connections\n  As the failover happens the existing three servers all cannot connect to the DB\n  AWS Auto Scaling detects this (any server not returning an http 200 status is deemed unhealthy), and replaces the three EC2 instances with new ones that establish new connections to the new RDS primary instance\n  The graph shows an unavailability period of about four minutes until at least one DB connection is re-established\n    [optional] Go to the Auto scaling group and AWS Elastic Load Balancer Target group consoles to see how EC2 instance and traffic routing was handled\n  5.2.4 RDS failure injection - conclusion  AWS RDS Database failover took less than a minute Time for AWS Auto Scaling to detect that the instances were unhealthy and to start up new ones took four minutes. This resulted in a four minute non-availability event.  5.2.5 [OPTIONAL] RDS failure injection - improving resiliency In this section you reduce the unavailability time from four minutes to under one minute.\nThis part of the RDS failure simulation is optional. If you are running this lab as part of a live workshop, then you may want to skip this and come back to it later.\n You observed before that failover of the RDS instance itself takes under one minute. However the servers you are running are configured such that they cannot recognize that the IP address for the RDS instance DNS name has changed from the primary to the standby. Availability is only regained once the servers fail to reach the primary, are marked unhealthy, and then are replaced. This accounts for the four minute delay. In this part of the lab you will update the server code to be more resilient to RDS failover. The new code can recognize underlying changes in IP address for the RDS instance DNS name\nUse either the Express Steps or Detailed Steps below:\nExpress Steps  Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation For the WebServersForResiliencyTesting Cloudformation stack  Redeploy the stack and Use current template Change the BootObject parameter to server_with_reconnect.py    Detailed Steps   Click here for detailed steps for updating the Cloudformation stack:    Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation Click on WebServersForResiliencyTesting Cloudformation stack Click the Update button Select Use current template then click Next On the Parameters page, find the BootObject parameter and replace the value there with server_with_reconnect.py Click Next Click Next Scroll to the bottom and under Change set preview note that you are changing the WebServerAutoscalingGroup and WebServerLaunchConfiguration. This CloudFormation deployment will modify the launch configuration to use the improved server code. Check I acknowledge that AWS CloudFormation might create IAM resources. Click Update stack Go the Events tab for the WebServersForResiliencyTesting Cloudformation stack and observe the progress. When the status is UPDATE_COMPLETE_CLEANUP_IN_PROGRESS you may continue.    RDS failure injections - observations Now repeat the RDS failure injection steps on this page, starting with 5.1 RDS failure injection .\n You will observe that the unavailability time is now under one minute What else is different compared to the previous time the RDS instance failed over?   Resources Learn more: After the lab see High Availability (Multi-AZ) for Amazon RDS for more details on high availability and failover support for DB instances using Multi-AZ deployments.\nHigh Availability (Multi-AZ) for Amazon RDS\n The primary DB instance switches over automatically to the standby replica if any of the following conditions occur:\n An Availability Zone outage The primary DB instance fails The DB instance\u0026rsquo;s server type is changed The operating system of the DB instance is undergoing software patching A manual failover of the DB instance was initiated using Reboot with failover    function prevStep(){ window.open(\"..\\/4_failure_injection_ec2\", \"_self\") } function nextStep(){ window.open(\"..\\/6_failure_injection_az\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_autonomous_monitoring_of_cryptographic_activity_with_kms/5_testing_the_workload_functionality/","title":"Testing the Workload Functionality","tags":[],"description":"","content":"Following the completion of section 4, we can complete the lab by testing the workload. We will achieve this by running a decrypt API call to our application. This will trigger a failed decrypt event which should result in our alarm being triggered and an SNS notification sent to the email address which you specified as an endpoint in the previous section.\nComplete the following steps to test the system functionality:\n5.1. Initiate a successful decryption operation Run the command shown below within your Cloud9 IDE, replacing the \u0026lt; encrypt key \u0026gt; with the key value that you took note of in section 2.4 as well as the \u0026lt; Application endpoint URL \u0026gt; with the OutputPattern1ApplicationEndpoint url you took note on section 2.3.3\nALBURL=\u0026quot;http://\u0026lt; Application endpoint URL \u0026gt;\u0026quot; curl --header \u0026quot;Content-Type: application/json\u0026quot; --request GET --data '{\u0026quot;Name\u0026quot;:\u0026quot;Andy Jassey\u0026quot;,\u0026quot;Key\u0026quot;:\u0026quot;\u0026lt;encrypt key\u0026gt;\u0026quot;}' $ALBURL/decrypt Once that is successful, you should see out put like below:\n{\u0026quot;Text\u0026quot;:\u0026quot;Welcome to ReInvent 2020!\u0026quot;} 5.2. Initiate an unsuccessful decryption operation Now that we have confirmed that the decrypt API is operational, let\u0026rsquo;s trigger a deliberate decryption failure to invoke our alerting.\nRun below command once again, but this time, pass on a wrong key for the encrypt key (you can just use whatever value).\nALBURL=\u0026quot;http://\u0026lt; Application endpoint URL \u0026gt;\u0026quot; curl --header \u0026quot;Content-Type: application/json\u0026quot; --request GET --data '{\u0026quot;Name\u0026quot;:\u0026quot;Andy Jassey\u0026quot;,\u0026quot;Key\u0026quot;:\u0026quot;some-random-false-key\u0026quot;}' $ALBURL/decrypt Once it is triggered, you should see output like below signifying that the decrypt procedure has failed, and in the background a failed KMS API has been called. :\n{\u0026quot;Message\u0026quot;:\u0026quot;Data decryption failed, make sure you have the correct key\u0026quot;} Make sure that you repeat this several times in a row, to ensure you we are triggering the alarm. This will result in email notification to the endpoint you defined earlier in the lab.\nNote:  CloudTrail can typically take up to 15 mins to pick up the API event and trigger your alarm. For more information about this, please visit Cloudtrail FAQ page  5.3. Observing the alarm. If all the components are configured correctly, you should receive an email notification triggered by the CloudWatch alarm similar to this:\n5.3.1. Click on the URL included in the email that will take you to the CloudWatch Alarm resource in AWS console.\n5.3.2. Observe the state changes under the History section, and notice each activity change as follows:\nCongratulations! you have completed the Pattern1 lab.\n END OF SECTION 5\n "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_splitting_sharing_cur_access/5_trigger_lambda/","title":"Trigger the Lambda When a CUR is Delivered","tags":[],"description":"","content":"It is assumed that you have completed 300_Automated_CUR_Updates_and_Ingestion, so there is an existing Lambda function that is being executed when a new CUR file is delivered. We will add code into this setup to trigger the new Lambda function.\n1 - Go to the CloudFormation service Dashboard\n2 - Select the current stack which updates the Glue database\n3 - Download the current template (crawler-cfn.yml file), and save this for later (if Teardown is required)\n4 - Open the template up in a text editor of your choice\n5 - A sample crawler file is below:\n./Code/crawler-cfn.md 6 - Update the ** AWSCURCrawlerLambdaExecutor** IAM role section, inside the PolicyName **AWSCURCrawlerLambdaExecutor** section:\nAdd the following Action:\n 'lambda:InvokeFunction' Edit the following line, and add the following resource\n - 'arn:aws:lambda:\u0026lt;region\u0026gt;:\u0026lt;accountID\u0026gt;:function:SubAcctSplit' 7 - Make the following amendments to the AWSCURInitializer Lambda function section, inside the else statement after the glue section:\n var lambda = new AWS.Lambda(); var params = { FunctionName: 'SubAcctSplit' }; lambda.invoke(params, function(err, data) { if (err) console.log(err, err.stack); // an error occurred else console.log(data); // successful response }); 8 - Save the new template file\n9 - In the CloudFormation console update the stack\n10 - Replace the current template with the new one, and upload your modified template\n11 - After the stack has successfully updated, you can test the function\n12 - Go to the S3 service dashboard, navigate to the source bucket and folder containing the current months original management/payer CUR file\n13 - Download the CUR file, and delete the object from the bucket\n14 - Re-upload the current CUR file back into its bucket\n15 - Navigate to the output bucket and folder for the current month\n16 - Check the Last modified time stamp on the object/s is/are the current time, and check that it has the correct Grantees in the permissions\nSetup is now complete for the payer account. When new CUR files are delivered, it will execute the Athena queries and extract the required data for the current month, and output it to the required S3 folder with the required permissions.\n function prevStep(){ window.open(\"..\\/4_lambda_function\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/6_sub_acct\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_aws_resource_optimization/5_ec2_updated_rec/","title":"Updated Amazon EC2 Resource Optimization recommendations","tags":[],"description":"","content":"NOTE: In order to complete this step you need to have Amazon EC2 Resource Optimization enabled, you can do that going to the AWS Cost Explorer, Recommendations (left bar) section.\nImportant: If you have just installed the CloudWatch agent at your instances it may take a couple of days for Amazon EC2 Resource Optimization to start to provide updated recommendations, so don\u0026rsquo;t worry if you don\u0026rsquo;t see the memory data during the first checks.\nNow that we have Memory data as a custom metric in CloudWatch let\u0026rsquo;s check how that affects the Amazon EC2 Resource Optimization recommendations.\nAmazon EC2 Resource Optimization offers right sizing recommendations in the AWS Cost Explorer without any additional cost. These recommendations identify likely idle and underutilized instances across your accounts, regions and tags. To generate these recommendations, AWS analyzes your historical EC2 resource usage (last 14 days using Amazon CloudWatch) and your existing reservation footprint to identify opportunities for cost savings. There are two types of recommended actions: Terminate if the instance is considered idle (max CPU utilization is at or below 1%) or Downsize if the instance is underutilized (max CPU utilization is between 1% and 40%).\nBy default Amazon EC2 Resource Optimization doesn\u0026rsquo;t need memory datapoint to provide recommendations, but if that information is available it will take that into consideration updating the Downsize recommendation for instances that now have max CPU and MEM utilization between 1% and 40% over the past 14 days. Let\u0026rsquo;s validate that with the following steps.\n  Navigate to the AWS Cost Explorer page   Select Recommendations in the left menu bar   Click on the View All link associated with the Amazon EC2 Resource Optimization Recommendations section.   In case you havent enabled the Amazon EC2 Resource Optimization please do so (no additional cost), it may take up to 24 hours in order to generate your first recommendations. Only regular or payer accounts can enable it and by default both linked and payer accounts will be able to access their rightsizing recommendations unless the payer account specifically prohibits it on the settings page (top right). Assuming you had enabled the Amazon EC2 Resource Optimization Recommendations, you will be presented with a screen that provides recommendations (if any exists). Click to view the Resource Optimization recommendations.    Optimization opportunities  The number of recommendations available based on your resources and usage Estimated monthly savings  The sum of the projected monthly savings associated with each of the recommendations provided Estimated savings (%)  The available savings relative to the direct Amazon EC2 costs (On-Demand) associated with the instances in the recommendation list  You can also filter your recommendations by the type of action (Idle and Underutilized), Linked Account, Region and Tag.\nUnderstanding the Amazon EC2 Resource Optimization recommendations.  In the example below we have a recommendation to downsize the t2.micro (1vCPU for a 2h 24m burst and 1GB RAM) to a t2.nano (1vCPU for a 1h 12m burst and 0.5 GB RAM) and save $12 USD per year.\nOver the past 14 days the maximum CPU utilization for this instance was only 9% and this instance was running for 86 hours and all of these were On Demand hours. Observe that there is no memory information available so Amazon EC2 Resource Optimization will ignore that datapoint and recommend to downsize to a t2.nano that have half of the memory available of a t2.micro.\nThat can be risky and waste engineer time when testing if the proposed right sizing option is valid or not. That said you can improve the accuracy of this recommendation with the CloudWatch agent we just installed.\nIn this other example we see a recommendation to downsize a r5.8xlarge (32 vCPU and 256GB RAM) to a r5.4xlarge (16 vCPU and 128GB RAM) and save $2,412 USD per year. On this case we have both CPU and Memory information available: the maximum CPU utilization was 21% and Memory was only 5%. That makes the case for downsize much stronger and the recommendation will even try estimate the CPU and Memory utilization for the new instance size. Keep in mind that this is just a simple estimation based on the past utilization data from CloudWatch, before executing the modification all the required load tests must be performed to avoid any impacts on your workload.\nAs explained above the Amazon EC2 Resource Optimization logic will recommend to downsize any instances where the maximum CPU utilization was between 1% to 40% over the past 14 days. If you do have memory information available the Amazon EC2 Resource Optimization will now consider to downsize instances that have both CPU and Memory maximum utilization between 1% and 40%. Idle recommendations are not impacted if memory data is available, so any EC2 instance that during the past 14 days never passed 1% of CPU utlization will be automatically flagged as idle.\n function prevStep(){ window.open(\"..\\/4_memory_plugin\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/6_ec2_rs_best_practices\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_5_cost_visualization/5_ri_coverage/","title":"View your Reserved Instance coverage","tags":[],"description":"","content":"You can view your Reserved Instance coverage to look for anomalies or changes in coverage. For compute resources you should purchase Savings Plans instead of Reserved Instances.\n  In Cost Explorer, click on Saved reports on the left:   Click on RI Coverage:   You will see the default RI Coverage report. It is for the Last 3 Months, and is for the instances within the EC2 service:   Scroll down below the graph and you can see a summary of the costs and usage. Note that depending on the instance type and size, the On-Demand costs will be different per hour:   To help focus where you need to, click on the down arrow next to ON-DEMAND COST to sort by costs descending. This will put the highest on-demand costs at the top, which is where you should focus your RI purchases:   You have now viewed your RI coverage, and have insight on where to increase your coverage.\n function prevStep(){ window.open(\"..\\/4_elasticity\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/6_custom_ec2\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/100_labs/100_walkthrough_of_the_well-architected_tool/5_view_report/","title":"Viewing and downloading the report","tags":[],"description":"","content":"  From the detail page for the workload you can identify the number of high and medium risk items:   You can also edit the improvement plan configuration. Scroll down and click on the Edit button next to the words Pillar priority:   Move the Reliability Pillar up by clicking the up icon to the right of the word, Reliability:   Click the Save button to save this configuration:   Click on the Properties tab:   Scroll down to set the improvement status:   From the detail page for the workload, click the AWS Well-Architected Framework navigation item (others navigation items would appear for any applied Well-Architected Lens):   Click the Generate report button to generate and download the report:   You can either open the file or save it to view it.\n   function prevStep(){ window.open(\"..\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/6_tear_down\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_5_cost_visualization/","title":"Level 100: Cost Visualization","tags":[],"description":"","content":"Authors  Nathan Besh, Cost Lead Well-Architected  Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com\nIntroduction This hands-on lab will guide you through the steps to perform visualization of your AWS cost and usage. The skills you learn will help you monitor your cost and usage, in alignment with the AWS Well-Architected Framework.\nGoals  Perform basic analysis through visualization of your cost and usage  Prerequisites  AWS Account Setup has been completed An account with usage for more than 1 month  Permissions required  Log in as the Cost Optimization team, created in AWS Account Setup  NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required.  Costs  https://aws.amazon.com/aws-cost-management/pricing/ Variable costs will be incurred Cost Explorer: $0.01 per 1,000 usage records for hourly granularity reporting  Time to complete  The lab should take approximately 15 minutes to complete  Steps:  View your cost and usage by service   View your cost and usage by account   View your Savings Plan coverage   View your Elasticity   View your Reserved Instance coverage   Create custom EC2 reports   Tear down    function prevStep(){ window.open(\"\", \"_self\") } function nextStep(){ window.open(\".\\/1_cost_usage_service\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool  Start Lab  "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_automated_deployment_of_web_application_firewall/","title":"Level 200: Automated Deployment of Web Application Firewall","tags":[],"description":"","content":"Authors  Ben Potter, Security Lead, Well-Architected  Introduction This hands-on lab will guide you through the steps to protect a workload from network based attacks using AWS Web Application Firewall (WAF) integrated with Amazon CloudFront. You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy AWS Web Application Firewall (WAF) with CloudFront integration to apply defense in depth methods. Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nGoals  Protecting network and host-level boundaries System security configuration and maintenance Enforcing service-level protection  Prerequisites  An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab.  Steps:  Configure AWS WAF   Configure Amazon CloudFront   Tear down   "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_5_cost_visualization/","title":"Level 200: Cost Visualization","tags":[],"description":"","content":"Last Updated May 2020\nAuthors  Spencer Marley, Commercial Architect Nathan Besh, Cost Lead, Well-Architected  Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com\nIntroduction This hands-on lab will guide you through the steps to visualize your cost and usage. The skills you learn will help you analyze your cost and usage, in alignment with the AWS Well-Architected Framework.\nGoals  Setup Amazon QuickSight Configure QuickSight to view your Cost and Usage reports Create a dashboard of cost and usage  Prerequisites  A management AWS Account Have your Cost and Usage Report (CUR) enabled as per 100_1_Account Setup  AWS Account Setup has been completed Cost_and_Usage_Analysis has been completed  Permissions required  Log in as the Cost Optimization team, created in AWS Account Setup  NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required.  Costs  QuickSight pricing  Approx $9-$12 monthly for QuickSight Authors Estimated additional costs should be \u0026lt;$5 a month for small accounts  Time to complete  The lab should take approximately 20 minutes to complete  Steps:  Create a data set   Create visualizations   Share your Analysis and Dashboard   Tear down    function prevStep(){ window.open(\"\", \"_self\") } function nextStep(){ window.open(\".\\/1_create_dataset\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool  Start Lab  "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_lambda_cross_account_bucket_policy/","title":"Level 300: Lambda Cross Account Using Bucket Policy","tags":[],"description":"","content":"Authors  Seth Eliot, Resiliency Lead, Well-Architected, AWS  Introduction This lab demonstrates configuration of an S3 bucket policy (which is a type of resource based policy) in AWS account 2 (the destination) that enables a Lambda function in AWS account 1 (the origin) to list the objects in that bucket using Python boto SDK. If you only have 1 AWS account simply repeat the instructions in that account and use the same account id.\nIf in classroom and you do not have 2 AWS accounts, buddy up to use each other\u0026rsquo;s accounts, agree who will be account #1 and who will be account #2.\nThe skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nGoals  S3 bucket policies Resource based policies versus identity based policies  Prerequisites  An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user with MFA enabled that can assume roles in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .  Steps:  Identify (or create) S3 bucket in account 2   Create role for Lambda in account 1   Create bucket policy for the S3 bucket in account 2   Create Lambda in account 1   Tear down   References \u0026amp; useful resources  https://docs.aws.amazon.com/AmazonS3/latest/dev/using-iam-policies.html https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_identity-vs-resource.html  "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_300_security_best_practices_day/","title":"Quest: AWS Security Best Practices Day","tags":[],"description":"This quest is the guide for an AWS led event including security best practices day. Includes identity &amp; access management, detective controls, infrastructure protection, data protection and incident response.","content":"Authors  Ben Potter, Security Lead, Well-Architected  About this Guide This quest is the guide for an AWS led event including security best practices day. Using your own AWS account you will learn through hands-on labs including identity \u0026amp; access management, detective controls, infrastructure protection, data protection and incident response. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nPrerequisites  An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .   Lab 1: Identity \u0026amp; Access Management For Lab 1 choose labs based on your interest or experience, its important to secure your AWS account so start with the introductory ones if you have not already completed:\nIntroductory Lab 1.1: AWS Account and Root User This hands-on lab will guide you through the introductory steps to configure a new AWS account and secure the root user.\nAWS Account and Root User  Lab 1.2 Basic Identity and Access Management User, Group, Role This hands-on lab will guide you through the introductory steps to configure AWS Identity and Access Management (IAM). You will use the AWS Management Console to guide you through how to configure your first IAM user, group and role for administrative access.\nBasic Identity and Access Management User, Group, Role  Advanced Lab 1.3 - IAM Permission Boundaries Delegating Role Creation This hands-on lab will guide you through the steps to configure an example AWS Identity and Access Management (IAM) permission boundary. AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature in which you use a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by the policy. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as a developer, the developer can then use this role to create additional user roles that are restricted to specific services and regions. This allows you to delegate access to create IAM roles and policies, without them exceeding the permissions in the permission boundary. We will also use a naming standard with a prefix, making it easier to control and organize policies and roles that your developers create.\nIAM Permission Boundaries Delegating Role Creation  Lab 1.4 - IAM Tag Based Access Control for EC2 This hands-on lab will guide you through the steps to configure example AWS Identity and Access Management (IAM) policies, and a AWS IAM role with associated permissions to use EC2 resource tags for access control. Using tags is powerful as it helps you scale your permission management, however you need to be careful about the management of the tags which you will learn in this lab. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as an EC2 administrator. This allows the EC2 administrator to create tags when creating resources only if they match the requirements, and control which existing resources and values they can tag.\nIAM Tag Based Access Control for EC2  Lab 2 - Automated Deployment of Detective Controls This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail, AWS Config, and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of each service.\nAutomated Deployment of Detective Controls  Lab 3 - Enable Security Hub AWS Security Hub gives you a comprehensive view of your high-priority security alerts and compliance status across AWS accounts. There are a range of powerful security tools at your disposal, from firewalls and endpoint protection to vulnerability and compliance scanners. But oftentimes this leaves your team switching back-and-forth between these tools to deal with hundreds, and sometimes thousands, of security alerts every day. With Security Hub, you now have a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services, such as Amazon GuardDuty, Amazon Inspector, and Amazon Macie, as well as from AWS Partner solutions. Your findings are visually summarized on integrated dashboards with actionable graphs and tables. You can also continuously monitor your environment using automated compliance checks based on the AWS best practices and industry standards your organization follows. Get started with AWS Security Hub in just a few clicks in the Management Console and once enabled, Security Hub will begin aggregating and prioritizing findings.\nEnable Security Hub  Lab 4 - Automated Deployment of VPC This hands-on lab will guide you through the steps to configure an Amazon VPC and outline some of the AWS security features. AWS CloudFormation will be used to automate the deployment and provide a repeatable way to re-use the template after this lab. The example CloudFormation template will deploy a completely new VPC incorporating a number of AWS security best practices which are: Networking subnets created in multiple availability zones for the following network tiers:\n Application Load Balancer - named ALB1 Application instances - named App1 Shared services - named Shared1 Databases - named DB1 VPC endpoints are created for private connectivity to AWS services. NAT Gateways are created to allow different subnets in the VPC to connect to the internet, without any direct ingress access being possible due to Route Table configurations. Network ACLs control access at each subnet layer. While VPC Flow Logs captures information about IP traffic and stores it in Amazon CloudWatch Logs. Do not follow tear down instructions until you have completed this quest, as the EC2 lab requires this VPC.  Automated Deployment of VPC  Lab 5 - Automated Deployment of EC2 Web Application This hands-on lab will guide you through the steps to configure a web application in Amazon EC2 with a defense in depth approach. The WordPress example CloudFormation template will deploy a basic WordPress content management system, incorporating a number of AWS security best practices. This example is not intended to be a comprehensive WordPress system, please consult Build a WordPress Website for more information.\nAutomated Deployment of EC2 Web Application  Lab 6 - Automated Deployment of Web Application Firewall This hands-on lab will guide you through the steps to protect a workload from network based attacks using AWS Web Application Firewall (WAF) integrated with Amazon CloudFront. You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy AWS Web Application Firewall (WAF) with CloudFront integration to apply defense in depth methods.\nAutomated Deployment of Web Application Firewall  Lab 7 - CloudFront for Web Application This hands-on lab will guide you through the steps to help protect a web application from network based attacks using Amazon CloudFront. You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy CloudFront.\nCloudFront for Web Application  Lab 8 - Incident Response with AWS Console and CLI This hands-on lab will guide you through a number of examples of how you could use the AWS Console and Command Line Interface (CLI) for responding to a security incident. It is a best practice to be prepared for an incident, and have appropriate detective controls enabled. You can find more best practices by reading the Security Pillar of the AWS Well-Architected Framework .\nIncident Response with AWS Console and CLI  "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/","title":"200 Labs","tags":[],"description":"","content":"List of labs available  Level 200: Cost and Usage Governance   Level 200: Pricing Models   Level 200: Cost and Usage Analysis   Level 200: Cost Visualization   Level 200: EC2 Right Sizing   Level 200: Pricing Model Analysis   Level 200: Enterprise Dashboards   Level 200: Workload Efficiency   Level 200: Licensing   Level 200: Cost Journey   "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_goals_and_targets/5_select_services/","title":"Service Selection","tags":[],"description":"","content":"Service Selection Evaluate Cost Licensing  Goal: Reduce database licensing costs Target: Within 1 year, convert 10 workloads with VendorX databases to Open Source databases Best Practice: Licensing Costs  Measures: Number of workloads converted Good/Bad: Good Why? When does it work well or not?: Works well to reduce licensing costs, significant effort in conversion may reduce overall savings. Contact/Contributor: natbesh@amazon.com    function prevStep(){ window.open(\"..\\/4_decommission_resources\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/6_resource_type_size_number\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_using_awscli_to_manage_wa_reviews/6_programmatic/","title":"Optional - Programmatic access via API","tags":[],"description":"","content":"Choose which programming language you wish to use  Python  PowerShell   Python version using Boto3 Library The lab uses AWS CLI to perform all of the tasks, but you can also use the AWS SDK for Python (Boto3) to perform the same steps. As a reference, you can download the code LabExample.py which will perform all of steps from the lab in a single python file. This file assumes you have already setup your AWS credential file, and uses the default profile for all interactions.\nThe code has been broken up into functions which accept various parameters, so you can pull those out and place them into integration points in your environment. There is error checking for most of the various API calls, but the code should not be considered production ready. Please review before implementing in your environment.\nPython code #!/usr/bin/env python3 # This is a simple python app for use with the Well-Architected labs # This will simulate all of the steps in the 200-level lab on using the # Well-Architected API calls # # This code is only for use in Well-Architected labs # *** NOT FOR PRODUCTION USE *** # # # Licensed under the Apache 2.0 and MITnoAttr License. # # Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;). You may not use this file except in compliance with the License. A copy of the License is located at # https://aws.amazon.com/apache2.0/ import botocore import boto3 import json import datetime import logging import jmespath import base64 from pkg_resources import packaging __author__ = \u0026#34;Eric Pullen\u0026#34; __email__ = \u0026#34;eppullen@amazon.com\u0026#34; __copyright__ = \u0026#34;Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\u0026#34; __credits__ = [\u0026#34;Eric Pullen\u0026#34;] # Default region listed here REGION_NAME = \u0026#34;us-east-1\u0026#34; blankjson = {} response = \u0026#34;\u0026#34; # Setup Logging logging.basicConfig( level=logging.DEBUG, format=\u0026#39;%(asctime)s.%(msecs)03d%(levelname)s%(module)s- %(funcName)s: %(message)s\u0026#39;, datefmt=\u0026#39;%Y-%m-%d%H:%M:%S\u0026#39;, ) logger = logging.getLogger() logging.getLogger(\u0026#39;boto3\u0026#39;).setLevel(logging.CRITICAL) logging.getLogger(\u0026#39;botocore\u0026#39;).setLevel(logging.CRITICAL) logging.getLogger(\u0026#39;s3transfer\u0026#39;).setLevel(logging.CRITICAL) logging.getLogger(\u0026#39;urllib3\u0026#39;).setLevel(logging.CRITICAL) # Helper class to convert a datetime item to JSON. class DateTimeEncoder(json.JSONEncoder): def default(self, z): if isinstance(z, datetime.datetime): return (str(z)) else: return super().default(z) def CreateNewWorkload( waclient, workloadName, description, reviewOwner, environment, awsRegions, lenses ): # Create your workload try: waclient.create_workload( WorkloadName=workloadName, Description=description, ReviewOwner=reviewOwner, Environment=environment, AwsRegions=awsRegions, Lenses=lenses ) except waclient.exceptions.ConflictException as e: workloadId = FindWorkload(waclient,workloadName) logger.error(\u0026#34;ERROR - The workload name %salready exists as workloadId %s\u0026#34; % (workloadName, workloadId)) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) def FindWorkload( waclient, workloadName ): # Finding your WorkloadId try: response=waclient.list_workloads( WorkloadNamePrefix=workloadName ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(\u0026#34;Full JSON:\u0026#34;,json.dumps(response[\u0026#39;WorkloadSummaries\u0026#39;], cls=DateTimeEncoder)) workloadId = response[\u0026#39;WorkloadSummaries\u0026#39;][0][\u0026#39;WorkloadId\u0026#39;] # print(\u0026#34;WorkloadId\u0026#34;,workloadId) return workloadId def DeleteWorkload( waclient, workloadId ): # Delete the WorkloadId try: response=waclient.delete_workload( WorkloadId=workloadId ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) def GetWorkload( waclient, workloadId ): # Get the WorkloadId try: response=waclient.get_workload( WorkloadId=workloadId ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(\u0026#34;Full JSON:\u0026#34;,json.dumps(response[\u0026#39;Workload\u0026#39;], cls=DateTimeEncoder)) workload = response[\u0026#39;Workload\u0026#39;] # print(\u0026#34;WorkloadId\u0026#34;,workloadId) return workload def disassociateLens( waclient, workloadId, lens ): # Disassociate the lens from the WorkloadId try: response=waclient.disassociate_lenses( WorkloadId=workloadId, LensAliases=lens ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) def associateLens( waclient, workloadId, lens ): # Associate the lens from the WorkloadId try: response=waclient.associate_lenses( WorkloadId=workloadId, LensAliases=lens ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) def listLens( waclient ): # List all lenses currently available try: response=waclient.list_lenses() except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(json.dumps(response)) lenses = jmespath.search(\u0026#34;LensSummaries[*].LensAlias\u0026#34;, response) return lenses def findQuestionId( waclient, workloadId, lensAlias, pillarId, questionTitle ): # Find a questionID using the questionTitle try: response=waclient.list_answers( WorkloadId=workloadId, LensAlias=lensAlias, PillarId=pillarId ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) answers = response[\u0026#39;AnswerSummaries\u0026#39;] while \u0026#34;NextToken\u0026#34; in response: response = waclient.list_answers(WorkloadId=workloadId,LensAlias=lensAlias,PillarId=pillarId,NextToken=response[\u0026#34;NextToken\u0026#34;]) answers.extend(response[\u0026#34;AnswerSummaries\u0026#34;]) jmesquery = \u0026#34;[?starts_with(QuestionTitle, `\u0026#34;+questionTitle+\u0026#34;`) == `true`].QuestionId\u0026#34; questionId = jmespath.search(jmesquery, answers) return questionId[0] def findChoiceId( waclient, workloadId, lensAlias, questionId, choiceTitle, ): # Find a choiceId using the choiceTitle try: response=waclient.get_answer( WorkloadId=workloadId, LensAlias=lensAlias, QuestionId=questionId ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) jmesquery = \u0026#34;Answer.Choices[?starts_with(Title, `\u0026#34;+choiceTitle+\u0026#34;`) == `true`].ChoiceId\u0026#34; choiceId = jmespath.search(jmesquery, response) return choiceId[0] def getAnswersForQuestion( waclient, workloadId, lensAlias, questionId ): # Find a answer for a questionId try: response=waclient.get_answer( WorkloadId=workloadId, LensAlias=lensAlias, QuestionId=questionId ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(json.dumps(response)) jmesquery = \u0026#34;Answer.SelectedChoices\u0026#34; answers = jmespath.search(jmesquery, response) # print(answers) return answers def updateAnswersForQuestion( waclient, workloadId, lensAlias, questionId, selectedChoices, notes ): # Update a answer to a question try: response=waclient.update_answer( WorkloadId=workloadId, LensAlias=lensAlias, QuestionId=questionId, SelectedChoices=selectedChoices, Notes=notes ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(json.dumps(response)) jmesquery = \u0026#34;Answer.SelectedChoices\u0026#34; answers = jmespath.search(jmesquery, response) return answers def listMilestones( waclient, workloadId ): # Find a milestone for a workloadId try: response=waclient.list_milestones( WorkloadId=workloadId, MaxResults=50 # Need to check why I am having to pass this parameter ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(\u0026#34;Full JSON:\u0026#34;,json.dumps(response[\u0026#39;MilestoneSummaries\u0026#39;], cls=DateTimeEncoder)) milestoneNumber = response[\u0026#39;MilestoneSummaries\u0026#39;] return milestoneNumber def createMilestone( waclient, workloadId, milestoneName ): # Create a new milestone with milestoneName try: response=waclient.create_milestone( WorkloadId=workloadId, MilestoneName=milestoneName ) except waclient.exceptions.ConflictException as e: milestones = listMilestones(waclient,workloadId) jmesquery = \u0026#34;[?starts_with(MilestoneName,`\u0026#34;+milestoneName+\u0026#34;`) == `true`].MilestoneNumber\u0026#34; milestoneNumber = jmespath.search(jmesquery,milestones) logger.error(\u0026#34;ERROR - The milestone name %salready exists as milestone %s\u0026#34; % (milestoneName, milestoneNumber)) return milestoneNumber[0] except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(\u0026#34;Full JSON:\u0026#34;,json.dumps(response[\u0026#39;MilestoneSummaries\u0026#39;], cls=DateTimeEncoder)) milestoneNumber = response[\u0026#39;MilestoneNumber\u0026#39;] return milestoneNumber def getMilestone( waclient, workloadId, milestoneNumber ): # Use get_milestone to return the milestone structure try: response=waclient.get_milestone( WorkloadId=workloadId, MilestoneNumber=milestoneNumber ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(\u0026#34;Full JSON:\u0026#34;,json.dumps(response[\u0026#39;Milestone\u0026#39;], cls=DateTimeEncoder)) milestoneResponse = response[\u0026#39;Milestone\u0026#39;] return milestoneResponse def getMilestoneRiskCounts( waclient, workloadId, milestoneNumber ): # Return just the RiskCount for a particular milestoneNumber milestone = getMilestone(waclient,workloadId,milestoneNumber) # print(\u0026#34;Full JSON:\u0026#34;,json.dumps(milestone[\u0026#39;Workload\u0026#39;][\u0026#39;RiskCounts\u0026#39;], cls=DateTimeEncoder)) milestoneRiskCounts = milestone[\u0026#39;Workload\u0026#39;][\u0026#39;RiskCounts\u0026#39;] return milestoneRiskCounts def listAllAnswers( waclient, workloadId, lensAlias, milestoneNumber=\u0026#34;\u0026#34; ): # Get a list of all answers try: if milestoneNumber: response=waclient.list_answers( WorkloadId=workloadId, LensAlias=lensAlias, MilestoneNumber=milestoneNumber ) else: response=waclient.list_answers( WorkloadId=workloadId, LensAlias=lensAlias ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) answers = response[\u0026#39;AnswerSummaries\u0026#39;] while \u0026#34;NextToken\u0026#34; in response: if milestoneNumber: response = waclient.list_answers(WorkloadId=workloadId,LensAlias=lensAlias,MilestoneNumber=milestoneNumber,NextToken=response[\u0026#34;NextToken\u0026#34;]) else: response = waclient.list_answers(WorkloadId=workloadId,LensAlias=lensAlias,NextToken=response[\u0026#34;NextToken\u0026#34;]) answers.extend(response[\u0026#34;AnswerSummaries\u0026#34;]) # print(\u0026#34;Full JSON:\u0026#34;,json.dumps(answers, cls=DateTimeEncoder)) return answers def getLensReview( waclient, workloadId, lensAlias, milestoneNumber=\u0026#34;\u0026#34; ): # Use get_lens_review to return the lens review structure try: if milestoneNumber: response=waclient.get_lens_review( WorkloadId=workloadId, LensAlias=lensAlias, MilestoneNumber=milestoneNumber ) else: response=waclient.get_lens_review( WorkloadId=workloadId, LensAlias=lensAlias ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(\u0026#34;Full JSON:\u0026#34;,json.dumps(response[\u0026#39;LensReview\u0026#39;], cls=DateTimeEncoder)) lensReview = response[\u0026#39;LensReview\u0026#39;] return lensReview def getLensReviewPDFReport( waclient, workloadId, lensAlias, milestoneNumber=\u0026#34;\u0026#34; ): # Use get_lens_review_report to return the lens review PDF in base64 structure try: if milestoneNumber: response=waclient.get_lens_review_report( WorkloadId=workloadId, LensAlias=lensAlias, MilestoneNumber=milestoneNumber ) else: response=waclient.get_lens_review_report( WorkloadId=workloadId, LensAlias=lensAlias ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(\u0026#34;Full JSON:\u0026#34;,json.dumps(response[\u0026#39;LensReviewReport\u0026#39;][\u0026#39;Base64String\u0026#39;], cls=DateTimeEncoder)) lensReviewPDF = response[\u0026#39;LensReviewReport\u0026#39;][\u0026#39;Base64String\u0026#39;] return lensReviewPDF def main(): boto3_min_version = \u0026#34;1.16.38\u0026#34; # Verify if the version of Boto3 we are running has the wellarchitected APIs included if (packaging.version.parse(boto3.__version__) \u0026lt; packaging.version.parse(boto3_min_version)): logger.error(\u0026#34;Your Boto3 version (%s) is less than %s. You must ugprade to run this script (pip3 upgrade boto3)\u0026#34; % (boto3.__version__, boto3_min_version)) exit() # STEP 1 - Configure environment logger.info(\u0026#34;1 - Starting Boto %sSession\u0026#34; % boto3.__version__) # Create a new boto3 session SESSION = boto3.session.Session() # Initiate the well-architected session using the region defined above WACLIENT = SESSION.client( service_name=\u0026#39;wellarchitected\u0026#39;, region_name=REGION_NAME, ) WORKLOADNAME = \u0026#39;WA Lab Test Workload\u0026#39; DESCRIPTION = \u0026#39;Test Workload for WA Lab\u0026#39; REVIEWOWNER = \u0026#39;WA Python Script\u0026#39; ENVIRONMENT= \u0026#39;PRODUCTION\u0026#39; AWSREGIONS = [REGION_NAME] LENSES = [\u0026#39;wellarchitected\u0026#39;, \u0026#39;serverless\u0026#39;] # STEP 2 - Creating a workload # https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_using_awscli_to_manage_wa_reviews/2_create_workload/ logger.info(\u0026#34;2 - Creating a new workload\u0026#34;) CreateNewWorkload(WACLIENT,WORKLOADNAME,DESCRIPTION,REVIEWOWNER,ENVIRONMENT,AWSREGIONS,LENSES) logger.info(\u0026#34;2 - Finding your WorkloadId\u0026#34;) workloadId = FindWorkload(WACLIENT,WORKLOADNAME) logger.info(\u0026#34;New workload created with id %s\u0026#34; % workloadId) logger.info(\u0026#34;2 - Using WorkloadId to remove and add lenses\u0026#34;) listOfLenses = listLens(WACLIENT) logger.info(\u0026#34;Lenses currently available: %s\u0026#34; % listOfLenses) workloadJson = GetWorkload(WACLIENT,workloadId) logger.info(\u0026#34;Workload ID \u0026#39;%s\u0026#39; has lenses \u0026#39;%s\u0026#39;\u0026#34; % (workloadId, workloadJson[\u0026#39;Lenses\u0026#39;])) logger.info(\u0026#34;Removing the serverless lens\u0026#34;) disassociateLens(WACLIENT,workloadId,[\u0026#39;serverless\u0026#39;]) workloadJson = GetWorkload(WACLIENT,workloadId) logger.info(\u0026#34;Now workload ID \u0026#39;%s\u0026#39; has lenses \u0026#39;%s\u0026#39;\u0026#34; % (workloadId, workloadJson[\u0026#39;Lenses\u0026#39;])) logger.info(\u0026#34;Adding serverless lens back into the workload\u0026#34;) associateLens(WACLIENT,workloadId,[\u0026#39;serverless\u0026#39;]) workloadJson = GetWorkload(WACLIENT,workloadId) logger.info(\u0026#34;Now workload ID \u0026#39;%s\u0026#39; has lenses \u0026#39;%s\u0026#39;\u0026#34; % (workloadId, workloadJson[\u0026#39;Lenses\u0026#39;])) # STEP 3 - Performing a review # https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_using_awscli_to_manage_wa_reviews/3_perform_review/ logger.info(\u0026#34;3 - Performing a review\u0026#34;) logger.info(\u0026#34;3 - STEP1 - Find the QuestionId and ChoiceID for a particular pillar question and best practice\u0026#34;) questionSearch = \u0026#34;How do you reduce defects, ease remediation, and improve flow into production\u0026#34; questionId = findQuestionId(WACLIENT,workloadId,\u0026#39;wellarchitected\u0026#39;,\u0026#39;operationalExcellence\u0026#39;,questionSearch) logger.info(\u0026#34;Found QuestionID of \u0026#39;%s\u0026#39; for the question text of \u0026#39;%s\u0026#39;\u0026#34; % (questionId, questionSearch)) choiceSet = [] choiceSet.append(findChoiceId(WACLIENT,workloadId,\u0026#39;wellarchitected\u0026#39;,questionId,\u0026#34;Use version control\u0026#34;)) logger.info(\u0026#34;Found choiceId of \u0026#39;%s\u0026#39; for the choice text of \u0026#39;Use version control\u0026#39;\u0026#34; % choiceSet) choiceSet.append(findChoiceId(WACLIENT,workloadId,\u0026#39;wellarchitected\u0026#39;,questionId,\u0026#34;Use configuration management systems\u0026#34;)) choiceSet.append(findChoiceId(WACLIENT,workloadId,\u0026#39;wellarchitected\u0026#39;,questionId,\u0026#34;Use build and deployment management systems\u0026#34;)) choiceSet.append(findChoiceId(WACLIENT,workloadId,\u0026#39;wellarchitected\u0026#39;,questionId,\u0026#34;Perform patch management\u0026#34;)) choiceSet.append(findChoiceId(WACLIENT,workloadId,\u0026#39;wellarchitected\u0026#39;,questionId,\u0026#34;Use multiple environments\u0026#34;)) logger.info(\u0026#34;All choices we will select for questionId of %sis %s\u0026#34; % (questionId, choiceSet)) logger.info(\u0026#34;3 - STEP2 - Use the QuestionID and ChoiceID to update the answer in well-architected review\u0026#34;) answers = getAnswersForQuestion(WACLIENT,workloadId,\u0026#39;wellarchitected\u0026#39;,questionId) logger.info(\u0026#34;Current answer for questionId \u0026#39;%s\u0026#39; is \u0026#39;%s\u0026#39;\u0026#34; % (questionId, answers)) logger.info(\u0026#34;Adding answers found in choices above (%s)\u0026#34; % choiceSet) updateAnswersForQuestion(WACLIENT,workloadId,\u0026#39;wellarchitected\u0026#39;,questionId,choiceSet,\u0026#39;Added by Python\u0026#39;) answers = getAnswersForQuestion(WACLIENT,workloadId,\u0026#39;wellarchitected\u0026#39;,questionId) logger.info(\u0026#34;Now the answer for questionId \u0026#39;%s\u0026#39; is \u0026#39;%s\u0026#39;\u0026#34; % (questionId, answers)) # STEP 4 - Saving a milestone # https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_using_awscli_to_manage_wa_reviews/4_save_milestone/ logger.info(\u0026#34;4 - Saving a Milestone\u0026#34;) logger.info(\u0026#34;4 - STEP1 - Create a Milestone\u0026#34;) milestones = listMilestones(WACLIENT,workloadId) milestoneCount = jmespath.search(\u0026#34;length([*].MilestoneNumber)\u0026#34;,milestones) logger.info(\u0026#34;Workload %shas %smilestones\u0026#34; % (workloadId, milestoneCount)) milestoneNumber = createMilestone(WACLIENT,workloadId,\u0026#39;Rev1\u0026#39;) logger.info(\u0026#34;Created Milestone #%scalled Rev1\u0026#34; % milestoneNumber) logger.info(\u0026#34;4 - STEP2 - List all Milestones\u0026#34;) milestones = listMilestones(WACLIENT,workloadId) milestoneCount = jmespath.search(\u0026#34;length([*].MilestoneNumber)\u0026#34;,milestones) logger.info(\u0026#34;Now workload %shas %smilestones\u0026#34; % (workloadId, milestoneCount)) logger.info(\u0026#34;4 - STEP3 - Retrieve the results from a milestone\u0026#34;) riskCounts = getMilestoneRiskCounts(WACLIENT,workloadId,milestoneNumber) logger.info(\u0026#34;Risk counts for all lenses for milestone %sare: %s\u0026#34; % (milestoneNumber,riskCounts)) logger.info(\u0026#34;4 - STEP4 - List all question and answers based from a milestone\u0026#34;) answers = listAllAnswers(WACLIENT,workloadId,\u0026#39;wellarchitected\u0026#39;,milestoneNumber) # STEP 5 - Viewing and downloading the report # https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_using_awscli_to_manage_wa_reviews/5_view_report/ logger.info(\u0026#34;5 - Viewing and downloading the report\u0026#34;) logger.info(\u0026#34;5 - STEP1 - Gather pillar and risk data for a workload\u0026#34;) lensReview = getLensReview(WACLIENT,workloadId,\u0026#39;wellarchitected\u0026#39;) logger.info(\u0026#34;The Well-Architected base framework has the following RiskCounts %s\u0026#34; % lensReview[\u0026#39;RiskCounts\u0026#39;]) logger.info(\u0026#34;5 - STEP2 - Generate and download workload PDF\u0026#34;) lensReviewBase64PDF = getLensReviewPDFReport(WACLIENT,workloadId,\u0026#39;wellarchitected\u0026#39;) # lensReviewPDF = base64.b64decode(lensReviewBase64PDF) # We will write the PDF to a file in the same directory with open(\u0026#34;WAReviewOutput.pdf\u0026#34;, \u0026#34;wb\u0026#34;) as fh: fh.write(base64.b64decode(lensReviewBase64PDF)) # STEP 6 - Teardown # https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_using_awscli_to_manage_wa_reviews/6_cleanup/ logger.info(\u0026#34;6 - Teardown\u0026#34;) # Allow user to keep the workload input(\u0026#34;\\n*** Press Enter to delete the workload or use ctrl-c to abort the script and keep the workload\u0026#34;) logger.info(\u0026#34;6 - STEP1 - Delete Workload\u0026#34;) DeleteWorkload(WACLIENT, workloadId) if __name__ == \u0026#34;__main__\u0026#34;: main()  PowerShell version using AWS Tools for PowerShell The lab uses AWS CLI to perform all of the tasks, but you can also use the AWS Tools for PowerShell to perform the same steps. As a reference, you can download the code LabExample.ps1 which will perform all of steps from the lab in a single PowerShell script. This file assumes you have already installed the AWS.Tools PowerShell module as well as the AWS.Tools.WellArchitected module using Install-AWSToolsModule command. It also assumes you have setup your AWS credential file and it will uses the default profile for all interactions.\nThere is no error checking for most of the various API calls, so the code should not be considered production ready. Please review before implementing in your environment.\nPowerShell code #!/usr/bin/env pwsh # This is a simple Powershell app for use with the Well-Architected labs # This will simulate all of the steps in the 200-level lab on using the # Well-Architected API calls # # This code is only for use in Well-Architected labs # *** NOT FOR PRODUCTION USE *** # # # Licensed under the Apache 2.0 and MITnoAttr License. # # Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;). You may not use this file except in compliance with the License. A copy of the License is located at # https://aws.amazon.com/apache2.0/ # Requires -Modules @{ModuleName=\u0026#39;AWSPowerShell.NetCore\u0026#39;;ModuleVersion=\u0026#39;3.3.618.0\u0026#39;} $__author__ = \u0026#34;Eric Pullen\u0026#34; $__email__ = \u0026#34;eppullen@amazon.com\u0026#34; $__copyright__ = \u0026#34;Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\u0026#34; $__credits__ = @({\u0026#34;Eric Pullen\u0026#34;}) # Default region listed here $REGION_NAME = \u0026#34;us-east-1\u0026#34; # Setup Log Message Routine to print current date/time to console function Log-Message { [CmdletBinding()] Param ( [Parameter(Mandatory=$true, Position=0)] [string]$LogMessage ) Write-Output (\u0026#34;{0} - {1}\u0026#34; -f (Get-Date), $LogMessage) } function Find-Workload { [CmdletBinding()] Param ( [Parameter(Mandatory=$true)] [string]$workloadName ) $response = Get-WATWorkloadList -WorkloadNamePrefix $workloadName if (!$response.WorkloadId) { Write-Warning (\u0026#34;Did not find a workload called \u0026#34;+$workloadName) -InformationAction Continue } return $response.WorkloadId } function Create-New-Workload { [CmdletBinding()] Param ( [Parameter(Mandatory=$true)] [string]$workloadName, [Parameter(Mandatory=$true)] [string]$description, [Parameter(Mandatory=$true)] [string]$reviewOwner, [Parameter(Mandatory=$true)] [string]$environment, [Parameter(Mandatory=$true)] [array]$awsRegions, [Parameter(Mandatory=$true)] [array]$lenses ) try { $response = New-WATWorkload -WorkloadName $workloadName -Description $description -ReviewOwner $reviewOwner -Environment $environment -AwsRegion $awsRegions -Lense $lenses $returnValue = $response.WorkloadId } catch [Amazon.WellArchitected.Model.ConflictException] { Write-Warning (\u0026#34;Conflict - Found a workload that already exists with the name \u0026#34;+$workloadName+\u0026#34;. Finding the workloadId to return\u0026#34;) -InformationAction Continue $returnValue = Find-Workload $workloadName } return $returnValue } function Create-New-Milestone { [CmdletBinding()] Param ( [Parameter(Mandatory=$true)] [string]$workloadId, [Parameter(Mandatory=$true)] [string]$milestoneName ) try { $response = New-WATMilestone -WorkloadId $workloadId -MilestoneName $milestoneName $returnValue = $response } catch [Amazon.WellArchitected.Model.ConflictException] { Write-Warning (\u0026#34;Conflict - Found a milestone that already exists with the name \u0026#34;+$milestoneName+\u0026#34;. Finding the milestone to return\u0026#34;) -InformationAction Continue $milestoneReturn = Get-WATMilestoneList -WorkloadId $workloadId $foundMilestone = $milestoneReturn.MilestoneSummaries | where {$_.MilestoneName -like $milestoneName+\u0026#34;*\u0026#34; } $returnValue = $foundMilestone } return $returnValue } $WORKLOADNAME = \u0026#39;WA Lab Test Workload\u0026#39; $DESCRIPTION = \u0026#39;Test Workload for WA Lab\u0026#39; $REVIEWOWNER = \u0026#39;WA Python Script\u0026#39; $ENVIRONMENT= \u0026#39;PRODUCTION\u0026#39; $AWSREGIONS = @($REGION_NAME) $LENSES = @(\u0026#39;wellarchitected\u0026#39;, \u0026#39;serverless\u0026#39;) Log-Message (\u0026#34;1 - Starting LabExample.ps1\u0026#34;) # STEP 2 - Creating a workload # https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_using_awscli_to_manage_wa_reviews/2_create_workload/ Log-Message (\u0026#34;2 - Creating new workload\u0026#34;) $createId = Create-New-Workload $WORKLOADNAME $DESCRIPTION $REVIEWOWNER $ENVIRONMENT $AWSREGIONS $LENSES Log-Message (\u0026#34;Created with new workloadId: \u0026#34;+$createId) Log-Message (\u0026#34;2 - Finding your WorkloadId for name \u0026#34;+$WORKLOADNAME) $workloadId = Find-Workload $WORKLOADNAME Log-Message (\u0026#34;New workload created with id \u0026#34; + $workloadId) Log-Message (\u0026#34;2 - Using WorkloadId to remove and add lenses\u0026#34;) $listOfLenses = Get-WATLenseList Log-Message (\u0026#34;Lenses currently available: \u0026#34;+$listOfLenses.LensAlias) $workload = Get-WATWorkload -WorkloadId $workloadId Log-Message (\u0026#34;WorkloadId \u0026#34;+$workloadId+\u0026#34; has lenses \u0026#39;\u0026#34;+$workload.Lenses+\u0026#34;\u0026#39;\u0026#34;) Log-Message (\u0026#34;Removing the serverless lens\u0026#34;) Remove-WATLense -WorkloadId $workloadId -LensAlias \u0026#34;serverless\u0026#34; -Force $workload = Get-WATWorkload -WorkloadId $workloadId Log-Message (\u0026#34;WorkloadId \u0026#34;+$workloadId+\u0026#34; has lenses \u0026#39;\u0026#34;+$workload.Lenses+\u0026#34;\u0026#39;\u0026#34;) Log-Message (\u0026#34;Adding serverless lens back into the workload\u0026#34;) Add-WATLense -WorkloadId $workloadId -LensAlias \u0026#34;serverless\u0026#34; -Force $workload = Get-WATWorkload -WorkloadId $workloadId Log-Message (\u0026#34;WorkloadId \u0026#34;+$workloadId+\u0026#34; has lenses \u0026#39;\u0026#34;+$workload.Lenses+\u0026#34;\u0026#39;\u0026#34;) # STEP 3 - Performing a review # https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_using_awscli_to_manage_wa_reviews/3_perform_review/ Log-Message (\u0026#34;3 - Performing a review\u0026#34;) Log-Message (\u0026#34;3 - STEP1 - Find the QuestionId and ChoiceID for a particular pillar question and best practice\u0026#34;) $questionSearch = \u0026#34;How do you reduce defects, ease remediation, and improve flow into production\u0026#34; $questionReturn = Get-WATAnswerList -WorkloadId $workloadId -LensAlias \u0026#34;wellarchitected\u0026#34; -PillarId \u0026#34;operationalExcellence\u0026#34; $foundQuestion = $questionReturn.AnswerSummaries | where { $_.QuestionTitle -like $questionSearch+\u0026#34;*\u0026#34; } $questionId = $foundQuestion.QuestionId Log-Message (\u0026#34;Found QuestionID of \u0026#34;+$questionId+\u0026#34; for the question text of \u0026#39;\u0026#34;+$questionSearch+\u0026#34;\u0026#39;\u0026#34;) $choiceSet = @() $answerReturn = Get-WATAnswer -WorkloadId $workloadId -LensAlias \u0026#34;wellarchitected\u0026#34; -QuestionId $questionId $answerSearch = \u0026#34;Use version control\u0026#34; $foundAnswer = $answerReturn.Answer.Choices | where {$_.Title -like $answerSearch+\u0026#34;*\u0026#34; } $choiceSet += $foundAnswer.ChoiceId Log-Message (\u0026#34;Found choiceId of \u0026#39;\u0026#34;+$choiceSet+\u0026#34;\u0026#39; for the choice text of \u0026#39;Use version control\u0026#39;\u0026#34;) $answerSearch = \u0026#34;Use configuration management systems\u0026#34; $foundAnswer = $answerReturn.Answer.Choices | where {$_.Title -like $answerSearch+\u0026#34;*\u0026#34; } $choiceSet += $foundAnswer.ChoiceId $answerSearch = \u0026#34;Use build and deployment management systems\u0026#34; $foundAnswer = $answerReturn.Answer.Choices | where {$_.Title -like $answerSearch+\u0026#34;*\u0026#34; } $choiceSet += $foundAnswer.ChoiceId $answerSearch = \u0026#34;Perform patch management\u0026#34; $foundAnswer = $answerReturn.Answer.Choices | where {$_.Title -like $answerSearch+\u0026#34;*\u0026#34; } $choiceSet += $foundAnswer.ChoiceId $answerSearch = \u0026#34;Use multiple environments\u0026#34; $foundAnswer = $answerReturn.Answer.Choices | where {$_.Title -like $answerSearch+\u0026#34;*\u0026#34; } $choiceSet += $foundAnswer.ChoiceId Log-Message (\u0026#34;All choices we will select for questionId of \u0026#34;+$questionId+\u0026#34; is \u0026#34; + $choiceSet) Log-Message (\u0026#34;3 - STEP2 - Use the QuestionID and ChoiceID to update the answer in well-architected review\u0026#34;) Log-Message (\u0026#34;Current answer for questionId \u0026#39;\u0026#34;+$questionId+\u0026#34;\u0026#39; is \u0026#39;\u0026#34;+$answerReturn.Answer.SelectedChoices+\u0026#34;\u0026#39;\u0026#34;) Log-Message (\u0026#34;Adding answers found in choices above \u0026#39;\u0026#34;+$choiceSet+\u0026#34;\u0026#39;\u0026#34;) $updateResponse = Update-WATAnswer -WorkloadId $workloadId -LensAlias \u0026#34;wellarchitected\u0026#34; -QuestionId $questionId -SelectedChoices $choiceSet -Notes \u0026#34;Question modified by PowerShell script\u0026#34; $answerReturn = Get-WATAnswer -WorkloadId $workloadId -LensAlias \u0026#34;wellarchitected\u0026#34; -QuestionId $questionId Log-Message (\u0026#34;Now the answer for questionId \u0026#39;\u0026#34;+$questionId+\u0026#34;\u0026#39; is \u0026#39;\u0026#34;+$answerReturn.Answer.SelectedChoices+\u0026#34;\u0026#39;\u0026#34;) # STEP 4 - Saving a milestone # https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_using_awscli_to_manage_wa_reviews/4_save_milestone/ Log-Message (\u0026#34;4 - Saving a Milestone\u0026#34;) Log-Message (\u0026#34;4 - STEP1 - Create a Milestone\u0026#34;) $milestoneReturn = Get-WATMilestoneList -WorkloadId $workloadId Log-Message (\u0026#34;Workload \u0026#34;+$workloadId+\u0026#34; has \u0026#34;+($milestoneReturn.MilestoneSummaries | measure ).Count+\u0026#34; milestones\u0026#34;) $createMilestoneResponse = Create-New-Milestone $workloadId \u0026#34;Rev1\u0026#34; $createdmilestoneNumber = $createMilestoneResponse.MilestoneNumber Log-Message (\u0026#34;Created Milestone #\u0026#34;+$createMilestoneResponse.MilestoneNumber+\u0026#34; called Rev1\u0026#34;) Log-Message (\u0026#34;4 - STEP2 - List all Milestones\u0026#34;) $milestoneReturn = Get-WATMilestoneList -WorkloadId $workloadId Log-Message (\u0026#34;Now workload \u0026#34;+$workloadId+\u0026#34; has \u0026#34;+($milestoneReturn.MilestoneSummaries | measure ).Count+\u0026#34; milestones\u0026#34;) Log-Message (\u0026#34;4 - STEP3 - Retrieve the results from a milestone\u0026#34;) $milestoneDetailReturn = Get-WATMilestone -WorkloadId $workloadId -MilestoneNumber $createdmilestoneNumber $riskCounts = $milestoneDetailReturn.Milestone.Workload.RiskCounts Log-Message (\u0026#34;Risk counts for all lenses for milestone \u0026#34;+$createdmilestoneNumber+\u0026#34; are: \u0026#34;+($riskCounts | Out-String)) Log-Message (\u0026#34;4 - STEP4 - List all question and answers based from a milestone\u0026#34;) $questionReturn = Get-WATAnswerList -WorkloadId $workloadId -LensAlias \u0026#34;wellarchitected\u0026#34; -MilestoneNumber $createdmilestoneNumber # STEP 5 - Viewing and downloading the report # https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_using_awscli_to_manage_wa_reviews/5_view_report/ Log-Message (\u0026#34;5 - Viewing and downloading the report\u0026#34;) Log-Message (\u0026#34;5 - STEP1 - Gather pillar and risk data for a workload\u0026#34;) $LensReviewReturn = Get-WATLensReview -WorkloadId $workloadId -LensAlias \u0026#34;wellarchitected\u0026#34; $LensReviewRiskCounts = $LensReviewReturn.LensReview.RiskCounts Log-Message (\u0026#34;The Well-Architected base framework has the following RiskCounts \u0026#34;+($LensReviewRiskCounts | Out-String)) Log-Message (\u0026#34;5 - STEP2 - Generate and download workload PDF\u0026#34;) $LensPDFReviewReturn = Get-WATLensReviewReport -WorkloadId $workloadId -LensAlias \u0026#34;wellarchitected\u0026#34; $filename = \u0026#39;WAReviewOutput.pdf\u0026#39; $bytes = [Convert]::FromBase64String($LensPDFReviewReturn.LensReviewReport.Base64String) [IO.File]::WriteAllBytes($filename, $bytes) # STEP 6 - Teardown # https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_using_awscli_to_manage_wa_reviews/6_cleanup/ Log-Message (\u0026#34;6 - Teardown\u0026#34;) # Allow user to keep the workload Read-Host -Prompt \u0026#34;Press any key to continue or CTRL+C to quit\u0026#34; Log-Message (\u0026#34;6 - STEP1 - Delete Workload\u0026#34;) Remove-WATWorkload -WorkloadId $workloadId -ClientRequestToken \u0026#34;ClientRequestToken1\u0026#34; -Force # ClientRequestToken is required at this time, but we are investigating if we can remove this in the future.   function prevStep(){ window.open(\"..\\/5_view_report\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/7_cleanup\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_fault_isolation_with_shuffle_sharding/6_impact_of_failures_shuffle_sharding/","title":"Impact of failures with shuffle sharding","tags":[],"description":"","content":"Break the application We will now introduce the poison pill into the workload by including the bug query-string with our requests and see how the updated the workload architectures handles it. As in our previous case, imagine that customer Alpha triggered the bug in the application again.\n  Include the query-string bug with a value of true and make a request as customer Alpha. The modified URL should look like this - http://shuffle-alb-1p2xbmzo541rr-1602891463.us-east-1.elb.amazonaws.com/?name=Alpha\u0026amp;bug=true\n  This should result in an Internal Server Error response on the browser indicating that the application has stopped working as expected on the instance that processed this request\n  Just like before, customer Alpha, not aware of this bug in the application, will retry the request. Refresh the page to simulate this as you did before. This request is routed to the other healthy instance in the shard for customer Alpha. The bug is triggered again and the other instance goes down as well. The entire shard is now affected.\n  All requests to this shard will now fail because there are no healthy instances in the shard. No matter how many times the page is refreshed, you will see a 502 Bad Gateway for customer Alpha showing that customer Alpha is experiencing complete downtime. At this point, the overall capacity of the fleet has decresed from 4 EC2 instances to 2 EC2 instances.\n  Due to shuffle sharding, all of the remaining customers are unaffected or have limited impact. Send requests as the following customers and refresh each request multiple times. You should notice that all customers will now receive a response, although some customers will only get responses from a single EC2 instance while others get it from 2 different EC2 instances.\n http://shuffle-alb-8vonmf2ywl5z-682850122.us-east-1.elb.amazonaws.com/?name=Bravo http://shuffle-alb-8vonmf2ywl5z-682850122.us-east-1.elb.amazonaws.com/?name=Charlie http://shuffle-alb-8vonmf2ywl5z-682850122.us-east-1.elb.amazonaws.com/?name=Delta http://shuffle-alb-8vonmf2ywl5z-682850122.us-east-1.elb.amazonaws.com/?name=Echo http://shuffle-alb-8vonmf2ywl5z-682850122.us-east-1.elb.amazonaws.com/?name=Foxtrot    The impact is localized to a specific shard and only customer Alpha is affected. Customers that have a shared EC2 instance with customer Alpha will only have 1 EC2 instance available to respond to requests. While this might lead to some degree of degradation for those customers, it is still an improvement over complete downtime. The scope of impact has now been reduced so that only 16.66% of customers are affected by the failure induced by the poison pill. With larger fleet and shard sizes, the number of combinations will increase resulting in customers having different degrees of degradation i.e. some customers will only have a fraction of their overall shard capacity affected instead of complete downtime.\n   Customer Name Nodes     Alpha Node-1 and Node-2   Bravo Node-1 and Node-3   Charlie Node-1 and Node-4   Delta Node-2 and Node-3   Echo Node-2 and Node-4   Foxtrot Node-3 and Node-4       With this shuffle sharded architecture, the scope of impact is further reduced by the combination of Workers used to generate shards. Here with six shards, if a customer experiences a problem, then the shard hosting them as well as the Workers mapped to that shard might be impacted. However, that shard represents only a fraction of the overall service. Since this is just a lab we kept it simple with only six shards, but with more shards, the scope of impact decreases further. Adding more shards requires adding more capacity (more workers). With higher number of Workers, it is possible to achieve a higher number of unique combinations resulting in exponential improvement of the scope of impact of failures.\n Fix the application Note: This is optional and does not need to be completed if you are planning on tearing down this lab as described in the next section. If you are planning on testing this lab further, please follow the instructions below to fix the application on the EC2 instances.\n  Click here for instructions to fix the application:   As in the previous sections, Systems Manager will be used to fix the application and return functionality to the users that are affected - Alpha, Bravo, and Charlie.\n  Go to the Outputs section of the CloudFormation stack and open the link for SSMDocument. This will take you to the Systems Manager console.\n  Click on Run command which will open a new tab on the browser\n  Scroll down to the Targets section and select Choose instances manually\n  In the list of instances, check the box next to the nodes that were affected. You can identify the nodes that were impacted by looking at the table above and determining the nodes mapped to the customer that introduced the poison pill. If you followed instructions in this guide and introduced the poison pill as customer Alpha, check the box next to the EC2 instances with the names Worker-1 and Worker-2.\n  Scroll down to the Output options section and uncheck the box next to Enable an S3 bucket. This will prevent Systems Manager from writing log files based on the command execution to S3.\n  Click on Run\n  You should see the command execution succeed in a few seconds\n  Once the command has finished execution, you can go back to the application and test it to verify it is working as expected. Make sure that the query-string bug is not included in the request. For example, http://shuffle-alb-1p2xbmzo541rr-1602891463.us-east-1.elb.amazonaws.com/?name=Alpha should return a valid response. Refresh the page a few times to make sure responses are being received from 2 different EC2 instances. Repeat this process for the other customers and verify that each customer is getting responses from 2 different EC2 instances.\n     function prevStep(){ window.open(\"..\\/5_implement_shuffle_sharding\", \"_self\") } function nextStep(){ window.open(\"..\\/7_cleanup\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_monitoring_linux_ec2_cloudwatch/6_cleanup/","title":"Teardown","tags":["Linux","Amazon Linux","EC2","CloudWatch","CloudWatch Dashboard"],"description":"","content":"Summary In order to monitor your compute resources to ensure they are performing properly, you must record performance related metrics. In this simple lab, you were able to create an EC2 instance and generate a simulated high-load situation to observe how CloudWatch can be used to monitor those resources. These are important concepts that you should use across all of your compute resources to ensure you are meeting PERF7 - \u0026ldquo;How do you monitor your resources to ensure they are performing?\u0026quot; Now that you know how to have an EC2 instance report its metrics to CloudWatch, you should define the Key Performance Indicators (KPIs) to measure workload performance. In some cases, such as a image rendering farm, a high-cpu may be a valid metric and not be indicative of performance. Once defined, you should establish CloudWatch alarms to ensure you are meeting your performance metrics.\nAdditional Tasks In addition to the lab tasks, feel free to use this lab deployment to test some additional CloudWatch features:\n Create a CPU or Memory alarm and test triggering the alarm by re-running step 5  Create a CloudWatch alarm for an instance    Zoom into various timeframes on the CloudWatch dashboard. Notice how it will link the two charts together when you zoom in. Use Metrics Explorer to look at other metrics deployed into the EC2 instance Explore the CloudFormation template to see how the various CloudWatch Agent configurations were deployed  Remove all the resources via CloudFormation  Go to the AWS CloudFormation console: https://console.aws.amazon.com/cloudformation Select the CloudFormation stack to delete and click Delete In the confirmation dialog, click Delete stack The Status changes to DELETE_IN_PROGRESS Click the refresh button to update and status will ultimately progress to DELETE_COMPLETE When complete, the stack will no longer be displayed. To see deleted stacks use the drop down next to the Filter text box. To see progress during stack deletion  Click the stack name Select the Events column Refresh to see new events    Delete the CloudWatch Dashboard  Go to CloudWatch Dashboards  Click the radio button next to the dashboard you created and then click Delete  References \u0026amp; useful resources  Using CloudWatch Dashboards  Monitoring your instances using CloudWatch  Connect to instance using Session Manager    function prevStep(){ window.open(\"..\\/5_generating_load\\/\", \"_self\") } function nextStep(){ window.open(\"\\/cost\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with PERF7 - \u0026ldquo;How do you monitor your resources to ensure they are performing?\u0026quot;\nClick here to access the Well-Architected Tool   Previous Step Complete this lab     "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_monitoring_windows_ec2_cloudwatch/6_cleanup/","title":"Teardown","tags":["Windows Server","Windows","EC2","CloudWatch","CloudWatch Dashboard"],"description":"","content":"Summary In order to monitor your compute resources to ensure they are performing properly, you must record performance related metrics. In this simple lab, you were able to create an EC2 instance and generate a simulated high-load situation to observe how CloudWatch can be used to monitor those resources. These are important concepts that you should use across all of your compute resources to ensure you are meeting PERF7 - \u0026ldquo;How do you monitor your resources to ensure they are performing?\u0026quot; Now that you know how to have an EC2 instance report its metrics to CloudWatch, you should define the Key Performance Indicators (KPIs) to measure workload performance. In some cases, such as a image rendering farm, a high-cpu may be a valid metric and not be indicative of performance. Once defined, you should establish CloudWatch alarms to ensure you are meeting your performance metrics.\nAdditional Tasks In addition to the lab tasks, feel free to use this lab deployment to test some additional CloudWatch features:\n Create a CPU or Memory alarm and test triggering the alarm by running the cpu_stress.ps1 script  https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/using-cloudwatch-createalarm.html   Zoom into various timeframes on the CloudWatch dashboard. Notice how it will link the two charts together when you zoom in. Use Metrics Explorer to look at other metrics deployed into the EC2 instance Explore the CloudFormation template to see how the various CW Agent configurations were deployed  Remove all the resources via CloudFormation  Go to the AWS CloudFormation console: https://console.aws.amazon.com/cloudformation Select the CloudFormation stack to delete and click Delete In the confirmation dialog, click Delete stack The Status changes to DELETE_IN_PROGRESS Click the refresh button to update and status will ultimately progress to DELETE_COMPLETE When complete, the stack will no longer be displayed. To see deleted stacks use the drop down next to the Filter text box. To see progress during stack deletion  Click the stack name Select the Events column Refresh to see new events    Delete the CloudWatch Dashboard  Go to CloudWatch Dashboards  Click the radio button next to the dashboard you created and then click Delete  References \u0026amp; useful resources  Using CloudWatch Dashboards  Monitoring your Windows instances using CloudWatch  Connect to a Windows instance using Session Manager    function prevStep(){ window.open(\"..\\/5_generating_load\\/\", \"_self\") } function nextStep(){ window.open(\"\\/cost\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with PERF7 - \u0026ldquo;How do you monitor your resources to ensure they are performing?\u0026quot;\nClick here to access the Well-Architected Tool   Previous Step Complete this lab     "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/100_labs/100_dependency_monitoring/6_cleanup/","title":"Tear down this lab","tags":[],"description":"","content":"The following instructions will remove the resources that you have created in this lab.\nCleaning up Amazon CloudWatch Resources  Go to the Amazon CloudWatch console at https://console.aws.amazon.com/cloudwatch and click on Alarms Search for the alarm WA-Lab-Dependency-Alarm and click on it Click on Delete on the top right hand corner Click Delete  Cleaning up AWS Systems Manager OpsCenter Resources  Go to the AWS Systems Manager console at https://console.aws.amazon.com/systems-manager and click on OpsCenter Click on the OpsItems tab, search by Title, select contains, and enter the value as S3 Data Writes Click on the OpsItem that has been created with the title S3 Data Writes failing Click on Set status on the top right hand corner, and select Resolved  Cleaning up the CloudFormation Stack  Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and click on the Dependency-Monitoring-Lab Click on Delete and then Delete stack  Thank you for using this lab.  function prevStep(){ window.open(\"..\\/5_bonus_content\\/\", \"_self\") } function nextStep(){ window.open(\"\\/cost\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment or workload, you should complete a milestone in the Well-Architected tool. This lab specifically helps you with OPS4 - \u0026ldquo;How do you design your workload so that you can understand its state?\u0026quot;\nClick here to access the Well-Architected Tool   Previous Step Complete this lab     "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_cur_queries/queries/database/","title":"Database","tags":[],"description":"","content":"These are queries for AWS Services under the Database product family .\nUse the clipboard in the top right of the text boxes below to copy all of the text to be pasted.\n You may need to change variables used as placeholders in your query. ${table_Name} is a common variable which needs to be replaced. Example: cur_db.cur_table\n Table of Contents   Amazon RDS   Query Description This query will output the daily sum per resource for all RDS purchase options across all RDS usage types.\nPricing Please refer to the Amazon RDS pricing page .\nSample Output Download SQL File Link to Code Copy Query  SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((\u0026quot;line_item_usage_start_date\u0026quot;), '%Y-%m-%d') AS day_line_item_usage_start_date, product_instance_type, line_item_operation, line_item_usage_type, line_item_line_item_type, pricing_term, product_product_family , SPLIT_PART(line_item_resource_id,':',7) AS line_item_resource_id, CASE product_database_engine WHEN '' THEN 'Not Applicable' ELSE product_database_engine END AS OS , sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END) \u0026quot;usage_quantity\u0026quot;, sum (\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;, sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0 WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0 WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;reservation_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (\u0026quot;reservation_unused_amortized_upfront_fee_for_billing_period\u0026quot; + \u0026quot;reservation_unused_recurring_fee\u0026quot;) WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN 0 ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) \u0026quot;amortized_cost\u0026quot;, sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (-\u0026quot;savings_plan_amortized_upfront_commitment_for_billing_period\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (-\u0026quot;reservation_amortized_upfront_fee_for_billing_period\u0026quot;) ELSE 0 END) \u0026quot;ri_sp_trueup\u0026quot;, sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN \u0026quot;line_item_unblended_cost\u0026quot; WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN \u0026quot;line_item_unblended_cost\u0026quot;ELSE 0 END) \u0026quot;ri_sp_upfront_fees\u0026quot; FROM ${table_name} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND product_product_name = 'Amazon Relational Database Service' AND line_item_line_item_type NOT IN ('Tax','Credit','Refund','EdpDiscount','Fee','RIFee') GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((\u0026quot;line_item_usage_start_date\u0026quot;),'%Y-%m-%d'), product_instance_type, line_item_operation, line_item_usage_type, line_item_line_item_type, pricing_term, product_product_family, product_database_engine, line_item_line_item_type, line_item_resource_id ORDER BY day_line_item_usage_start_date, usage_quantity, unblended_cost;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         Amazon RDS - Monthly Cost grouped by Usage Type and Resource Tag   Query Description This query will output the total monthly blended costs for RDS grouped by usage type and a specified tag (e.g. Environment:Test,Dev,Prod). The query can be modified to adjust the Cost dataset from Blended to Unblended by adjusting the specified cost column (line_item_blended_cost -\u0026gt; line_item_unblended_cost). This query would be helpful to visualize a quick monthly breakdown of cost components for RDS usage with a specific tag (Environment:Test,Dev,Prod).\nPricing Please refer to the Amazon RDS pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT line_item_usage_type, month, resource_tags_user_environment, SUM(CAST(line_item_blended_cost AS decimal(16,8))) AS sum_line_item_blended_cost FROM ${table_name} WHERE year = '2020' AND (month BETWEEN '1' AND '12' OR month BETWEEN '01' AND '12') AND line_item_product_code='AmazonRDS' AND resource_tags_user_environment = 'dev' GROUP BY 1,2,3 HAVING sum(line_item_blended_cost) \u0026gt; 0 ORDER BY line_item_usage_type, month, resource_tags_user_environment;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         Amazon DynamoDB   Query Description This query will output the total monthly sum per resource for all DynamoDB purchase options (including reserved capacity) across all DynamoDB usage types (including data transfer and storage costs). The unblended cost will be summed and in descending order.\nPricing Please refer to the DynamoDB pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, month, product_location, SPLIT_PART(line_item_resource_id, 'table/', 2) as line_item_resource_id, (CASE WHEN line_item_line_item_type LIKE '%Fee' THEN 'DynamoDB Reserved Capacity' WHEN line_item_line_item_type = 'DiscountedUsage' THEN 'DynamoDB Reserved Capacity' ELSE 'DynamoDB Usage' END) as purchase_option_line_item_line_item_type, (CASE WHEN product_product_family = 'Data Transfer' THEN 'DynamoDB Data Transfer' WHEN product_product_family LIKE '%Storage' THEN 'DynamoDB Storage' ELSE 'DynamoDB Usage' END) as usage_type_product_product_family, SUM(CAST(line_item_usage_amount AS double)) as sum_line_item_usage_amount, SUM(CAST(line_item_blended_cost AS decimal(16,8))) AS sum_line_item_unblended_cost, reservation_reservation_a_r_n FROM ${table_name} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND line_item_product_code = 'AmazonDynamoDB' AND line_item_line_item_type NOT IN ('Tax','Credit','Refund','EdpDiscount') GROUP BY bill_payer_account_id, line_item_usage_account_id, month, product_location, line_item_resource_id, line_item_line_item_type, product_product_family, reservation_reservation_a_r_n ORDER BY sum_line_item_unblended_cost DESC    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         Amazon Redshift   Query Description This query will provide daily unblended and amortized cost as well as usage information per linked account for Amazon Redshift. The output will include detailed information about the resource id (cluster name), usage type, and API operation. The usage amount and cost will be summed and the cost will be in descending order. This query includes RI and SP true up which will show any upfront fees to the account that purchased the pricing model.\nPricing Please refer to the Redshift pricing page . Please refer to the Redshift Cost Optimization Whitepaper for Cost Optimization techniques.\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d') AS day_line_item_usage_start_date, product_instance_type, SPLIT_PART(line_item_resource_id,':',7) as split_line_item_resource_id, line_item_operation, line_item_usage_type, line_item_line_item_type, pricing_term, product_usage_family, product_product_family, sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END) \u0026quot;usage_quantity\u0026quot;, sum (\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;, sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0 WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0 WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;reservation_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (\u0026quot;reservation_unused_amortized_upfront_fee_for_billing_period\u0026quot; + \u0026quot;reservation_unused_recurring_fee\u0026quot;) WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN 0 ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) \u0026quot;amortized_cost\u0026quot;, sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (-\u0026quot;savings_plan_amortized_upfront_commitment_for_billing_period\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (-\u0026quot;reservation_amortized_upfront_fee_for_billing_period\u0026quot;) ELSE 0 END) \u0026quot;ri_sp_trueup\u0026quot;, sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN \u0026quot;line_item_unblended_cost\u0026quot; WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN \u0026quot;line_item_unblended_cost\u0026quot;ELSE 0 END) \u0026quot;ri_sp_upfront_fees\u0026quot; FROM ${table_name} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND product_product_name = 'Amazon Redshift' AND line_item_line_item_type NOT IN ('Tax','Credit','Refund','EdpDiscount') GROUP BY 1,2,3,4,5,6,7,8,9,10,11 ORDER BY day_line_item_usage_start_date, product_product_family, unblended_cost DESC;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         Amazon ElastiCache   Query Description This query will output the total monthly sum per resource for all Amazon ElastiCache purchase options (including reserved instances) across all ElastiCache instances types. The unblended and amortized cost will be summed and in descending order.\nPricing Please refer to the Amazon ElastiCache pricing page .\nSample Output Download SQL File Link to Code Copy Query select bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-01') AS month_line_item_usage_start_date, SPLIT_PART(line_item_resource_id,':',7) as split_line_item_resource_id, SPLIT_PART(line_item_usage_type ,':',2) AS split_line_item_usage_type, (CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN 'Reserved Instance' WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN 'OnDemand' ELSE 'Others' END) purchase_option_line_item_line_item_type, sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END) sum_line_item_usage_amount, sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0 ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) sum_line_item_unblended_cost, sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0 WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0 WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;reservation_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (\u0026quot;reservation_unused_amortized_upfront_fee_for_billing_period\u0026quot; + \u0026quot;reservation_unused_recurring_fee\u0026quot;) WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN 0 ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) \u0026quot;amortized_cost\u0026quot;, sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (-\u0026quot;savings_plan_amortized_upfront_commitment_for_billing_period\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (-\u0026quot;reservation_amortized_upfront_fee_for_billing_period\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN (-\u0026quot;line_item_unblended_cost\u0026quot; ) ELSE 0 END) \u0026quot;ri_sp_trueup\u0026quot;, sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN \u0026quot;line_item_unblended_cost\u0026quot; WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN \u0026quot;line_item_unblended_cost\u0026quot;ELSE 0 END) \u0026quot;ri_sp_upfront_fees\u0026quot; FROM ${table_name} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND product_product_name = 'Amazon ElastiCache' AND product_product_family = 'Cache Instance' AND line_item_line_item_type NOT IN ('Tax','Credit','Refund','EdpDiscount','Fee','RIFee') GROUP BY DATE_FORMAT((line_item_usage_start_date),'%Y-%m-01'), bill_payer_account_id, line_item_usage_account_id, line_item_line_item_type, line_item_resource_id, line_item_usage_type ORDER BY month_line_item_usage_start_date, sum_line_item_usage_amount desc, sum_line_item_unblended_cost    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback       CUR queries are provided as is. We recommend validating your data by comparing it against your monthly bill and Cost Explorer prior to making any financial decisions. If you wish to provide feedback on these queries, there is an error, or you want to make a suggestion, please email: curquery@amazon.com\n "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_organization_data_cur_connection/6_bonus_org_tags/","title":"Bonus Organization Tags","tags":[],"description":"","content":"Update the Amazon Lambda Function to retrieve AWS Organizations tags To get the Organizations tags, we need to update the Lambda function to pull this information too. We will be swapping from csv to json as this will allow us to have flexibility with Amazon Athena as different accounts might have different tags.\n Go to the Lambda service page:  Search for the function Lambda_Org_Data and click on the name   Scroll down to the Function Code section and replace the code with the one below, change (account id) to your Management Account ID and (Region) to the Region you are deploying in:\n#!/usr/bin/env python3 #Lambda Function Code - Lambda_Org_Data import boto3 from botocore.exceptions import ClientError from botocore.client import Config import os import json import datetime def myconverter(o): if isinstance(o, datetime.datetime): return o.__str__() def list_accounts(): bucket = os.environ[\u0026quot;BUCKET_NAME\u0026quot;] #Using enviroment varibles below the lambda will use your S3 bucket tags_check = os.environ[\u0026quot;TAGS\u0026quot;] sts_connection = boto3.client('sts') acct_b = sts_connection.assume_role( RoleArn=\u0026quot;arn:aws:iam::(account id):role/OrganizationLambdaAccessRole\u0026quot;, RoleSessionName=\u0026quot;cross_acct_lambda\u0026quot; ) ACCESS_KEY = acct_b['Credentials']['AccessKeyId'] SECRET_KEY = acct_b['Credentials']['SecretAccessKey'] SESSION_TOKEN = acct_b['Credentials']['SessionToken'] # create service client using the assumed role credentials client = boto3.client( \u0026quot;organizations\u0026quot;, region_name=\u0026quot;us-east-1\u0026quot;, #Using the Organization client to get the data. This MUST be us-east-1 regardless of region you have the lamda in aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY, aws_session_token=SESSION_TOKEN, ) paginator = client.get_paginator(\u0026quot;list_accounts\u0026quot;) #Paginator for a large list of accounts response_iterator = paginator.paginate() with open('/tmp/org.json', 'w') as f: # Saving in the temporay folder in the lambda for response in response_iterator: # extracts the needed info for account in response[\u0026quot;Accounts\u0026quot;]: aid = account[\u0026quot;Id\u0026quot;] if tags_check != '': tags_list = client.list_tags_for_resource(ResourceId=aid) #gets the lists of tags for this account for tag in os.environ.get(\u0026quot;TAGS\u0026quot;).split(\u0026quot;,\u0026quot;): #looking at tags in the enviroment variables split by a space for org_tag in tags_list['Tags']: if tag == org_tag['Key']: #if the tag found on the account is the same as the current one in the environent varibles, add it to the data value = org_tag['Value'] kv = {tag : value} account.update(kv) data = json.dumps(account, default = myconverter) #converts datetime to be able to placed in json f.write(data) f.write('\\n') print(\u0026quot;respose gathered\u0026quot;) try: s3 = boto3.client('s3', '(Region)', config=Config(s3={'addressing_style': 'path'})) s3.upload_file( '/tmp/org.json', bucket, \u0026quot;organisation-data/org.json\u0026quot;) #uploading the file with the data to s3 print(\u0026quot;org data in s3\u0026quot;) except Exception as e: print(e) def lambda_handler(event, context): list_accounts()    Scroll down to Environment variables and click Edit\n  Click Add Environment variables. In the new empty box write TAGS under key and a list of tags from your Organisation you would like to include separated by a comma. Click Save.\n   Scroll to Function code section and Click Deploy.\n  You can now test the function by clicking Test at the top.\n  If you tested the csv version then you need to go to your S3 but and delete the csv file that is there as you will now be using json.\n Update the Organizations Data Table In this section we will update the Organization table in Athena to include the tags specified above.\n Go to the Athena service page  We are going to update the table we created earlier with the tags your chose in the Lambda section. Copy and paste the below query replacing:    Change (bucket-name) with your chosen bucket name from before\n  Add the Tags name as they appear in the Organizations page into the query in replace of the (Tag1) that you chose in your lambda function etc\n  Remove the \u0026hellip;\n  Click Run Query\n CREATE EXTERNAL TABLE IF NOT EXISTS managementcur.organisation_data ( `Id` string, `Arn` string, `Email` string, `Name` string, `Status` string, `JoinedMethod` string, `JoinedTimestamp` string, `(Tag1)` string, `(Tag2)` string, ... ) ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe' WITH SERDEPROPERTIES ( 'serialization.format' = '1' ) LOCATION 's3://(bucket-name)/organisation-data/' TBLPROPERTIES ('has_encrypted_data'='false');    There will now be additional columns with your tags in them.  If you wish to add more tags at a later date you must repeat Step 4 from the lambda section, adding the tags to the list of Environment variables and replacing the Athena table with the tags appended.\n  function prevStep(){ window.open(\"..\\/5_join_cost_intelligence_dashboard\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/7_teardown\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_aws_resource_optimization/6_ec2_rs_best_practices/","title":"Amazon EC2 Right Sizing Best Practices","tags":[],"description":"","content":"  Start simple: idle resources, non-critical development/QA and previous generation instances will require less testing hours and provide quick wins (The Amazon EC2 Launch time statistics can be used to identify instances that have been running longer than others and is a good statistic to sort your Amazon EC2 instances by).\n  Right Size before performing a migration: If you skip right sizing to save time, your migration speed might increase, but you will end up with higher cloud infrastructure spend for a potentially longer period of time. Instead, leverage the test and QA cycles during a migration exercise to test several instance types and families. Also, take that opportunity to test different sizes and burstable instances like the t family.\n  The best right sizing starts on day 1: As you perform right sizing analysis, and ultimately rightsize resources, ensure any learnings are being shared across your organization and influencing the design of new workloads.\n  Measure Twice, Cut Once: Test, then test some more: The last thing you want is for a new resource type to be uncapable of handling load, or functioning incorrectly.\n  Test once and perform multiple right sizing: Aggregate instances per autoscaling group and tags to scale right sizing activities.\n  Combine Reserved Instance or Savings Plans strategy with Right Sizing to maximize savings: For Standard RIs and EC2 Instance SP: Perform your pricing model purchases after rightsizing and for Convertible RIs, exchange them after rightsizing. Compute Savings plan will automatically adjust the commitment for the new environment.\n  Ignore burstable instance families (T types): These families are designed to typically run at low CPU percentages for significant periods of time and shouldnt be part of the instance types being analyzed for rightsizing.\n   function prevStep(){ window.open(\"..\\/5_ec2_updated_rec\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/7_tear_down\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_5_cost_visualization/6_custom_ec2/","title":"Create custom EC2 reports","tags":[],"description":"","content":"We will now create some custom EC2 reports, which will help to show ongoing costs related to EC2 instances and their associated usage.\n  From the left menu click Cost Explorer, click Reports, and click and click Monthly costs by service:   You will have the default breakdown by Service. Click on the Service filter on the right, select EC2-Instances (Elastic Compute Cloud - Compute) and EC2-Other, then click Apply filters:   You will now have monthly EC2 Instance and Other costs:   Change the Group by to Usage Type:   Change it to a Daily Line graph, then select More filters:   click on Purchase Option, select On Demand and click Apply filters, which will ensure we are only looking at On-Demand costs:   These are your on-demand EC2 costs, you should setup a report like this for your services that have the highest usage or costs. We will now save this, click on Save as\u0026hellip;:   Enter a report name and click Save Report \u0026gt;:   Now click on the Service filter, and de-select EC2-Instances, so that only EC2-Other is selected:   Now you can clearly see what makes up the Other charges, typically these are EBS volumes, Data Transfer and other costs associated with EC2 usage. Click Save as\u0026hellip; (do NOT click Save):   Enter a report name and click Save Report \u0026gt;:   You can access these by clicking on Saved Reports:   Here you can see both reports that were saved, note they do not have a lock symbol - which is reserved for AWS configured reports:    function prevStep(){ window.open(\"..\\/5_ri_coverage\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/7_tear_down\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/100_labs/100_inventory_patch_management/6_maintenance_windows/","title":"Creating Maintenance Windows and Scheduling Automated Operations Activities","tags":[],"description":"","content":"AWS Systems Manager: Maintenance Windows AWS Systems Manager Maintenance Windows let you define a schedule for when to perform potentially disruptive actions on your instances such as patching an operating system (OS), updating drivers, or installing software. Each Maintenance Window has a schedule, a duration, a set of registered targets, and a set of registered tasks. With Maintenance Windows, you can perform tasks like the following:\n Installing applications, updating patches, installing or updating SSM Agent, or executing PowerShell commands and Linux shell scripts by using a Systems Manager Run Command task Building Amazon Machine Images (AMIs), boot-strapping software, and configuring instances by using Systems Manager Automation Executing AWS Lambda functions that trigger additional actions such as scanning your instances for patch updates Running AWS Step Function state machines to perform tasks such as removing an instance from an Elastic Load Balancing environment, patching the instance, and then adding the instance back to the Elastic Load Balancing environment   NoteTo register Step Function tasks you must use the AWS CLI.\n 6.1 Setting up Maintenance Windows  Create the role that allows Systems Manager to tasks in Maintenance Windows on your behalf:  Navigate to the IAM console . In the navigation pane, choose Roles, and then choose Create role. In the Select type of trusted entity section, verify that the default AWS service is selected. In the Choose the service that will use this role section, choose EC2. This allows EC2 instances to call AWS services on your behalf. Choose Next: Permissions.   Under Attached permissions policy:  Search for AmazonSSMMaintenanceWindowRole. Check the box next to AmazonSSMMaintenanceWindowRole in the list. Choose Next: Review.   In the Review section:  Enter a Role name, such as SSMMaintenanceWindowRole. Enter a Role description, such as Role for Amazon SSMMaintenanceWindow. Choose Create role. Upon success you will be returned to the Roles screen.   To enable the service to run tasks on your behalf, we need to edit the trust relationship for this role:  Choose the role you just created to enter its Summary page. Choose the Trust relationships tab. Choose Edit trust relationship. Delete the current policy, and then copy and paste the following policy into the Policy Document field:    { \u0026quot;Version\u0026quot;:\u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;:[ { \u0026quot;Sid\u0026quot;:\u0026quot;\u0026quot;, \u0026quot;Effect\u0026quot;:\u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;:{ \u0026quot;Service\u0026quot;:[ \u0026quot;ec2.amazonaws.com\u0026quot;, \u0026quot;ssm.amazonaws.com\u0026quot;, \u0026quot;sns.amazonaws.com\u0026quot; ] }, \u0026quot;Action\u0026quot;:\u0026quot;sts:AssumeRole\u0026quot; } ] } Choose Update Trust Policy. You will be returned to the now updated Summary page for your role. Copy the Role ARN to your clipboard by choosing the double document icon at the end of the ARN.  When you register a task with a Maintenance Window, you specify the role you created, which the service will assume when it runs tasks on your behalf. To register the task, you must assign the IAM PassRole policy to your IAM user account. The policy in the following procedure provides the minimum permissions required to register tasks with a Maintenance Window.\nTo create the IAM PassRole policy for your Administrators IAM user group:  In the IAM console navigation pane, choose Policies, and then choose Create policy. On the Create policy page, in the Select a service area, next to Service choose Choose a service, and then choose IAM. In the Actions section, search for PassRole and check the box next to it when it appears in the list. In the Resources section, choose \u0026ldquo;You choose actions that require the role resource type.\u0026rdquo;, and then choose Add ARN to restrict access. The Add ARN(s) window will open. In the Add ARN(s) window, in the Specify ARN for role field, delete the existing entry, paste in the role ARN you created in the previous procedure, and then choose Add to return to the Create policy window. Choose Review policy. On the Review Policy page, type a name in the Name box, such as SSMMaintenanceWindowPassRole and then choose Create policy. You will be returned to the Policies page.   To assign the IAM PassRole policy to your Administrators IAM user group:  In the IAM console navigation pane, choose Groups, and then choose your Administrators group to reach its Summary page. Under the permissions tab, choose Attach Policy. On the Attach Policy page, search for SSMMaintenanceWindowPassRole, check the box next to it in the list, and choose Attach Policy. You will be returned to the Summary page for the group.    Creating Maintenance Windows To create a Maintenance Window , you must do the following:\n Create the window and define its schedule and duration. Assign targets for the window. Assign tasks to run during the window.  After you complete these steps, the Maintenance Window runs according to the schedule you defined and runs the tasks on the targets you specified. After a task is finished, Systems Manager logs the details of the execution.\n6.2 Create a Patch Maintenance Window First, you must create the window and define its schedule and duration:\n Open the AWS Systems Manager console . In the navigation pane, choose Maintenance Windows and then choose Create a Maintenance Window. In the Provide maintenance window details section:  In the Name field, type a descriptive name to help you identify this Maintenance Window, such as PatchTestWorkloadWebServers. (Optional) you may enter a description in the Description field. Choose Allow unregistered targets if you want to allow a Maintenance Window task to run on managed instances, even if you have not registered those instances as targets.   NoteIf you choose Allow unregistered targets, then you can choose the unregistered instances (by instance ID) when you register a task with the Maintenance Window. If you don\u0026rsquo;t, then you must choose previously registered targets when you register a task with the Maintenance Window.\n  Specify a schedule for the Maintenance Window by using one of the scheduling options:  Under Specify with, accept the default Cron schedule builder. Under Window starts, choose the third option, specify Every Day at, and select a time, such as 02:00. In the Duration field, type the number of hours the Maintenance Window should run, such as \u0026lsquo;3\u0026rsquo; hours. In the Stop initiating tasks field, type the number of hours before the end of the Maintenance Window that the system should stop scheduling new tasks to run, such as 1 hour before the window closes. Allow enough time for initiate activities to complete before the close of the maintenance window.     (Optionally) to have the maintenance window execute more rapidly while engaged with the lab:  Under Window starts, choose Every 30 minutes to have the tasks execute on every hour and every half hour. Set the Duration to the minimum 1 hours. Set the Stop initiation tasks to the minimum 0 hours.   Choose Create maintenance window. The system returns you to the Maintenance Window page. The state of the Maintenance Window you just created is Enabled.  6.3 Assigning Targets to Your Patch Maintenance Window After you create a Maintenance Window, you assign targets where the tasks will run.\n On the Maintenance windows page, choose the Window ID of your maintenance window to enter its Details page. Choose Actions in the top right of the window and select Register targets. On the Register target page under Maintenance window target details:  In the Target Name field, enter a name for the targets, such as TestWebServers. (Optional) Enter a description in the Description field. (Optional) Specify a name or work alias in the Owner information field.   Note: Owner information is included in any CloudWatch Events that are raised while running tasks for these targets in this Maintenance Window.\n  In the Targets section, under Select Targets by:  Choose the default Specifying tags to target instances by using Amazon EC2 tags that were previously assigned to the instances. Under Tags, enter \u0026lsquo;Workload\u0026rsquo; as the key and Test as the value. The option to add and additional tag key/value pair will appear. Add a second key/value pair using InstanceRole as the key and WebServer as the value.   Choose Register target at the bottom of the page to return to the maintenance window details page.  If you want to assign more targets to this window, choose the Targets tab, and then choose Register targetto register new targets. With this option, you can choose a different means of targeting. For example, if you previously targeted instances by instance ID, you can register new targets and target instances by specifying Amazon EC2 tags.\n6.4 Assigning Tasks to Your Patch Maintenance Window After you assign targets, you assign tasks to perform during the window:\n From the details page of your maintenance window, choose Actions in the top right of the window and select Register Run command task. On the Register Run command task page:  In the Name field, enter a name for the task, such as PatchTestWorkloadWebServers. (Optional) Enter a description in the Description field.   In the Command document section:  Choose the search icon, select Platform, and then choose Linux to display all the available commands that can be applied to Linux instances. Choose AWS-RunPatchBaseline in the list. Leave the Task priority at the default value of 1 (1 is the highest priority). Tasks in a Maintenance Window are scheduled in priority order, with tasks that have the same priority scheduled in parallel.   In the Targets section:  For Target by, select Selecting registered target groups. Select the group you created from the list.   In the Rate control section:  For Concurrency, leave the default targets selected and specify 1. For Error threshold, leave the default errors selected and specify 1.   In the Role section, specify the role you defined with the AmazonSSMMaintenanceWindowRole. It will be SSMMaintenanceWindowRole if you followed the suggestion in the instructions above. In Output options, leave Enable writing to S3 clear.  (Optionally) Specify Output options to record the entire output to a preconfigured S3 bucket and optional S3 key prefix   NoteOnly the last 2500 characters of a command document\u0026rsquo;s output are displayed in the console. To capture the complete output define and S3 bucket to receive the logs.\n  In SNS notifications, leave Enable SNS notifications clear.  (Optional) Specify SNS notifications to a preconfigured SNS Topic on all events or a specific event type for either the entire command or on a per-instance basis.   In the Parameters section, under Operation, select Install. Choose Register Run command task to complete the task definition and return to the details page.  6.5 Review Maintenance Window Execution  After allowing enough time for your maintenance window to complete:  Navigate to the AWS Systems Manager console . Choose Maintenance Windows, and then select the Window ID for your new maintenance window.   On the Maintenance window ID details page, choose History. Select a Windows execution ID and choose View details. On the Command ID details page, scroll down to the Targets and outputs section, select an Instance ID, and choose View output. Choose Step 1 - Output and review the output. Choose Step 2 - Output and review the output.  You have now configured a maintenance window, assigned targets, assigned tasks, and validated successful execution. The same procedures can be used to schedule the execution of any AWS Systems Manager Document .\n function prevStep(){ window.open(\"..\\/5_patch_mgmt\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/7_create_sns_topic\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_automated_cur_query_and_email_delivery/6_cloudwatch_event/","title":"Customize query strings and create scheduled CloudWatch event","tags":[],"description":"","content":"  In you local path where AutoCURDelivery.zip is located. Unzip and re-open config.yml in a text editor.\n  Find Body_Text, insert a description of new query MTD_Inter_AZ_DT.\nMTD_Inter_AZ_DT - Month to date inter-AZ data transfer split by resource ID   Find the section Query_String_List, add following new query string at the bottom of file (note the indent should be same as other query strings), save config.yml.\n - MTD_Inter_AZ_DT: SELECT year ,month(line_item_usage_start_date) month ,line_item_product_code as Product_Name ,line_item_resource_id as Resource_Id ,line_item_usage_type as Usage_Type ,sum(line_item_usage_amount) as \u0026quot;Inter_AZ_Data_Transfer(GB)\u0026quot; ,sum(line_item_unblended_cost) as \u0026quot;Cost($)\u0026quot; FROM CUR_DB WHERE \u0026quot;line_item_usage_type\u0026quot; like '%Bytes%' AND \u0026quot;line_item_usage_type\u0026quot; like '%Regional%' AND year='CUR_YEAR' AND month='CUR_MONTH' GROUP BY 1,2,3,4,5 ORDER BY sum(\u0026quot;line_item_unblended_cost\u0026quot;) desc    The paramemters CUR_DB, CUR_MONTH, CUR_YEAR are replaced when function is running\n Add config.yml back into AutoCURDelivery.zip, and upload zip file to S3.\n  Goto Lambda console, update function code path to above S3 path where new zip file is located, click Save.\n  Perform another Test of the function.\n  Check the cost \u0026amp; utilization report in the mail your recipient receives, there should be one more tab added in the excel file for month to date inter-az data transfer cost.\n  We will now create a scheduled Cloudwatch event to trigger Lambda function periodically\n Go to the Cloudwatch dashboard, under Events click Rules\n  Click Create rule.\n  In Event Source, choose Schedule, use default fixed rate of 5 minutes.\n  In Targets click Add Target and choose Lambda function in the drop-down box.\n  Choose the function Auto_CUR_Delivery, click Configure details   Configure a name 5_min_auto_cur_delivery, click Create rule.   Wait for 5 minutes, your recipients should receive a cost \u0026amp; utilization report mail, and continually receive report mail every other 5 minutes.\n  To stop event triggering, choose the rule 5_min_auto_cur_delivery, click Actions and select Disable.   Now you have completed this lab to query CUR with customized query strings from Athena and send it via SES periodically. To explore more, you can define your own query strings in config.yml and configure CloudWatch event rule to the rate as required.\n function prevStep(){ window.open(\"..\\/5_create_lambda_function\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/7_tear_down\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_1_aws_account_setup/6_quicksight/","title":"Setup Amazon QuickSight","tags":[],"description":"","content":"This will setup Amazon QuickSight, so that users in the Cost Optimization Account can create analysis' and dashboards.\nSetup QuickSight for the first time   Log into the console in the Cost Optimization account as an IAM user with the required permissions, go to the Amazon QuickSight console:   If you havent used QuickSight before click on Sign up for QuickSight, otherwise skip this step and proceed to Setup QuickSight IAM Policy below:   Select the Standard edition, and click Continue:   Select the region which should be the same as your CUR file source, Enter your QuickSight account name, Notification email address, select Amazon Athena and click Choose S3 buckets:   Select S3 Buckets You Can Access Across AWS, under Use a different bucket enter the CUR bucket name, click Add S3 bucket and click Finish:   Click Finish:   Setup QuickSight IAM Policy   Go to the IAM Dashboard\n  Click Policies and search for the AWSQuickSightS3Policy, click on the AWSQuickSightS3Policy policy:   Click Edit policy,   We will add the s3 resource arn:aws:s3:::cost* below the existing s3 bucket. This will allow QuickSight to access any S3 bucket starting with cost, so Cost Optimization users can easily create new datasets without requiring additional QuickSight privileges. Click Review policy:   Click Save changes:   Congratulations - QuickSight is now setup for your users. The Cost Optimization team can self manage QuickSight, and access to data sets in S3 with the correct bucket name.\n  function prevStep(){ window.open(\"..\\/5_account_settings\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/7_cost_explorer\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_splitting_sharing_cur_access/6_sub_acct/","title":"Sub Account Crawler Setup","tags":[],"description":"","content":"The final step is to setup the sub account to automatically scan the S3 folders each morning using a Glue Crawler, and update a local Athena database.\n1 - Login to the sub account as an IAM user with the required permissions, and go into the Glue console.\n2 - Add a Crawler with the following details:\n Include path: the S3 bucket in the account with the delivered CURs Exclude patterns:  **.json, **.yml, **.sql, **.csv, **.gz, **.zip (1 per line) 3 - Create a new role for the crawler to use 4 - Create a daily schedule to update the tables each morning before you come into work 5 - Create a new database\n6 - Review the crawler configuration and finish: 7 - Run the crawler, and check that it has added tables.\n8 - Go into Athena and execute a preview query to verify access and the data.\nYou have now given the sub account access to their specific CUR files as extracted from the Management/Payer CUR file. This will be automatically updated on any new versions delivered, or any new months delivered.\n function prevStep(){ window.open(\"..\\/5_trigger_lambda\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/7_tear_down\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_aws_resource_optimization/6_tear_down/","title":"Tear down","tags":[],"description":"","content":"No tear down is required for this lab.\n function prevStep(){ window.open(\"..\\/5_ec2_rs_best_practices\\/\", \"_self\") } function nextStep(){ window.open(\"\\/cost\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with COST6 - \u0026ldquo;How do you meet cost targets when you select resource type, size and number?\u0026quot;\nClick here to access the Well-Architected Tool   Previous Step Complete this lab     "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_deploy_and_update_cloudformation/6_cleanup/","title":"Tear down this lab","tags":[],"description":"","content":"When a CloudFormation stack is deleted, CloudFormation will automatically delete the resources that it created.\nIf you are attending an in-person workshop and were provided with an AWS account by the instructor:\n There is no need to tear down the lab. Feel free to continue exploring. Log out of your AWS account when done.  If you are using your own AWS account:\n You may leave these resources deployed for as long as you want. When you are ready to delete these resources, see the following instructions  Remove AWS CloudFormation provisioned resources You will now delete the CloudFormationLab CloudFormation stack.\nHow to delete an AWS CloudFormation stack  Go to the AWS CloudFormation console: https://console.aws.amazon.com/cloudformation Select the CloudFormation stack to delete and click Delete In the confirmation dialog, click Delete stack The Status changes to DELETE_IN_PROGRESS Click the refresh button to update and status will ultimately progress to DELETE_COMPLETE When complete, the stack will no longer be displayed. To see deleted stacks use the drop down next to the Filter text box. To see progress during stack deletion  Click the stack name Select the Events column Refresh to see new events     References \u0026amp; useful resources  What is AWS CloudFormation?  CloudFormation AWS Resource and Property Types Reference    function prevStep(){ window.open(\"..\\/5_add_ec2\\/\", \"_self\") } function nextStep(){ window.open(\"\\/cost\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with REL 8 How do you implement change?\nClick here to access the Well-Architected Tool   Previous Step Complete this lab     "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/100_labs/100_walkthrough_of_the_well-architected_tool/6_tear_down/","title":"Tear down this lab","tags":[],"description":"","content":"In order to take down the lab environment, you simply delete the workload you created.\n  Select Workloads on the left navigation:   Select the radio button next to the Workload for AWS Workshop and then click the Delete button.   Confirm the deletion by clicking the Delete button on the dialog:    function prevStep(){ window.open(\"..\\/5_view_report\\/\", \"_self\") } function nextStep(){ window.open(\"\\/cost\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Congratulations! \nClick here to access the Well-Architected Tool   Previous Step Complete this lab     "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_2_cost_and_usage_governance/6_tear_down/","title":"Teardown","tags":[],"description":"","content":"Log onto the console as your regular user with the required permissions.\nDelete the IAM policies We will delete the IAM policies created, as they are no longer applied to any groups.\n  Log on to the console as your regular user with the required permissions, go to the IAM service page:   Click on Policies on the left:   3.Click on Filter Policies and select Customer managed:  Select the policy you want to delete Region_Restrict:   Click on Policy actions, and select Delete:   Click on Delete:   Perform the same steps above to delete the Ec2_FamilyRestrict and EC2EBS_Restrict policies.\n  Click on Groups:   Select the CostTest group, click Group Actions, click Delete Group:   Click Yes, Delete:   Click Users:   Select TestUser1, and click Delete user:   Click Yes, delete:   Go to the EC2 dashboard:   Click Security Groups on the left:   Select the security groups you took note of, ensure you have the correct groups that were created. Click Actions, select Delete Security Groups:   Triple check they are the groups you wrote down, and click Yes, Delete:   Confirm there are no io1 unattached EBS volumes, go to the EC2 dashboard, click on Elastic Block Store, click Volumes. You can sort by the Created column to help identify volumes that were not terminated as part of this lab.\n   function prevStep(){ window.open(\"..\\/5_ec2_volume_type\\/\", \"_self\") } function nextStep(){ window.open(\"\\/cost\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with COST2 - \u0026ldquo;How do you govern usage?\u0026quot;\nClick here to access the Well-Architected Tool   Previous Step Complete this lab     "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_pricing_model_analysis/6_tear_down/","title":"Teardown","tags":[],"description":"","content":"Savings Plan analysis is a critical requirement of cost optimization, so there is no specific tear down for this lab.\nThe following resources were created in this lab:\n S3 Bucket: (custom name) Lambda Functions: SPTool_ODPricing_Download and SPTool_SPPricing_Download IAM Role: SPTool_Lambda IAM Policy: s3_pricing_lambda CloudWatch Event, Rule: SPTool-Pricing Glue Crawlers: OD_Pricing and SP_Pricing IAM Role: AWSGlueServiceRole-SPToolPricing Glue Database: Pricing Athena Views: pricing.pricing and costmaster.SP_USage QuickSight Dataset: SP_Usage QuickSight Analysis: sp_usage analysis   function prevStep(){ window.open(\"..\\/5_format_dashboard\\/\", \"_self\") } function nextStep(){ window.open(\"\\/cost\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with COST7 - \u0026ldquo;How do you use pricing models to reduce cost?\u0026quot;\nClick here to access the Well-Architected Tool   Previous Step Complete this lab     "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_autonomous_monitoring_of_cryptographic_activity_with_kms/6_teardown/","title":"Teardown","tags":[],"description":"","content":"The following steps will remove the services which are deployed in the lab.\n6.1. Remove the CloudWatch Alarm 6.1.1. From the CloudWatch console, select Alarms from the left-hand dashboard, and select the alarms which you created in the lab using the radio box.\n6.1.2. From the Actions button, select Delete and confirm the alarm deletion.\n6.2. Remove the CloudWatch Metric Filter From the CloudWatch console, select Log group under Logs and locate the log group which you created in the lab.\nUnder the Metric filters section, click on 1 filter and check the radio button next to the metric filter that you created.\nSelect the Delete button and confirm the deletion of the metric filter.\n6.3. Remove the CloudWatch Log Group From the CloudWatch console, select Log group under Logs and select the log group which you created in the lab.\nSelect the Actions button and delete the log group confirming the deletion.\n6.4. Remove the Trail from CloudTrail From the CloudTrail console, select Trails from the left-hand menu.\nUse the radio button to select the trail that you created and select the Delete button to delete the Trail.\n6.5. Remove the SNS topic From the SNS console, select Topics and then select the name of the topic which you created, and select the Delete button, confirming the deletion in the next dialog box.\n6.6. Remove the ECS Cluster From the ECS console, select Clusters in the left-hand-menu and select the cluster which you created for the lab.\n Select the Task tab and remove all active tasks for the cluster, confirming the deletion in the next window. Select the service tab and select the Delete button, confirming the delete in the next window.  6.7. Remove the ECR Repository From the ECR console, select Repositories in the left-hand menu, highlight the repository name in the main panel and select the Delete button, confirming the delete in the next window.\n6.8. Remove the Application Stack From the CloudFormation console, select the pattern1-app and select the Delete button, confirming the deletion in the next window.\n6.9. Remove the Base Stack Firstly, remove the data from the S3 bucket you created, or the deletion of the stack will fail. To do this, go to the S3 console and find the bucket which you created. Select the bucket and delete all the objects confirming the deletion in the next dialog box.\nNow, from the CloudFormation console, select the pattern1-base and select the Delete button, confirming the deletion in the next window.\n6.10. Finally Remove The Cloud9 Environment From the Cloud9 IDE highlight the environment which you created and select the delete button\n"},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/6_failure_injection_az/","title":"Test Resiliency Using Availability Zone (AZ) Failure Injection","tags":[],"description":"","content":"6.1 AZ failure injection This failure injection will simulate a critical problem with one of the three AWS Availability Zones (AZs) used by your service. AWS Availability Zones are powerful tools for helping build highly available applications. If an application is partitioned across AZs, companies are better isolated and protected from issues such as lightning strikes, tornadoes, earthquakes and more.\n  Go to the RDS Dashboard in the AWS Console at http://console.aws.amazon.com/rds and note which Availability Zone the AWS RDS primary DB instance is in.\n Note: If you previously ran the RDS Failure Injection test, you must wait until the console shows the AZs for the primary and standby instances as swapped, before running this test A good way to run the AZ failure injection is first in an AZ other than this - we\u0026rsquo;ll call this Scenario 1 Then try it again in the same AZ as the AWS RDS primary DB instance - we\u0026rsquo;ll call this Scenario 2 Taking down two out of the three AZs this way is an unlikely use case, however it will show how AWS systems work to maintain service integrity despite extreme circumstances. And executing this way illustrates the impact and response under the two different scenarios.    To simulate failure of an AZ, select one of the Availability Zones used by your service (us-east-2a, us-east-2b, or us-east-2c) as \u0026lt;az\u0026gt;\n For scenario 1 select an AZ that is neither primary nor secondary for your RDS DB instance. Given the following RDS console you would choose us-east-2c For scenario 2 select the AZ that is primary for your RDS DB instance. Given the following RDS console you would choose us-east-2b    use your VPC ID as \u0026lt;vpc-id\u0026gt;\n  Select one (and only one) of the scripts/programs below. (choose the language that you setup your environment for).\n   Language Command     Bash ./fail_az.sh \u0026lt;az\u0026gt; \u0026lt;vpc-id\u0026gt;   Python python fail_az.py \u0026lt;vpc-id\u0026gt; \u0026lt;az\u0026gt;   Java java -jar app-resiliency-1.0.jar AZ \u0026lt;vpc-id\u0026gt; \u0026lt;az\u0026gt;   C# .\\AppResiliency AZ \u0026lt;vpc-id\u0026gt; \u0026lt;az\u0026gt;   PowerShell .\\fail_az.ps1 \u0026lt;az\u0026gt; \u0026lt;vpc-id\u0026gt;      The specific output will vary based on the command used.\n Note whether an RDS failover was initiated. This would be the case if you selected the AZ containing the AWS RDS primary DB instance    6.2 System response to AZ failure Watch how the service responds. Note how AWS systems help maintain service availability. Test if there is any non-availability, and if so then how long.\n6.2.1 System availability Refresh the service website several times\n Scenario 1: If you selected an AZ not containing the AWS RDS primary DB instance then you should see uninterrupted availability Scenario 2: If you selected the AZ containing the AWS RDS primary DB instance, then an availability loss similar to what you saw with RDS fault injection testing will occur.  6.2.2 Scenario 1 - Load balancer and web server tiers This scenario is similar to the EC2 failure injection test because there is only one EC2 server per AZ in our architecture. Look at the same screens you as for that test:\n EC2 Instances  Load Balancer Target group  Auto Scaling Groups   One difference from the EC2 failure test that you will observe is that auto scaling will bring up the replacement EC2 instance in an AZ that already has an EC2 instance as it attempts to balance the requested three EC2 instances across the remaining AZs.\n6.2.3 Scenario 2 - Load balancer, web server, and data tiers This scenario is similar to a combination of the RDS failure injection along with EC2 failure injection. In addition to the EC2 related screens look at the Amazon RDS console , navigate to your DB screen and observe the following tabs:\n Configuration Monitoring Logs \u0026amp; Events  6.2.4 AZ failure injection - conclusion This similarity between scenario 1 and the EC2 failure test, and between scenario 2 and the RDS failure test is illustrative of how an AZ failure impacts your system. The resources in that AZ will have no or limited availability. With the strong partitioning and isolation between Availability Zones however, resources in the other AZs continue to provide your service with needed functionality. Scenario 1 results in loss of the load balancer and web server capabilities in one AZ, while Scenario 2 adds to that the additional loss of the data tier. By ensuring that every tier of your system is in multiple AZs, you create a partitioned architecture resilient to failure.\n6.2.5 AZ failure recovery This step is optional. To simulate the AZ returning to health do the following:\n Go to the Auto Scaling Group console  Select the WebServersforResiliencyTesting auto scaling group Actions \u0026raquo; Edit In the Subnet field add any ResiliencyVPC-PrivateSubnets that are missing (there should be three total) and Save Go to the Network ACL console  Look at the NACL entries for the VPC called ResiliencyVPC For any of these NACLs that are not Default do the following  Select the NACL Actions \u0026raquo; Edit subnet associations Uncheck all boxes and click Edit Actions \u0026raquo; Delete network ACL     Note how the auto scaling redistributes the EC2 serves across the availability zones   function prevStep(){ window.open(\"..\\/5_failure_injection_rds\", \"_self\") } function nextStep(){ window.open(\"..\\/7_failure_injection_optional\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_remote_configuration_installation_and_viewing_cloudwatch_logs/6_view_cw_logs/","title":"View your CloudWatch Logs","tags":[],"description":"","content":"Now that the CloudWatch Agent is up and running on your EC2 Instance, lets go ahead and view those logs and metrics from the Console. CloudWatch is a useful place to view logs because it is centralized, meaning you can switch between examining logs from many sources.\nViewing Logs:\n Open the CloudWatch console . On the left side menu, choose Log groups under Logs. On that screen, enter securitylablogs in the search bar. Click on the log group that appears in the results.  You will see these log streams: cw-agent-logs, apache-access-logs, apache-error-logs, yum-logs, and ssh-logs. Click through all of them to view the logs from each of these services.  You should see a record of log events. This is the data being collected on your EC2 instance, and then sent to CloudWatch by the CloudWatch Agent installed on the instance.  Recap: In this section, you explored log files generated by your EC2 instance in the CloudWatch console. The CloudWatch console provides a unified location to view a variety of logs, enabling you to investigate or monitor security activity in a central location. Using the CloudWatch console illustrates the security best practice of analyzing logs, findings, and metrics centrally.\n function prevStep(){ window.open(\"..\\/5_generate_logs\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/7_export_to_s3\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/200_labs/200_automating_operations_with_playbooks_and_runbooks/6_tracibility_of_your_operational_activities/","title":"Visibility on Operational activities.","tags":[],"description":"","content":"Following the completion of section 4, we can complete the lab by testing the workload. We will achieve this by running a decrypt API call to our application. This will trigger a failed decrypt event which should result in our alarm being triggered and an SNS notification sent to the email address which you specified as an endpoint in the previous section.\nComplete the following steps to test the system functionality:\n5.1. Initiate a successful decryption operation Run the command shown below within your Cloud9 IDE, replacing the \u0026lt; encrypt key \u0026gt; with the key value that you took note of in section 2.4 as well as the \u0026lt; Application endpoint URL \u0026gt; with the OutputPattern1ApplicationEndpoint url you took note on section 2.3.3\nALBURL=\u0026quot;http://\u0026lt; Application endpoint URL \u0026gt;\u0026quot; curl --header \u0026quot;Content-Type: application/json\u0026quot; --request GET --data '{\u0026quot;Name\u0026quot;:\u0026quot;Andy Jassey\u0026quot;,\u0026quot;Key\u0026quot;:\u0026quot;\u0026lt;encrypt key\u0026gt;\u0026quot;}' $ALBURL/decrypt Once that is successful, you should see out put like below:\n{\u0026quot;Text\u0026quot;:\u0026quot;Welcome to ReInvent 2020!\u0026quot;} 5.2. Initiate an unsuccessful decryption operation Now that we have confirmed that the decrypt API is operational, let\u0026rsquo;s trigger a deliberate decryption failure to invoke our alerting.\nRun below command once again, but this time, pass on a wrong key for the encrypt key (you can just use whatever value).\nALBURL=\u0026quot;http://\u0026lt; Application endpoint URL \u0026gt;\u0026quot; curl --header \u0026quot;Content-Type: application/json\u0026quot; --request GET --data '{\u0026quot;Name\u0026quot;:\u0026quot;Andy Jassey\u0026quot;,\u0026quot;Key\u0026quot;:\u0026quot;some-random-false-key\u0026quot;}' $ALBURL/decrypt Once it is triggered, you should see output like below signifying that the decrypt procedure has failed, and in the background a failed KMS API has been called. :\n{\u0026quot;Message\u0026quot;:\u0026quot;Data decryption failed, make sure you have the correct key\u0026quot;} Make sure that you repeat this several times in a row, to ensure you we are triggering the alarm. This will result in email notification to the endpoint you defined earlier in the lab.\nNote:  CloudTrail can typically take up to 15 mins to pick up the API event and trigger your alarm. For more information about this, please visit Cloudtrail FAQ page  5.3. Observing the alarm. If all the components are configured correctly, you should receive an email notification triggered by the CloudWatch alarm similar to this:\n5.3.1. Click on the URL included in the email that will take you to the CloudWatch Alarm resource in AWS console.\n5.3.2. Observe the state changes under the History section, and notice each activity change as follows:\nCongratulations! you have completed the Pattern1 lab.\n END OF SECTION 5\n "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_aws_resource_optimization/","title":"Level 100: EC2 Rightsizing","tags":[],"description":"","content":"Authors  Arthur Basbaum, AWS Cloud Economics  Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com\nIntroduction This hands-on lab will give you an overview on Amazon CloudWatch and AWS Resource Optimization and how to prioritize your EC2 Rightsizing efforts.\nGoals  Learn how to check metrics like CPU, Network and Disk usage on Amazon CloudWatch Enable and use the AWS Resource Optimization and get EC2 Right Sizing recommendations Learn how to filter AWS Resource Optimization report and focus only on the less complex high saving cases  Prerequisites  Root user access to the management account Enable AWS Resource Optimization at AWS Cost Explorer \u0026gt; Recommendations no additional cost.  Permissions required  Log in as the Cost Optimization team, created in AWS Account Setup  NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required.  Steps:  Getting to know Amazon Cloudwatch   Using Amazon EC2 Resource Optimization Recommendations   Download the Amazon EC2 Resource Optimization CSV File and sort it to find quick wins   Action the recommendations   Amazon EC2 Rightsizing Best Practices   Tear down    function prevStep(){ window.open(\"\", \"_self\") } function nextStep(){ window.open(\".\\/1_cloudwatch_intro\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool  Start Lab  "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_automated_iam_user_cleanup/","title":"Level 200: Automated IAM User Cleanup","tags":[],"description":"","content":"Authors  Pierre Liddle, Principal Security Architect Byron Pogson, Solutions Architect  Introduction This hands-on lab will guide you through the steps to deploy a AWS Lambda function with AWS Serverless Application Model (SAM) to provide regular insights on IAM User/s and AWS Access Key usage within your account. You will use the AWS SAM CLI to package your deployment. Skills learned will help you secure your AWS account in alignment with the AWS Well-Architected Framework .\nThe AWS Lambda function is triggered by a regular scheduled event in Amazon CloudWatch Events. Once the Lambda function runs to check the status of the AWS IAM Users and associated IAM Access Keys the results are sent the designated email contact via Amazon SNS. A check is also performed for unused roles and unused permissions identifed by IAM Access Analyzer. The logs from the AWS Lambda function are captured in Amazon CloudWatch Logs for review and trouble shooting purposes.\nGoals  Identify orphaned IAM Users and AWS Access Keys Take action to automatically remove IAM Users and AWS Access Keys no longer needed Reduce identity sprawl  Prerequisites  An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab. Select region with support for AWS Lambda from the list: AWS Regions and Endpoints . AWS Serverless Application Model (SAM) installed and configured. The AWS Serverless Application Model (SAM) is an open-source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. With just a few lines per resource, you can define the application you want and model it using YAML. During deployment, SAM transforms and expands the SAM syntax into AWS CloudFormation syntax, enabling you to build serverless applications faster.  Steps:  Deploying IAM Lambda Cleanup with AWS SAM   Tear down   References \u0026amp; useful resources  AWS Identity and Access Management User Guide  What is IAM Access Analyzer?  IAM Best Practices and Use Cases  AWS SAM CLI  AWS Serverless Application Model (SAM)   "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_aws_resource_optimization/","title":"Level 200: EC2 Right Sizing","tags":[],"description":"","content":"Authors  Jeff Kassel, AWS Technical Account Manager Arthur Basbaum, AWS Cloud Economics  Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com\nIntroduction This hands-on lab will guide you through the steps to install the CloudWatch agent to collect memory utilization (% GB consumption) and analyze how that new datapoint can help during EC2 right sizing exercises with the AWS Resource Optimization tool.\nGoals  Learn how to check metrics like CPU, Network and Disk usage on Amazon CloudWatch Learn how to install and collect Memory data through a custom metric at Amazon CloudWatch Enable AWS Resource Optimization and observe how the recommendations are impacted by this new datapoint (Memory)  Prerequisites  Root user access to the management account Enable AWS Resource Optimization at AWS Cost Explorer \u0026gt; Recommendations no additional cost.  Permissions required  Root user access to the management account NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required.  Steps:  Getting to know Amazon Cloudwatch   Create an IAM Role to use with Amazon CloudWatch Agent   Attach CloudWatch IAM role to selected EC2 Instances   Cloudwatch Agent Manual Install   Updated Amazon EC2 Resource Optimization recommendations   Amazon EC2 Right Sizing Best Practices   Tear down    function prevStep(){ window.open(\"\", \"_self\") } function nextStep(){ window.open(\".\\/1_cloudwatch_intro\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool  Start Lab  "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_lambda_cross_account_iam_role_assumption/","title":"Level 300: Lambda Cross Account IAM Role Assumption","tags":[],"description":"","content":"Authors  Ben Potter, Security Lead, Well-Architected  Introduction This lab demonstrates a Lambda function in AWS account 1 (the origin) using Python boto SDK to assume an IAM role in account 2 (the destination), then list the buckets. If you only have 1 AWS account simply repeat the instructions in that account and use the same account id.\nIf in classroom and you do not have 2 AWS accounts, buddy up to use each other\u0026rsquo;s accounts, agree who will be account #1 and who will be account #2.\nThe skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nGoals  Cross account role assumption Lambda assuming another role  Prerequisites  An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user with MFA enabled that can assume roles in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .  Steps:  Create role for Lambda in account 2   Create role for Lambda in account 1   Create Lambda in account 1   Tear down   References \u0026amp; useful resources https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html\n"},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_managing_credentials_and_authentication/","title":"Quest: Managing Credentials &amp; Authentication","tags":[],"description":" ","content":"Authors  Ben Potter, Security Lead, Well-Architected  About this Guide This guide will help you improve your security in the AWS Well-Architected area of Identity \u0026amp; Access Management . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nPrerequisites  An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .   New AWS Account Setup and Securing Root User Walkthrough This hands-on lab will guide you through the introductory steps to configure a new AWS account and secure the root user.\nAWS Account and Root User  Further Considerations  Federate Identity Using SAML: Leveraging a SAML provider  Separate production, non-production and different workloads using different AWS accounts: AWS Multiple Account Billing Strategy   Basic Identity and Access Management User, Group, Role Walkthrough This hands-on lab will guide you through the introductory steps to configure AWS Identity and Access Management (IAM). You will use the AWS Management Console to guide you through how to configure your first IAM user, group and role for administrative access.\nBasic Identity and Access Management User, Group, Role  Automated IAM User Cleanup Walkthrough This hands-on lab will guide you through the steps to deploy an AWS Lambda function with AWS Serverless Application Model (SAM) to provide regular insights on IAM User/s and AWS Access Key usage within your account.\nIAM Tag Based Access Control for EC2  IAM Permission Boundaries Delegating Role Creation Walkthrough This hands-on lab will guide you through the steps to configure an example AWS Identity and Access Management (IAM) permission boundary. AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature in which you use a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by the policy. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as a developer, the developer can then use this role to create additional user roles that are restricted to specific services and regions. This allows you to delegate access to create IAM roles and policies, without them exceeding the permissions in the permission boundary. We will also use a naming standard with a prefix, making it easier to control and organize policies and roles that your developers create.\nIAM Permission Boundaries Delegating Role Creation  IAM Tag Based Access Control for EC2 Walkthrough This hands-on lab will guide you through the steps to configure example AWS Identity and Access Management (IAM) policies, and a AWS IAM role with associated permissions to use EC2 resource tags for access control. Using tags is powerful as it helps you scale your permission management, however you need to be careful about the management of the tags which you will learn in this lab. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as an EC2 administrator. This allows the EC2 administrator to create tags when creating resources only if they match the requirements, and control which existing resources and values they can tag.\nIAM Tag Based Access Control for EC2  "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/","title":"300 Labs","tags":[],"description":"","content":"List of labs available  Level 300: Automated Athena CUR Query and E-mail Delivery   Level 300: Automated CUR Updates and Ingestion   Level 300: CUR Queries   Level 300: Splitting the CUR and Sharing Access   Level 300: Organization Data CUR Connection   "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_goals_and_targets/6_resource_type_size_number/","title":"Resource Type, Size &amp; Number","tags":[],"description":"","content":"Resource Type, Size, \u0026amp; Number Resource Size Over-provisioning  Goal: Minimize waste due to over-provisioning Target: Waste due to compute over-provisioing in the 1st tier of production workloads not to exceed 10% of tier compute cost, as reported by tool X. Waste due to compute over-provisioining in the 2nd tier of production workloads not to exceed 5% of tier compute cost. Best Practice: Data-based selection  Measures: % of over provisioning Good/Bad: Good Why? When does it work well or not?: Provides a $ amount, allows headroom for demand spikes \u0026amp; time to scale Contact/Contributor: natbesh@amazon.com    function prevStep(){ window.open(\"..\\/5_select_services\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/7_pricing_models\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_pricing_model_analysis/","title":"Level 200: Pricing Model Analysis","tags":[],"description":"","content":"Last Updated February 2021\nYour browser doesn't support video, or if you're on GitHub head to https://wellarchitectedlabs.com to watch the video.  Authors  Nathan Besh, Cost Lead, Well-Architected (AWS) Nataliya Godunok, Technical Account Manager (AWS)  Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com\nIntroduction This hands-on lab will guide you through setting up pricing and usage data sources, then creating a visualization to view your costs over time for EC2 in Savings Plans rates, allowing you to make low risk, high return purchases for pricing models. The data sources will also allow you to do custom allocation of discounts for your organization (a separate lab).\nYou will create two pricing data sources, by using Lambda to download the AWS price list files (On Demand EC2 pricing, and Savings Plans rates from all regions) and extract the pricing components required. You can configure CloudWatch Events to periodically run these functions to ensure you have the most up to date pricing and the latest instances in your data.\nThe pricing files are then combined with your Cost and Usage Report (CUR), to provide an hourly usage report with multiple pricing dimensions, this allows you query and analyze your usage by on demand rates, savings plan rates, or the difference between the two (discount level).\nFinally you create a visualization with calculations in QuickSight which allows you to view your usage patterns, and also perform analysis to understand the commitment levels that are right for your business.\nNOTE: this lab demonstrates EC2 savings plans only, but can be extended to cover other services such as Fargate or Lambda.\nGoals  Setup the pricing and usage data sources Create the visualization for recommendations and analysis  Prerequisites  An AWS Account An Amazon QuickSight Account A Cost and Usage Report (CUR) Amazon Athena and QuickSight have been setup Completed the Cost and Usage Analysis lab  Completed the Cost and Usage Visualization lab  Basic knowledge of AWS Lambda, Amazon Athena and Amazon Quicksight  Permissions required  Log in as the Cost Optimization team, created in AWS Account Setup  Additional: Create a Lambda function with assiciated IAM roles, trigger it via CloudWatch  Costs  Small accounts approximately \u0026lt;$5  Time to complete  The lab should take approximately 50-60 minutes to complete  Steps:  Create Pricing Data Sources   Create the Usage Data Source   Setup QuickSight Dashboard   Create the Recommendation Dashboard   Format the Recommendation Dashboard   Teardown    function prevStep(){ window.open(\"\", \"_self\") } function nextStep(){ window.open(\".\\/1_pricing_sources\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool  Start Lab  "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_using_awscli_to_manage_wa_reviews/7_cleanup/","title":"Teardown","tags":[],"description":"","content":"Summary You have learned how to use the various AWS CLI commands to work with the AWS Well-Architected Tool.\nRemove all the resources  Using the delete-workload API , you can remove the workload from the WA Tool. aws wellarchitected delete-workload --workload-id \u0026quot;\u0026lt;WorkloadId\u0026gt;\u0026quot;   References \u0026amp; useful resources  AWS CLI - wellarchitected  AWS Well-Architected Tool Documentation  AWS Well-Architected Boto3 Reference  AWS Well-Architected API Reference    function prevStep(){ window.open(\"..\\/6_programmatic\\/\", \"_self\") } function nextStep(){ window.open(\"\\/cost\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Congratulations! \nClick here to access the Well-Architected Tool   Previous Step Complete this lab     "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_fault_isolation_with_shuffle_sharding/7_cleanup/","title":"Teardown","tags":[],"description":"","content":"The following instructions will remove the resources that you have created in this lab.\nCleaning up the CloudFormation Stack  Sign in to the AWS Management Console and navigate to the AWS CloudFormation console - https://console.aws.amazon.com/cloudformation/ Select the stack Shuffle-sharding-lab, and delete the stack.   function prevStep(){ window.open(\"..\\/6_impact_of_failures_shuffle_sharding\", \"_self\") } function nextStep(){ window.open(\"..\\/8_resources\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_organization_data_cur_connection/7_teardown/","title":"Teardown","tags":[],"description":"","content":"The following resources were created in this lab:\n Amazon QuickSight Dataset: organisation_data Amazon Athena Table: organisation_data Amazon CloudWatch Event, Rule: Lambda_Org_Data AWS Lambda Functions: Lambda_Org_Data IAM Policy: LambdaOrgPolicy IAM Role: LambdaOrgRole IAM Policy: ListOrganizations IAM Role: OrganizationLambdaAccessRole S3 Bucket: (custom name)   function prevStep(){ window.open(\"..\\/6_bonus_org_tags\\/\", \"_self\") } function nextStep(){ window.open(\"\\/cost\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with COST3 - \u0026ldquo;How do you monitor usage and cost?\u0026quot;\nClick here to access the Well-Architected Tool   Previous Step Complete this lab     "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_cur_queries/queries/end_user_computing/","title":"End User Computing","tags":[],"description":"","content":"These are queries for AWS Services under the End User Computing product family .\nUse the clipboard in the top right of the text boxes below to copy all of the text to be pasted.\n You may need to change variables used as placeholders in your query. ${table_Name} is a common variable which needs to be replaced. Example: cur_db.cur_table\n Table of Contents   Amazon WorkSpaces   Query Description This query will provide unblended cost and usage information per linked account for Amazon WorkSpaces. The output will include detailed information about the resource id (WorkSpace ID), usage type, running mode, product bundle, and API operation. The cost will be summed and in descending order.\nPricing Please refer to the WorkSpaces pricing page . If you are interested in Cost Optimization, please refer to the AWS Solution, Amazon WorkSpaces Cost Optimizer .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d') AS day_line_item_usage_start_date, SPLIT_PART(line_item_resource_id,'/',2) as split_line_item_resource_id, SPLIT_PART(product_bundle,'-',1) as split_product_bundle, product_operating_system, product_memory, product_storage, product_vcpu, product_running_mode, product_license, product_software_included, pricing_unit, SUM(CAST(line_item_usage_amount AS double)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS decimal(16,8))) AS sum_line_item_unblended_cost FROM ${table_name} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND product_product_name = 'Amazon WorkSpaces' AND line_item_line_item_type NOT IN ('Tax','Credit','Refund','EdpDiscount','Fee','RIFee') GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d'), line_item_resource_id, product_bundle, product_operating_system, product_memory, product_storage, product_vcpu, product_running_mode, product_license, product_software_included, pricing_unit ORDER BY day_line_item_usage_start_date, sum_line_item_unblended_cost DESC;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         Amazon WorkSpaces - Auto Stop   Query Description AutoStop Workspaces are cost effective when used for several hours per day. If AutoStop Workspaces run for more than 80 hrs per month it is more cost effective to switch to AlwaysOn mode. This query shows AutoStop Workspaces which ran more that 80 hrs in previous month. If usage pattern for these Workspaces is the same month over month it\u0026rsquo;s possible to optimize cost with switching to AlwaysOn mode. For example: Windows PowerPro (8 vCPU, 32GB RAM) bundle in eu-west-1 runs for 400 hrs per month. In AutoStop mode it costs $612/month ($8.00/month + 400 * $1.53/hour) while if used in AlwaysOn mode it would cost $141/month\nPricing Please refer to the WorkSpaces pricing page . If you are interested in Cost Optimization, please refer to the AWS Solution, Amazon WorkSpaces Cost Optimizer .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, SPLIT_PART(line_item_resource_id, '/', 2) AS split_line_item_resource_id, product_region, product_operating_system, product_bundle_description, product_software_included, product_license, product_rootvolume, product_uservolume, pricing_unit, sum_line_item_usage_amount, CAST(total_cost_per_resource AS decimal(16, 8)) AS \u0026quot;sum_line_item_unblended_cost(incl monthly fee)\u0026quot; FROM ( SELECT bill_payer_account_id, line_item_usage_account_id, line_item_resource_id, product_operating_system, pricing_unit, product_region, product_bundle_description, product_rootvolume, product_uservolume, product_software_included, product_license, SUM(\u0026quot;line_item_usage_amount\u0026quot;) AS \u0026quot;sum_line_item_usage_amount\u0026quot;, SUM(SUM(\u0026quot;line_item_unblended_cost\u0026quot;)) OVER (PARTITION BY \u0026quot;line_item_resource_id\u0026quot;) AS \u0026quot;total_cost_per_resource\u0026quot;, SUM(SUM(\u0026quot;line_item_usage_amount\u0026quot;)) OVER (PARTITION BY \u0026quot;line_item_resource_id\u0026quot;, \u0026quot;pricing_unit\u0026quot;) AS \u0026quot;usage_amount_per_resource_and_pricing_unit\u0026quot; FROM $ {table_name} WHERE line_item_product_code = 'AmazonWorkSpaces' -- get previous month AND month = cast(month(current_timestamp + -1 * interval '1' MONTH) AS VARCHAR) -- get year for previous month AND year = cast(year(current_timestamp + -1 * interval '1' MONTH) AS VARCHAR) AND line_item_line_item_type = 'Usage' AND line_item_usage_type like '%AutoStop%' GROUP BY line_item_usage_account_id, line_item_resource_id, product_operating_system, pricing_unit, product_region, product_bundle_description, product_rootvolume, product_uservolume, bill_payer_account_id, product_software_included, product_license ) WHERE -- return only workspaces which ran more than 80 hrs \u0026quot;usage_amount_per_resource_and_pricing_unit\u0026quot; \u0026gt; 80 ORDER BY total_cost_per_resource DESC, line_item_resource_id, line_item_usage_account_id, product_operating_system, pricing_unit    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         Amazon AppStream 2.0   Query Description This query will provide unblended cost and usage information per linked account for Amazon AppStream 2.0. The output will include detailed information about the product family, product instance type, pricing unit, region along with usage amount. The cost will be summed and in descending order.\nPricing Please refer to the Amazon AppStream 2.0 pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,'%Y-%m') AS month_line_item_usage_start_date, product_product_family, -- Stopped Instance, Streaming Instance, User Fees product_instance_type, -- stream.TYPE.SIZE pricing_unit, -- Hrs, Users product_region, SUM(CAST(line_item_usage_amount AS double)) AS sum_line_item_usage_amount, CASE line_item_line_item_type WHEN 'DiscountedUsage' THEN SUM(CAST(reservation_effective_cost AS decimal(16,8))) WHEN 'Usage' THEN SUM(CAST(line_item_unblended_cost AS decimal(16,8))) ELSE SUM(CAST(line_item_unblended_cost AS decimal(16,8))) END AS sum_line_item_unblended_cost_reservation_effective_cost FROM ${table_name} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND product_product_name = 'Amazon AppStream' AND line_item_line_item_type NOT IN ('Tax','Credit','Refund','EdpDiscount','Fee','RIFee') GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,'%Y-%m'), product_product_family, product_instance_type, pricing_unit, product_region, line_item_line_item_type ORDER BY month_line_item_usage_start_date, sum_line_item_usage_amount desc, sum_line_item_unblended_cost_reservation_effective_cost, product_product_family;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback       CUR queries are provided as is. We recommend validating your data by comparing it against your monthly bill and Cost Explorer prior to making any financial decisions. If you wish to provide feedback on these queries, there is an error, or you want to make a suggestion, please email: curquery@amazon.com\n "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/100_labs/100_inventory_patch_management/7_create_sns_topic/","title":"Creating a Simple Notification Service Topic","tags":[],"description":"","content":"Amazon Simple Notification Service (Amazon SNS) coordinates and manages the delivery or sending of messages to subscribing endpoints or clients. In Amazon SNS, there are two types of clients: publishers and subscribers. These are also referred to as producers and consumers. Publishers communicate asynchronously with subscribers by producing and sending a message to a topic, which is a logical access point and communication channel. Subscribers (i.e., web servers, email addresses, Amazon SQS queues, AWS Lambda functions) consume or receive the message or notification over one of the supported protocols (i.e., Amazon SQS, HTTP/S, email, SMS, Lambda) when they are subscribed to the topic.\n6.1 Create and Subscribe to an SNS Topic To create and subscribe to an SNS topic:\n Navigate to the SNS console at https://console.aws.amazon.com/sns/. Choose Create topic. In the Create new topic window:  In the Topic name field, enter AdminAlert. In the Display name field, enter AdminAlert. Choose Create topic.   On the Topic details: AdminAlert page, choose Create subscription. In the Create subscription window:  Select Email from the Protocol list. Enter your email address in the Endpoint field. Choose Create subscription.   You will receive an email request for confirmation. Your Subscription ID will remain PendingConfirmation until you confirm your subscription by clicking through the link to Confirm subscription in the email. Refresh the page after confirming your subscription to view the populated Subscription ARN.  You can now use this SNS topic to send notifications to your Administrator user.\n function prevStep(){ window.open(\"..\\/6_maintenance_windows\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/8_cleanup\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_1_aws_account_setup/7_cost_explorer/","title":"Enable AWS Cost Explorer","tags":[],"description":"","content":"AWS Cost Explorer has an easy-to-use interface that lets you visualize, analyze, and manage your AWS costs and usage over time. You must enable it before you can use it within your accounts, once enabled it is enabled for ALL accounts and controlled through IAM policies. The default configuration of Cost Explorer is free, however we will enable hourly granularity, which incurrs an additional cost - AWS Cost Management Pricing .\n  Log in to your Management account with administrative privileges, and go to the Billing console:   Select Cost Explorer from the left menu:   Click on Enable Cost Explorer:   You will receive notification that Cost Explorer has been enabled, and data will be populated:   After 24hrs, go into Cost Explorer:   Click Settings in the top right:   Select Hourly and Resource Level Data, and click Save:   This will incur costs depending on the number of resources you are running.\n  function prevStep(){ window.open(\"..\\/6_quicksight\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/8_cost_tags\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_remote_configuration_installation_and_viewing_cloudwatch_logs/7_export_to_s3/","title":"Export Logs to S3","tags":[],"description":"","content":"After collecting logs, you may want to export logs from CloudWatch to an S3 Bucket. This is useful as storing data in S3 is more cost effective and reliable than storing it in CloudWatch, making S3 a good option for long-term storage and archival of log files.\n Open up the CloudWatch console . On the left side menu, choose Log groups under Logs. On that screen, enter securitylablogs in the search bar. Click on the log group that appears in the results. Click Actions and Export data to Amazon S3 in the top menu.   You will have to fill out information about what data to export.\n In the From field, set the YYYY/MM/DD field to todays date (the date you are doing this lab). This is the earliest creation date of logs you want to export. In the To field, set set the YYYY/MM/DD field to tomorrows date (the date after the day you are doing this lab). This is the latest creation date of logs you want to export. Leave the Stream prefix field blank, as we want to export all logs. This field allows you to select which logs you want to export. Set S3BucketName to the bucket name you entered in your CloudFormation stack, likely wa-lab-\u0026lt;your-account-id\u0026gt;-\u0026lt;date\u0026gt;. This is the bucket your logs will be exported to. Set S3 bucket prefix to lablogs. This is the subdirectory your exported logs will be stored in.    Click Export\n  Click on the View export tasks in the pop up box that appears. This will bring you to a list of Export tasks performed from CloudWatch\n  Click the radio bubble next to the most recent export. Click View in Amazon S3 to open these logs in the S3 bucket you created.  You should now see folders corresponding to all of the log streams you viewed earlier. You can explore these logs and download the .gz files if youd like to see their contents.  Recap: In this portion of the lab, you exported logs from CloudWatch to S3, a good way to archive logs for long term storage. This demonstrates an important component of the security best practice of configuring logging centrally - the ability to extract meaningful insights from large volumes of log data. Compared with CloudWatch, storing log files in S3 is more cost-effective and allows you to use lifecycle policies on your stored logs. As the volume of logs generated by your workloads increase, so does the value of storing these data in S3. It also enables you to analyze logs from Athena, as you will see in the next section.\n function prevStep(){ window.open(\"..\\/6_view_cw_logs\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/8_query_from_athena\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_splitting_sharing_cur_access/7_tear_down/","title":"Tear Down","tags":[],"description":"","content":"We will tear down this lab, removing any data, resources and configuration that it created. We will restore any modified code or resources to their original state before the lab.\n7.1 Sub Account 1 - Log into the sub account as an IAM user with the required privileges\n2 - Go to the Glue service dashboard\n3 - Delete the created database and tables\n4 - Delete the recurring Glue crawler\n7.2 Management/Payer Account 1 - Log into the management/payer account as an IAM user with the required privileges\n2 - Go to the Cloudformation service dashboard, and select the CUR update stack\n3 - Update the stack and use the original Template yml file\n4 - Go to the Lambda service dashboard\n5 - Delete the SubAcctSplit and S3LinkedPutACL Lambda functions\n6 - Go to the IAM service dashboard\n7 - Delete the LambdaSubAcctSplit and Lambda_Put_Linked_S3ACL roles\n8 - Delete the LambdaSubAcctSplit and Lambda_S3Linked_PutACL policies\n9 - Go to the Athena service dashboard\n10 - Delete the **create_linked_** and **delete_linked_** Athena saved queries\n11 - Delete any temp tables\n12 - Go into the S3 service dashboard\n13 - Delete the S3 output folder\n function prevStep(){ window.open(\"..\\/6_sub_acct\\/\", \"_self\") } function nextStep(){ window.open(\"\\/cost\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with COST 3 - \u0026ldquo;How do you monitor usage and cost?\u0026quot;\nClick here to access the Well-Architected Tool   Previous Step Complete this lab     "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_5_cost_visualization/7_tear_down/","title":"Tear down","tags":[],"description":"","content":"We will delete both custom reports that were created.\n  Click on Saved reports on the left menu:   Select the checkbox next to the two custom reports that you created above. Click on Delete:   Verify the names of the reports you are going to delete, click Delete:   The reports are no longer listed in the reports available:    function prevStep(){ window.open(\"..\\/6_custom_ec2\\/\", \"_self\") } function nextStep(){ window.open(\"\\/cost\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with COST1 - \u0026ldquo;How do you implement cloud financial management?\u0026quot;\nClick here to access the Well-Architected Tool   Previous Step Complete this lab     "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_aws_resource_optimization/7_tear_down/","title":"Tear down","tags":[],"description":"","content":" Terminate the EC2 Instance Delete the IAM role CloudWatchAgentServerRole   function prevStep(){ window.open(\"..\\/6_ec2_rs_best_practices\\/\", \"_self\") } function nextStep(){ window.open(\"\\/cost\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with COST 6. \u0026ldquo;How do you meet cost targets when you select resource type, size and number?\u0026quot;\nClick here to access the Well-Architected Tool   Previous Step Complete this lab     "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_automated_cur_query_and_email_delivery/7_tear_down/","title":"Teardown","tags":[],"description":"","content":" Delete IAM role Lambda_Auto_CUR_Delivery_Role and policy Lambda_Auto_CUR_Delivery_Access Delete Lambda function Auto_CUR_Delivery Delete CloudWatch event 5_min_auto_cur_delivery Delete SES configuration Delete S3 bucket for CUR query results storing   function prevStep(){ window.open(\"..\\/6_cloudwatch_event\\/\", \"_self\") } function nextStep(){ window.open(\"\\/cost\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with COST 3 - \u0026ldquo;How do you monitor usage and cost?\u0026quot;\nClick here to access the Well-Architected Tool   Previous Step Complete this lab     "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/200_labs/200_automating_operations_with_playbooks_and_runbooks/7_teardown/","title":"Teardown","tags":[],"description":"","content":"The following steps will remove the services which are deployed in the lab.\n6.1. Remove the CloudWatch Alarm 6.1.1. From the CloudWatch console, select Alarms from the left-hand dashboard, and select the alarms which you created in the lab using the radio box.\n6.1.2. From the Actions button, select Delete and confirm the alarm deletion.\n6.2. Remove the CloudWatch Metric Filter From the CloudWatch console, select Log group under Logs and locate the log group which you created in the lab.\nUnder the Metric filters section, click on 1 filter and check the radio button next to the metric filter that you created.\nSelect the Delete button and confirm the deletion of the metric filter.\n6.3. Remove the CloudWatch Log Group From the CloudWatch console, select Log group under Logs and select the log group which you created in the lab.\nSelect the Actions button and delete the log group confirming the deletion.\n6.4. Remove the Trail from CloudTrail From the CloudTrail console, select Trails from the left-hand menu.\nUse the radio button to select the trail that you created and select the Delete button to delete the Trail.\n6.5. Remove the SNS topic From the SNS console, select Topics and then select the name of the topic which you created, and select the Delete button, confirming the deletion in the next dialog box.\n6.6. Remove the ECS Cluster From the ECS console, select Clusters in the left-hand-menu and select the cluster which you created for the lab.\n Select the Task tab and remove all active tasks for the cluster, confirming the deletion in the next window. Select the service tab and select the Delete button, confirming the delete in the next window.  6.7. Remove the ECR Repository From the ECR console, select Repositories in the left-hand menu, highlight the repository name in the main panel and select the Delete button, confirming the delete in the next window.\n6.8. Remove the Application Stack From the CloudFormation console, select the pattern1-app and select the Delete button, confirming the deletion in the next window.\n6.9. Remove the Base Stack Firstly, remove the data from the S3 bucket you created, or the deletion of the stack will fail. To do this, go to the S3 console and find the bucket which you created. Select the bucket and delete all the objects confirming the deletion in the next dialog box.\nNow, from the CloudFormation console, select the pattern1-base and select the Delete button, confirming the deletion in the next window.\n6.10. Finally Remove The Cloud9 Environment From the Cloud9 IDE highlight the environment which you created and select the delete button\n"},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/7_failure_injection_optional/","title":"Test Resiliency Using Failure Injection - Optional steps","tags":[],"description":"","content":"The following are optional lab steps you may run to further explore failure injection\n7.1 S3 failure injection  Failure of S3 means that the image will not be available You may ONLY do this testing if you supplied your own websiteimage reference to an S3 bucket you control  7.1.1 Bucket name You will need to know the bucket name where your image is. For example if the websiteimage value you supplied was \u0026quot;https://s3.us-east-2.amazonaws.com/my-awesome-bucketname/my_image.jpg\u0026quot;, then the bucket name is my-awesome-bucketname\nFor this failure simulation it is most straightforward to use the AWS Console as follows. (If you are interested in doing this using the AWS CLI then see here - choose either AWS Console or AWS CLI)\n7.1.2 AWS Console  Navigate to the S3 console: https://console.aws.amazon.com/s3 Select the bucket name where the image is located Select the object, then select the \u0026ldquo;Permissions\u0026rdquo; tab Select the \u0026ldquo;Public Access\u0026rdquo; radio button, and deselect the \u0026ldquo;Read object\u0026rdquo; checkbox and Save To re-enable access (after testing), do the same steps, tick the \u0026ldquo;Read object\u0026rdquo; checkbox and Save  7.1.3 System response to S3 failure What is the expected effect? How long does it take to take effect?\n Note that due to browser caching you may still see the image on refreshing the site. On most systems Shift-F5 does a clean refresh with no cache  How would you diagnose if this is a larger problem than permissions?\n7.2 More testing you can do You can use drift detection in the CloudFormation console to see what had changed, or work on code to heal the failure modes.\n function prevStep(){ window.open(\"..\\/6_failure_injection_az\", \"_self\") } function nextStep(){ window.open(\"..\\/8_cleanup\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_basic_ec2_with_waf_protection/","title":"Level 200: Basic EC2 Web Application Firewall Protection","tags":[],"description":"","content":"Authors  Ben Potter, Security Lead, Well-Architected  Introduction This hands-on lab will guide you through the introductory steps to protect an Amazon EC2 workload from network based attacks. You will use the AWS Management Console and AWS CloudFormation to guide you through how to secure an Amazon EC2 based web application with defense in depth methods. Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nGoals  Protecting network and host-level boundaries System security configuration and maintenance Enforcing service-level protection  Prerequisites  An AWS account that you are able to use for testing, that is not used for production or other purposes.\nNOTE: You will be billed for any applicable AWS resources used if you complete this lab. Select region with support for AWS WAF for Application Load Balancers from list: AWS Regions and Endpoints .  Steps:  Launch Instance   Create AWS WAF Rules   Create Application Load Balancer with WAF integration   Tear down   References \u0026amp; useful resources Amazon Elastic Compute Cloud User Guide for Linux Instances Tutorial: Configure Apache Web Server on Amazon Linux 2 to Use SSL/TLS AWS WAF, AWS Firewall Manager, and AWS Shield Advanced Developer Guide "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_control_human_access/","title":"Quest: Control Human Access","tags":[],"description":" ","content":"Authors  Ben Potter, Security Lead, Well-Architected  About this Guide This guide will help you improve your security in the AWS Well-Architected area of Identity \u0026amp; Access Management . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nPrerequisites  An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .   Basic Identity and Access Management User, Group, Role Walkthrough This hands-on lab will guide you through the introductory steps to configure AWS Identity and Access Management (IAM). You will use the AWS Management Console to guide you through how to configure your first IAM user, group and role for administrative access.\nBasic Identity and Access Management User, Group, Role  IAM Permission Boundaries Delegating Role Creation Walkthrough This hands-on lab will guide you through the steps to configure an example AWS Identity and Access Management (IAM) permission boundary. AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature in which you use a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by the policy. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as a developer, the developer can then use this role to create additional user roles that are restricted to specific services and regions. This allows you to delegate access to create IAM roles and policies, without them exceeding the permissions in the permission boundary. We will also use a naming standard with a prefix, making it easier to control and organize policies and roles that your developers create.\nIAM Permission Boundaries Delegating Role Creation  IAM Tag Based Access Control for EC2 Walkthrough This hands-on lab will guide you through the steps to configure example AWS Identity and Access Management (IAM) policies, and a AWS IAM role with associated permissions to use EC2 resource tags for access control. Using tags is powerful as it helps you scale your permission management, however you need to be careful about the management of the tags which you will learn in this lab. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as an EC2 administrator. This allows the EC2 administrator to create tags when creating resources only if they match the requirements, and control which existing resources and values they can tag.\nIAM Tag Based Access Control for EC2  "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_goals_and_targets/","title":"Level 100: Goals and Targets","tags":[],"description":"","content":" Your browser doesn't support video, or if you're on GitHub head to https://wellarchitectedlabs.com to watch the video.  Last Updated February 2021\nAuthors  Nathan Besh, Cost Lead Well-Architected  Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com\nIntroduction This lab will be used as a collaboration space for people to learn, contribute and discuss Cost Optimization goals. The goals are categorized according to the W-A questions and best practices. Each goal has a target and supporting information to help explain the goal/target and how you can adapt and use it best for your organization, along with any potential caveats.\nThe goals and targets listed should be treated as suggestions (some stronger than others!), to help promote thought \u0026amp; discussion - not necessarily direct implementation in your organization.\nThere is also an optional space to add contacts for the contributor, if you wish to be contacted and open for a discussion you can use this field.\nGoals  Create goals and targets for your organization  Prerequisites  None  Permissions required  None  Costs  There are no costs for this lab  Time to complete  N/A  Steps:  Cloud Financial Management   Govern Usage   Monitor Cost and Usage   Decommission Resources   Service Selection   Resource Type, Size \u0026amp; Number   Pricing Models   Data Transfer   Manage Demand \u0026amp; Supply Resources   Evaluate New Services    function prevStep(){ window.open(\"\", \"_self\") } function nextStep(){ window.open(\".\\/1_cloud_financial_management\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool  Start Lab  "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_goals_and_targets/7_pricing_models/","title":"Pricing Models","tags":[],"description":"","content":"Pricing Models Savings Plans Purchasing Savings Plans  Goal: Minimize resource cost by using Savings Plans Target: Unpurchased Savings Plans offering more than 10% discount not to exceed $100 Best Practice: Commitment Discounts - Savings Plans  Measures: $ value of unpurchased plans above 10% discount Good/Bad: Good Why? When does it work well or not?: Maximizes high quality, low risk SP consumption, \u0026amp; ensures SP\u0026rsquo;s that dont make business sense are not purchased Contact/Contributor: natbesh@amazon.com  Purchasing Savings Plans  Goal: Minimize resource cost by using Savings Plans Target: Ensure 80% of my on-demand usage is covered with Savings Plans Best Practice: Commitment Discounts - Savings Plans  Measures: Coverage % Good/Bad: Bad Why? When does it work well or not?: It ignores the return on investment in SP \u0026amp; the risk. If you run \u0026lt;100% uptime or small resources with licensed operating systems and/or software, there can be low returns \u0026amp; also high risks. It can force a purchase which would be a bad business decision Contact/Contributor: natbesh@amazon.com   Reserved Instances Purchase Reserved Instances  Goal: Minimize resource cost by using Reserved Instances Target: Unpurchased Reserved Instances offering more than 10% discount, with a pay-off period of less than 9 months must not exceed $300 Best Practice: Commitment Discounts - Reserved Instances  Measures: $ value of unpurchased RI\u0026rsquo;s above 10% discount that pay off in less than 9 months Good/Bad: Good Why? When does it work well or not?: Maximizes high quality, low risk RI consumption, \u0026amp; ensures RI\u0026rsquo;s that dont make business sense are not purchased Contact/Contributor: natbesh@amazon.com  Purchase Reserved Instances  Goal: Maximize discounts by maximizing use of purchased Reserved Instances Target: Have cost of low RI utilization below $100 for any purchased RI Best Practice: Commitment Discounts - Reserved Instances  Measures: Wasted $ per RI Good/Bad: Good Why? When does it work well or not?: Minimizes waste from unused RI\u0026rsquo;s, it ensures you only act when it makes business sense \u0026amp; effort can be recovered by making a change. Contact/Contributor: natbesh@amazon.com  Purchase Reserved Instances  Goal: Maximize discounts by maximizing use of purchased Reserved Instances Target: Have RI utilization above 80% Best Practice: Commitment Discounts - Reserved Instances  Measures: RI utilization % Good/Bad: Bad Why? When does it work well or not?: High amounts of effort required to know whether you\u0026rsquo;re actually losing money, and how much money. It can work in environments that have only a few different resource types running. This target can lead to large effort resulting in minimal gains or even a loss. Contact/Contributor: natbesh@amazon.com   function prevStep(){ window.open(\"..\\/6_resource_type_size_number\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/8_data_transfer\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_fault_isolation_with_shuffle_sharding/8_resources/","title":"References &amp; useful resources","tags":[],"description":"","content":" AWS re:Invent 2019: Introducing The Amazon Builders Library (DOP328)  Amazon Builders' Library: Workload isolation using shuffle sharding  AWS re:Invent 2018: Architecture Patterns for Multi-Region Active-Active Applications (ARC209-R2)  AWS re:Invent 2018: How AWS Minimizes the Blast Radius of Failures (ARC338)  AWS re:Invent 2019: Innovation and operation of the AWS global network infrastructure (NET339)   AWS Well-Architected Best Practices Use bulkhead architectures : Like the bulkheads on a ship, this pattern ensures that a failure is contained to a small subset of requests/users so that the number of impaired requests is limited, and most can continue without error. Bulkheads for data are usually called partitions or shards, while bulkheads for services are known as cells.\nThank you for using this lab.  function prevStep(){ window.open(\"..\\/7_cleanup\\/\", \"_self\") } function nextStep(){ window.open(\"\\/cost\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Congratulations! With completion of this lab you have learned several best practices. Consider how you can implement these, and update the Well-Architected Review for your workloads: REL 10 How do you use fault isolation to protect your workload? - Use bulkhead architectures\nClick here to access the Well-Architected Tool   Previous Step Complete this lab     "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_organization_data_cur_connection/","title":"Level 300: Organization Data CUR Connection","tags":[],"description":"","content":"Last Updated December 2020\nAuthors  Stephanie Gooch, Commercial Architect (AWS)  Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com\nIntroduction This lab will show you how to combine your organizations information with your AWS Cost \u0026amp; Usage Report, this will enable you to view cost \u0026amp; usage in a way that is more relevant to your organization. It will guide you through the process of setting up an AWS Lambda function to extract the data from AWS Organizations, such as account ID and name, and place it into Amazon S3. From there, Amazon Athena will be able to read this data to produce a table that can be connected to your AWS Cost \u0026amp; Usage Report to enrich it.\nArchitecture Goals  Combine your AWS Organizations information with your CUR Allows you to view costs against accounts with names you provide enriching the data  Prerequisites  Access to the management AWS Account of the AWS Organisation to deploy a cross account role A sub account within the Organization Completed the Account Setup Lab 100_1_AWS_Account_Setup  Completed the Cost and Usage Analysis lab 200_4_Cost_and_Usage_Analysis  Completed the Cost Visualization Lab 200_5_Cost_Visualization   Permissions required Be able to create the below in the management account:\n IAM role and policy  Be able to create the below in a sub account where your CUR data is accessible:\n Amazon S3 Bucket AWS Lambda function IAM role and policy Amazon CloudWatch trigger Amazon Athena Table  Optional  Completed the Enterprise Dashboards lab 200_Enterprise_Dashboards   Costs  Estimated costs should be \u0026lt;$5 a month for small Organization Amazon QuickSight pricing   Time to complete  30 minutes  Steps:  Create Static Resources   Create Automation Resources   Utilize Organization Data Source   Visualize Organization Data in QuickSight   Join with the Enterprise Cost Intelligence Dashboard   Bonus Organization Tags   Teardown    function prevStep(){ window.open(\"\", \"_self\") } function nextStep(){ window.open(\".\\/1_create_static_resources_source\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool  Start Lab  "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_cur_queries/queries/global/","title":"Global Queries","tags":[],"description":"","content":"These are queries which return information about global usage.\nUse the clipboard in the top right of the text boxes below to copy all of the text to be pasted.\n You may need to change variables used as placeholders in your query. ${table_Name} is a common variable which needs to be replaced. Example: cur_db.cur_table\n Table of Contents   Account   Query Description This query will provide monthly unblended and amortized costs per linked account for all services. The query also includes ri_sp_trueup and ri_sip_upfront_fees columns to allow you to visualize the calculated difference between unblended and amortized costs. Unblended = Amortized + True-up + Upfront Fees, the +/- logic has already been included for you in the columns. We are showing in our example the exclusion of Route 53 Domains, as the monthly charges for these do not match between CUR and Cost Explorer. Finally we are excluding discounts, credits, refunds and taxes.\nPricing Please refer to the AWS pricing page .\nCost Explorer Links These links are provided as an example to compare CUR report output to Cost Explorer output.\nUnblended Cost Link Amortized Cost Link Sample Output: Download SQL File: Link to file Query Preview: SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-01') AS month_line_item_usage_start_date , sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0 ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) \u0026quot;sum_line_item_unblended_cost\u0026quot; , sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0 WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0 WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;reservation_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (\u0026quot;reservation_unused_amortized_upfront_fee_for_billing_period\u0026quot; + \u0026quot;reservation_unused_recurring_fee\u0026quot;) WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN 0 ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) \u0026quot;amortized_cost\u0026quot; , sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (-\u0026quot;savings_plan_amortized_upfront_commitment_for_billing_period\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (-\u0026quot;reservation_amortized_upfront_fee_for_billing_period\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN (-\u0026quot;line_item_unblended_cost\u0026quot; ) ELSE 0 END) \u0026quot;ri_sp_trueup\u0026quot; , sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN \u0026quot;line_item_unblended_cost\u0026quot; WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN \u0026quot;line_item_unblended_cost\u0026quot;ELSE 0 END) \u0026quot;ri_sp_upfront_fees\u0026quot; FROM ${table_name} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND line_item_usage_type != 'Route53-Domains' AND line_item_line_item_type != 'Tax' AND line_item_line_item_type != 'EdpDiscount' AND line_item_line_item_type != 'Credit' AND line_item_line_item_type != 'Refund' AND line_item_line_item_type != 'BundledDiscount' GROUP BY bill_payer_account_id, line_item_usage_account_id, 3 ORDER BY month_line_item_usage_start_date ASC, sum_line_item_unblended_cost DESC;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         Region   Query Description This query will provide monthly unblended and amortized costs per linked account for all services by region where the service is operating. The query also includes ri_sp_trueup and ri_sip_upfront_fees columns to allow you to visualize the calculated difference between unblended and amortized costs. Unblended = Amortized + True-up + Upfront Fees, the +/- logic has already been included for you in the columns. We are showing in our example the exclusion of Route 53 Domains, as the monthly charges for these do not match between CUR and Cost Explorer. Finally we are excluding discounts, credits, refunds and taxes.\nPricing Please refer to the AWS pricing page .\nCost Explorer Links These links are provided as an example to compare CUR report output to Cost Explorer output.\nUnblended Cost Link Amortized Cost Link Sample Output: Download SQL File: Link to file Query Preview: SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-01') AS month_line_item_usage_start_date, CASE product_region WHEN NULL THEN 'Global' WHEN '' THEN 'Global' WHEN 'global' THEN 'Global' ELSE product_region END AS product_region , sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0 ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) \u0026quot;sum_line_item_unblended_cost\u0026quot; , sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0 WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0 WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;reservation_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (\u0026quot;reservation_unused_amortized_upfront_fee_for_billing_period\u0026quot; + \u0026quot;reservation_unused_recurring_fee\u0026quot;) WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN 0 ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) \u0026quot;amortized_cost\u0026quot; , sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (-\u0026quot;savings_plan_amortized_upfront_commitment_for_billing_period\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (-\u0026quot;reservation_amortized_upfront_fee_for_billing_period\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN (-\u0026quot;line_item_unblended_cost\u0026quot; ) ELSE 0 END) \u0026quot;ri_sp_trueup\u0026quot; , sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN \u0026quot;line_item_unblended_cost\u0026quot; WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN \u0026quot;line_item_unblended_cost\u0026quot;ELSE 0 END) \u0026quot;ri_sp_upfront_fees\u0026quot; FROM ${table_name} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND line_item_usage_type != 'Route53-Domains' AND line_item_line_item_type != 'Tax' AND line_item_line_item_type != 'EdpDiscount' AND line_item_line_item_type != 'Credit' AND line_item_line_item_type != 'Refund' AND line_item_line_item_type != 'BundledDiscount' GROUP BY bill_payer_account_id, line_item_usage_account_id, 3, 4;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         Service   Query Description This query will provide monthly unblended and amortized costs per linked account for all services by service. We have additionally broken out Data Transfer for each service. The query also includes ri_sp_trueup and ri_sip_upfront_fees columns to allow you to visualize the calculated difference between unblended and amortized costs. Unblended = Amortized + True-up + Upfront Fees, the +/- logic has already been included for you in the columns. We are showing in our example the exclusion of Route 53 Domains, as the monthly charges for these do not match between CUR and Cost Explorer. Finally we are excluding discounts, credits, refunds and taxes.\nPricing Please refer to the AWS pricing page .\nCost Explorer Links These links are provided as an example to compare CUR report output to Cost Explorer output.\nUnblended Cost Link Amortized Cost Link Sample Output: Download SQL File: Link to file Query Preview: SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-01') AS month_line_item_usage_start_date, CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage' AND \u0026quot;product_product_family\u0026quot; = 'Data Transfer') THEN CONCAT('DataTransfer-',\u0026quot;line_item_product_code\u0026quot;) ELSE \u0026quot;line_item_product_code\u0026quot; END \u0026quot;service_line_item_product_code\u0026quot; , sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0 ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) \u0026quot;sum_line_item_unblended_cost\u0026quot; , sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0 WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0 WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;reservation_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (\u0026quot;reservation_unused_amortized_upfront_fee_for_billing_period\u0026quot; + \u0026quot;reservation_unused_recurring_fee\u0026quot;) WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN 0 ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) \u0026quot;amortized_cost\u0026quot; , sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (-\u0026quot;savings_plan_amortized_upfront_commitment_for_billing_period\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (-\u0026quot;reservation_amortized_upfront_fee_for_billing_period\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN (-\u0026quot;line_item_unblended_cost\u0026quot; ) ELSE 0 END) \u0026quot;ri_sp_trueup\u0026quot; , sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN \u0026quot;line_item_unblended_cost\u0026quot; WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN \u0026quot;line_item_unblended_cost\u0026quot;ELSE 0 END) \u0026quot;ri_sp_upfront_fees\u0026quot; FROM ${table_name} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND line_item_usage_type != 'Route53-Domains' AND line_item_line_item_type != 'Tax' AND line_item_line_item_type != 'EdpDiscount' AND line_item_line_item_type != 'Credit' AND line_item_line_item_type != 'Refund' AND line_item_line_item_type != 'BundledDiscount' GROUP BY bill_payer_account_id, line_item_usage_account_id, 3, 4 ORDER BY month_line_item_usage_start_date ASC, sum_line_item_unblended_cost DESC;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         Bill Details by Service   Query Description This query will provide a monthly cost summary by AWS Service Charge which is an approximation to the monthly bill in the billing console.\nPricing Please refer to the AWS pricing page .\nSample Output: Download SQL File: Link to file Query Preview: SELECT DATE_FORMAT((line_item_usage_start_date),'%Y-%m-01') AS month_line_item_usage_start_date, bill_bill_type, CASE WHEN (\u0026quot;product_product_family\u0026quot; = 'Data Transfer') THEN 'Data Transfer' ELSE replace(replace(replace(\u0026quot;product_product_name\u0026quot;, 'Amazon '),'Amazon'),'AWS ') END \u0026quot;product_product_name\u0026quot;, product_location, line_item_line_item_description, sum(line_item_unblended_cost) AS round_sum_line_item_unblended_cost, sum(line_item_usage_amount) AS sum_line_item_usage_amount FROM ${table_name} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') GROUP BY 1, bill_bill_type, 3, product_location, line_item_line_item_description HAVING sum(line_item_usage_amount) \u0026gt; 0 ORDER BY month_line_item_usage_start_date, bill_bill_type, product_product_name, product_location, line_item_line_item_description;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback       CUR queries are provided as is. We recommend validating your data by comparing it against your monthly bill and Cost Explorer prior to making any financial decisions. If you wish to provide feedback on these queries, there is an error, or you want to make a suggestion, please email: curquery@amazon.com\n "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_enterprise_dashboards/","title":"Level 200: Enterprise Dashboards","tags":[],"description":"","content":"Last Updated December 2020\nYour browser doesn't support video, or if you're on GitHub head to https://wellarchitectedlabs.com to watch the video.  Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: quicksightcostdashboards@amazon.com\nIntroduction The goal of the Enterprise Dashboards is to remove the complexities of cost \u0026amp; usage analysis, and provide enterprises with a clear understanding of something, to enable them to make the right business decisions quickly. The Enterprise Dashboard are made up of multiple templates known as modules to help you gain insight into different aspects of your cost and usage as well as enable your teams to better understand the cost of their applications and opportunities to optimize. Every dashboard complements the other modules so you can grow your reporting analytics and gain additional insight. Using separate modules provides greater flexibility, allowing you to customize existing modules and take advantage of the new templates without overwriting your existing customizations. If the dashboards were in a single report it would overwrite all customizations each time you create the latest template. This hands-on lab will guide you through the steps to copy and customize the QuickSight dashboard to better leverage your cost and usage report.\n The Cost Intelligence Dashboard is an interactive, customizable and business accessible QuickSight dashboard to help customers create the foundation for their own Cost Management and Optimization reporting tool. The Data Transfer Dashboard allows your organization to understand their data transfer cost and usage across all AWS products so you can take action on optimization opportunities.  Interested in a detailed description of the dashboards options to get the dashboards in a single view? Download read the FAQ Note: This QuickSight dashboard is not an official AWS dashboard and should be used as a self-service tool. We recommend validating your data by comparing the aggregate un-grouped Payer and Linked Account spend for a prior month.\nGoals  Create the Cost Intelligence dashboard Distribute your dashboards in your organization  Prerequisites  An AWS Account with Cost Optimization team permissions An Amazon Enterprise Edition QuickSight Account A Cost and Usage Report (CUR) Amazon Athena and QuickSight have been setup Completed the Cost and Usage Analysis lab  Completed the Cost and Usage Visualization lab  Requested template access here   Permissions required  Log in as the Cost Optimization team, created in AWS Account Setup  Access to AWS CLI  Costs  Small accounts approximately \u0026lt;$5 when using your free QuickSight trial  Time to complete  Cost Intelligence Dashboard: Should take approximately 45-60 minutes to complete  Optional Advanced Setup: Should take approximately 15-30 minutes to complete   Data Transfer Dashboard: Should take approximately 15-20 minutes to complete  Steps:  Create Cost Intelligence Dashboard   Modify Cost Intelligence Dashboard   Create Data Transfer Cost Analysis Dashboard   Distribute Dashboards   Tear down    function prevStep(){ window.open(\"\", \"_self\") } function nextStep(){ window.open(\".\\/1_create_cost_intelligence\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool  Start Lab  "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_1_aws_account_setup/8_cost_tags/","title":"Enable AWS-Generated Cost Allocation Tags","tags":[],"description":"","content":"Enabling AWS-Generated Cost Allocation Tags, generates a cost allocation tag containing resource creator information. This is automatically applied to resources that are created, and allows you to view and allocate costs based on who created a resource.\n  Log in to your Master account as an IAM user with the required permissions, and go to the Billing console:   Select Cost Allocation Tags from the left menu:   Click on Activate to enable the tags:   You will see that it is activated:    function prevStep(){ window.open(\"..\\/7_cost_explorer\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/9_tear_down\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_remote_configuration_installation_and_viewing_cloudwatch_logs/8_query_from_athena/","title":"Query logs from S3 using Athena","tags":[],"description":"","content":"With your log data now stored in S3, you will utilize Amazon Athena - a serverless interactive query service. You will run SQL queries on your log files to extract information from them. In this section, we will focus on the Apache access logs, although Athena can be used to query any of your log files. It is possible to query your log data from CloudWatch Insights, however, Athena querying allows you to pull data from files stored in S3, as well as other sources, where Insights only allows to query data in CloudWatch. Athena supports SQL querying - an industry standard language.\n  Open up the Athena console .\n  If this is the first time you are using Athena:\n Click Get Started to go to the Query Editor. Set up a query result location by clicking the link that appears at the top of the page.    If this is not the first time you are using Athena:\n Set up a query result location by clicking Settings in the top right corner of the page.    Enter the following into the Query result location field, replacing REPLACE_ME_BUCKETNAME with the name of the S3 bucket you created, likely wa-lab-\u0026lt;your-account-id\u0026gt;-\u0026lt;date\u0026gt;.\n  s3://REPLACE_ME_BUCKETNAME/athenaqueries/\nClick Save.  You should now see the blank query editor, as seen in the image below. This is where you will enter SQL queries to manipulate and extract information from your log files.  Enter the following command in New query 1 box to create a new Database , which will hold the Table containing our log data. This command creates a database called security_lab_logs.  CREATE database security_lab_logs Press Run query to execute this command. Once complete, you should see Query successful. displayed in the results box. On the left side menu, click the dropdown under Database and select the newly created database called security_lab_logs.  You will create a table within this database to hold our logs. Click the plus icon next to New query 1 to open a new query editor tab. Copy the SQL code below into the editor to create a table from our log data.  Replace REPLACE_ME_BUCKET with the name of the bucket you created to your logs in S3, likely wa-lab-\u0026lt;your-last-name\u0026gt;-\u0026lt;date\u0026gt;. You will need to identify the folder your logs are in for REPLACE_ME_STRING. Follow these steps to identify the path.  Open the S3 console . Open the bucket you created, likely wa-lab-\u0026lt;your-last-name\u0026gt;-\u0026lt;date\u0026gt;. Open the lablogs folder. You should see a folder with a long, random looking string (e.g. c848ff11-df30-481c-8d9f-5805741606d3). This string is what you should use for REPLACE_ME_STRING.      CREATE EXTERNAL TABLE IF NOT EXISTS `security_lab_apache_access_logs` ( request_date string, request_timestamp string, request_ip string, request_method string, request string, response_code int, response_size int, user_client_data string, garbage string ) ROW FORMAT SERDE \u0026#39;org.apache.hadoop.hive.serde2.RegexSerDe\u0026#39; WITH SERDEPROPERTIES ( \u0026#34;input.regex\u0026#34; = \u0026#34;([^ ]*)T([^ ]*)Z ([^ ]*) (?:[^ ]* [^ ]*) (?:[^\\\u0026#34;]*) \\\u0026#34;([^ ]*) ([^\\\u0026#34;]*)\\\u0026#34; ([^ ]*) ([^ ]*) (?:\\\u0026#34;[^\\\u0026#34;]*\\\u0026#34;) (\\\u0026#34;[^\\\u0026#34;]*\\\u0026#34;)([^\\n]*)\u0026#34; ) LOCATION \u0026#39;s3://REPLACE_ME_BUCKET/lablogs/REPLACE_ME_STRING/apache-access-logs/\u0026#39; TBLPROPERTIES ( \u0026#39;compressionType\u0026#39; = \u0026#39;gzip\u0026#39; ); Lets break this down a little.\n The CREATE EXTERNAL TABLE... statement creates your new table and defines its columns, such as request_date, request_timestamp, and so on. The ROW FORMAT SERDE statement specifies that the table rows are formatted using the RegEx SerDe (serializer/deserializer). The WITH SERDEPROPERTIES statement specifies the RegEx input format of your log files. This is how your raw log data is converted into columns. The LOCATION statement specifies the source of your table data, which is the S3 bucket containing your log files. The TBLPROPERTIES statement specifies that your log files are initially compressed using the GZIP format.  Click Run query. One it is successfully finished, you should see your new 'security_lab_apache_access_logs' table in the left side menu under Tables. Next, you will query data from this table. Click the plus icon next to New query 2 to open a new query editor tab. First, view your whole table. Copy the following commands into the query editor and press Run query. You should see the table in the Results box.  SELECT * FROM security_lab_logs.security_lab_apache_access_logs limit 15 Lets say you want to view only a certain column from this table, such as the response code frequency and size of the response. Replace the query you just made with the code below to do so.  SELECT response_code, count(response_code) AS count FROM security_lab_logs.security_lab_apache_access_logs WHERE response_code IS NOT NULL GROUP BY response_code ORDER BY count desc This isolates the response_code and response_size columns from your table and creates a new column called count, which is the frequency of each response type.\nIf you needed to track different metrics for your workload, you can always use different Athena queries to do so, but for the purposes of this lab, we will just be focusing on response code frequency.\n Click Run query. In the Results box, you should see a table similar to the one below.  Recap: In this section, you analyzed information from your workloadss log files using Amazon Athena. Although you only focused on the response codes and sizes in this lab, Athena can be used to query any data from S3, making it a powerful tool to analyze log files without directly accessing them. This demonstrates the best practices of enabling people to perform actions at a distance and keeping people away from data. Youve been able to minimize direct interaction with your data and instances - first by using Systems Manager for configuration, and now through S3 and Athena for log analysis.\n function prevStep(){ window.open(\"..\\/7_export_to_s3\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/9_create_quicksight_dashboard\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/100_labs/100_inventory_patch_management/8_cleanup/","title":"Removing Lab Resources","tags":[],"description":"","content":" NoteWhen the lab is complete, remove the resources you created. Otherwise you will be charged for any resources that are not covered in the AWS Free Tier.\n 7.1 Remove resources created with CloudFormation  Navigate to the CloudFormation dashboard at https://console.aws.amazon.com/cloudformation/:  Select your first stack. Choose Actions and choose delete stack. Select your second stack. Choose Actions and choose delete stack .   Navigate to Systems Manager console at https://console.aws.amazon.com/systems-manager/:  Choose State Manager. Select the association you created. Choose Delete.   If you created an S3 bucket to store detailed output, delete the bucket and associated data:  Navigate to the S3 console https://s3.console.aws.amazon.com/s3/. Select the bucket. Choose Delete and provide the bucket name to confirm deletion.   If you created the optional SNS Topic, delete the SNS topic:  Navigate to the SNS console https://console.aws.amazon.com/sns/. Select your AdminAlert SNS topic from the list. Choose Actions and select Delete topics.   If you created a Maintenance Window, delete the Maintenance Window:  Navigate to the Systems Manager console at https://console.aws.amazon.com/systems-manager/. Choose Maintenance Windows. Select the maintenance window you created. Choose Delete. In the Delete maintenance window window, choose Delete.   If you do not intend to continue to use the Administrator account you created, delete the account:  Navigate to the IAM console at https://console.aws.amazon.com/iam/. Choose Users. Select your user from the list. Choose Delete user. Select the check box next to \u0026ldquo;One or more of these users have recently accessed AWS. Deleting them could affect running systems. Check the box to confirm that you want to delete these users.\u0026rdquo;. Choose Yes, delete. When next you navigate within the console you will be returned to the account login page.   If you do intend to continue to use the Administrator account you created, we strongly suggest you enable MFA .  Thank you for using this lab.\n function prevStep(){ window.open(\"..\\/7_create_sns_topic\\/\", \"_self\") } function nextStep(){ window.open(\"\\/cost\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment or workload, you should complete a milestone in the Well-Architected tool. This lab specifically helps you with OPS5 - \u0026ldquo;How do you reduce defects, ease remediation, and improve flow into production?\u0026quot;\nClick here to access the Well-Architected Tool   Previous Step Complete this lab     "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/8_cleanup/","title":"Tear down this lab","tags":[],"description":"","content":"If you are attending an in-person workshop and were provided with an AWS account by the instructor:\n There is no need to tear down the lab. Feel free to continue exploring. Log out of your AWS account when done.  If you are using your own AWS account:\n You may leave these resources deployed for as long as you want. When you are ready to delete these resources, see the following instructions  Remove manually provisioned resources Some resources were created by the failure simulation scripts. You need to remove these first\n Go to the Network ACL console  Look at the NACL entries for the VPC called ResiliencyVPC For any of these NACLs that are not Default do the following  Select the NACL Actions \u0026raquo; Edit subnet associations Uncheck all boxes and click Edit Actions \u0026raquo; Delete network ACL    Remove AWS CloudFormation provisioned resources As part of lab setup you have deployed several AWS CloudFormation stacks. These directions will show you:\n How to delete an AWS CloudFormation stack In what specific order the stacks must be deleted  How to delete an AWS CloudFormation stack   Go to the AWS CloudFormation console: https://console.aws.amazon.com/cloudformation\n  Select the CloudFormation stack to delete and click Delete\n  In the confirmation dialog, click Delete stack\n  The Status changes to DELETE_IN_PROGRESS\n  Click the refresh button to update and status will ultimately progress to DELETE_COMPLETE\n  When complete, the stack will no longer be displayed. To see deleted stacks use the drop down next to the Filter text box.\n  To see progress during stack deletion\n Click the stack name Select the Events column Refresh to see new events    Delete workshop CloudFormation stacks  Since AWS resources deployed by AWS CloudFormation stacks may have dependencies on the stacks that were created before, then deletion must occur in the opposite order they were created Stacks with the same ordinal can be deleted at the same time. All stacks for a given ordinal must be DELETE_COMPLETE before moving on to the next ordinal  Single region If you deployed the single region option, then delete your stacks in the following order\n   Order CloudFormation stack     1 WebServersforResiliencyTesting   1 MySQLforResiliencyTesting       2 ResiliencyVPC   2 DeployResiliencyWorkshop    Multi region If you deployed the multi region option, then see these instructions for the order in which to delete the CloudFormation stacks Delete remaining resources Delete Lambda execution role used to create custom resource This role was purposely not deleted by the CloudFormation stack, because CloudFormation needs it to delete the custom resource it was used to create. Choose ONE: AWS CLI or AWS Console.\n Do this step only after ALL CloudFormation stacks are DELETE_COMPLETE  Using AWS CLI:\naws iam delete-role-policy --role-name LambdaCustomResourceRole-SecureSsmForRds --policy-name LambdaCustomResourcePolicy aws iam delete-role --role-name LambdaCustomResourceRole-SecureSsmForRds  Using AWS Console:\n Go to the IAM Roles Console: https://console.aws.amazon.com/iam/home#/roles Search for SecureSsmForRds Check the box next to LambdaCustomResourceRole-SecureSsmForRds Click Delete role button Click Yes, delete button  Delete Systems Manager parameter The password(s) for your Amazon RDS instances were stored in AWS Systems Manager secure parameter store. These steps will verify the parameter(s) were deleted, and if not then guide you to deleting them. Choose ONE: AWS CLI or AWS Console.\n single region You only need to do the following steps in us-east-2 multi region Do the following steps for both us-east-2 and **us-west-**2  Using AWS CLI:\n  In the following command use the workshop name supplied in step 1.4.4. when you ran the step function state machine. If you kept the defaults, the command will work as-is:\n aws ssm delete-parameter --name 300-ResiliencyofEC2RDSandS3    If you get ParameterNotFound then the password was already deleted by the CloudFormation stack (as expected).\nUsing AWS Console:\n Select the region Wait until ResiliencyVPC CloudFormation stack is DELETE_COMPLETE in the region Go to the AWS Console for AWS Systems Manager parameter store  Look for the parameter created for your infrastructure. If you used our default values, this will be named 300-ResiliencyofEC2RDSandS3 If it is not present (check all regions you deployed to) then you are finished If it is present then  Click on the parameter name Click the Delete button Click Delete again     References \u0026amp; useful resources  EC2 Auto Scaling Groups  What Is an Application Load Balancer?  High Availability (Multi-AZ) for Amazon RDS  Amazon RDS Under the Hood: Multi-AZ  Regions and Availability Zones  Injecting Chaos to Amazon EC2 using AWS System Manager  Build a serverless multi-region, active-active backend solution in an hour    function prevStep(){ window.open(\"..\\/7_failure_injection_optional\\/\", \"_self\") } function nextStep(){ window.open(\"\\/cost\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with REL 12 How do you test reliability?\nClick here to access the Well-Architected Tool   Previous Step Complete this lab     "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_certificate_manager_request_public_certificate/","title":"Level 200: AWS Certificate Manager Request Public Certificate","tags":[],"description":"","content":"Authors  Ben Potter, Security Lead, Well-Architected  Introduction AWS Certificate Manager is a service that lets you easily provision, manage, and deploy public and private Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with AWS services and your internal connected resources. SSL/TLS certificates are used to secure network communications and establish the identity of websites over the Internet as well as resources on private networks. AWS Certificate Manager removes the time-consuming manual process of purchasing, uploading, and renewing SSL/TLS certificates.\nGoals  Request AWS Certificate Manager public certificate  Prerequisites  An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . See pricing for further information on AWS Certificate Manager.  Steps:  Requesting a public certificate using the console   Tear down   "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_workload_efficiency/","title":"Level 200: Workload Efficiency","tags":[],"description":"","content":"Last Updated May 2020\nYour browser doesnt support video, or if you're on GitHub head to https://wellarchitectedlabs.com to watch the video.  Authors  Nathan Besh, Cost Lead, Well-Architected  Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com\nIntroduction This hands-on lab will guide you through the steps to measure the efficiency of a workload. It shows you how to get the overall efficiency, then look deeper for patterns in usage to be able to allocate different weights to different outputs of a system.\nThe lab uses a simple web application to demonstrate the efficiency, but will teach you the techniques so that it can be applied to ANY workload you have, whether its born in the cloud or legacy.\nThe first time you perform this lab it is recommended to use the sample files supplied, then you can use your own application and billing files for each workload you have.\nGoals  Setup the applicaion data source Combine the application and cost data sources Create the visualization for efficiency  Prerequisites  An AWS Account An Amazon QuickSight Account Amazon Athena and QuickSight have been setup Completed the Cost and Usage Analysis lab  Completed the Cost and Usage Visualization lab   Permissions required  Log in as the Cost Optimization team, created in AWS Account Setup   Costs  \u0026lt;$5 depending on the size of your data sources, and existing QuickSight subscription  Time to complete  The lab should take approximately 50-60 minutes to complete  Steps:  Create the Data Sources   Create the efficiency data source   Create the Visualizations   Teardown    function prevStep(){ window.open(\"\", \"_self\") } function nextStep(){ window.open(\".\\/1_data_sources\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool  Start Lab  "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_control_programmatic_access/","title":"Quest: Control Programmatic Access","tags":[],"description":" ","content":" Labs coming soon  "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_goals_and_targets/8_data_transfer/","title":"Data Transfer","tags":[],"description":"","content":"Data Transfer Data transfer modelling   Goal: Reduce unnecessary data transfer Target: Data transfer pattern between 1st \u0026amp; 2nd tier, must be within 10% of the pattern between the internet and 1st tier Best Practice: Data transfer modelling  Measures: Number of reports delivered, number of teams delivering Good/Bad: Bad Why? When does it work well or not?: Hard to convert to $, but it could help to indicate there is an issue - but again hard to quantify the cost of the issue Contact/Contributor: natbesh@amazon.com   function prevStep(){ window.open(\"..\\/7_pricing_models\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/9_manage_demand_supply\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_licensing/","title":"Level 200: Licensing","tags":[],"description":"","content":" Your browser doesn't support video, or if you're on GitHub head to https://wellarchitectedlabs.com to watch the video.  Authors  Nathan Besh, Cost Lead, Well-Architected (AWS)  Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com\nIntroduction This hands-on lab will guide you through analyzing your cost and usage for licensing costs, and the cost of licensing in your workloads. You will then be shown how to analyze the data and decide if it is beneficial to change to non-licensed software. In this lab we show the techniques using Operating System licences, but these techniques can be applied to any licensed software.\nYou will first setup a data source (if required) using the sample provided. This is a Cost and Usage report that contains licensed usage. You may use your own cost and usage report, however you need to ensure there is licensed usage and modify the examples in the lab.\nYou analyze the CUR to discover the costs of operating system licenses, and als the cost of running licensed operating systems. With this data you will make an analysis of the cost savings by switching to an unlicensed operating system. We have also provided an additional data set with the changes applied, to simulate this change and verify the savings.\nGoals  Discover the cost of licensed software in your cost and usage Analyze and understand the benefit of moving to unlicensed software  Prerequisites  An AWS Account (Optional) Your own Cost and Usage Report with licensed software (RHEL - RedHat linux) usage Completed the AWS Account Setup lab  Completed the Cost and Usage Analysis lab   Permissions required  Log in as the Cost Optimization team, created in AWS Account Setup  The following additional permissions: ec2:DescribeImages, ec2:DescribeVpcs, ec2:DescribeSubnets are optional, as you can complete the lab without them - however you will not be able to access pages in the EC2 console  Costs  Approximately \u0026lt;$5  Time to complete  The lab should take approximately 15-20 minutes to complete  Steps:  Create Pricing Data Source   Analyze and Understand Licensing   Teardown    function prevStep(){ window.open(\"\", \"_self\") } function nextStep(){ window.open(\".\\/1_pricing_sources\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool  Start Lab  "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_cur_queries/queries/machine_learning/","title":"Machine Learning","tags":[],"description":"","content":"These are queries for AWS Services under the Machine Learning product family .\nUse the clipboard in the top right of the text boxes below to copy all of the text to be pasted.\n You may need to change variables used as placeholders in your query. ${table_Name} is a common variable which needs to be replaced. Example: cur_db.cur_table\n Table of Contents   Amazon Rekognition   Query Description This query will provide daily unblended and usage information per linked account for Amazon Rekognition. The output will include detailed information about the usage type and usage region. The cost will be summed by day, account, and usage type, and displayed in descending order.\nPricing Please refer to the Rekognition pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d') AS day_line_item_usage_start_date, line_item_usage_type, product_region, SUM(CAST(line_item_usage_amount AS double)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS decimal(16,8))) AS sum_line_item_unblended_cost FROM {$table_name} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND line_item_product_code = 'AmazonRekognition' AND line_item_line_item_type NOT IN ('Tax','Credit','Refund','EdpDiscount','Fee','RIFee') GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d'), line_item_usage_type, product_region ORDER BY sum_line_item_unblended_cost DESC;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         Amazon SageMaker   Query Description This query will provide daily unblended cost and usage information per resource ID for Amazon SageMaker. The output will include detailed information about associated usage types. The cost and usage will be summed by day, account, resource ID, and usage type, and displayed in descending order.\nPricing Please refer to the SageMaker pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d') as day_line_item_usage_start_date, line_item_resource_id, line_item_usage_type, SUM(CAST(line_item_usage_amount AS double)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS decimal(16,8))) AS sum_line_item_unblended_cost FROM {$table_name} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND line_item_product_code = 'AmazonSageMaker' AND line_item_line_item_type NOT IN ('Tax','Credit','Refund','EdpDiscount','Fee','RIFee') GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d'), line_item_resource_id, line_item_usage_type ORDER by sum_line_item_unblended_cost DESC;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         Amazon Textract   Query Description This query will provide daily unblended and usage information per linked account for Amazon Textract. The output will include detailed information about the usage type and usage region. The cost and usage will be summed by day, account, and usage type, and displayed in descending order.\nPricing Please refer to the Textract pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d') AS day_line_item_usage_start_date, line_item_usage_type, product_region, SUM(CAST(line_item_usage_amount AS double)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS decimal(16,8))) AS sum_line_item_unblended_cost FROM {$table_name} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND line_item_product_code = 'AmazonTextract' AND line_item_line_item_type NOT IN ('Tax','Credit','Refund','EdpDiscount','Fee','RIFee') GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d'), line_item_usage_type, product_region ORDER BY sum_line_item_unblended_cost DESC;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback       CUR queries are provided as is. We recommend validating your data by comparing it against your monthly bill and Cost Explorer prior to making any financial decisions. If you wish to provide feedback on these queries, there is an error, or you want to make a suggestion, please email: curquery@amazon.com\n "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_remote_configuration_installation_and_viewing_cloudwatch_logs/9_create_quicksight_dashboard/","title":"Create a QuickSight Visualization","tags":[],"description":"","content":"Amazon QuickSight is a Business Intelligence service that allows you to streamline delivery of data insights. Prior to building out a dashboard, you need to prepare QuickSight by creating an account and linking your table from Athena. Using QuickSight, you will build out a dashboard to display responses to site visits on your website.\n Open the QuickSight console . If this is your first time accessing QuickSight, you will need to create your QuickSight account.  You may be prompted to sign up for QuickSight - click Sign up for QuickSight. Ensure that the AWS Account number shown matches your AWS Account. If not, click the log in again link to sign in with the correct account. Choose the edition you would like to use. For this lab, the Standard edition is sufficient. On the next screen, you will need to configure some options.  First, select the QuickSight region. This should be the same region you have been operating in for the rest of the lab. Enter a QuickSight account name and Notification email address. Ensure that the Amazon Athena box is checked. This allows QuickSight to access Athena databases. Check the Amazon S3 box, on the screen that follows. Click the tick box next to the bucket you created for this lab, likely called wa-lab-\u0026lt;your-last-name\u0026gt;-\u0026lt;date\u0026gt;, also select the Write permission for Athena Workgroup tick box for that bucket. This gives permissions for QuickSight to access data in the S3 bucket and write to the Athena Workgroup used for querying. Click Finish.   Click Finish. On the next screen, click Go to QuickSight.    If this is not your first time using QuickSight, you will need to give QuickSight permission to access log files from your S3 bucket.  Click on your user icon in the top right corner of the screen. Click Manage QuickSight. Click Security \u0026amp; Permissions. QuickSight may prompt you to change your region to N. Virginia to do so. Click on the user icon and change your region if needed. Under QuickSight access to AWS Services, click Add or remove. Tick the tick box next to Amazon Athena. This allows QuickSight to access Athena databases. Tick the tick box next to Amazon S3. If S3 is already ticked, click **Details **and then Select S3 buckets. Click the tick box next to the bucket you created for this lab, likely called wa-lab-\u0026lt;your-last-name\u0026gt;-\u0026lt;date\u0026gt;, also select the Write permission for Athena Workgroup tick box for that bucket. This gives permissions for QuickSight to access data in the S3 bucket and write to the Athena Workgroup used for querying. Click Finish. Click Update. Click on the QuickSight logo in the top right corner of the page to return to the QuickSight homepage. If you changed your region to N. Virginia to change permissions, switch it back to the region you have been operating in for this lab.   Now you can create your analysis dashboard. Click the New analysis button in the top left corner of the screen. Click the New dataset button in the top left corner of the screen to link your Athena database to QuickSight. On the screen that appears, click Athena Enter a Data source name, such as security-lab-log-data. Leave the Athena workgroup as [ primary ]. Click Create data source. In the Choose your table screen, click the dropdown menu and select security_lab_logs, the database you created in Athena. In the table selection menu that appears, select security_lab_apache_access_logs. Click Select.  In the Finish data set creation screen, select the Directly query your data radio bubble. Click Visualize. You should now see a screen similar to the one below.  Now, you will create a bar graph showing frequency of response types by day. To do so, click on the Vertical bar chart in the Visual types section in the bottom left. You will see three Field wells at the top of the page. You will set these fields by dragging and dropping fields from the left side menu into the field wells.  Set X axis to request_date. Set the Value and Group/Color to response_code.    Your graph should be created similar to the one above. You have now created a graph that displays the count of each response type your page received each day. Since this architecture was deployed for this lab, your graph will be very simple - with only one date on the X Axis. Now, you can publish this analysis into a QuickSight dashboard. Click the Share button in the top right corner of the screen. In the dropdown menu, click Publish dashboard. Select Publish new dashboard as in the pop-up menu. Enter a name, such as Security-Lab-CW-Apache-Responses. Click Publish dashboard.  This will take you to your new dashboard with a pop-up window to share the dashboard with users. This allows you to send the dashboard to other people. For the purposes of this lab, you can close out of this window by clicking the X in the top right corner. You should now see your QuickSight dashboard. This dashboard will be very simple, only containing the one graph created in your analysis. However, this should elucidate the purpose of QuickSight. You are able to communicate important information about your website to a broad audience, all without directly touching the data needed.  Recap: In this section, you built out a QuickSight visualization to display the responses generated by hits on your website. In doing this, you create a digestible source of data and information that anyone can understand. This also demonstrates the security best practices of analyzing logs centrally and keeping people away from data, as your log files are centrally graphed in QuickSight and the source log data is abstracted from viewers on many levels.\n function prevStep(){ window.open(\"..\\/8_query_from_athena\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/10_recap\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_1_aws_account_setup/9_tear_down/","title":"Tear down","tags":[],"description":"","content":"This exercise covered fundamental steps that are required for all AWS accounts to enable Cost Optimization. There is no specific tear down for this lab.\nCancel your QuickSight subscription To cancel your QuickSight subscription follow the steps below.\n  Click on your profile icon in the top right, select Manage QuickSight:   Click on Account settings:   Cluck on Unsubscribe:   Review the notifications, click Unsubscribe:    function prevStep(){ window.open(\"..\\/8_cost_tags\\/\", \"_self\") } function nextStep(){ window.open(\"\\/cost\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment or workload, you should complete a milestone in the Well-Architected tool. This lab specifically helps you with COST2 - \u0026ldquo;How do you govern usage?\u0026rdquo; and COST3 - \u0026ldquo;How do you monitor usage and cost?\u0026quot;\nClick here to access the Well-Architected Tool   Previous Step Complete this lab     "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_cloudfront_for_web_application/","title":"Level 200: CloudFront for Web Application","tags":[],"description":"","content":"Authors  Ben Potter, Security Lead, Well-Architected  Introduction This hands-on lab will guide you through the steps to help protect a web application from network based attacks using Amazon CloudFront. You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy CloudFront. Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nGoals  Protecting network and host-level boundaries System security configuration and maintenance Enforcing service-level protection  Prerequisites  An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab. A web application to configure as the origin to CloudFront.  Steps:  Configure CloudFront - EC2 or Load Balancer   Tear down   "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_detect_and_investigate_events/","title":"Quest: Detect &amp; Investigate Events","tags":[],"description":" ","content":"Authors  Ben Potter, Security Lead, Well-Architected  About this Guide This guide will help you improve your security in the AWS Well-Architected area of Detective Controls . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nPrerequisites  An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .   Automated Deployment of Detective Controls Introduction This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail, AWS Config, and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of each service.\nStart the Lab!   Enable Security Hub Introduction AWS Security Hub gives you a comprehensive view of your high-priority security alerts and compliance status across AWS accounts. There are a range of powerful security tools at your disposal, from firewalls and endpoint protection to vulnerability and compliance scanners. But oftentimes this leaves your team switching back-and-forth between these tools to deal with hundreds, and sometimes thousands, of security alerts every day. With Security Hub, you now have a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services, such as Amazon GuardDuty, Amazon Inspector, and Amazon Macie, as well as from AWS Partner solutions. Your findings are visually summarized on integrated dashboards with actionable graphs and tables. You can also continuously monitor your environment using automated compliance checks based on the AWS best practices and industry standards your organization follows. Get started with AWS Security Hub in just a few clicks in the Management Console and once enabled, Security Hub will begin aggregating and prioritizing findings.\nStart the Lab!   Further Learning: AWS CloudTrail User Guide AWS CloudFormation User Guide Amazon GuardDuty User Guide AWS Config User Guide "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_goals_and_targets/9_manage_demand_supply/","title":"Manage Demand &amp; Supply Resources","tags":[],"description":"","content":"Manage Demand, and Supply Resources Supply resources dynamically Demand vs Supply  Goal: Minimize unused resources Target: Workload resourcing should only deviate by a maximum of $x from workload demand Best Practice: Dynamic-based supply  Measures: Deviation % and $ Good/Bad: Good Why? When does it work well or not?: Ensure there is not an excess of resources compared to the workload demand, and if there is variable demand that the resourcing follows this closely. Contact/Contributor: natbesh@amazon.com  Supply resources Supply vs business/demand  Goal: Minimize unused resources Target: Non-production or development/test workloads must not have more than 5% (by cost) of resources available outside of business hours Best Practice: Time-based supply  Measures: Deviation % Good/Bad: Good Why? When does it work well or not?: Can be hard to track when development/test activities occur outside of defined hours. Contact/Contributor: natbesh@amazon.com   Manage demand Supply vs business/demand  Goal: Minimize unused resources Target: Workload demand should not vary by more than 10% on any single day. Best Practice: Manage Demand  Measures: Deviation % Good/Bad: Good Why? When does it work well or not?: Will be unachievable when demand cannot be altered. Works well when demand can be throttled or buffered to smooth out the peaks. Contact/Contributor: natbesh@amazon.com   function prevStep(){ window.open(\"..\\/8_data_transfer\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/10_evaluate_new_services\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cost_journey/","title":"Level 200: Cost Journey","tags":[],"description":"","content":" Your browser doesn't support video, or if you're on GitHub head to https://wellarchitectedlabs.com to watch the video.  Last Updated March 2021\nAuthors  Nathan Besh, Cost Lead Well-Architected (AWS) Tom McMeekin, Solutions Architect (AWS)  Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com\nIntroduction In this lab you will create your organizations cost optimization journey. It will show you where your workload and organization is currently at, in terms of capability, and what lies ahead - so you can plan and resource accordingly.\nThe lab will create a lambda function, which reads all the AWS Well-Architected reviews in the account, and produce a webpage with an image showing your journey, as below.\nGoals  Create a cost optimization journey for each workload with a Well-Architected review  Prerequisites  Performed at least one AWS Well-Architected review on a workload  Permissions required  Create and manage an S3 bucket Create an IAM role to run a lambda function Create and run a lambda function Access to the Well-Architected service  Costs  Estimated costs are \u0026lt;$5 for an average customer (lambda execution, s3 storage)  Time to complete  15minutes  Steps:  Configure Services   Create Journey   Teardown    function prevStep(){ window.open(\"\", \"_self\") } function nextStep(){ window.open(\".\\/1_configure_services\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool  Start Lab  "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_cur_queries/queries/management__governance/","title":"Management &amp; Governance","tags":[],"description":"","content":"These are queries for AWS Services under the Management \u0026amp; Governance product family .\nUse the clipboard in the top right of the text boxes below to copy all of the text to be pasted.\n You may need to change variables used as placeholders in your query. ${table_Name} is a common variable which needs to be replaced. Example: cur_db.cur_table\n Table of Contents   AWS Config   Query Description This query will provide daily unblended and usage information per linked account for AWS Config. The output will include detailed information about the usage type and usage region. The cost will be summed by day, account, and usage type.\nPricing Please refer to the AWS Config pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d') AS day_line_item_usage_start_date, product_region, CASE WHEN line_item_usage_type LIKE '%%ConfigurationItemRecorded%%' THEN 'ConfigurationItemRecorded' WHEN line_item_usage_type LIKE '%%ActiveConfigRules%%' THEN 'ActiveConfigRules' WHEN line_item_usage_type LIKE '%%SecurityHubConfigRules%%' THEN 'SecurityHubConfigRules' WHEN line_item_usage_type LIKE '%%ConfigRuleEvaluations%%' THEN 'ConfigRuleEvaluations' ELSE 'Others' END AS case_line_item_usage_type, SUM(CAST(line_item_usage_amount AS double)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS decimal(16,8))) AS sum_line_item_unblended_cost FROM ${tableName} WHERE (year = '2020' AND month IN ('1','01') OR year = '2020' AND month IN ('2','02')) AND product_product_name = 'AWS Config' AND line_item_line_item_type NOT IN ('Tax','Credit','Refund','EdpDiscount','Fee','RIFee') GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d'), product_region, line_item_usage_type ORDER BY day_line_item_usage_start_date, sum_line_item_usage_amount, sum_line_item_unblended_cost, case_line_item_usage_type;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         AWS CloudTrail   Query Description This query will provide monthly unblended and usage information per linked account for AWS CloudTrail. The output will include detailed information about the usage type and usage region. The cost will be summed by month, account, and usage type, and displayed in descending order.\nPricing Please refer to the CloudTrail pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,'%Y-%m') AS month_line_item_usage_start_date, product_product_name, SPLIT_PART(line_item_usage_type,'-',2) as split_line_item_usage_type, SUM(CAST(line_item_usage_amount AS double)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS decimal(16,8))) AS sum_line_item_unblended_cost FROM ${table_name} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND product_product_name = 'AWS CloudTrail' AND line_item_line_item_type NOT IN ('Tax','Credit','Refund','EdpDiscount','Fee','RIFee') GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,'%Y-%m'), SPLIT_PART(line_item_usage_type,'-',2), product_product_name ORDER BY sum_line_item_unblended_cost desc, month_line_item_usage_start_date, sum_line_item_usage_amount;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         AWS CloudWatch   Query Description This query will provide monthly unblended and usage information per linked account for AWS CloudWatch. The output will include detailed information about the usage type. The cost will be summed by month, account, and usage type, and displayed in descending order.\nPricing Please refer to the CloudWatch pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,'%Y-%m') month_line_item_usage_start_date, CASE WHEN line_item_usage_type LIKE '%%Requests%%' THEN 'Requests' WHEN line_item_usage_type LIKE '%%DataProcessing-Bytes%%' THEN 'DataProcessing' WHEN line_item_usage_type LIKE '%%TimedStorage-ByteHrs%%' THEN 'Storage' WHEN line_item_usage_type LIKE '%%DataScanned-Bytes%%' THEN 'DataScanned' WHEN line_item_usage_type LIKE '%%AlarmMonitorUsage%%' THEN 'AlarmMonitors' WHEN line_item_usage_type LIKE '%%DashboardsUsageHour%%' THEN 'Dashboards' WHEN line_item_usage_type LIKE '%%MetricMonitorUsage%%' THEN 'MetricMonitor' WHEN line_item_usage_type LIKE '%%VendedLog-Bytes%%' THEN 'VendedLogs' WHEN line_item_usage_type LIKE '%%GMD-Metrics%%' THEN 'GetMetricData' ELSE 'Others' END AS line_item_usage_type, line_item_operation, SUM(CAST(line_item_usage_amount AS double)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS decimal(16,8))) AS sum_line_item_unblended_cost FROM ${tableName} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND product_product_name = 'AmazonCloudWatch' AND line_item_line_item_type NOT IN ('Tax','Credit','Refund','EdpDiscount','Fee','RIFee') GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,'%Y-%m'), line_item_usage_type, line_item_operation ORDER BY sum_line_item_unblended_cost desc;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback       CUR queries are provided as is. We recommend validating your data by comparing it against your monthly bill and Cost Explorer prior to making any financial decisions. If you wish to provide feedback on these queries, there is an error, or you want to make a suggestion, please email: curquery@amazon.com\n "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_remote_configuration_installation_and_viewing_cloudwatch_logs/10_recap/","title":"Lab Recap","tags":[],"description":"","content":"In this lab, you explored Well-Architected security best practices in the context of monitoring application and system logs.\nFirst, you deployed a CloudFormation template containing an EC2 web server instance, an S3 bucket, and networking infrastructure for the lab. Then, you installed, configured, and started up the CloudWatch agent remotely. Using Systems Manager Run Command demonstrates the best practices of enabling people to perform actions at a distance and reducing attack surface by enabling you to close ports on your instance and avoid having to SSH directly into it. Storing the agent configuration file in Parameter Store highlighted the best practice of configuring services and resources centrally by maintaining reusable configuration data in AWS.\nThen, you generated some logs and viewed them in CloudWatch, illustrating the principle of analyzing logs centrally as you were able to view all your raw log data in a single location in CloudWatch. From there, you exported these logs to S3. This provides a method to store logs long term more cost-effectively in CloudWatch. Doing this facilitates the best practice of configuring logging centrally by enabling you to extract meaningful insights from large volumes of log data.\nIn Athena, you were able to query this S3 data using serverless SQL commands. You created a table within a database to track responses to site visits. In QuickSight, you directly used the results of your work in Athena to create a visualization. This keeps with the theme of enabling people to perform actions from a distance by abstracting the raw log data from users in both QuickSight and Athena. Additionally, you analyzed logs centrally in both services, generating useful insights from your application.\nOverall, you should now have a better understanding of how to collect log data in a secure manner. You kept people away from directly accessing data and instances, which reduced the exposure of your workload. You also centrally implemented service and application logging, a valuable practice to monitor and investigate any security threats.\n function prevStep(){ window.open(\"..\\/9_create_quicksight_dashboard\\/\", \"_self\") } function nextStep(){ window.open(\"..\\/11_teardown\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool   Previous Step Next Step     "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_cloudfront_with_waf_protection/","title":"Level 200: CloudFront with WAF Protection","tags":[],"description":"","content":"Authors  Ben Potter, Security Lead, Well-Architected  Introduction This hands-on lab will guide you through the steps to protect a workload from network based attacks using Amazon CloudFront and AWS Web Application Firewall (WAF). You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy CloudFront with WAF integration to apply defense in depth methods. Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nGoals  Protecting network and host-level boundaries System security configuration and maintenance Enforcing service-level protection  Prerequisites  An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab.  Steps:  Launch Instance   Configure AWS WAF   Configure Amazon CloudFront   Tear down this lab   "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_defend_against_new_threats/","title":"Quest: Defend Against New Threats","tags":[],"description":" ","content":" Labs coming soon Check out:\n AWS Security Blog  AWS Security Bulletins    "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_goals_and_targets/10_evaluate_new_services/","title":"Evaluate New Services","tags":[],"description":"","content":"Evaluate new services Develop a workload review process  Goal: Perform Well-Architected reviews throughout a workloads lifecycle Target: For Tier1 workloads each stage has a full review, for Tier2 and below workloads the minimum requirement is: In the development stage review Security, Reliability and Operational Excellence. In Performance test review Performance Efficiency and Cost Optimization. Best Practice: Optimize Over Time  Measures: % of workloads reviewed in each stage Good/Bad: Good Why? When does it work well or not?: Ensures the correct level review is performed inline with organization requirements. It also allows for tradeoffs if the organization is prioritizing speed to market for some workloads. Contact/Contributor: natbesh@amazon.com   Review and analyze this workload regularly  Goal: Perform Well-Architected reviews regularly on active workloads Target: Tier1 workloads will have milestones with all pillars completed at least every 3 months, for Tier2 workloads milestones with all pillars are completed at least every 6 months, all other workloads are to have milestones with all pillars completed yearly. Best Practice: Optimize Over Time  Measures: Number of milestones completed, number of teams delivering Good/Bad: Good Why? When does it work well or not?: Ensures workloads are actively maintained and inline with best practices. Will result in higher operational costs if there are minimal changes in the workload, and no additional services/features available. Contact/Contributor: natbesh@amazon.com   function prevStep(){ window.open(\"..\\/9_manage_demand_supply\\/\", \"_self\") } function nextStep(){ window.open(\"\\/cost\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your organization, you should complete a milestone in the Well-Architected tool. This lab specifically helps you with COST2 - \u0026ldquo;How do you govern usage?\u0026quot;\nClick here to access the Well-Architected Tool   Previous Step Complete this lab     "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_cur_queries/queries/networking__content_delivery/","title":"Networking &amp; Content Delivery","tags":[],"description":"","content":"These are queries for AWS Services under the Networking \u0026amp; Content Delivery product family .\nUse the clipboard in the top right of the text boxes below to copy all of the text to be pasted.\n You may need to change variables used as placeholders in your query. ${table_Name} is a common variable which needs to be replaced. Example: cur_db.cur_table\n Table of Contents   Amazon API Gateway   Query Description This query provides daily unblended cost and usage information about Amazon API Gateway usage including the resource id. The usage amount and cost will be summed and the cost will be in descending order.\nPricing Please refer to the Amazon API Gateway pricing page for more details.\nSample Output Download SQL File Link to file Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, SPLIT_PART(line_item_resource_id, 'apis/', 2) as split_line_item_resource_id, DATE_FORMAT(line_item_usage_start_date,'%Y-%m-%d') AS day_line_item_usage_start_date, CASE WHEN line_item_usage_type LIKE '%%ApiGatewayRequest%%' OR line_item_usage_type LIKE '%%ApiGatewayHttpRequest%%' THEN 'Requests' WHEN line_item_usage_type LIKE '%%DataTransfer%%' THEN 'Data Transfer' WHEN line_item_usage_type LIKE '%%Message%%' THEN 'Messages' WHEN line_item_usage_type LIKE '%%Minute%%' THEN 'Minutes' WHEN line_item_usage_type LIKE '%%CacheUsage%%' THEN 'Cache Usage' ELSE 'Other' END AS case_line_item_usage_type, SUM(CAST(line_item_usage_amount AS double)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS decimal(16,8))) AS sum_line_item_unblended_cost FROM ${table_name} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND product_product_name = 'Amazon API Gateway' AND line_item_line_item_type NOT IN ('Tax','Credit','Refund','EdpDiscount','Fee','RIFee') GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,'%Y-%m-%d'), line_item_resource_id, line_item_usage_type ORDER BY sum_line_item_unblended_cost DESC;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         Amazon CloudFront   Query Description This query provides daily unblended cost and usage information about Amazon CloudFront usage including the distribution name, region, and operation. The usage amount and cost will be summed and the cost will be in descending order.\nPricing Please refer to the Amazon CloudFront pricing page for more details.\nSample Output Download SQL File Link to file Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,'%Y-%m-%d') AS day_line_item_usage_start_date, product_region, product_product_family, -- NOTE: product_product_family used in place of large line_item_usage_type CASE line_item_operation, SPLIT_PART(line_item_resource_id, 'distribution/', 2) as split_line_item_resource_id, SUM(CAST(line_item_usage_amount AS double)) as sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS decimal(16,8))) as sum_line_item_unblended_cost FROM ${table_name} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND line_item_product_code = 'AmazonCloudFront' AND line_item_line_item_type NOT IN ('Tax','Credit','Refund','EdpDiscount','Fee','RIFee') GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,'%Y-%m-%d'), product_region, product_product_family, line_item_operation, line_item_resource_id ORDER BY sum_line_item_unblended_cost DESC;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         Data Transfer   Query Description This query provides daily unblended cost and usage information about Data Transfer usage including resource id that sourced the traffic, the product code corresponding to the source traffic, and the to/from locations of the usage. The usage amount and cost will be summed and the cost will be in descending order.\nPricing Please refer to each individual service pricing page for more details on how data transfer charges are handled for that service.\nSample Output Download SQL File Link to file Copy Query SELECT line_item_product_code, line_item_usage_account_id , date_format(line_item_usage_start_date,'%Y-%m-%d') AS date_line_item_usage_start_date, line_item_usage_type, product_from_location, product_to_location, product_product_family, line_item_resource_id, SUM(CAST(line_item_usage_amount AS double)) as sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS decimal(16,8))) as sum_line_item_unblended_cost FROM ${tableName} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND product_product_family = 'Data Transfer' AND line_item_line_item_type = 'Usage' AND line_item_line_item_type NOT IN ('Tax','Credit','Refund','EdpDiscount','Fee','RIFee') GROUP BY line_item_product_code, line_item_usage_account_id, date_format(line_item_usage_start_date, '%Y-%m-%d'), line_item_resource_id, line_item_usage_type, product_from_location, product_to_location, product_product_family ORDER BY sum_line_item_unblended_cost DESC;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         Data Transfer - MSK   Query Description This query provides monthly unblended cost and usage information about Data Transfer related to Amazon MSK including resource id. The usage amount and cost will be summed and the cost will be in descending order.\nPricing Please refer to the Amazon MSK pricing page for more details.\nSample Output: Download SQL File: Link to file Query Preview: SELECT line_item_product_code, line_item_usage_account_id, date_format(line_item_usage_start_date,'%Y-%m-%d') AS date_line_item_usage_start_date, line_item_resource_id, line_item_usage_type, line_item_line_item_description, product_product_family, sum(line_item_usage_amount)/1024 as sum_line_item_usage_amount, round(sum(line_item_unblended_cost),2) as sum_line_item_unblended_cost FROM ${table_name} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND line_item_product_code = 'AmazonMSK' AND line_item_usage_type LIKE '%DataTransfer%' AND line_item_line_item_type NOT IN ('Tax','Credit','Refund','EdpDiscount','Fee','RIFee') GROUP BY line_item_product_code, line_item_usage_account_id, date_format(line_item_usage_start_date, '%Y-%m-%d'), line_item_resource_id, line_item_usage_type, product_product_family, line_item_line_item_description ORDER BY sum_line_item_unblended_cost desc    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         AWS Direct Connect   Query Description: The query will output AWS Direct Connect charges split by Direct Connect port charges and Data Transfer charges for a specific resource using Direct Connect. They query will output port speed metrics and transfer source and destination locations.\nPricing Please refer to the AWS Direct Connect pricing page for more details.\nSample Output: Download SQL File: Link to file Query Preview: SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,'%Y-%m') AS month_line_item_usage_start_date, product_port_speed, product_product_family, product_transfer_type, product_from_location, product_to_location, product_direct_connect_location, pricing_unit, line_item_operation, line_item_resource_id, SUM(CAST(line_item_usage_amount AS double)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS decimal(16,8))) AS sum_line_item_unblended_cost FROM ${table_name} WHERE year = '2020' AND (month = BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09' ) AND product_product_name = 'AWS Direct Connect' AND product_transfer_type NOT IN ('IntraRegion Inbound','InterRegion Inbound') AND line_item_line_item_type NOT IN ('Tax','Credit','Refund','EdpDiscount','Fee','RIFee') GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,'%Y-%m'), line_item_resource_id, line_item_operation, product_port_speed, product_product_family, product_transfer_type, product_from_location, product_to_location, product_direct_connect_location, pricing_unit ORDER BY sum_line_item_unblended_cost Desc    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         NAT Gateway   Query Description This query provides monthly unblended cost and usage information about NAT Gateway Usage including resource id. The usage amount and cost will be summed and the cost will be in descending order.\nPricing Please refer to the VPC pricing page for more details.\nSample Output: Download SQL File: Link to file Query Preview: SELECT bill_payer_account_id, line_item_usage_account_id, line_item_resource_id, DATE_FORMAT(line_item_usage_start_date,'%Y-%m') AS month_line_item_usage_start_date, CASE WHEN line_item_usage_type LIKE '%%NatGateway-Bytes' THEN 'NAT Gateway Data Processing Charge' -- Charge for per GB data processed by NatGateways WHEN line_item_usage_type LIKE '%%NatGateway-Hours' THEN 'NAT Gateway Hourly Charge' -- Hourly charge for NAT Gateways ELSE line_item_usage_type END AS line_item_usage_type, SUM(CAST(line_item_usage_amount AS double)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS decimal(16,8))) AS sum_line_item_unblended_cost FROM ${table_name} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND product_product_family = 'NAT Gateway' AND line_item_line_item_type NOT IN ('Tax','Credit','Refund','EdpDiscount','Fee','RIFee') GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,'%Y-%m'), line_item_resource_id, line_item_usage_type ORDER BY sum_line_item_unblended_cost DESC, sum_line_item_usage_amount;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         NAT Gateway Idle   Query Description This query shows cost and usage of NAT Gateways which didn\u0026rsquo;t receive any traffic last month and ran for more than 336 hrs. Resources returned by this query could be considered for deletion.\nPricing Please refer to the VPC pricing page for more details.\nSample Output: Download SQL File: Link to file Query Preview: SELECT bill_payer_account_id, line_item_usage_account_id, SPLIT_PART(line_item_resource_id, ':', 6) split_line_item_resource_id, product_region, pricing_unit, sum_line_item_usage_amount, CAST(cost_per_resource AS decimal(16, 8)) AS \u0026quot;sum_line_item_unblended_cost\u0026quot; FROM ( SELECT line_item_resource_id, product_region, pricing_unit, line_item_usage_account_id, bill_payer_account_id, SUM(line_item_usage_amount) AS sum_line_item_usage_amount, SUM(SUM(line_item_unblended_cost)) OVER (PARTITION BY line_item_resource_id) AS cost_per_resource, SUM(SUM(line_item_usage_amount)) OVER (PARTITION BY line_item_resource_id, pricing_unit) AS usage_per_resource_and_pricing_unit, COUNT(pricing_unit) OVER (PARTITION BY line_item_resource_id) AS pricing_unit_per_resource FROM ${table_name} WHERE line_item_product_code = 'AmazonEC2' AND line_item_usage_type like '%Nat%' -- get previous month AND month = cast(month(current_timestamp + -1 * interval '1' MONTH) AS VARCHAR) -- get year for previous month AND year = cast(year(current_timestamp + -1 * interval '1' MONTH) AS VARCHAR) AND line_item_line_item_type = 'Usage' GROUP BY line_item_resource_id, product_region, pricing_unit, line_item_usage_account_id, bill_payer_account_id ) WHERE -- filter only resources which ran more than half month (336 hrs) usage_per_resource_and_pricing_unit \u0026gt; 336 AND pricing_unit_per_resource = 1 ORDER BY cost_per_resource DESC    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         AWS Transit Gateway   Query Description This query provides monthly unblended cost and usage information about AWS Transit Gateway Usage including attachment type, and resource id. The usage amount and cost will be summed and the cost will be in descending order.\nPricing Please refer to the TGW pricing page for more details.\nSample Output: Download SQL File: Link to file Query Preview: SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,'%Y-%m') AS month_line_item_usage_start_date, CASE WHEN line_item_resource_id like 'arn%' THEN CONCAT(SPLIT_PART(line_item_resource_id,'/',2),' - ',product_location) ELSE CONCAT(line_item_resource_id,' - ',product_location) END AS line_item_resource_id, product_location, product_attachment_type, pricing_unit, CASE WHEN pricing_unit = 'hour' THEN 'Hourly charges' WHEN pricing_unit = 'GigaBytes' THEN 'Data processing charges' END AS pricing_unit, SUM(CAST(line_item_usage_amount AS double)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS decimal(16,8))) AS sum_line_item_unblended_cost FROM ${table_name} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND product_group = 'AWSTransitGateway' AND line_item_line_item_type NOT IN ('Tax','Credit','Refund','EdpDiscount','Fee','RIFee') GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,'%Y-%m'), line_item_resource_id, product_location, product_attachment_type, pricing_unit ORDER BY sum_line_item_unblended_cost DESC, month_line_item_usage_start_date, sum_line_item_usage_amount, product_attachment_type;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         Network Usage   Query Description This query provides daily unblended cost and usage information about AWS Network Usage including VPCPeering, PublicIP, InterZone, LoadBalancing, and resource id. Usage will be in ascending order and cost will be in descending order.\nPricing The Pricing Calculator is a useful tool for assisting with cost estimates for data transfer costs. To aid in Cost Analysis we highly recommend implementing the Data Transfer Cost Analysis Dashboard .\nSample Output Download SQL File Link to file Query Preview SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d') AS day_line_item_usage_start_date, line_item_operation, line_item_resource_id, SUM(CAST(line_item_usage_amount AS double)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS decimal(16,8))) AS sum_line_item_unblended_cost FROM ${table_name} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND line_item_operation IN ( 'LoadBalancing-PublicIP-In', 'LoadBalancing-PublicIP-Out', 'InterZone-In', 'InterZone-Out', 'PublicIP-In', 'PublicIP-Out', 'VPCPeering-In', 'VPCPeering-Out' ) AND line_item_line_item_type NOT IN ('Tax','Credit','Refund','EdpDiscount') GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d'), line_item_operation, line_item_resource_id ORDER BY day_line_item_usage_start_date ASC, sum_line_item_usage_amount DESC, sum_line_item_unblended_cost DESC;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback       CUR queries are provided as is. We recommend validating your data by comparing it against your monthly bill and Cost Explorer prior to making any financial decisions. If you wish to provide feedback on these queries, there is an error, or you want to make a suggestion, please email: curquery@amazon.com\n "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_remote_configuration_installation_and_viewing_cloudwatch_logs/11_teardown/","title":"Lab Teardown","tags":[],"description":"","content":"Deleting the QuickSight visualization\n Visit the QuickSight console . On the home screen, click the three dots under the analysis called security_lab_apache_access_log.... Click Delete, and click Delete again on the screen that pops up. Next, click on the Manage data button in the top right corner of the page. Under Your Data Sets, click on security_lab_apache_acc.... Click Delete data set and click Delete on the pop up window.  Deleting Athena database/table.\n Visit the Athena console . In the left side menu, under Tables, click the three dots next to security_lab_apache_access_logs. Click Delete table in the menu that appears. Click Yesin the pop up box Create a new query. Type DROP database security_lab_logs in the query editor and press Run query.  Deleting the Systems Manager stored parameter\n Visit the Systems Manager console . In the left side menu, click on Parameter store. Click the box next to your created parameter, likely called AmazonCloudWatch-securitylab-cw-config. Click Delete. Click Delete parameters on the pop up that appears.  Deleting the S3 Bucket\n Visit the S3 console . Click the button next to your created bucket, likely called wa-lab-\u0026lt;your-last-name\u0026gt;-\u0026lt;date\u0026gt;. Click Delete, follow the instructions in the pop-up to delete your bucket.  Tearing down the CloudFormation stack.\n Visit the CloudFormation console . Click on the stack you created for this lab, likely called security-cw-lab. Click Delete and then Delete stack on the window that pops up.   function prevStep(){ window.open(\"..\\/9_create_quicksight_dashboard\\/\", \"_self\") } function nextStep(){ window.open(\"\\/cost\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with SEC4 - How do you detect and investigate security events? and SEC6 - How do you protect your compute resources?\nClick here to access the Well-Architected Tool   Previous Step Complete this lab     "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_protect_networks/","title":"Quest: Protect Networks","tags":[],"description":" ","content":"Authors  Ben Potter, Security Lead, Well-Architected  About this Guide This guide will help you improve your security in the AWS Well-Architected area of Infrastructure Protection . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nPrerequisites  An AWS account that you are able to use for testing, that is not used for production or other purposes.\nNOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .   CloudFront with WAF Protection Introduction This hands-on lab will guide you through the steps to protect a workload from network based attacks using Amazon CloudFront and AWS Web Application Firewall (WAF). You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy CloudFront with WAF integration.\nStart the Lab!  "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_cur_queries/queries/security_identity__compliance/","title":"Security, Identity, &amp; Compliance","tags":[],"description":"","content":"These are queries for AWS Services under the Security, Identity, \u0026amp; Compliance product family .\nUse the clipboard in the top right of the text boxes below to copy all of the text to be pasted.\n You may need to change variables used as placeholders in your query. ${table_Name} is a common variable which needs to be replaced. Example: cur_db.cur_table\n Table of Contents   Amazon GuardDuty   Query Description This query provides daily unblended cost and usage information about Amazon GuardDuty Usage. The usage amount and cost will be summed.\nPricing Please refer to the Amazon GuardDuty pricing page for more details.\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d') AS day_line_item_usage_start_date, line_item_usage_type, TRIM(REPLACE(product_group, 'Security Services - Amazon GuardDuty ', '')) AS trim_product_group, pricing_unit, SUM(CAST(line_item_usage_amount AS double)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS decimal(16,8))) AS sum_line_item_unblended_cost FROM ${tableName} WHERE (year = '2020' AND month IN ('1','01') OR year = '2020' AND month IN ('2','02')) AND product_product_name = 'Amazon GuardDuty' AND line_item_line_item_type NOT IN ('Tax','Credit','Refund','EdpDiscount','Fee','RIFee') GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d'), line_item_usage_type, product_group, pricing_unit ORDER BY day_line_item_usage_start_date, sum_line_item_usage_amount, sum_line_item_unblended_cost, trim_product_group;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         Amazon Cognito   Query Description This query provides daily unblended cost and usage information about Amazon Cognito Usage. The usage amount and cost will be summed.\nPricing Please refer to the Amazon Cognito pricing page for more details.\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d') AS day_line_item_usage_start_date, product_product_name, line_item_operation, SUM(CAST(line_item_usage_amount AS double)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS decimal(16,8))) AS sum_line_item_unblended_cost FROM ${tableName} WHERE (year = '2020' AND month IN ('1','01') OR year = '2020' AND month IN ('2','02')) AND product_product_name = 'Amazon Cognito' AND line_item_line_item_type NOT IN ('Tax','Credit','Refund','EdpDiscount','Fee','RIFee') GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d'), product_product_name, line_item_operation ORDER BY day_line_item_usage_start_date, sum_line_item_usage_amount, sum_line_item_unblended_cost, line_item_operation;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         AWS WAF   Query Description This query provides daily unblended cost and usage information about AWS WAF Usage including web acl, rule id, and region. The usage amount and cost will be summed and the cost will be in descending order.\nPricing Please refer to the WAF pricing page for more details.\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,'%Y-%m-%d') AS day_line_item_usage_start_date, SPLIT_PART(SPLIT_PART(line_item_resource_id,'/',2),'+',1) AS split_webaclid_line_item_resource_id, SPLIT_PART(SPLIT_PART(line_item_resource_id,'/',2),'+',2) AS split_ruleid_line_item_resource_id, line_item_usage_type, product_group, product_group_description, product_location, product_location_type, line_item_line_item_description, pricing_unit, SUM(CAST(line_item_usage_amount AS double)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS decimal(16,8))) AS sum_line_item_unblended_cost FROM ${tableName} WHERE (year = '2020' AND month IN ('1','01') OR year = '2020' AND month IN ('2','02')) AND product_product_name = 'AWS WAF' AND line_item_line_item_type NOT IN ('Tax','Credit','Refund','EdpDiscount','Fee','RIFee') GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,'%Y-%m-%d'), line_item_resource_id, line_item_usage_type, product_group, product_group_description, product_location, product_location_type, line_item_line_item_description, pricing_unit ORDER BY day_line_item_usage_start_date, sum_line_item_usage_amount, sum_line_item_unblended_cost, product_group;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback       CUR queries are provided as is. We recommend validating your data by comparing it against your monthly bill and Cost Explorer prior to making any financial decisions. If you wish to provide feedback on these queries, there is an error, or you want to make a suggestion, please email: curquery@amazon.com\n "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_protect_compute/","title":"Quest: Protect Compute","tags":[],"description":" ","content":"Authors  Ben Potter, Security Lead, Well-Architected  About this Guide This guide will help you improve your security in the AWS Well-Architected area of Infrastructure Protection . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nPrerequisites  An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .   Automated Deployment of VPC Introduction This hands-on lab will guide you through the steps to configure an Amazon VPC and outline some of the AWS security features. AWS CloudFormation will be used to automate the deployment and provide a repeatable way to re-use the template after this lab.\nStart the Lab!  Automated Deployment of EC2 Web Application Introduction This hands-on lab will guide you through the steps to configure a web application in Amazon EC2 with a defense in depth approach. You must have first deployed the Automated Deployment of VPC lab.\nStart the Lab!  "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_cur_queries/queries/storage/","title":"Storage","tags":[],"description":"","content":"These are queries for AWS Services under the Storage product family .\nUse the clipboard in the top right of the text boxes below to copy all of the text to be pasted.\n You may need to change variables used as placeholders in your query. ${table_Name} is a common variable which needs to be replaced. Example: cur_db.cur_table\n Table of Contents   Amazon S3   Query Description This query provides daily unblended cost and usage information for Amazon S3. The output will include detailed information about the resource id (bucket name) and usage type. The usage amount and cost will be summed and the cost will be in descending order.\nPricing Please refer to the S3 pricing page . Please refer to understanding your AWS billing and usage reports for Amazon S3 to understand of of the usage types populated for S3 use.\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d') AS day_line_item_usage_start_date, line_item_resource_id, product_product_name, line_item_usage_type, CASE WHEN line_item_usage_type LIKE '%%EarlyDelete-ByteHrs' THEN 'S3: Early Delete - Glacier' WHEN line_item_usage_type LIKE '%%EarlyDelete-SIA' THEN 'S3: Early Delete - Standard Infrequent Access' WHEN line_item_usage_type LIKE '%%Requests-Tier3' THEN 'S3: API Requests - Glacier' WHEN (line_item_usage_type LIKE '%%Requests-Tier4' OR line_item_usage_type LIKE '%%Requests-SIA') THEN 'S3: API Requests - Standard Infrequent Access' WHEN (line_item_usage_type LIKE '%%Requests-Tier1' OR line_item_usage_type LIKE '%%Requests-Tier2') THEN 'S3: API Requests - Standard' WHEN line_item_usage_type LIKE '%%Retrieval-SIA' THEN 'S3: Data Retrieval - Standard Infrequent Access' WHEN line_item_usage_type LIKE '%%Peak-Restore-Bytes-Delta' THEN 'S3: Data Retrieval - Glacier' WHEN (line_item_usage_type LIKE '%%AWS-In-Bytes' OR line_item_usage_type LIKE '%%AWS-In-ABytes') THEN 'S3: Data Transfer - Region to Region (In)' WHEN (line_item_usage_type LIKE '%%AWS-Out-Bytes' OR line_item_usage_type LIKE '%%AWS-Out-ABytes') THEN 'S3: Data Transfer - Region to Region (Out)' WHEN line_item_usage_type LIKE '%%CloudFront-In-Bytes' THEN 'S3: Data Transfer - CloudFront (In)' WHEN line_item_usage_type LIKE '%%CloudFront-Out-Bytes' THEN 'S3: Data Transfer - CloudFront (Out)' WHEN line_item_usage_type LIKE '%%DataTransfer-Regional-Bytes' THEN 'S3: Data Transfer - Inter AZ' WHEN (line_item_usage_type LIKE '%%DataTransfer-In-aBytes' OR line_item_usage_type LIKE '%%DataTransfer-In-Bytes') THEN 'S3: Data Transfer - Internet (In)' WHEN (line_item_usage_type LIKE '%%DataTransfer-Out-aBytes' OR line_item_usage_type LIKE '%%DataTransfer-Out-Bytes') THEN 'S3: Data Transfer - Internet (Out)' WHEN line_item_usage_type LIKE '%%TimedStorage-GlacierByteHrs' THEN 'S3: Storage - Glacier' WHEN line_item_usage_type LIKE '%%TimedStorage-RRS-ByteHrs' THEN 'S3: Storage - Reduced Redundancy' WHEN (line_item_usage_type LIKE '%%TimedStorage-SIA-ByteHrs' OR line_item_usage_type LIKE '%%TimedStorage-SIA-SmObjects') THEN 'S3: Storage - Standard Infrequent Access' WHEN line_item_usage_type LIKE '%%TimedStorage-ByteHrs' THEN 'S3: Storage - Standard' ELSE 'Other Requests' END as line_item_usage_type_group, SUM(CAST(line_item_usage_amount AS double)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS decimal(16,8))) AS sum_line_item_unblended_cost FROM ${table_name} WHERE (year = '2020' AND month IN ('7','9') OR year = '2020' AND month IN ('07','09')) AND ((\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonGlacier%') OR (\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonS3%')) AND line_item_line_item_type NOT IN ('Tax','Credit','Refund','EdpDiscount','Fee','RIFee') GROUP BY 1,2,3,4,5,6 ORDER By day_line_item_usage_start_date ASC, line_item_usage_account_id, line_item_usage_type_group DESC, sum_line_item_unblended_cost DESC;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         Amazon EBS Snapshots   Query Description This query provides daily unblended cost and usage information about Amazon EBS Snapshot Usage per account including region. The usage amount and cost will be summed and the cost will be in descending order.\nPricing Please refer to the Amazon EBS pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,'%Y-%m-%d') AS date_line_item_usage_start_date, product_region, SUM(CAST(line_item_usage_amount AS double)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS decimal(16,8))) AS sum_line_item_unblended_cost FROM ${tableName} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND product_product_name = 'Amazon Elastic Compute Cloud' AND line_item_usage_type LIKE '%%EBS%%Snapshot%%' AND product_product_family LIKE 'Storage Snapshot' AND line_item_line_item_type NOT IN ('Tax','Credit','Refund','EdpDiscount','Fee','RIFee') GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,'%Y-%m-%d'), product_region ORDER BY sum_line_item_unblended_cost DESC, sum_line_item_usage_amount DESC, date_line_item_usage_start_date ASC;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         Amazon EBS Volumes   Query Description This query provides daily unblended cost and usage information about Amazon EBS Volume Usage per account. The usage amount and cost will be summed and the cost will be in descending order.\nPricing Please refer to the Amazon EBS pricing page . Please refer to the Amazon EBS Volume Charges page for more info on the calculations used on your bill.\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,'%Y-%m') AS month_line_item_usage_start_date, CASE SPLIT_PART(line_item_usage_type,':',2) WHEN 'VolumeUsage' THEN 'EBS - Magnetic' WHEN 'VolumeUsage.gp2' THEN 'EBS - SSD(gp2)' WHEN 'VolumeUsage.piops' THEN 'EBS - SSD(io1)' WHEN 'VolumeUsage.st1' THEN 'EBS - HDD(st1)' WHEN 'VolumeUsage.sc1' THEN 'EBS - HDD(sc1)' WHEN 'VolumeIOUsage' THEN 'EBS - I/O Requests' WHEN 'VolumeP-IOPS.piops' THEN 'EBS - Provisioned IOPS' ELSE SPLIT_PART(line_item_usage_type,':',2) END as line_item_usage_type, SUM(CAST(line_item_usage_amount AS double)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS decimal(16,8))) AS sum_line_item_unblended_cost FROM ${tableName} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND product_product_name = 'Amazon Elastic Compute Cloud' AND line_item_usage_type LIKE '%%EBS%%Volume%%' AND product_product_family IN ('Storage','System Operation') AND line_item_line_item_type NOT IN ('Tax','Credit','Refund','EdpDiscount','Fee','RIFee') GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,'%Y-%m'), line_item_usage_type ORDER BY sum_line_item_unblended_cost DESC;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         Amazon EFS   Query Description This query will provide daily unblended cost and usage information per linked account for Amazon EFS. The output will include detailed information about the resource id (File System), usage type, and API operation. The usage amount and cost will be summed and the cost will be in descending order.\nPricing Please refer to the Amazon EFS pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d') AS day_line_item_usage_start_date, SPLIT_PART(line_item_resource_id, 'file-system/', 2) AS split_line_item_resource_id, line_item_usage_type, product_product_family, product_storage_class, pricing_unit, line_item_operation, SUM(CAST(line_item_usage_amount AS double)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS decimal(16,8))) AS sum_line_item_unblended_cost FROM ${table_name} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND product_product_name = 'Amazon Elastic File System' AND line_item_line_item_type NOT IN ('Tax','Credit','Refund','EdpDiscount','Fee','RIFee') GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d'), line_item_usage_type, pricing_unit, product_product_family, product_storage_class, line_item_resource_id, line_item_operation ORDER BY day_line_item_usage_start_date, sum_line_item_usage_amount, sum_line_item_unblended_cost DESC, line_item_usage_type;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         Amazon FSx   Query Description This query will provide daily unblended cost and usage information per linked account for Amazon FSx. The output will include detailed information about the resource id (FSx file system), usage type, and Storage type. The usage amount and cost will be summed and the cost will be in descending order.\nPricing Please refer to the Amazon FSx pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d') AS day_line_item_usage_start_date, SPLIT_PART(line_item_resource_id, ':', 6) as split_line_item_resource_id, product_deployment_option, line_item_usage_type, product_product_family, pricing_unit, SUM(CAST(line_item_usage_amount AS double)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS decimal(16,8))) AS sum_line_item_unblended_cost FROM ${table_name} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND product_product_name = 'Amazon FSx' AND line_item_line_item_type NOT IN ('Tax','Credit','Refund','EdpDiscount','Fee','RIFee') GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d'), SPLIT_PART(line_item_resource_id, ':', 6), product_deployment_option, line_item_usage_type, product_product_family, pricing_unit ORDER BY day_line_item_usage_start_date, sum_line_item_usage_amount, sum_line_item_unblended_cost;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         AWS Backup   Query Description This query will provide daily unblended cost and usage information per linked account for AWS Backup. The output will include detailed information about the usage type, product family, pricing unit and others. The usage amount and cost will be summed and the cost will be in descending order.\nPricing Please refer to the AWS Backup pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date), '%Y-%m-%d') AS day_line_item_usage_start_date, pricing_unit, product_product_family, line_item_usage_type, line_item_operation, SPLIT_PART(line_item_usage_type, '-', 4) as split_line_item_usage_type, SUM(CAST(line_item_usage_amount AS double)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS decimal(16,8))) AS sum_line_item_unblended_cost FROM ${table_name} WHERE year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09') AND product_product_name like '%Backup%' AND line_item_line_item_type NOT IN ('Tax','Credit','Refund','EdpDiscount','Fee','RIFee') GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),'%Y-%m-%d'), line_item_usage_type, product_product_family, pricing_unit, line_item_operation ORDER BY day_line_item_usage_start_date, sum_line_item_usage_amount, sum_line_item_unblended_cost, line_item_usage_type;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback       CUR queries are provided as is. We recommend validating your data by comparing it against your monthly bill and Cost Explorer prior to making any financial decisions. If you wish to provide feedback on these queries, there is an error, or you want to make a suggestion, please email: curquery@amazon.com\n "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_classify_data/","title":"Quest: Classify Data","tags":[],"description":" ","content":"Labs coming soon Check out:\n Amazon Macie User Guide   "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_protect_data_at_rest/","title":"Quest: Protect Data at Rest","tags":[],"description":" ","content":"Authors  Ben Potter, Security Lead, Well-Architected  About this Guide This guide will help you improve your security in the AWS Well-Architected area of Data Protection . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nPrerequisites  An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .   Create a Data Bunker Account Introduction In this lab we will create a secure data bunker. A data bunker is a secure account which will hold important security data in a secure location. Ensure that only members of your security team have access to this account. In this lab we will create a new security account, create a secure S3 bucket in that account and then turn on CloudTrail for our organisation to send these logs to the bucket in the secure data account. You may want to also think about what other data you need in there such as secure backups.\nStart the Lab!   Further Learning  S3: Protecting Data Using Server-Side Encryption with AWS KMSManaged Keys  Opt-in to Default Encryption for New EBS Volumes   "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_protect_data_in_transit/","title":"Quest: Protect Data in Transit","tags":[],"description":" ","content":"Labs coming soon Check out:\n AWS Certificate Manager - Getting Started  AWS Encryption SDK  AWS Site-to-Site VPN User Guide  AWS Client VPN User Guide   "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_incident_response/","title":"Quest: Incident Response","tags":[],"description":" ","content":"Authors  Ben Potter, Security Lead, Well-Architected  About this Guide This guide will help you improve your security in the AWS Well-Architected area of Incident Response . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nPrerequisites  An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .   Incident Response with AWS Console and CLI Introduction This hands-on lab will guide you through a number of examples of how you could use the AWS Console and Command Line Interface (CLI) for responding to a security incident. It is a best practice to be prepared for an incident, and have appropriate detective controls enabled.\nStart the Lab!   Further Learning AWS Security Incident Response Guide "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_cur_queries/query_help/","title":"CUR Query Library Help","tags":[],"description":"","content":"The CUR Query Library Help section is intended to provide tips and information about navigating the CUR dataset. We will cover beginner topics like getting started with querying the CUR, filtering query results, common query format, links to public documentation, and getting product information. We will also cover advanced topics like understanding your AWS Cost Datasets while working with the CUR data.\nUse the clipboard in the top right of the text boxes below to copy all of the text to be pasted.\n You may need to change variables used as placeholders in your query. ${table_Name} is a common variable which needs to be replaced. Example: cur_db.cur_table\n Table of Contents   Cost and Usage Report Documentation Links   Helpful Links   Cost and Usage Report Data Dictionary   Cost and Usage Report User Guide   Understanding your AWS Cost Datasets   Cost and Usage Report Analysis Tutorial   SQL Tutorials   Introduction to Amazon Athena     function prevStep(){ window.open(\"\", \"_self\") }   Links Feedback         AWS Product Descriptions and Pricing Units   Query Description This query will provide AWS Product Descriptions and Pricing Units including Product Name, Product Family, and Operations. This query is intended to be used to help you understand the data published in the CUR and assist in query development.\nPricing Please refer to the AWS pricing page for any service you are interested in.\nSample Output Download SQL File Link to Code Copy Query SELECT product_product_name, product_product_family, line_item_operation, pricing_unit, product_description, product_usage_family FROM ${tableName} WHERE line_item_line_item_type = 'Usage' GROUP BY product_product_name, product_product_family, line_item_operation, pricing_unit, product_description, product_usage_family ORDER BY product_product_name, product_product_family, line_item_operation, pricing_unit, product_description, product_usage_family;    function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         Query Format Overview   Query Format Overview Below is an overview of the key elements of an Athena Query.\nThe SELECT statement is used to select data you require and the FROM specifies which table from which database.\nEach column from the data you would like returned is placed between the SELECT and the FROM separated by a comma.\nThe AS command is used to rename a column or table with an alias.\nThe WHERE clause is used to filter records. The WHERE clause is used to extract only those records that fulfil a specified condition. For example WHERE year = \u0026lsquo;2020\u0026rsquo;The GROUP BY clause divides the output of a SELECT statement into groups of rows containing matching values. A simple GROUP BY clause may contain any expression composed of input columns or it may be an ordinal number selecting an output column by position (starting at one).\nThe ORDER BY clause is used to sort a result set by one or more output expressions. These can be ascending ASC or descending DESC.\nAdditional Functions The CASE statement goes through conditions and returns a value when the first condition is met (like an IF-THEN-ELSE statement). So, once a condition is true, it will stop reading and return the result. If no conditions are true, it returns the value in the ELSE clause.\nWe may use this to add information to the Athena query based on what results we get back. The example below shows how we have added the Spot Instance item into the data if there is nothing in the pricing term column.\nCASE pricing_term WHEN 'Reserved' THEN 'Reserved Instance' WHEN 'OnDemand' THEN 'OnDemand' WHEN '' THEN 'Spot Instance' ELSE 'Other' END AS Reservation  The CAST() function converts a value (of any type) into a specified datatype. This is often use to change a column that\u0026rsquo;s is a Double to a Decimal. This is because a Double is used for binary and decimal floating-point arithmetic whereas a Decimal is more useful for financial reporting. You can see this in the example below.\nCAST(line_item_unblended_cost AS decimal(16,8))  The SUM() function returns the total sum of a numeric column. This is often used to calculate the total spend on a service to give you a complete total.\nSUM(CAST(line_item_unblended_cost AS decimal(16,8)))  The ROUND() function rounds a number to a specified number of decimal places.\n    Understanding Unblended vs Blended vs Net vs Amortized Cost and Usage Data   Cost Data - Understanding Cost Data Each customer is unique and so are your needs when it comes to viewing and reporting on cost data.\nUnblended and blended costs allow customers with consolidated billing to understand their cost and usage in management payer and linked member accounts. Unblended costs refers to the cost incurred for the usage by an individual account while blended costs refers to costs within a consolidated billing family looking at the total usage for all accounts compared to the linked member account\u0026rsquo;s usage. The ability to view linked account\u0026rsquo;s unblended cost data allows you to more easily reconcile your charges across your payer and linked accounts. [1]\nThe amortized cost metric reflects the effective cost of the upfront and monthly reservation fees spread across the billing period. By default, Cost Explorer shows the fees for Reserved Instances and Savings Plans as a spike on the day that you\u0026rsquo;re charged, but if you choose to show costs as amortized costs, the costs are amortized over the billing period.\nAmortization, simply stated is breaking down cost into an effective daily rate and enables you to see your costs in accrual-based accounting as opposed to cash-based accounting. For example within the context of a Reserved Instance, if you pay $365 for an All Upfront RI for one year and you have a matching instance that uses that RI, that instance costs you $1 a day, amortized. Unblended and blended costs would be instead report on the full initial $365 cost on the day of purchase. [2]\nIn the Well-Architected Labs CUR Query Library we are focused on examples showing unblended costs, amortized costs where applicable, and if applicable we will show true-up and upfront fee columns to allow you to calculate and visuallize the difference between unblended and amortized costs.\nUnblended vs Blended vs Amortized vs Net Unblended/Blended  Consolidated Billing / Management Accounts with Linked Member Accounts Applicable to Reserved Instances, Savings Plans, and On-Demand Instance, as well as pricing tiers (e.g. AWS Free Tier and S3 volume tiers). Unblended costs are associated with the current cost of the product, while blended rates are the averaged costs of the Reserved Instances, On-Demand Instances, and pricing tiered products that are used by member accounts in an organization in AWS Organizations. AWS calculates blended costs by multiplying the blended rate for each service with an account\u0026rsquo;s usage of that service. [4]\nAmortized Applicable to Reserved Instances and Savings Plans, amortizing is when you distribute one-time reservation costs across the billing period that is affected by that cost. For example, if you pay $365 for an All Upfront RI for one year and you have a matching instance that uses that RI, that instance costs you $1 a day, amortized.\nIn Cost Explorer amortized costs are estimated by combining unblended costs with the amortized portion of your upfront and recurring fees. The unused portion of your upfront fees and recurring charges are shown on the first of the month. [5]\nNet In some cases, customers operating at scale on AWS may be able to take advantage of specialized discounts. The net unblended costs reflect usage costs after these discounts are applied while the net amortized costs adds additional logic to amortize discount-related information, in addition to your Savings Plans or Reservation-related charges. [3]\nCUR Table Data You can see the data provided by Billing and Cost Management for you to calculate your unblended and amortized costs in the following Cost and Usage Reports columns.\nIn CUR many columns are dynamic, and their visibility in Cost and Usage Reports depends on the usage of product in the billing period. [6] You will need to adjust your queries for RIs and SPs depending on your purchase of these products during the billing period that you are querying.\nReserved Instances Unblended You can use the following columns to understand the unblended costs of your RIs for the billing period. The values for these columns appear for RI line items with reservation_reservation_a_r_n filled in. Note: When trying to calculate amortized total costs, lineItem/UnblendedCost should be removed for Fee line items.\nlineItem/UnblendedCost - CUR Column: line_item_unblended_cost - Description: Cost on the day that you're charged - Operation: Sum/Add - Filter: Not needed for Reserved Instances  Amortized You can use the following columns to understand the amortized costs of your RIs for the billing period. The values for these columns appear only for RI subscription line items (also known as RI Fee line items) and not for the actual instances using the RIs. [7] Typically you would add these two columns (reservation_unused_recurring_fee and reservation_unused_amortized_upfront_fee_for_billing_period) together, along with reservation_effective_cost documented below, to get the total amortized total costs. Additionally you will also need to ignore lineItem/UnblendedCost for Fee line items. See the examples at the bottom of this section for additional clarity.\nreservation/unusedRecurringFee - CUR Column: reservation_unused_recurring_fee - Description: Initial upfront RI fee amortized for Partial Upfront RIs and No Upfront RIs - Operation: Sum/Add - Filter: line_item_line_item_type = 'RIFee' reservation/unusedAmortizedUpfrontFeeForBillingPeriod - CUR Column: reservation_unused_amortized_upfront_fee_for_billing_period - Description: Unused portion of Initial upfront RI fee amortized for Partial Upfront RIs and No - Operation: Sum/Add - Filter: line_item_line_item_type = 'RIFee' lineItem/UnblendedCost - CUR Column: line_item_unblended_cost - Description: Cost on the day that you're charged [as filtered, this is the upfront RI fee] - Operation: Ignore (set to 0 in calculation) - Filter: Where reservation_reservation_a_r_n \u0026lt;\u0026gt; '' and line_item_line_item_type = 'Fee'  If you wish to compare unblended costs to amortized costs you can subtract the following reservation_amortized_upfront_fee_for_billing_period and add in the upfront RI fee from the line_item_unblended_cost, ignored right above in the amortized cost calculation.\nreservation/amortizedUpfrontFeeForBillingPeriod - CUR Column: reservation_amortized_upfront_fee_for_billing_period - Description: Initial upfront RI fee amortized for All Upfront RIs and Partial Upfront RIs - Operation: Sum/Subtract - Filter: line_item_line_item_type = 'RIFee'  The values for these columns appear for actual instances using RIs and represent the Discounted Usage (also known as DiscountedUsage line items). [8]\nreservation/EffectiveCost - CUR Column: reservation_effective_cost - Description: The sum of both the upfront and hourly rate of your RI, averaged into an effective hourly rate. - Additional Breakdown of this column: reservation_amortized_upfront_cost_for_usage + reservation_recurring_fee_for_usage - Operation: Sum/Add - Filter: Where line_item_line_item_type = 'DiscountedUsage'  Savings Plans Unblended You can use the following columns to understand the unblended costs of your SPs for the billing period. The values for these columns appear for SP line items with savings_plan_savings_plan_a_r_n filled in. When trying to calculate unblended costs, line_item_unblended_cost should be removed for SavingsPlanNegation line items. [9]\nlineItem/UnblendedCost - CUR Column: line_item_unblended_cost - Description: Cost on the day that you're charged - Operation: Sum/Add - Filter: line_item_line_item_type != 'SavingsPlanNegation'  Amortized You can use the following columns to understand the amortized costs of your SPs for the billing period. The values for these columns appear only for SP subscription line items (also known as SavingsPlanCoveredUsage and SavingsPlanRecurringFee line items) and not for the actual instances using the SPs. For the amortized costs you must also ignore SavingsPlanNegation and SavingsPlanUpfrontFee line items, these can be used to show you the difference between unblended and amortized costs.\nTypically you would add these two columns (savings_plan_savings_plan_effective_cost and savings_plan_total_commitment_to_date) together, and subtract the third (savings_plan_used_commitment). See the examples at the bottom of this section for additional clarity.\nsavingsPlan/SavingsPlanEffectiveCost - CUR Column: savings_plan_savings_plan_effective_cost - Description: Proportion of the SP monthly commitment amount for Upfront and Recurring - Operation: Sum/Add - Filter: line_item_line_item_type = 'SavingsPlanCoveredUsage' savingsPlan/TotalCommitmentToDate - CUR Column: savings_plan_total_commitment_to_date - Description: The total amortized upfront commitment and recurring commitment - Operation: Sum/Add - Filter: line_item_line_item_type = 'SavingsPlanRecurringFee' savingsPlan/UsedCommitment - CUR Column: savings_plan_used_commitment - Description: The total dollar amount of the Savings Plan commitment used - Operation: Sum/Subtract - Filter: line_item_line_item_type = 'SavingsPlanRecurringFee'  If you wish to compare unblended costs to amortized costs you can subtract the following two columns for AmortizedUpfrontCommitmentForBillingPeriod and the lineItem/UnblendedCost fropm the SavingsPlanNegation line items, and finally add the SavingsPlanUpfrontFee fee from the lineItem/UnblendedCost, ignored above in the amortized cost calculation. [10]\nsavingsPlan/AmortizedUpfrontCommitmentForBillingPeriod - CUR Column: savings_plan_amortized_upfront_commitment_for_billing_period - Description: The amount of upfront fee a Savings Plan subscription is costing you. Applies to all upfront and partial upfront. - Operation: Sum/Subtract - Filter: line_item_line_item_type = 'SavingsPlanRecurringFee' lineItem/UnblendedCost - CUR Column: line_item_unblended_cost - Description: Cost on the day that you're charged [as filtered, this is the upfront SP fee] - Operation: Sum/Subtract - Filter: line_item_line_item_type = 'SavingsPlanNegation' lineItem/UnblendedCost - CUR Column: line_item_unblended_cost - Description: Cost on the day that you're charged [as filtered, this is the Savings Plans discount applied] - Operation: Sum/Add - Filter: line_item_line_item_type = 'SavingsPlanUpfrontFee'  Example The following snippits from the SELECT portion of the SQL query shows you how to put the above documented logic into practical usage. We will use this same logic across queries where RIs and SPs are used in the CUR query library. When comparing Cost Explorer unblended costs to your CUR queries, use sum_line_item_unblended_cost. When comparing Cost Explorer amortized cost to your CUR queries, use amortized_cost. The ri_sp_trueup and ri_sp_upfront_fees are included in these examples to help you reconcile the difference between unblended and amortized costs. You will find that sum_line_item_unblended_cost = amortized_cost + ri_sp_trueup + ri_sp_upfront_fees. We have already taken care of the additions and subtractions in the values, so you can just add them together.\nAccounts have both RI and SP usage during the billing period\nDATE_FORMAT((line_item_usage_start_date),'%Y-%m-01') AS month_line_item_usage_start_date , sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0 ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) \u0026quot;sum_line_item_unblended_cost\u0026quot; , sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0 WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0 WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;reservation_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (\u0026quot;reservation_unused_amortized_upfront_fee_for_billing_period\u0026quot; + \u0026quot;reservation_unused_recurring_fee\u0026quot;) WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN 0 ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) \u0026quot;amortized_cost\u0026quot; , sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (-\u0026quot;savings_plan_amortized_upfront_commitment_for_billing_period\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (-\u0026quot;reservation_amortized_upfront_fee_for_billing_period\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN (-\u0026quot;line_item_unblended_cost\u0026quot; ) ELSE 0 END) \u0026quot;ri_sp_trueup\u0026quot; , sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN \u0026quot;line_item_unblended_cost\u0026quot; WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN \u0026quot;line_item_unblended_cost\u0026quot; ELSE 0 END) \u0026quot;ri_sp_upfront_fees\u0026quot;  Accounts have SP usage but no RI usage during the billing period\nDATE_FORMAT((line_item_usage_start_date),'%Y-%m-01') AS month_line_item_usage_start_date , sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0 ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) \u0026quot;sum_line_item_unblended_cost\u0026quot; , sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0 WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0 -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;reservation_effective_cost\u0026quot; -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (\u0026quot;reservation_unused_amortized_upfront_fee_for_billing_period\u0026quot; + \u0026quot;reservation_unused_recurring_fee\u0026quot;) -- WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN 0 ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) \u0026quot;amortized_cost\u0026quot; , sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (-\u0026quot;savings_plan_amortized_upfront_commitment_for_billing_period\u0026quot;) -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (-\u0026quot;reservation_amortized_upfront_fee_for_billing_period\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN (-\u0026quot;line_item_unblended_cost\u0026quot; ) ELSE 0 END) \u0026quot;ri_sp_trueup\u0026quot; , sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN \u0026quot;line_item_unblended_cost\u0026quot; -- WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN \u0026quot;line_item_unblended_cost\u0026quot; ELSE 0 END) \u0026quot;ri_sp_upfront_fees\u0026quot;  [1] https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/con-bill-blended-rates.html\n[2] https://docs.aws.amazon.com/cur/latest/userguide/amortized-reservation.html\n[3] https://aws.amazon.com/blogs/aws-cost-management/understanding-your-aws-cost-datasets-a-cheat-sheet/\n[4] https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/con-bill-blended-rates.html#Blended_CB\n[5] https://console.aws.amazon.com/cost-management/home?#/custom\n[6] https://docs.aws.amazon.com/cur/latest/userguide/product-columns.html\n[7] https://docs.aws.amazon.com/cur/latest/userguide/amortized-reservation.html\n[8] https://docs.aws.amazon.com/cur/latest/userguide/reservation-columns.html\n[9] https://docs.aws.amazon.com/cur/latest/userguide/cur-sp.html\n[10] https://docs.aws.amazon.com/cur/latest/userguide/Lineitem-columns.html\n  function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         Retrieving Data from CUR   We will learn how to find out what data is available for querying in the CUR files, this will show what columns there are.\u000bLets discuss a couple common examples when exploring the CUR dataset.\nExample 1 - Answers the question What are all the columns and data in the CUR table?  SELECT *\u000bFROM\u000b${table_name}\u000bLIMIT 10;  This is helpful when trying to understand what data is available in the CUR. Use the CSV Export functionality in Athena to obtain a copy of the query results making it is easier to see all of the columns and data.\nIf you are interested in just the column name, you can use the DDL statement: SHOW TABLES IN $your table name .\nExample 2 - Answers the question of What are all the columns from the CUR, where a specific value is in the column?  SELECT * FROM ${table_name}\u000bWHERE \u0026quot;line_item_line_item_type\u0026quot; LIKE '%Usage% LIMIT 10;  This is a helpful technique when searching for a particular Charge Type. Take note that the column values are case sensitive.\nExample 3 - Answers the question What services are available in my CUR?  SELECT DISTINCT \u0026quot;line_item_product_code\u0026quot; FROM ${table_name}\u000bLIMIT 10;\u000b This query shows the available services within your CUR. The SELECT DISTINCT statement is helpful when looking for unique values in a specific column.\nThe column product_product_name contains similar information and would be a better choice for querying if you are looking for third party marketplace products since line_item_product_code provides a unique id. Be advised that when using the column product_product_name certain line_item_line_item_type\u0026rsquo;s (EdpDiscount, PrivateRateDiscount) do not populate this column.\nExample 4 - Answers the question \u0026ldquo;What billing periods are available?\u0026rdquo;  SELECT distinct bill_billing_period_start_date FROM ${table_name}\u000bLIMIT 10;  Additional helpful getting started queries can be found in the in 200 level Cost and Usage Analysis Lab .\n  function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback         Filtering Query Results   Filtering query results will allow for precise data retrieval. The WHERE clause is used to extract only those records that fulfill a specified condition. Lets review an example where we match a filtered Cost Explorer view with a CUR query to achieve the same results.\n WHERE year = '2020' AND (month BETWEEN '8' AND '10') AND product_product_name = ('AWS Glue') AND line_item_usage_type LIKE '%Crawler% AND product_region = 'us-east-1 AND line_item_line_item_type NOT IN ('Tax','Credit','Refund')  We will explain what the query filter is doing as well as a screenshot of the equivalent filter in Cost Explorer:\n Time frame to be August to October 2020   The Service to be AWS Glue The region to be us-east-1 The usage type to include usage like %Crawler%   To exclude Charge Types like credit, refund, and tax  You can mimic this example using the following query:\nLink to Code You can configure Cost Explorer like the below screenshot:\nIn Usage Type just type Crawler and select all region-Crawler-DPU-Hour entries.\nYou should now be able to align the unblended cost for AWS Glue using both CUR and Cost Explorer. This is a common approach used to validate your CUR query is working as expected.\n  function prevStep(){ window.open(\"\", \"_self\") }   Help \u0026amp; Feedback       CUR queries are provided as is. We recommend validating your data by comparing it against your monthly bill and Cost Explorer prior to making any financial decisions. If you wish to provide feedback on these queries, there is an error, or you want to make a suggestion, please email: curquery@amazon.com\n Contributing Community contributions are encouraged and welcome. Please follow the Contribution Guide . The goal is to pull together useful CUR queries in to a single library that is open, standardized, and maintained.\nContributors Please refer to the CUR Query Library Contributors section .\n"},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_cur_queries/query_help/contribution-guide/","title":"Library Contribution Guide","tags":[],"description":"","content":"Contribution Process For ease of use and education, it is crucial we maintain uniformity and consistency. To contribute your query please follow the steps below:\n Develop your query following the CUR Library Style Guide rules defined below Execute and validate the query using your own Cost and Usage Report Validate your query results against your Cost Explorer Save your query as a .sql file without underscores as seperators (_i.e. rds_mysql_engine_query.sql NOT rdsmysqlenginequery.sql _). Clone the github repo and perform a pull request to contribute to the labs using this guide  Place your query in the folder \u0026lsquo;static/Cost/300_CUR_Queries/Code/\u0026lt; AWS Product Category Folder \u0026gt;/your_query.sql\u0026rsquo; (NOTE: please check below for more information about the folder structure)  Contributions will take between 2-3 weeks to review, validate and publish. Not all queries will be published if they overlap with an existing query.\n CUR Library Style Guide For a reference of a properly constructed query please reference the Examples below:\nSELECT SECTION RULES:\nIt is recommended to sanitize the query of your account and resource related data. Please reference the Fictitious names and numbers documentation. Be advised, during the review process, queries submitted with customer account data will be sanitized.\nTo fill fields with dummy data, use the following select statement (example shown fills bill_payer_account_id with dummy data):\n EXAMPLE: '111122223333' as bill_payer_account_id EXAMPLE: '444455556666' as line_item_usage_account_id  For line_item_resource_id there is not a reference to all resources in the safenames document. Where possible use:\n EXAMPLE: \u0026lt;resource id\u0026gt;' as line_item_resource_id  COLUMN NAMING:\nIf a column requires a specific name to be defined ( i.e. after running a sum function ), use the name of the outer most function followed by the column name:\n EXAMPLE: SUM(CAST(line_item_usage_amount AS double)) AS sum_line_item_usage_amount, EXAMPLE: SUM(CAST(line_item_usage_amount AS double)) AS sum_line_item_usage_amount, EXAMPLE: SPLIT_PART(line_item_resource_id, 'crawler/', 2) AS split_line_item_resource_id, EXAMPLE: SPLIT_PART(line_item_usage_type ,':',2) AS split_line_item_usage_type  For date_format use the value defined by the format:\n EXAMPLE: DATE_FORMAT(line_item_usage_start_date,'%Y-%m') AS month_line_item_usage_start_date, EXAMPLE: DATE_FORMAT(line_item_usage_start_date,'%Y-%m-%d') AS day_line_item_usage_start_date,  If multiple nested functions are used, use the name of the left-most function only followed by the column name:\n EXAMPLE: TRIM(REPLACE(product_group, 'Security Services - Amazon GuardDuty ', '')) AS trim_product_group,  FROM SECTION RULES:\nUse ${table_name} as the variable for the customer table name.\nWHERE SECTION RULES:\nTo aggregate data, always try to use the default CUR partitions as defined by the CUR CFN template. The data is partitioned on year and month. Below is an example on how we are formatting this WHERE statement:\n EXAMPLE: year = '2020' AND (month BETWEEN '7' AND '9' OR month BETWEEN '07' AND '09')  For month we use both mm vs. m as per the example above as previous CFN templates have included both formats.\nORDER BY SECTION RULES:\nOrder is currently at authors discretion. You can use the selected data including your functions used in your select, a column alias, or you can substitute with a column number. It is most readable if you use column aliases in the SELECT and ORDER BY clauses.\n EXAMPLE: ${table_name} EXAMPLE: ${payer_id}  OTHER:\n Variables use a dollar sign ($) curly brackets {} and a name with fields separated by underscore _. Rule: For fields without spaces, do not use quotes   around field name. Rule: Queries should end with a semi-colon. Rule: Review Domain Markdown Process defined below. Rule: Must be run on a CUR Athena Database before loaded. Compare data against the testing accounts Cost Explorer data.  Query Folders We have split the CUR queries up into folders matching the AWS Cloud Product groups . This is to enable users to easily find the query they are looking for based on the service they are interested in.\nWhen adding a query please consider these points:\n Check if the service your query focuses on has a AWS Product Category folder and add it in there if it does If not create a folder using the AWS Product Category name If it is a multi product query, choose the AWS Product Category with the most influence on the query If it crosses 3+ services or is a global query, use the Global folder If it is a unique CUR query not related to cost, use the Global folder and we will sort the location as needed  "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_cur_queries/query_help/contributors/","title":"CUR Query Library Contributors","tags":[],"description":"","content":"Introduction The CUR Library is intended to be an open source library of Cost \u0026amp; Usage Reports queries that can be shared. We want to thank all contributors both from Amazon and External parties for helping make this a robust tool for navigating the CUR.\nLibrary Authors  Chris Strzelczyk, Sr. TAM, AWS Enterprise Support Bill Pfeiffer, Sr. TAM, AWS Enterprise Support Stephanie Gooch, Cost Optimization Lead, AWS OPTICS Matthew Brend, Enterprise Support Lead, AWS Enterprise Support Jonathan Banas, Sr. TAM, AWS Enterprise Support Alee Whitman, Commercial Architect, AWS Optics Justin Marks, TAM, AWS Enterprise Support  Query Contributors  Alon Jupiter, Sr. TAM, AWS Enterprise Support Paul Abruzzo, Enterprise Support Lead, AWS Enterprise Support Benjamin Lecoq, Sr. TAM, AWS Enterprise Support Oleksandr Moskalenko, TAM, AWS Enterprise Support Rony Blum, Sr. TAM, AWS Enterprise Support Jonathan Rudge, Sr.TAM, AWS Enterprise Support Luis Osses, Sr. Edge Specialist Solutions Architect Alee Whitman, Commercial Architect, Optics Arabinda Pani, Sr. Partner Solutions Architect, Database Yuriy Prykhodko, Sr. TAM, AWS Enterprise Support  "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/builders_guide/","title":"Builders Guide for 300 - Testing for Resiliency of EC2, RDS, and S3","tags":[],"description":"","content":"Introduction This guide contains the instructions for how to build the Lambda functions, the web application, and the modifications needed for the AWS CloudFormation templates' parameters as well as the JSON passed to the AWS Step Functions state machine to perform the deployment.\nThis guide will also give some specific instructions on the limitations of how you can deploy and what AWS regions it can be run in.\nPrerequisites  An AWS Account that you are able to use for tesintg, that is not used for production or other purposes. Python installer program (pip) Go language development environment Comfort with JSON   License Documentation License Licensed under the Creative Commons Share Alike 4.0 license.\nBuilding and uploading the AWS Lambda Functions Each function also has a makefile included. This make file will use pip to install dependent packages, then zip the entire directory\u0026rsquo;s contents into a zip file that will be located one directory up. You can deploy these to the region you wish to run the Lambda functions using the AWS Command Line Interface (CLI) as follows:\n% cd \u0026lt;LambdaDirectory\u0026gt; % make % cd .. % aws s3 cp \u0026lt;lambda\u0026gt;.zip s3://\u0026lt;S3 bucket\u0026gt;/\u0026lt;directory prefix\u0026gt;/\u0026lt;lambda\u0026gt;.zip  Debugging the AWS Lambda Functions The Lambda functions are all written in Python. They can be run on the command line with the python debugger, pdb, as follows:\n% python -m pdb \u0026lt;lambda_function\u0026gt;.py  The lambda functions all have an event that is passed in the main function that can be used to test your environment. The parameters are the same as they are to the AWS Step Functions state machine:\n log_level: This is the python logger logging level. To make it verbose in the logs, use the value \u0026ldquo;DEBUG\u0026rdquo; region_name: This is the region that the infrastructure is going to be deployed to secondary_region_name: This is the region where the red replica for this region will be deployed. (optional) workshop: A name to be added to the tags of the deployed infrastructure cfn_region: This is the region where the bucket that contains the AWS CloudFormation template is located cfn_bucket: This is the name of the S3 bucket where the AWS CloudFormation template is stored. folder: This is the apparent \u0026ldquo;folder\u0026rdquo; (actually a key prefix) where the CloudFormation template is located in the cfn_bucket. boot_bucket: This is the bucket in the region_name where the boot scripts and executables are located. boot_prefix: This is the apparent \u0026ldquo;folder\u0026rdquo; (actually a key prefix) where the boot scripts and executables are located. boot_object: This is the script executed on the instances to bootstrap the application.  This is an JSON string that looks like the following:\n{ 'log_level' : 'DEBUG', 'region_name' : 'us-west-2', 'secondary_region_name' : 'us-east-2', 'workshop' : '300 - Testing for Resiliency', 'cfn_region' : 'us-east-2', 'cfn_bucket' : 'aws-well-architected-labs-ohio', 'folder' : 'Reliability/', 'boot_bucket' : 'aws-well-architected-labs-ohio', 'boot_prefix' : 'Reliability/', 'boot_object' : 'bootstrap300Reliability.sh', 'websiteimage' : 'https://aws-well-architected-labs-ohio.s3.us-east-2.amazonaws.com/images/Cirque_of_the_Towers.jpg' }  There is considerable \u0026ldquo;shared knowledge\u0026rdquo; between the state machine functions that is all hard-coded, like stack names.\nThe state machine passes state of stacks between functions to indicate if the stack has been deployed or not. These take the form of a nested JSON object:\n{ 'vpc' : { 'stackname' : 'ResiliencyVPC', 'status' : 'CREATE_COMPLETE' } }  There will be a status for each stack as they deploy to prevent any attempt to deploy when a previous stack is either not present, or not complete. The applications all have the relevant nested stacks passed in the debug event, so you need to ensure you test them in the same order that the state machine deploys them within.\nThe Troubleshooting guide has additional details on how to debug the function when it is executing in AWS Lambda.\nBuilding and Uploading the Web Application The web application is written in the Go programming langauage. You must have the go language installed where you are building the executable. There is also a makefile to build this application. You can also upload the executable using the same method as follows:\n% cd go % make % aws s3 cp FragileWebApp s3://\u0026lt;S3 bucket\u0026gt;/\u0026lt;diretory prefix\u0026gt;/FragileWebapp  The web application is very fragile in that it will always write an entry on every hit it receives. This will cause the application to be tightly coupled to the database (a violation of the AWS Well-Architected Reliability Pillar!). However, it is small and easy to understand and deploy.\nThe Bootstrapping Script The bootstrapping script assumes 4 things:\n The name of the SQL to run to create the table used is hardcoded to \u0026ldquo;createIPTable.sql\u0026rdquo; The password is hardcoded to match the hardcoded password in the CloudFormation template that creates the RDS instance. The name of the Executable is \u0026ldquo;FragileWebApp\u0026rdquo;  The bucket location(s) should really be passed as a 5th and/or 6th command line variable and is marked as TODOs.\nThe SQL in the Bootstrapping Script The database and table are hard coded to match what the executable is expecting. There are also commands required to support AWS Database Migration Service (DMS) replication to set the retention configuration of the binlog, and add permisssions for the user that AWS DMS uses.\nDeploying the State Machine The AWS Step Functions state machine must be deployed in the same region as the bucket where you uploaded the zipped code. This is because the Lambda functions can only be created in the same AWS Region as the location of the bucket. In addition, the Lambda functions must be in the same AWS Region as the state machine in order for the state machine to invoke it.\nCloudFormation templates The CloudFomation templates and the bootstrapping scripts need to be deployed in the same region. This is not a limitation, except for the fact that the parameters built in the Lambda function make this assumption. Also, the Amazon Machine Images (AMIs) for the web servers are only mapped into us-east-2 (Ohio) and us-west-2 (Oregon).\n"},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/troubleshooting_guide/","title":"Troubleshooting Guide for 300 - Testing for Resiliency of EC2, RDS, and S3","tags":[],"description":"","content":"Introduction The purpose of this guide is to prepare for the expected questions and problems.\nCommon AWS Account Problems If running these labs on your own, you will need to use an AWS account that meets the following qualifications. If you are at a live workshop, you may have been supplied with an AWS account for the lab. If not, and you cannot remedy your account issues, please see a proctor who can help pair you with another student who does have these permissions and you can pair lab.\nYou will need to be able to log into the console as a user with permissions to run CloudFormation. If you do not have permission to run CloudFormation, please create a new IAM User with these permissions or use a different AWS account.\nThe next most common problem in deploying the test application is exceeding the default limit of 5 Elastic IPs in an account. The VPC is created with 3 NAT Gateways, which each require an EIP. You will either will have to release some that you are using, or use a different AWS account.\nThe service linked roles may exist already in an account. If they do, you will see a failure to deploy the first CFN stack for lambda_functions_for_deploy.json. You should delete the stack and redeploy it, but please make sure you are appropriately setting the Boolean parameters of the deployment machine stack. If at a live workshop, please see a proctor if you need more help with this.\nProblems with Service Linked Roles If you dont see the existing service linked IAM Role and try to create it in the deployment machine, it will not deploy. It will fail back with an error that the Role already exists under another name. Simply set the parameter to false and redeploy.\nProblems with the Step Functions State Machine and/or Lambda Functions The state machine is idempotent and can be re-run if something times out.\nIf a function fails, you can debug it by creating a test for the Lambda Function. For example, to the test the DeployVPC Lambda function, navigate to the StepFunctions console, and select the DeployVPC function in the Visual Workflow, and click on the Input in the Step details on the right:\nOnce youve clicked the Input, you can select the input and copy it into the copy buffer:\nThen navigate to the Lambda console and click on the DeployVPC Lambda Function:\nYou can then click the down arrow to the left of the Test button with the grayed text Select a test event.. and click on Configure test events:\nName the event TestDeployVPC and insert the copied input from the step function, then click Create:\nNow you can click the Test button to execute the test:\nAfter execution, you can click on the Details and see the log of the function to determine what went wrong: You can also go to the CloudWatch logs to see details of the execution.\nDeployRDS step fails If you get the following error for DeployRDS then there is an obsolete DB password stored in SSM Parameter store. This can happen if you had a problem with deployment, stopped it, and then restarted it.\nThe ciphertext refers to a customer master key that does not exist, does not exist in this region, or you are not allowed to access.\nSolution:\n In the AWS console go to SSM Parameter store  Delete the parameters stored there Go to the CloudFormation console and delete (roll back) the ResiliencyVPC stack Resume by re-starting deployment of the infrastructure  Note you will need to use a new name, such as BuildResiliency2    RDSStackCompleteChoice -\u0026gt; DeployFailedStatus If your deployment machine fails and looks like this\nAnd if the following is true:\n  click on the RDSStackCompleteChoice stage of your workflow\n  select Output\n  RDS stack shows status as CREATE_IN_PROGRESS\n\u0026quot;rds\u0026quot;: { \u0026quot;stackname\u0026quot;: \u0026quot;MySQLforResiliencyTesting\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;CREATE_IN_PROGRESS\u0026quot; }    Then it is likley that your RDS deployment timed out before the workflow could complete. Do the following to continue:\n  Go to the CloudFormation console   Verify the status for MySQLforResiliencyTesting is CREATE_COMPLETE\n  Go back to your state machine\n  Click New Execution\n  Give your execution a new name, unique from previous ones (such as \u0026ldquo;BuildResiliency3\u0026rdquo;)\n   The workflow will quickly determine which stacks have already been deployed, and start immediately on the final (web server) stack.  Problems Executing the Scripts If you are not using Amazon Linux, you will need to install the AWS CLI.\nInstalling jq is pretty easy, just download the executable and install it where the PATH will see it.\nOlder versions of bash and the windows bash implementation complain about the { and } characters in the sed commands. They can be deleted in older version of bash (but note that they are required in newer versions of bash).\nAssisting with the Failure Tests Failure modes individual The shell script will delete the first instance it finds running the VPC. There shouldnt be a problem with the jq parser of the JSON returned from the ec2 describe-instances but it is theoretically possible. If you get a jq error, it is more likely the web layer has not actually been created correctly.\nIf you see nulls in output messages, it is possible that you are not specifying the correct VPC ID.\nInstalling boto3 via pip on an Amazon Linux instance will cause you to have errors on the command line:\n $ aws help Traceback (most recent call last): File \u0026quot;/usr/local/bin/aws\u0026quot;, line 19, in \\\u0026lt;module\\\u0026gt; import awscli.clidriver File \u0026quot;/usr/local/lib/python2.7/dist-packages/awscli/clidriver.py\u0026quot;, line 19, in \\\u0026lt;module\\\u0026gt; from botocore.hooks import AliasedEventEmitter ImportError: cannot import name AliasedEventEmitter  To fix this, you need to remove the aws-cli, downgrade boto, and install an older version of the aws-cli:\n $ sudo yum remove aws-cli $ sudo yum downgrade python27-botocore 1.8 $ sudo yum install aws-cli-1.14.9  EC2 Instance Failure Some additional questions to ask yourself:\n  Open fail_instance.sh/fail_instance.py/InstanceFailover.java/InstanceFailover.cs in an editor. How could you make this randomly select an instance?\n  What are the concerns if you have hundreds or thousands of instances?\n  What if you dont have an Auto Scaling group? How could you recover?\n  How do they test EC2 AutoRecovery? The answer is to open a support ticket. This requires extra effort on our side.\n  How would you undo this failure mode?\n  RDS Failover The console does not update the active AZ of the RDS instance until an unknown amount of time passes (~5 min). This can make it appear it didnt fail over, but it did. You can see the failover in the events log of RDS.\nIf you see nulls in output messages, it is possible that you are not specifying the correct VPC ID.\nSome additional questions to ask yourself:\n  Why didnt the Auto Scaling Group terminate them and replace the instances? Or why did it?\n  How could you make the application resilient to the transient failure?\n  What if this was a single AZ RDS?\n  How would you fail the S3 portion of the application?\n  AZ Failure The Java and C# implementations have some improved error checking over the bash implementation, but essentially perform the same logic.\nThis is what the failure simulation does:\n  Loop through all the Auto Scaling Groups by calling Auto Scalings DescribeAutoScalingGroups; for each group, look at the AZs it is configured for. If the desired AZ in in the list, we can reconfigure the group by calling UpdateAutoScalingGroup to update the AZs to the list without this AZ in it.\n  Call EC2s DescribeSubnets to identify the subnets in the AZ desired within the VPC. It then creates a NACL, adds entries to block ingress and egress of all ports and protocols, then calls EC2s ReplaceNetworkAclAssociation to associate the subnets with the NACL. This will cause the ELB to route traffic to the other AZs since it has Cross AZ enabled.\n  Loop through all the RDS Instances by calling RDS DescribeInstances. If this instances AvailabilityZone is this AZ, then if it is an RDS Multi-AZ, call RDS RebootDBInstance with ForceFailover set to True.\n  Some additional questions to ask yourself:\n  What is the expected effect? How long does it take to take effect? Look at the Target Group Targets to see them go unhealthy, also watch the EC2 instances to see the one in the target AZ shutdown and be restarted in one of the other AZs.\n  What would you do if the ASG was only in one AZ? You could call the AutoScaling SuspendProcesses and then get the list of instances in the group and call EC2 StopInstances or TerminateInstances\n  How would you undo all these changes?\n  Region Failure Unfortunately, you need a DNS domain registered to effect a Region failover, so you wont be able to perform this failure. However, you can think about how you would simulate it.\nRoute53 uses HealthChecks to see if the destination is available. You could use the network ACL modification above that relates to the AZ to specify that change on the health check endpoint. (A SecurityGroup modification would likely work as well). This would cause the HeathCheck to fail and the record set to use the second region. You would want to also have an alert fire on this and send a call to DMS StopReplicationTask to stop the replication if you are using DMS to replicate data to an instance in a second AWS Region.\nSome additional questions to ask yourself:\nHow would you fail back? You could set up a DMS instance that is configured for the source and target going the other way, then recover by call StartReplicationTask, using a load and cdc configuration.\n"},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_enterprise_dashboards/code/6_view6/","title":"","tags":[],"description":"","content":"Data Transfer View This view will be used to create the main Data Transfer Cost Analysis dashboard page.\nWe recommend large customers with over 500 linked accounts, or more than $5M a month in invoiced cost, display 1 or 2 months previous data instead of 3. Modify the INTERVAL in the statements below to less than 3 months for improved performance.\n     Click here - to expand data transfer view query OR   Modify the following SQL query for data_transfer_view:\n  Update line 21 replace (database).(tablename) with your CUR database and table name\n  Optional: Adjust the look back from \u0026lsquo;3\u0026rsquo; months to desired time-frame in row 32,33\n CREATE OR REPLACE VIEW data_transfer_view AS SELECT DISTINCT \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot; , \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot; , \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot; , \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot; , \u0026quot;product_product_name\u0026quot; \u0026quot;product_name\u0026quot; , \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot; , \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot; , \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot; , \u0026quot;line_item_usage_type\u0026quot; \u0026quot;usage_type\u0026quot; , \u0026quot;product_from_location\u0026quot; \u0026quot;from_location\u0026quot; , \u0026quot;product_to_location\u0026quot; \u0026quot;to_location\u0026quot; , \u0026quot;line_item_resource_id\u0026quot; \u0026quot;resource_id\u0026quot; , (\u0026quot;sum\u0026quot;((CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END)) / 1024) \u0026quot;TBs\u0026quot; , \u0026quot;sum\u0026quot;((CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END)) \u0026quot;usage_quantity\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;line_item_blended_cost\u0026quot;) \u0026quot;blended_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;pricing_public_on_demand_cost\u0026quot;) \u0026quot;public_cost\u0026quot; , \u0026quot;line_item_blended_rate\u0026quot; \u0026quot;blended_rate\u0026quot; , \u0026quot;line_item_unblended_rate\u0026quot; \u0026quot;unblended_rate\u0026quot; , \u0026quot;pricing_public_on_demand_rate\u0026quot; \u0026quot;public_ondemand_rate\u0026quot; , \u0026quot;product_transfer_type\u0026quot; \u0026quot;data_transfer_type\u0026quot; FROM (database).(tablename) WHERE (((((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Bytes%') AND ((((\u0026quot;line_item_usage_type\u0026quot; LIKE '%In%') OR (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Out%')) OR (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Regional%')) AND ((NOT (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Cloudfront%')) AND (\u0026quot;line_item_usage_type\u0026quot; \u0026lt;\u0026gt; 'DataTransfer-In-Bytes')))) AND ((\u0026quot;product_from_location\u0026quot; = '') OR (\u0026quot;product_from_location\u0026quot; LIKE '%(%'))) AND (NOT (\u0026quot;line_item_line_item_type\u0026quot; IN ('Tax', 'RIFee', 'Fee', 'Refund', 'Credit')))) AND (\u0026quot;line_item_blended_cost\u0026quot; \u0026gt; 0.0)) AND ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '3' MONTH)) AND (CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_date) - INTERVAL '3' MONTH))) GROUP BY \u0026quot;line_item_product_code\u0026quot;, \u0026quot;bill_billing_period_start_date\u0026quot;, \u0026quot;line_item_usage_account_id\u0026quot;, \u0026quot;bill_payer_account_id\u0026quot;, \u0026quot;product_product_name\u0026quot;, \u0026quot;line_item_line_item_type\u0026quot;, \u0026quot;line_item_operation\u0026quot;, \u0026quot;product_region\u0026quot;, \u0026quot;line_item_usage_type\u0026quot;, \u0026quot;product_from_location\u0026quot;, \u0026quot;product_to_location\u0026quot;, \u0026quot;line_item_resource_id\u0026quot;, \u0026quot;line_item_blended_rate\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot;, \u0026quot;product_transfer_type\u0026quot;, \u0026quot;product_usagetype\u0026quot;, \u0026quot;pricing_public_on_demand_cost\u0026quot;, \u0026quot;pricing_public_on_demand_rate\u0026quot;, \u0026quot;line_item_unblended_rate\u0026quot;, \u0026quot;line_item_unblended_cost\u0026quot;, \u0026quot;line_item_blended_cost\u0026quot;          Click here - to expand Create view in Athena using aws cli     Copy the code below to a new file and name it create-data-transfer-view-query.json. Then replace the values as follows-\n  \u0026lt;your database\u0026gt;.\u0026lt;your table\u0026gt; = Your database.table\n  \u0026lt;your s3 bucket\u0026gt; = Your s3 bucket\n  \u0026lt;your Athena Workgroup\u0026gt; = Your Athena WorkGroup\n  Optional: Adjust the look back from \u0026lsquo;3\u0026rsquo; months to desired time-frame in QueryString\n{ \u0026quot;QueryString\u0026quot;: \u0026quot;CREATE OR REPLACE VIEW data_transfer_view AS SELECT DISTINCT line_item_product_code product_code, bill_billing_period_start_date billing_period, bill_payer_account_id payer_account_id, line_item_usage_account_id linked_account_id, product_product_name product_name, line_item_line_item_type charge_type, line_item_operation operation, product_region region, line_item_usage_type usage_type, product_from_location from_location , product_to_location to_location , line_item_resource_id resource_id , (sum((CASE WHEN (line_item_line_item_type = 'Usage') THEN line_item_usage_amount ELSE 0 END)) / 1024) TBs , sum((CASE WHEN (line_item_line_item_type = 'Usage') THEN line_item_usage_amount ELSE 0 END)) usage_quantity, sum(line_item_blended_cost) blended_cost , sum(line_item_unblended_cost) unblended_cost , sum(pricing_public_on_demand_cost) public_cost , line_item_blended_rate blended_rate, line_item_unblended_rate unblended_rate, pricing_public_on_demand_rate public_ondemand_rate, product_transfer_type data_transfer_type FROM \u0026lt;your database\u0026gt;.\u0026lt;your table\u0026gt; WHERE (((((line_item_usage_type LIKE '%Bytes%') AND ((((line_item_usage_type LIKE '%In%') OR (line_item_usage_type LIKE '%Out%')) OR (line_item_usage_type LIKE '%Regional%')) AND ((NOT (line_item_usage_type LIKE '%Cloudfront%')) AND (line_item_usage_type \u0026lt;\u0026gt; 'DataTransfer-In-Bytes')))) AND ((product_from_location = '') OR (product_from_location LIKE '%(%'))) AND (NOT (line_item_line_item_type IN ('Tax', 'RIFee', 'Fee', 'Refund', 'Credit')))) AND (line_item_blended_cost \u0026gt; 0.0)) AND ((bill_billing_period_start_date \u0026gt;= (date_trunc('month', current_timestamp) - INTERVAL '3' MONTH)) AND (CAST(concat(\\year\\, '-', \\month\\, '-01') AS date) \u0026gt;= (date_trunc('month', current_date) - INTERVAL '3' MONTH))) GROUP BY line_item_product_code, bill_billing_period_start_date, line_item_usage_account_id, bill_payer_account_id, product_product_name, line_item_line_item_type, line_item_operation, product_region, line_item_usage_type, product_from_location, product_to_location, line_item_resource_id, line_item_blended_rate, line_item_line_item_description, product_transfer_type, product_usagetype, pricing_public_on_demand_cost, pricing_public_on_demand_rate, line_item_unblended_rate, line_item_unblended_cost, line_item_blended_cost\u0026quot;, \u0026quot;QueryExecutionContext\u0026quot;: { \u0026quot;Database\u0026quot;: \u0026quot;costmaster\u0026quot;, \u0026quot;Catalog\u0026quot;: \u0026quot;AWSDataCatalog\u0026quot; }, \u0026quot;ResultConfiguration\u0026quot;: { \u0026quot;OutputLocation\u0026quot;: \u0026quot;s3://\u0026lt;your S3 bucket\u0026gt;/tmp\u0026quot; }, \u0026quot;WorkGroup\u0026quot;: \u0026quot;\u0026lt;your Athena Workgroup\u0026gt;\u0026quot; }      Run the following command in a terminal window from the folder where you created create-data-transfer-view-query.json\n aws athena start-query-execution --cli-input-json file://create-data-transfer-view-query.json    To check query execution status\n aws athena get-query-execution --query-execution-id \u0026lt;QueryExecutionId returned from previus command\u0026gt; --region us-east-1    Response:\n      Confirm the view is working, run the following Athena query and you should receive 10 rows of data:\n select * from costmaster.data_transfer_view limit 10;   "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_organization_data_cur_connection/code/6_view6/","title":"","tags":[],"description":"","content":"Data Transfer View This view will be used to create the main Data Transfer Cost Analysis dashboard page.\nWe recommend large customers with over 500 linked accounts, or more than $5M a month in invoiced cost, display 1 or 2 months previous data instead of 3. Modify the INTERVAL in the statements below to less than 3 months for improved performance.\n     Click here - to expand data transfer view query OR   Modify the following SQL query for data_transfer_view:\n  Update line 21 replace (database).(tablename) with your CUR database and table name\n  Optional: Adjust the look back from \u0026lsquo;3\u0026rsquo; months to desired time-frame in row 32,33\n CREATE OR REPLACE VIEW data_transfer_view AS SELECT DISTINCT \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot; , \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot; , \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot; , \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot; , \u0026quot;product_product_name\u0026quot; \u0026quot;product_name\u0026quot; , \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot; , \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot; , \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot; , \u0026quot;line_item_usage_type\u0026quot; \u0026quot;usage_type\u0026quot; , \u0026quot;product_from_location\u0026quot; \u0026quot;from_location\u0026quot; , \u0026quot;product_to_location\u0026quot; \u0026quot;to_location\u0026quot; , \u0026quot;line_item_resource_id\u0026quot; \u0026quot;resource_id\u0026quot; , (\u0026quot;sum\u0026quot;((CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END)) / 1024) \u0026quot;TBs\u0026quot; , \u0026quot;sum\u0026quot;((CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END)) \u0026quot;usage_quantity\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;line_item_blended_cost\u0026quot;) \u0026quot;blended_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;pricing_public_on_demand_cost\u0026quot;) \u0026quot;public_cost\u0026quot; , \u0026quot;line_item_blended_rate\u0026quot; \u0026quot;blended_rate\u0026quot; , \u0026quot;line_item_unblended_rate\u0026quot; \u0026quot;unblended_rate\u0026quot; , \u0026quot;pricing_public_on_demand_rate\u0026quot; \u0026quot;public_ondemand_rate\u0026quot; , \u0026quot;product_transfer_type\u0026quot; \u0026quot;data_transfer_type\u0026quot; FROM (database).(tablename) WHERE (((((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Bytes%') AND ((((\u0026quot;line_item_usage_type\u0026quot; LIKE '%In%') OR (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Out%')) OR (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Regional%')) AND ((NOT (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Cloudfront%')) AND (\u0026quot;line_item_usage_type\u0026quot; \u0026lt;\u0026gt; 'DataTransfer-In-Bytes')))) AND ((\u0026quot;product_from_location\u0026quot; = '') OR (\u0026quot;product_from_location\u0026quot; LIKE '%(%'))) AND (NOT (\u0026quot;line_item_line_item_type\u0026quot; IN ('Tax', 'RIFee', 'Fee', 'Refund', 'Credit')))) AND (\u0026quot;line_item_blended_cost\u0026quot; \u0026gt; 0.0)) AND ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '3' MONTH)) AND (CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_date) - INTERVAL '3' MONTH))) GROUP BY \u0026quot;line_item_product_code\u0026quot;, \u0026quot;bill_billing_period_start_date\u0026quot;, \u0026quot;line_item_usage_account_id\u0026quot;, \u0026quot;bill_payer_account_id\u0026quot;, \u0026quot;product_product_name\u0026quot;, \u0026quot;line_item_line_item_type\u0026quot;, \u0026quot;line_item_operation\u0026quot;, \u0026quot;product_region\u0026quot;, \u0026quot;line_item_usage_type\u0026quot;, \u0026quot;product_from_location\u0026quot;, \u0026quot;product_to_location\u0026quot;, \u0026quot;line_item_resource_id\u0026quot;, \u0026quot;line_item_blended_rate\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot;, \u0026quot;product_transfer_type\u0026quot;, \u0026quot;product_usagetype\u0026quot;, \u0026quot;pricing_public_on_demand_cost\u0026quot;, \u0026quot;pricing_public_on_demand_rate\u0026quot;, \u0026quot;line_item_unblended_rate\u0026quot;, \u0026quot;line_item_unblended_cost\u0026quot;, \u0026quot;line_item_blended_cost\u0026quot;          Click here - to expand Create view in Athena using aws cli     Copy the code below to a new file and name it create-data-transfer-view-query.json. Then replace the values as follows-\n  \u0026lt;your database\u0026gt;.\u0026lt;your table\u0026gt; = Your database.table\n  \u0026lt;your s3 bucket\u0026gt; = Your s3 bucket\n  \u0026lt;your Athena Workgroup\u0026gt; = Your Athena WorkGroup\n  Optional: Adjust the look back from \u0026lsquo;3\u0026rsquo; months to desired time-frame in QueryString\n{ \u0026quot;QueryString\u0026quot;: \u0026quot;CREATE OR REPLACE VIEW data_transfer_view_cli AS SELECT DISTINCT line_item_product_code product_code, bill_billing_period_start_date billing_period, bill_payer_account_id payer_account_id, line_item_usage_account_id linked_account_id, product_product_name product_name, line_item_line_item_type charge_type, line_item_operation operation, product_region region, line_item_usage_type usage_type, product_from_location from_location , product_to_location to_location , line_item_resource_id resource_id , (sum((CASE WHEN (line_item_line_item_type = 'Usage') THEN line_item_usage_amount ELSE 0 END)) / 1024) TBs , sum((CASE WHEN (line_item_line_item_type = 'Usage') THEN line_item_usage_amount ELSE 0 END)) usage_quantity, sum(line_item_blended_cost) blended_cost , sum(line_item_unblended_cost) unblended_cost , sum(pricing_public_on_demand_cost) public_cost , line_item_blended_rate blended_rate, line_item_unblended_rate unblended_rate, pricing_public_on_demand_rate public_ondemand_rate, product_transfer_type data_transfer_type FROM \u0026lt;your database\u0026gt;.\u0026lt;your table\u0026gt; WHERE (((((line_item_usage_type LIKE '%Bytes%') AND ((((line_item_usage_type LIKE '%In%') OR (line_item_usage_type LIKE '%Out%')) OR (line_item_usage_type LIKE '%Regional%')) AND ((NOT (line_item_usage_type LIKE '%Cloudfront%')) AND (line_item_usage_type \u0026lt;\u0026gt; 'DataTransfer-In-Bytes')))) AND ((product_from_location = '') OR (product_from_location LIKE '%(%'))) AND (NOT (line_item_line_item_type IN ('Tax', 'RIFee', 'Fee', 'Refund', 'Credit')))) AND (line_item_blended_cost \u0026gt; 0.0)) AND ((bill_billing_period_start_date \u0026gt;= (date_trunc('month', current_timestamp) - INTERVAL '3' MONTH)) AND (CAST(concat(\\year\\, '-', \\month\\, '-01') AS date) \u0026gt;= (date_trunc('month', current_date) - INTERVAL '3' MONTH))) GROUP BY line_item_product_code, bill_billing_period_start_date, line_item_usage_account_id, bill_payer_account_id, product_product_name, line_item_line_item_type, line_item_operation, product_region, line_item_usage_type, product_from_location, product_to_location, line_item_resource_id, line_item_blended_rate, line_item_line_item_description, product_transfer_type, product_usagetype, pricing_public_on_demand_cost, pricing_public_on_demand_rate, line_item_unblended_rate, line_item_unblended_cost, line_item_blended_cost\u0026quot;, \u0026quot;QueryExecutionContext\u0026quot;: { \u0026quot;Database\u0026quot;: \u0026quot;costmaster\u0026quot;, \u0026quot;Catalog\u0026quot;: \u0026quot;AWSDataCatalog\u0026quot; }, \u0026quot;ResultConfiguration\u0026quot;: { \u0026quot;OutputLocation\u0026quot;: \u0026quot;s3://\u0026lt;your S3 bucket\u0026gt;/tmp\u0026quot; }, \u0026quot;WorkGroup\u0026quot;: \u0026quot;\u0026lt;your Athena Workgroup\u0026gt;\u0026quot; }      Run the following command in a terminal window from the folder where you created create-data-transfer-view-query.json\n aws athena start-query-execution --cli-input-json file://create-data-transfer-view-query.json    To check query execution status\n aws athena get-query-execution --query-execution-id \u0026lt;QueryExecutionId returned from previus command\u0026gt; --profile qs --region us-east-1    Response:\n      Confirm the view is working, run the following Athena query and you should receive 10 rows of data:\n select * from costmaster.data_transfer_view limit 10;   "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_enterprise_dashboards/code/1_view1/","title":"","tags":[],"description":"","content":"View 1 - Summary View This view will be used to create the main Usage Cost Summary dashboard page. Use one of the following queries depending on whether you have Reserved Instances, or Savings Plans.\nWe recommend large customers with over 500 linked accounts or more than $10M a month in invoiced cost update the usage date field in the query from \u0026lsquo;day\u0026rsquo; to \u0026lsquo;month\u0026rsquo; for improved performance\n     Click here - if you have both Savings Plans and Reserved Instances   Modify the following SQL query for View1:\n  Update line 74 replace (database).(tablename) with your CUR database and table name\n  Optional: Adjust the granularity to monthly, by changing \u0026lsquo;day\u0026rsquo; to \u0026lsquo;month\u0026rsquo; in row 6\n  Optional: Adjust the look back from \u0026lsquo;7\u0026rsquo; months to desired time-frame in row 75\n CREATE OR REPLACE VIEW summary_view AS\rSELECT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;bill_invoice_id\u0026quot; \u0026quot;invoice_id\u0026quot;\r, \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot;\r, CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN 'Running_Usage' WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN 'Running_Usage' WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN 'Running_Usage' ELSE 'non_usage' END \u0026quot;charge_category\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END \u0026quot;purchase_option\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot; ELSE '' END \u0026quot;ri_sp_arn\u0026quot;\r, \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot;\r, \u0026quot;product_product_name\u0026quot; \u0026quot;product_name\u0026quot;\r, CASE WHEN (\u0026quot;bill_billing_entity\u0026quot; = 'AWS Marketplace' AND \u0026quot;line_item_line_item_type\u0026quot; NOT LIKE '%Discount%') THEN \u0026quot;Product_Product_Name\u0026quot; WHEN (\u0026quot;product_servicecode\u0026quot; = '') THEN \u0026quot;line_item_product_code\u0026quot; ELSE \u0026quot;product_servicecode\u0026quot; END \u0026quot;service\u0026quot;\r, \u0026quot;product_product_family\u0026quot; \u0026quot;product_family\u0026quot;\r, \u0026quot;line_item_usage_type\u0026quot; \u0026quot;usage_type\u0026quot;\r, \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot;\r, \u0026quot;line_item_line_item_description\u0026quot; \u0026quot;item_description\u0026quot;\r, \u0026quot;line_item_availability_zone\u0026quot; \u0026quot;availability_zone\u0026quot;\r, \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot;\r, CASE\rWHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, '.', 1) ELSE \u0026quot;product_instance_type_family\u0026quot; END \u0026quot;instance_type_family\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 1) ELSE \u0026quot;product_instance_type\u0026quot; END \u0026quot;instance_type\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 2), '/', 1) ELSE \u0026quot;product_operating_system\u0026quot; END \u0026quot;platform\u0026quot; , \u0026quot;product_tenancy\u0026quot; \u0026quot;tenancy\u0026quot;\r, \u0026quot;product_physical_processor\u0026quot; \u0026quot;processor\u0026quot;\r, \u0026quot;product_processor_features\u0026quot; \u0026quot;processor_features\u0026quot;\r, \u0026quot;product_database_engine\u0026quot; \u0026quot;database_engine\u0026quot;\r, \u0026quot;product_group\u0026quot; \u0026quot;product_group\u0026quot;\r, \u0026quot;product_from_location\u0026quot; \u0026quot;product_from_location\u0026quot;\r, \u0026quot;product_to_location\u0026quot; \u0026quot;product_to_location\u0026quot;\r, \u0026quot;product_current_generation\u0026quot; \u0026quot;current_generation\u0026quot;\r, \u0026quot;line_item_legal_entity\u0026quot; \u0026quot;legal_entity\u0026quot;\r, \u0026quot;bill_billing_entity\u0026quot; \u0026quot;billing_entity\u0026quot;\r, \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot;\r, approx_distinct(\u0026quot;Line_item_resource_id\u0026quot;) \u0026quot;resource_id_count\u0026quot;\r, sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END) \u0026quot;usage_quantity\u0026quot;\r, sum (\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;reservation_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (\u0026quot;reservation_unused_amortized_upfront_fee_for_billing_period\u0026quot; + \u0026quot;reservation_unused_recurring_fee\u0026quot;)\rWHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN 0 ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) \u0026quot;amortized_cost\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (-\u0026quot;savings_plan_amortized_upfront_commitment_for_billing_period\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (-\u0026quot;reservation_amortized_upfront_fee_for_billing_period\u0026quot;) ELSE 0 END) \u0026quot;ri_sp_trueup\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN \u0026quot;line_item_unblended_cost\u0026quot;ELSE 0 END) \u0026quot;ri_sp_upfront_fees\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; \u0026lt;\u0026gt; 'SavingsPlanNegation') THEN \u0026quot;pricing_public_on_demand_cost\u0026quot; ELSE 0 END) \u0026quot;public_cost\u0026quot; FROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '7' MONTH)) AND (CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_date) - INTERVAL '7' MONTH)))\rGROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34\r         Click here - if you have Savings Plans, but do not have Reserved Instances   The query is the same as the first query, except some of lines have been commented out. If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View1:\n  Update line 78 replace (database).(tablename) with your CUR database and table name\n  Optional: Adjust the granularity to monthly, by changing \u0026lsquo;day\u0026rsquo; to \u0026lsquo;month\u0026rsquo; in row 6\n  Optional: Adjust the look back from \u0026lsquo;7\u0026rsquo; months to desired timeframe in row 79\n CREATE OR REPLACE VIEW summary_view AS\rSELECT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;bill_invoice_id\u0026quot; \u0026quot;invoice_id\u0026quot;\r, \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot;\r, CASE -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN 'Running_Usage' WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN 'Running_Usage' WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN 'Running_Usage' ELSE 'non_usage' END \u0026quot;charge_category\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END \u0026quot;purchase_option\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;\rELSE '' END \u0026quot;ri_sp_arn\u0026quot;\r, \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot;\r, \u0026quot;product_product_name\u0026quot; \u0026quot;product_name\u0026quot;\r, CASE WHEN (\u0026quot;bill_billing_entity\u0026quot; = 'AWS Marketplace' AND \u0026quot;line_item_line_item_type\u0026quot; NOT LIKE '%Discount%') THEN \u0026quot;Product_Product_Name\u0026quot; WHEN (\u0026quot;product_servicecode\u0026quot; = '') THEN \u0026quot;line_item_product_code\u0026quot; ELSE \u0026quot;product_servicecode\u0026quot; END \u0026quot;service\u0026quot;\r, \u0026quot;product_product_family\u0026quot; \u0026quot;product_family\u0026quot;\r, \u0026quot;line_item_usage_type\u0026quot; \u0026quot;usage_type\u0026quot;\r, \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot;\r, \u0026quot;line_item_line_item_description\u0026quot; \u0026quot;item_description\u0026quot;\r, \u0026quot;line_item_availability_zone\u0026quot; \u0026quot;availability_zone\u0026quot;\r, \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, '.', 1) ELSE \u0026quot;product_instance_type_family\u0026quot; END \u0026quot;instance_type_family\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 1) ELSE \u0026quot;product_instance_type\u0026quot; END \u0026quot;instance_type\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 2), '/', 1) ELSE \u0026quot;product_operating_system\u0026quot; END \u0026quot;platform\u0026quot; , \u0026quot;product_tenancy\u0026quot; \u0026quot;tenancy\u0026quot;\r, \u0026quot;product_physical_processor\u0026quot; \u0026quot;processor\u0026quot;\r, \u0026quot;product_processor_features\u0026quot; \u0026quot;processor_features\u0026quot;\r, \u0026quot;product_database_engine\u0026quot; \u0026quot;database_engine\u0026quot;\r, \u0026quot;product_group\u0026quot; \u0026quot;product_group\u0026quot;\r, \u0026quot;product_from_location\u0026quot; \u0026quot;product_from_location\u0026quot;\r, \u0026quot;product_to_location\u0026quot; \u0026quot;product_to_location\u0026quot;\r, \u0026quot;product_current_generation\u0026quot; \u0026quot;current_generation\u0026quot;\r, \u0026quot;line_item_legal_entity\u0026quot; \u0026quot;legal_entity\u0026quot;\r, \u0026quot;bill_billing_entity\u0026quot; \u0026quot;billing_entity\u0026quot;\r, \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot;\r, \u0026quot;count\u0026quot;(DISTINCT \u0026quot;Line_item_resource_id\u0026quot;) \u0026quot;resource_id_count\u0026quot;\r, sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END) \u0026quot;usage_quantity\u0026quot;\r, sum (\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;reservation_effective_cost\u0026quot; -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (\u0026quot;reservation_unused_amortized_upfront_fee_for_billing_period\u0026quot; + \u0026quot;reservation_unused_recurring_fee\u0026quot;)\r-- WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN 0 ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) \u0026quot;amortized_cost\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (-\u0026quot;savings_plan_amortized_upfront_commitment_for_billing_period\u0026quot;) -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (-\u0026quot;reservation_amortized_upfront_fee_for_billing_period\u0026quot;) ELSE 0 END) \u0026quot;ri_sp_trueup\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN \u0026quot;line_item_unblended_cost\u0026quot;\r-- WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0 END) \u0026quot;ri_sp_upfront_fees\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; \u0026lt;\u0026gt; 'SavingsPlanNegation') THEN \u0026quot;pricing_public_on_demand_cost\u0026quot; ELSE 0 END) \u0026quot;public_cost\u0026quot; FROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '7' MONTH)) AND (CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_date) - INTERVAL '7' MONTH)))\rGROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32,33,34\r         Click here - if you have Reserved Instances, but do not have Savings Plans   The query is the same as the first query, except some of lines have been commented out. If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View1:\n  Update line 74 replace (database).(tablename) with your CUR database and table name\n  Optional: Adjust the granularity to monthly, by changing \u0026lsquo;day\u0026rsquo; to \u0026lsquo;month\u0026rsquo; in row 6\n  Optional: Adjust the look back from \u0026lsquo;7\u0026rsquo; months to desired timeframe in row 75\n CREATE OR REPLACE VIEW summary_view AS\rSELECT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;bill_invoice_id\u0026quot; \u0026quot;invoice_id\u0026quot;\r, \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot;\r, CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN 'Running_Usage' -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN 'Running_Usage' WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN 'Running_Usage' ELSE 'non_usage' END \u0026quot;charge_category\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END \u0026quot;purchase_option\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot; ELSE '' END \u0026quot;ri_sp_arn\u0026quot;\r, \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot;\r, \u0026quot;product_product_name\u0026quot; \u0026quot;product_name\u0026quot;\r, CASE WHEN (\u0026quot;bill_billing_entity\u0026quot; = 'AWS Marketplace' AND \u0026quot;line_item_line_item_type\u0026quot; NOT LIKE '%Discount%') THEN \u0026quot;Product_Product_Name\u0026quot; WHEN (\u0026quot;product_servicecode\u0026quot; = '') THEN \u0026quot;line_item_product_code\u0026quot; ELSE \u0026quot;product_servicecode\u0026quot; END \u0026quot;service\u0026quot;\r, \u0026quot;product_product_family\u0026quot; \u0026quot;product_family\u0026quot;\r, \u0026quot;line_item_usage_type\u0026quot; \u0026quot;usage_type\u0026quot;\r, \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot;\r, \u0026quot;line_item_line_item_description\u0026quot; \u0026quot;item_description\u0026quot;\r, \u0026quot;line_item_availability_zone\u0026quot; \u0026quot;availability_zone\u0026quot;\r, \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, '.', 1) ELSE \u0026quot;product_instance_type_family\u0026quot; END \u0026quot;instance_type_family\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 1) ELSE \u0026quot;product_instance_type\u0026quot; END \u0026quot;instance_type\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 2), '/', 1) ELSE \u0026quot;product_operating_system\u0026quot; END \u0026quot;platform\u0026quot; , \u0026quot;product_tenancy\u0026quot; \u0026quot;tenancy\u0026quot;\r, \u0026quot;product_physical_processor\u0026quot; \u0026quot;processor\u0026quot;\r, \u0026quot;product_processor_features\u0026quot; \u0026quot;processor_features\u0026quot;\r, \u0026quot;product_database_engine\u0026quot; \u0026quot;database_engine\u0026quot;\r, \u0026quot;product_group\u0026quot; \u0026quot;product_group\u0026quot;\r, \u0026quot;product_from_location\u0026quot; \u0026quot;product_from_location\u0026quot;\r, \u0026quot;product_to_location\u0026quot; \u0026quot;product_to_location\u0026quot;\r, \u0026quot;product_current_generation\u0026quot; \u0026quot;current_generation\u0026quot;\r, \u0026quot;line_item_legal_entity\u0026quot; \u0026quot;legal_entity\u0026quot;\r, \u0026quot;bill_billing_entity\u0026quot; \u0026quot;billing_entity\u0026quot;\r, \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot;\r, \u0026quot;count\u0026quot;(DISTINCT \u0026quot;Line_item_resource_id\u0026quot;) \u0026quot;resource_id_count\u0026quot;\r, sum(CASE -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END) \u0026quot;usage_quantity\u0026quot;\r, sum (\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;\r, sum(CASE\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot; -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;) -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;reservation_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (\u0026quot;reservation_unused_amortized_upfront_fee_for_billing_period\u0026quot; + \u0026quot;reservation_unused_recurring_fee\u0026quot;)\rWHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN 0 ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) \u0026quot;amortized_cost\u0026quot;\r, sum(CASE\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (-\u0026quot;savings_plan_amortized_upfront_commitment_for_billing_period\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (-\u0026quot;reservation_amortized_upfront_fee_for_billing_period\u0026quot;) ELSE 0 END) \u0026quot;ri_sp_trueup\u0026quot;\r, sum(CASE\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN \u0026quot;line_item_unblended_cost\u0026quot; ELSE 0 END) \u0026quot;ri_sp_upfront_fees\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; \u0026lt;\u0026gt; 'SavingsPlanNegation') THEN \u0026quot;pricing_public_on_demand_cost\u0026quot; ELSE 0 END) \u0026quot;public_cost\u0026quot; FROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '7' MONTH)) AND (CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_date) - INTERVAL '7' MONTH)))\rGROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32,33,34\r         Click here - if you do not have Reserved Instances, and do not have Savings Plans   The query is the same as the first query, except some of lines have been commented out. If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View1:\n  Update line 84 replace (database).(tablename) with your CUR database and table name\n  Optional: Adjust the granularity to monthly, by changing \u0026lsquo;day\u0026rsquo; to \u0026lsquo;month\u0026rsquo; in row 6\n  Optional: Adjust the look back from \u0026lsquo;7\u0026rsquo; months to desired timeframe in row 85\n CREATE OR REPLACE VIEW summary_view AS\rSELECT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;bill_invoice_id\u0026quot; \u0026quot;invoice_id\u0026quot;\r, \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot;\r, CASE -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN 'Running_Usage' -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN 'Running_Usage' WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN 'Running_Usage' ELSE 'non_usage' END \u0026quot;charge_category\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END \u0026quot;purchase_option\u0026quot;\r,CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN '' ELSE '' END \u0026quot;ri_sp_arn\u0026quot;\r, \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot;\r, \u0026quot;product_product_name\u0026quot; \u0026quot;product_name\u0026quot;\r, CASE WHEN (\u0026quot;bill_billing_entity\u0026quot; = 'AWS Marketplace' AND \u0026quot;line_item_line_item_type\u0026quot; NOT LIKE '%Discount%') THEN \u0026quot;Product_Product_Name\u0026quot; WHEN (\u0026quot;product_servicecode\u0026quot; = '') THEN \u0026quot;line_item_product_code\u0026quot; ELSE \u0026quot;product_servicecode\u0026quot; END \u0026quot;service\u0026quot;\r, \u0026quot;product_product_family\u0026quot; \u0026quot;product_family\u0026quot;\r, \u0026quot;line_item_usage_type\u0026quot; \u0026quot;usage_type\u0026quot;\r, \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot;\r, \u0026quot;line_item_line_item_description\u0026quot; \u0026quot;item_description\u0026quot;\r, \u0026quot;line_item_availability_zone\u0026quot; \u0026quot;availability_zone\u0026quot;\r, \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, '.', 1) ELSE \u0026quot;product_instance_type_family\u0026quot; END \u0026quot;instance_type_family\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 1) ELSE \u0026quot;product_instance_type\u0026quot; END \u0026quot;instance_type\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 2), '/', 1) ELSE \u0026quot;product_operating_system\u0026quot; END \u0026quot;platform\u0026quot; , \u0026quot;product_tenancy\u0026quot; \u0026quot;tenancy\u0026quot;\r, \u0026quot;product_physical_processor\u0026quot; \u0026quot;processor\u0026quot;\r, \u0026quot;product_processor_features\u0026quot; \u0026quot;processor_features\u0026quot;\r, \u0026quot;product_database_engine\u0026quot; \u0026quot;database_engine\u0026quot;\r, \u0026quot;product_group\u0026quot; \u0026quot;product_group\u0026quot;\r, \u0026quot;product_from_location\u0026quot; \u0026quot;product_from_location\u0026quot;\r, \u0026quot;product_to_location\u0026quot; \u0026quot;product_to_location\u0026quot;\r, \u0026quot;product_current_generation\u0026quot; \u0026quot;current_generation\u0026quot;\r, \u0026quot;line_item_legal_entity\u0026quot; \u0026quot;legal_entity\u0026quot;\r, \u0026quot;bill_billing_entity\u0026quot; \u0026quot;billing_entity\u0026quot;\r, \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot;\r, \u0026quot;count\u0026quot;(DISTINCT \u0026quot;Line_item_resource_id\u0026quot;) \u0026quot;resource_id_count\u0026quot;\r, sum(CASE -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END) \u0026quot;usage_quantity\u0026quot;\r, sum (\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_unblended_cost\u0026quot;\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot; -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;) -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;reservation_effective_cost\u0026quot; -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (\u0026quot;reservation_unused_amortized_upfront_fee_for_billing_period\u0026quot; + \u0026quot;reservation_unused_recurring_fee\u0026quot;)\r-- WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN 0 ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) \u0026quot;amortized_cost\u0026quot;\r, CAST(sum(CASE\rWHEN (line_item_line_item_type = 'Usage') THEN 0\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (-\u0026quot;savings_plan_amortized_upfront_commitment_for_billing_period\u0026quot;) -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (-\u0026quot;reservation_amortized_upfront_fee_for_billing_period\u0026quot;) ELSE 0 END) AS double) \u0026quot;ri_sp_trueup\u0026quot;\r, CAST(sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN 0\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN \u0026quot;line_item_unblended_cost\u0026quot;\r-- WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0 END) AS Double) \u0026quot;ri_sp_upfront_fees\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; \u0026lt;\u0026gt; 'SavingsPlanNegation') THEN \u0026quot;pricing_public_on_demand_cost\u0026quot; ELSE 0 END) \u0026quot;public_cost\u0026quot; FROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '7' MONTH)) AND (CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_date) - INTERVAL '7' MONTH)))\rGROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32,33,34\r       Confirm the view is working, run the following Athena query and you should receive 10 rows of data:\n select * from costmaster.summary_view\rlimit 10\r   "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_enterprise_dashboards/code/2_view2/","title":"","tags":[],"description":"","content":"View 2 - EC2 Running Costs This view will be used to create the EC2 Running Costs dashboard page. Use one of the following queries depending on whether you have Reserved Instances, or Savings Plans.\n    Click here - if you have both Savings Plans and Reserved Instances   Modify the following SQL query for View2 - EC2_Running_Cost:\n  Update line 17 replace (database).(tablename) with your CUR database and table name\n CREATE OR REPLACE VIEW \u0026quot;ec2_running_cost\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, (CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END) \u0026quot;purchase_option\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage' THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot;\rWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage' THEN \u0026quot;reservation_effective_cost\u0026quot;\rWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'Usage' THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0 END) \u0026quot;amortized_cost\u0026quot;\r, \u0026quot;round\u0026quot;(\u0026quot;sum\u0026quot;(\u0026quot;line_item_usage_amount\u0026quot;), 2) \u0026quot;usage_quantity\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((((((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2')) AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer')) AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%')) AND (((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') OR (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage')) OR (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage')))\rGROUP BY 1, 2, 3, 4,5,6,7\r         Click here - if you have Savings Plans, but do not have Reserved Instances   The query is the same as the first query, except some of lines have been commented out. If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View2 - EC2_Running_Cost:\n  Update line 21 replace (database).(tablename) with your CUR database and table name\n CREATE OR REPLACE VIEW \u0026quot;ec2_running_cost\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, (CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END) \u0026quot;purchase_option\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage' THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot;\r-- WHEN \u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage' THEN \u0026quot;reservation_effective_cost\u0026quot;\rWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'Usage' THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0 END) \u0026quot;amortized_cost\u0026quot;\r, \u0026quot;round\u0026quot;(\u0026quot;sum\u0026quot;(\u0026quot;line_item_usage_amount\u0026quot;), 2) \u0026quot;usage_quantity\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE (\r(\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%') AND ((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') OR\r(\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') -- OR -- (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage')\r))\rGROUP BY 1, 2, 3, 4,5,6,7\r         Click here - if you have Reserved Instances, but do not have Savings Plans   The query is the same as the first query, except some of lines have been commented out. If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View2 - EC2_Running_Cost:\n  Update line 21 replace (database).(tablename) with your CUR database and table name\n CREATE OR REPLACE VIEW \u0026quot;ec2_running_cost\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, (CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END) \u0026quot;purchase_option\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\r-- WHEN \u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage' THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot;\rWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage' THEN \u0026quot;reservation_effective_cost\u0026quot;\rWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'Usage' THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0 END) \u0026quot;amortized_cost\u0026quot;\r, \u0026quot;round\u0026quot;(\u0026quot;sum\u0026quot;(\u0026quot;line_item_usage_amount\u0026quot;), 2) \u0026quot;usage_quantity\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE (\r(\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%') AND ((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') -- OR\r-- (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') OR (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage')))\rGROUP BY 1, 2, 3, 4,5,6,7\r         Click here - if you do not have Reserved Instances, and do not have Savings Plans   The query is the same as the first query, except some of lines have been commented out. If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View2 - EC2_Running_Cost:\n  Update line 21 replace (database).(tablename) with your CUR database and table name\n CREATE OR REPLACE VIEW \u0026quot;ec2_running_cost\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, (CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END) \u0026quot;purchase_option\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\r-- WHEN \u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage' THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot;\r-- WHEN \u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage' THEN \u0026quot;reservation_effective_cost\u0026quot;\rWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'Usage' THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0 END) \u0026quot;amortized_cost\u0026quot;\r, \u0026quot;round\u0026quot;(\u0026quot;sum\u0026quot;(\u0026quot;line_item_usage_amount\u0026quot;), 2) \u0026quot;usage_quantity\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE (\r(\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%') AND ((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') -- OR\r-- (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') -- OR --(\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage')\r))\rGROUP BY 1, 2, 3, 4,5,6,7\r       Confirm the view is working, run the following Athena query and you should receive 10 rows of data:\n select * from costmaster.ec2_running_cost\rlimit 10\r   "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_enterprise_dashboards/code/3_view3/","title":"","tags":[],"description":"","content":"View 3 - Compute Savings Plan Eligible Spend This view will be used to create the Compute Savings Plan Eligible Spend dashboard page. Use one of the following queries depending on whether you have Reserved Instances, or Savings Plans.\n    Click here - if you have both Savings Plans and Reserved Instances   Modify the following SQL query for View3 - Compute Savings PlaneEligible spend:\n  Update line 22, replace (database).(tablename) with your CUR database and table name\n CREATE OR REPLACE VIEW \u0026quot;compute_savings_plan_eligible_spend\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN ((((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') AND (NOT (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%'))) AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer')) AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%')) THEN\rCASE\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-Concurrency%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Fargate%') THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0\rEND\rELSE 0 END) \u0026quot;unblended_cost\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY)) AND (((\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-GB-Second%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-GB-Second%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-Concurrency%')) OR (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Fargate%')) AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%Spot%') AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%')) GROUP BY 1, 2, 3, 4,5,6\r         Click here - if you have Savings Plans, but do not have Reserved Instances   If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View3 - Compute Savings PlaneEligible spend:\n  Update line 22, replace (database).(tablename) with your CUR database and table name\n CREATE OR REPLACE VIEW \u0026quot;compute_savings_plan_eligible_spend\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN ((((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') AND (NOT (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%'))) AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer')) AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%')) THEN\rCASE\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-Concurrency%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Fargate%') THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0\rEND\rELSE 0 END) \u0026quot;unblended_cost\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY)) AND (((\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-GB-Second%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-GB-Second%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-Concurrency%')) OR (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Fargate%')) AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%Spot%') AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%')) GROUP BY 1, 2, 3, 4,5,6\r         Click here - if you have Reserved Instances, but do not have Savings Plans   If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View3 - Compute Savings PlaneEligible spend:\n  Update line 22, replace (database).(tablename) with your CUR database and table name\n CREATE OR REPLACE VIEW \u0026quot;compute_savings_plan_eligible_spend\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN ((((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') AND (NOT (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%'))) AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer')) AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%')) THEN\rCASE\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-Concurrency%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Fargate%') THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0\rEND\rELSE 0 END) \u0026quot;unblended_cost\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY)) AND (((\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-GB-Second%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-GB-Second%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-Concurrency%')) OR (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Fargate%')) AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%Spot%') AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%')) GROUP BY 1, 2, 3, 4,5,6\r         Click here - if you do not have Reserved Instances, and do not have Savings Plans   If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View3 - Compute Savings PlaneEligible spend:\n  Update line 22, replace (database).(tablename) with your CUR database and table name\n CREATE OR REPLACE VIEW \u0026quot;compute_savings_plan_eligible_spend\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN ((((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') AND (NOT (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%'))) AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer')) AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%')) THEN\rCASE\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-Concurrency%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Fargate%') THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0\rEND\rELSE 0 END) \u0026quot;unblended_cost\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY)) AND (((\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-GB-Second%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-GB-Second%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-Concurrency%')) OR (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Fargate%')) AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%Spot%') AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%')) GROUP BY 1, 2, 3, 4,5,6\r       Confirm the view is working, run the following Athena query and you should receive 10 rows of data:\n select * from costmaster.compute_savings_plan_eligible_spend\rlimit 10\r   "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_enterprise_dashboards/code/4_view4/","title":"","tags":[],"description":"","content":"View 4 - S3 This view will be used to create the S3 dashboard page. Use one of the following queries depending on whether you have Reserved Instances, or Savings Plans.\n    Click here - if you have both Savings Plans and Reserved Instances   Modify the following SQL query for View4 - S3:\n  Update line 22, replace (database).(tablename) with your CUR database and table name\n CREATE OR REPLACE VIEW \u0026quot;s3_view\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;line_item_resource_id\u0026quot; \u0026quot;resource_id\u0026quot;\r, \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot;\r, \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot;\r, \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot;\r, \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot;\r, \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot;\rELSE 0\rEND) \u0026quot;usage_quantity\u0026quot;\r, \u0026quot;sum\u0026quot;(\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;\r, \u0026quot;sum\u0026quot;(\u0026quot;pricing_public_on_demand_cost\u0026quot;) \u0026quot;public_cost\u0026quot;\rFROM (ADD YOUR CUR TABLE NAME)\rWHERE ((((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '3' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY))) AND (\u0026quot;line_item_operation\u0026quot; LIKE '%Storage%')) AND ((\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonGlacier%') OR (\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonS3%')))\rGROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,11,12\r         Click here - if you have Savings Plans, but do not have Reserved Instances   If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View4 - S3:\n  Update line 22, replace (database).(tablename) with your CUR database and table name\n CREATE OR REPLACE VIEW \u0026quot;s3_view\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;line_item_resource_id\u0026quot; \u0026quot;resource_id\u0026quot;\r, \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot;\r, \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot;\r, \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot;\r, \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot;\r, \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot;\rELSE 0\rEND) \u0026quot;usage_quantity\u0026quot;\r, \u0026quot;sum\u0026quot;(\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;\r, \u0026quot;sum\u0026quot;(\u0026quot;pricing_public_on_demand_cost\u0026quot;) \u0026quot;public_cost\u0026quot;\rFROM (ADD YOUR CUR TABLE NAME)\rWHERE ((((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '3' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY))) AND (\u0026quot;line_item_operation\u0026quot; LIKE '%Storage%')) AND ((\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonGlacier%') OR (\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonS3%')))\rGROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,11,12\r         Click here - if you have Reserved Instances, but do not have Savings Plans   If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View4 - S3:\n  Update line 22, replace (database).(tablename) with your CUR database and table name\n CREATE OR REPLACE VIEW \u0026quot;s3_view\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;line_item_resource_id\u0026quot; \u0026quot;resource_id\u0026quot;\r, \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot;\r, \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot;\r, \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot;\r, \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot;\r, \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot;\rELSE 0\rEND) \u0026quot;usage_quantity\u0026quot;\r, \u0026quot;sum\u0026quot;(\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;\r, \u0026quot;sum\u0026quot;(\u0026quot;pricing_public_on_demand_cost\u0026quot;) \u0026quot;public_cost\u0026quot;\rFROM (ADD YOUR CUR TABLE NAME)\rWHERE ((((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '3' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY))) AND (\u0026quot;line_item_operation\u0026quot; LIKE '%Storage%')) AND ((\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonGlacier%') OR (\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonS3%')))\rGROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,11,12\r         Click here - if you do not have Reserved Instances, and do not have Savings Plans   If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View4 - S3:\n  Update line 22, replace (database).(tablename) with your CUR database and table name\n CREATE OR REPLACE VIEW \u0026quot;s3_view\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;line_item_resource_id\u0026quot; \u0026quot;resource_id\u0026quot;\r, \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot;\r, \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot;\r, \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot;\r, \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot;\r, \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot;\rELSE 0\rEND) \u0026quot;usage_quantity\u0026quot;\r, \u0026quot;sum\u0026quot;(\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;\r, \u0026quot;sum\u0026quot;(\u0026quot;pricing_public_on_demand_cost\u0026quot;) \u0026quot;public_cost\u0026quot;\rFROM (ADD YOUR CUR TABLE NAME)\rWHERE ((((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '3' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY))) AND (\u0026quot;line_item_operation\u0026quot; LIKE '%Storage%')) AND ((\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonGlacier%') OR (\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonS3%')))\rGROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,11,12\r       Confirm the view is working, run the following Athena query and you should receive 10 rows of data:\n select * from costmaster.s3_view\rlimit 10   "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_organization_data_cur_connection/code/1_view1/","title":"","tags":[],"description":"","content":"View 1 - Summary View This view will be used to create the main Usage Cost Summary dashboard page. Use one of the following queries depending on whether you have Reserved Instances, or Savings Plans.\nWe recommend large customers with over 500 linked accounts or more than $10M a month in invoiced cost update the usage date field in the query from \u0026lsquo;day\u0026rsquo; to \u0026lsquo;month\u0026rsquo; for improved performance\n     Click here - if you have both Savings Plans and Reserved Instances   Modify the following SQL query for View1:\n  Update line 74 replace (database).(tablename) with your CUR database and table name\n  Optional: Adjust the granularity to monthly, by changing \u0026lsquo;day\u0026rsquo; to \u0026lsquo;month\u0026rsquo; in row 6\n  Optional: Adjust the look back from \u0026lsquo;7\u0026rsquo; months to desired time-frame in row 75\n CREATE OR REPLACE VIEW summary_view AS\rSELECT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;bill_invoice_id\u0026quot; \u0026quot;invoice_id\u0026quot;\r, \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot;\r, CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN 'Running_Usage' WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN 'Running_Usage' WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN 'Running_Usage' ELSE 'non_usage' END \u0026quot;charge_category\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END \u0026quot;purchase_option\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;ELSE '' END \u0026quot;ri_sp_arn\u0026quot;\r, \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot;\r, \u0026quot;product_product_name\u0026quot; \u0026quot;product_name\u0026quot;\r, CASE WHEN (\u0026quot;bill_billing_entity\u0026quot; = 'AWS Marketplace' AND \u0026quot;line_item_line_item_type\u0026quot; NOT LIKE '%Discount%') THEN \u0026quot;Product_Product_Name\u0026quot; WHEN (\u0026quot;product_servicecode\u0026quot; = '') THEN \u0026quot;line_item_product_code\u0026quot; ELSE \u0026quot;product_servicecode\u0026quot; END \u0026quot;service\u0026quot;\r, \u0026quot;product_product_family\u0026quot; \u0026quot;product_family\u0026quot;\r, \u0026quot;line_item_usage_type\u0026quot; \u0026quot;usage_type\u0026quot;\r, \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot;\r, \u0026quot;line_item_line_item_description\u0026quot; \u0026quot;item_description\u0026quot;\r, \u0026quot;line_item_availability_zone\u0026quot; \u0026quot;availability_zone\u0026quot;\r, \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot;\r, CASE\rWHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, '.', 1) ELSE \u0026quot;product_instance_type_family\u0026quot; END \u0026quot;instance_type_family\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 1) ELSE \u0026quot;product_instance_type\u0026quot; END \u0026quot;instance_type\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 2), '/', 1) ELSE \u0026quot;product_operating_system\u0026quot; END \u0026quot;platform\u0026quot; , \u0026quot;product_tenancy\u0026quot; \u0026quot;tenancy\u0026quot;\r, \u0026quot;product_physical_processor\u0026quot; \u0026quot;processor\u0026quot;\r, \u0026quot;product_processor_features\u0026quot; \u0026quot;processor_features\u0026quot;\r, \u0026quot;product_database_engine\u0026quot; \u0026quot;database_engine\u0026quot;\r, \u0026quot;product_group\u0026quot; \u0026quot;product_group\u0026quot;\r, \u0026quot;product_from_location\u0026quot; \u0026quot;product_from_location\u0026quot;\r, \u0026quot;product_to_location\u0026quot; \u0026quot;product_to_location\u0026quot;\r, \u0026quot;product_current_generation\u0026quot; \u0026quot;current_generation\u0026quot;\r, \u0026quot;line_item_legal_entity\u0026quot; \u0026quot;legal_entity\u0026quot;\r, \u0026quot;bill_billing_entity\u0026quot; \u0026quot;billing_entity\u0026quot;\r, \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot;\r, approx_distinct(\u0026quot;Line_item_resource_id\u0026quot;) \u0026quot;resource_id_count\u0026quot;\r, sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END) \u0026quot;usage_quantity\u0026quot;\r, sum (\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;reservation_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (\u0026quot;reservation_unused_amortized_upfront_fee_for_billing_period\u0026quot; + \u0026quot;reservation_unused_recurring_fee\u0026quot;)\rWHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN 0 ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) \u0026quot;amortized_cost\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (-\u0026quot;savings_plan_amortized_upfront_commitment_for_billing_period\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (-\u0026quot;reservation_amortized_upfront_fee_for_billing_period\u0026quot;) ELSE 0 END) \u0026quot;ri_sp_trueup\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN \u0026quot;line_item_unblended_cost\u0026quot;ELSE 0 END) \u0026quot;ri_sp_upfront_fees\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; \u0026lt;\u0026gt; 'SavingsPlanNegation') THEN \u0026quot;pricing_public_on_demand_cost\u0026quot; ELSE 0 END) \u0026quot;public_cost\u0026quot; FROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '7' MONTH)) AND (CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_date) - INTERVAL '7' MONTH)))\rGROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34\r         Click here - if you have Savings Plans, but do not have Reserved Instances   The query is the same as the first query, except some of lines have been commented out. If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View1:\n  Update line 77 replace (database).(tablename) with your CUR database and table name\n  Optional: Adjust the granularity to monthly, by changing \u0026lsquo;day\u0026rsquo; to \u0026lsquo;month\u0026rsquo; in row 6\n  Optional: Adjust the look back from \u0026lsquo;7\u0026rsquo; months to desired timeframe in row 78\n CREATE OR REPLACE VIEW summary_view AS\rSELECT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;bill_invoice_id\u0026quot; \u0026quot;invoice_id\u0026quot;\r, \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot;\r, CASE -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN 'Running_Usage' WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN 'Running_Usage' WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN 'Running_Usage' ELSE 'non_usage' END \u0026quot;charge_category\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END \u0026quot;purchase_option\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;ELSE '' END \u0026quot;ri_sp_arn\u0026quot;\r, \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot;\r, \u0026quot;product_product_name\u0026quot; \u0026quot;product_name\u0026quot;\r, CASE WHEN (\u0026quot;bill_billing_entity\u0026quot; = 'AWS Marketplace' AND \u0026quot;line_item_line_item_type\u0026quot; NOT LIKE '%Discount%') THEN \u0026quot;Product_Product_Name\u0026quot; WHEN (\u0026quot;product_servicecode\u0026quot; = '') THEN \u0026quot;line_item_product_code\u0026quot; ELSE \u0026quot;product_servicecode\u0026quot; END \u0026quot;service\u0026quot;\r, \u0026quot;product_product_family\u0026quot; \u0026quot;product_family\u0026quot;\r, \u0026quot;line_item_usage_type\u0026quot; \u0026quot;usage_type\u0026quot;\r, \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot;\r, \u0026quot;line_item_line_item_description\u0026quot; \u0026quot;item_description\u0026quot;\r, \u0026quot;line_item_availability_zone\u0026quot; \u0026quot;availability_zone\u0026quot;\r, \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, '.', 1) ELSE \u0026quot;product_instance_type_family\u0026quot; END \u0026quot;instance_type_family\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 1) ELSE \u0026quot;product_instance_type\u0026quot; END \u0026quot;instance_type\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 2), '/', 1) ELSE \u0026quot;product_operating_system\u0026quot; END \u0026quot;platform\u0026quot; , \u0026quot;product_tenancy\u0026quot; \u0026quot;tenancy\u0026quot;\r, \u0026quot;product_physical_processor\u0026quot; \u0026quot;processor\u0026quot;\r, \u0026quot;product_processor_features\u0026quot; \u0026quot;processor_features\u0026quot;\r, \u0026quot;product_database_engine\u0026quot; \u0026quot;database_engine\u0026quot;\r, \u0026quot;product_group\u0026quot; \u0026quot;product_group\u0026quot;\r, \u0026quot;product_from_location\u0026quot; \u0026quot;product_from_location\u0026quot;\r, \u0026quot;product_to_location\u0026quot; \u0026quot;product_to_location\u0026quot;\r, \u0026quot;product_current_generation\u0026quot; \u0026quot;current_generation\u0026quot;\r, \u0026quot;line_item_legal_entity\u0026quot; \u0026quot;legal_entity\u0026quot;\r, \u0026quot;bill_billing_entity\u0026quot; \u0026quot;billing_entity\u0026quot;\r, \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot;\r, \u0026quot;count\u0026quot;(DISTINCT \u0026quot;Line_item_resource_id\u0026quot;) \u0026quot;resource_id_count\u0026quot;\r, sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END) \u0026quot;usage_quantity\u0026quot;\r, sum (\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;reservation_effective_cost\u0026quot; -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (\u0026quot;reservation_unused_amortized_upfront_fee_for_billing_period\u0026quot; + \u0026quot;reservation_unused_recurring_fee\u0026quot;)\r-- WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN 0 ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) \u0026quot;amortized_cost\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (-\u0026quot;savings_plan_amortized_upfront_commitment_for_billing_period\u0026quot;) -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (-\u0026quot;reservation_amortized_upfront_fee_for_billing_period\u0026quot;) ELSE 0 END) \u0026quot;ri_sp_trueup\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN \u0026quot;line_item_unblended_cost\u0026quot;\r-- WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0 END) \u0026quot;ri_sp_upfront_fees\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; \u0026lt;\u0026gt; 'SavingsPlanNegation') THEN \u0026quot;pricing_public_on_demand_cost\u0026quot; ELSE 0 END) \u0026quot;public_cost\u0026quot; FROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '7' MONTH)) AND (CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_date) - INTERVAL '7' MONTH)))\rGROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32,33,34\r         Click here - if you have Reserved Instances, but do not have Savings Plans   The query is the same as the first query, except some of lines have been commented out. If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View1:\n  Update line 74 replace (database).(tablename) with your CUR database and table name\n  Optional: Adjust the granularity to monthly, by changing \u0026lsquo;day\u0026rsquo; to \u0026lsquo;month\u0026rsquo; in row 6\n  Optional: Adjust the look back from \u0026lsquo;7\u0026rsquo; months to desired timeframe in row 75\n CREATE OR REPLACE VIEW summary_view AS\rSELECT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;bill_invoice_id\u0026quot; \u0026quot;invoice_id\u0026quot;\r, \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot;\r, CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN 'Running_Usage' -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN 'Running_Usage' WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN 'Running_Usage' ELSE 'non_usage' END \u0026quot;charge_category\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END \u0026quot;purchase_option\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;ELSE '' END \u0026quot;ri_sp_arn\u0026quot;\r, \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot;\r, \u0026quot;product_product_name\u0026quot; \u0026quot;product_name\u0026quot;\r, CASE WHEN (\u0026quot;bill_billing_entity\u0026quot; = 'AWS Marketplace' AND \u0026quot;line_item_line_item_type\u0026quot; NOT LIKE '%Discount%') THEN \u0026quot;Product_Product_Name\u0026quot; WHEN (\u0026quot;product_servicecode\u0026quot; = '') THEN \u0026quot;line_item_product_code\u0026quot; ELSE \u0026quot;product_servicecode\u0026quot; END \u0026quot;service\u0026quot;\r, \u0026quot;product_product_family\u0026quot; \u0026quot;product_family\u0026quot;\r, \u0026quot;line_item_usage_type\u0026quot; \u0026quot;usage_type\u0026quot;\r, \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot;\r, \u0026quot;line_item_line_item_description\u0026quot; \u0026quot;item_description\u0026quot;\r, \u0026quot;line_item_availability_zone\u0026quot; \u0026quot;availability_zone\u0026quot;\r, \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, '.', 1) ELSE \u0026quot;product_instance_type_family\u0026quot; END \u0026quot;instance_type_family\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 1) ELSE \u0026quot;product_instance_type\u0026quot; END \u0026quot;instance_type\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 2), '/', 1) ELSE \u0026quot;product_operating_system\u0026quot; END \u0026quot;platform\u0026quot; , \u0026quot;product_tenancy\u0026quot; \u0026quot;tenancy\u0026quot;\r, \u0026quot;product_physical_processor\u0026quot; \u0026quot;processor\u0026quot;\r, \u0026quot;product_processor_features\u0026quot; \u0026quot;processor_features\u0026quot;\r, \u0026quot;product_database_engine\u0026quot; \u0026quot;database_engine\u0026quot;\r, \u0026quot;product_group\u0026quot; \u0026quot;product_group\u0026quot;\r, \u0026quot;product_from_location\u0026quot; \u0026quot;product_from_location\u0026quot;\r, \u0026quot;product_to_location\u0026quot; \u0026quot;product_to_location\u0026quot;\r, \u0026quot;product_current_generation\u0026quot; \u0026quot;current_generation\u0026quot;\r, \u0026quot;line_item_legal_entity\u0026quot; \u0026quot;legal_entity\u0026quot;\r, \u0026quot;bill_billing_entity\u0026quot; \u0026quot;billing_entity\u0026quot;\r, \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot;\r, \u0026quot;count\u0026quot;(DISTINCT \u0026quot;Line_item_resource_id\u0026quot;) \u0026quot;resource_id_count\u0026quot;\r, sum(CASE -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END) \u0026quot;usage_quantity\u0026quot;\r, sum (\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;\r, sum(CASE\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot; -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;) -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;reservation_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (\u0026quot;reservation_unused_amortized_upfront_fee_for_billing_period\u0026quot; + \u0026quot;reservation_unused_recurring_fee\u0026quot;)\rWHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN 0 ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) \u0026quot;amortized_cost\u0026quot;\r, sum(CASE\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (-\u0026quot;savings_plan_amortized_upfront_commitment_for_billing_period\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (-\u0026quot;reservation_amortized_upfront_fee_for_billing_period\u0026quot;) ELSE 0 END) \u0026quot;ri_sp_trueup\u0026quot;\r, sum(CASE\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN \u0026quot;line_item_unblended_cost\u0026quot;ELSE 0 END) \u0026quot;ri_sp_upfront_fees\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; \u0026lt;\u0026gt; 'SavingsPlanNegation') THEN \u0026quot;pricing_public_on_demand_cost\u0026quot; ELSE 0 END) \u0026quot;public_cost\u0026quot; FROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '7' MONTH)) AND (CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_date) - INTERVAL '7' MONTH)))\rGROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32,33,34\r         Click here - if you do not have Reserved Instances, and do not have Savings Plans   The query is the same as the first query, except some of lines have been commented out. If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View1:\n  Update line 84 replace (database).(tablename) with your CUR database and table name\n  Optional: Adjust the granularity to monthly, by changing \u0026lsquo;day\u0026rsquo; to \u0026lsquo;month\u0026rsquo; in row 6\n  Optional: Adjust the look back from \u0026lsquo;7\u0026rsquo; months to desired timeframe in row 85\n CREATE OR REPLACE VIEW summary_view AS\rSELECT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;bill_invoice_id\u0026quot; \u0026quot;invoice_id\u0026quot;\r, \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot;\r, CASE -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN 'Running_Usage' -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN 'Running_Usage' WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN 'Running_Usage' ELSE 'non_usage' END \u0026quot;charge_category\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END \u0026quot;purchase_option\u0026quot;\r,CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN '' ELSE '' END \u0026quot;ri_sp_arn\u0026quot;\r, \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot;\r, \u0026quot;product_product_name\u0026quot; \u0026quot;product_name\u0026quot;\r, CASE WHEN (\u0026quot;bill_billing_entity\u0026quot; = 'AWS Marketplace' AND \u0026quot;line_item_line_item_type\u0026quot; NOT LIKE '%Discount%') THEN \u0026quot;Product_Product_Name\u0026quot; WHEN (\u0026quot;product_servicecode\u0026quot; = '') THEN \u0026quot;line_item_product_code\u0026quot; ELSE \u0026quot;product_servicecode\u0026quot; END \u0026quot;service\u0026quot;\r, \u0026quot;product_product_family\u0026quot; \u0026quot;product_family\u0026quot;\r, \u0026quot;line_item_usage_type\u0026quot; \u0026quot;usage_type\u0026quot;\r, \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot;\r, \u0026quot;line_item_line_item_description\u0026quot; \u0026quot;item_description\u0026quot;\r, \u0026quot;line_item_availability_zone\u0026quot; \u0026quot;availability_zone\u0026quot;\r, \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, '.', 1) ELSE \u0026quot;product_instance_type_family\u0026quot; END \u0026quot;instance_type_family\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 1) ELSE \u0026quot;product_instance_type\u0026quot; END \u0026quot;instance_type\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 2), '/', 1) ELSE \u0026quot;product_operating_system\u0026quot; END \u0026quot;platform\u0026quot; , \u0026quot;product_tenancy\u0026quot; \u0026quot;tenancy\u0026quot;\r, \u0026quot;product_physical_processor\u0026quot; \u0026quot;processor\u0026quot;\r, \u0026quot;product_processor_features\u0026quot; \u0026quot;processor_features\u0026quot;\r, \u0026quot;product_database_engine\u0026quot; \u0026quot;database_engine\u0026quot;\r, \u0026quot;product_group\u0026quot; \u0026quot;product_group\u0026quot;\r, \u0026quot;product_from_location\u0026quot; \u0026quot;product_from_location\u0026quot;\r, \u0026quot;product_to_location\u0026quot; \u0026quot;product_to_location\u0026quot;\r, \u0026quot;product_current_generation\u0026quot; \u0026quot;current_generation\u0026quot;\r, \u0026quot;line_item_legal_entity\u0026quot; \u0026quot;legal_entity\u0026quot;\r, \u0026quot;bill_billing_entity\u0026quot; \u0026quot;billing_entity\u0026quot;\r, \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot;\r, \u0026quot;count\u0026quot;(DISTINCT \u0026quot;Line_item_resource_id\u0026quot;) \u0026quot;resource_id_count\u0026quot;\r, sum(CASE -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END) \u0026quot;usage_quantity\u0026quot;\r, sum (\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_unblended_cost\u0026quot;\r--\tWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot; -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;) -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;reservation_effective_cost\u0026quot; -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (\u0026quot;reservation_unused_amortized_upfront_fee_for_billing_period\u0026quot; + \u0026quot;reservation_unused_recurring_fee\u0026quot;)\r-- WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN 0 ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) \u0026quot;amortized_cost\u0026quot;\r, sum(CASE\rWHEN (line_item_line_item_type = 'Usage') THEN 0\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (-\u0026quot;savings_plan_amortized_upfront_commitment_for_billing_period\u0026quot;) -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (-\u0026quot;reservation_amortized_upfront_fee_for_billing_period\u0026quot;) ELSE 0 END) \u0026quot;ri_sp_trueup\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN 0\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN \u0026quot;line_item_unblended_cost\u0026quot;\r-- WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0 END) \u0026quot;ri_sp_upfront_fees\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; \u0026lt;\u0026gt; 'SavingsPlanNegation') THEN \u0026quot;pricing_public_on_demand_cost\u0026quot; ELSE 0 END) \u0026quot;public_cost\u0026quot; FROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '7' MONTH)) AND (CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_date) - INTERVAL '7' MONTH)))\rGROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32,33,34\r       Confirm the view is working, run the following Athena query and you should receive 10 rows of data:\n select * from costmaster.summary_view\rlimit 10\r   "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_organization_data_cur_connection/code/2_view2/","title":"","tags":[],"description":"","content":"View 2 - EC2 Running Costs This view will be used to create the EC2 Running Costs dashboard page. Use one of the following queries depending on whether you have Reserved Instances, or Savings Plans.\n    Click here - if you have both Savings Plans and Reserved Instances   Modify the following SQL query for View2 - EC2_Running_Cost:\n  Update line 17 replace (database).(tablename) with your CUR database and table name\n CREATE OR REPLACE VIEW \u0026quot;ec2_running_cost\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, (CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END) \u0026quot;purchase_option\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage' THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot;\rWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage' THEN \u0026quot;reservation_effective_cost\u0026quot;\rWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'Usage' THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0 END) \u0026quot;amortized_cost\u0026quot;\r, \u0026quot;round\u0026quot;(\u0026quot;sum\u0026quot;(\u0026quot;line_item_usage_amount\u0026quot;), 2) \u0026quot;usage_quantity\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((((((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2')) AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer')) AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%')) AND (((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') OR (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage')) OR (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage')))\rGROUP BY 1, 2, 3, 4,5,6,7\r         Click here - if you have Savings Plans, but do not have Reserved Instances   The query is the same as the first query, except some of lines have been commented out. If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View2 - EC2_Running_Cost:\n  Update line 21 replace (database).(tablename) with your CUR database and table name\n CREATE OR REPLACE VIEW \u0026quot;ec2_running_cost\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, (CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END) \u0026quot;purchase_option\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\r--\tWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage' THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot;\rWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage' THEN \u0026quot;reservation_effective_cost\u0026quot;\rWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'Usage' THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0 END) \u0026quot;amortized_cost\u0026quot;\r, \u0026quot;round\u0026quot;(\u0026quot;sum\u0026quot;(\u0026quot;line_item_usage_amount\u0026quot;), 2) \u0026quot;usage_quantity\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE (\r(\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%') AND ((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') -- OR\r-- (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') OR (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage')))\rGROUP BY 1, 2, 3, 4,5,6,7\r         Click here - if you have Reserved Instances, but do not have Savings Plans   The query is the same as the first query, except some of lines have been commented out. If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View2 - EC2_Running_Cost:\n  Update line 21 replace (database).(tablename) with your CUR database and table name\n CREATE OR REPLACE VIEW \u0026quot;ec2_running_cost\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, (CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END) \u0026quot;purchase_option\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\r-- WHEN \u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage' THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot;\rWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage' THEN \u0026quot;reservation_effective_cost\u0026quot;\rWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'Usage' THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0 END) \u0026quot;amortized_cost\u0026quot;\r, \u0026quot;round\u0026quot;(\u0026quot;sum\u0026quot;(\u0026quot;line_item_usage_amount\u0026quot;), 2) \u0026quot;usage_quantity\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE (\r(\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%') AND ((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') -- OR\r-- (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') OR (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage')))\rGROUP BY 1, 2, 3, 4,5,6,7\r         Click here - if you do not have Reserved Instances, and do not have Savings Plans   The query is the same as the first query, except some of lines have been commented out. If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View2 - EC2_Running_Cost:\n  Update line 21 replace (database).(tablename) with your CUR database and table name\n CREATE OR REPLACE VIEW \u0026quot;ec2_running_cost\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, (CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END) \u0026quot;purchase_option\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\r-- WHEN \u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage' THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot;\r-- WHEN \u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage' THEN \u0026quot;reservation_effective_cost\u0026quot;\rWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'Usage' THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0 END) \u0026quot;amortized_cost\u0026quot;\r, \u0026quot;round\u0026quot;(\u0026quot;sum\u0026quot;(\u0026quot;line_item_usage_amount\u0026quot;), 2) \u0026quot;usage_quantity\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE (\r(\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%') AND ((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') -- OR\r-- (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') OR --(\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage')\r))\rGROUP BY 1, 2, 3, 4,5,6,7\r       Confirm the view is working, run the following Athena query and you should receive 10 rows of data:\n select * from costmaster.ec2_running_cost\rlimit 10\r   "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_organization_data_cur_connection/code/3_view3/","title":"","tags":[],"description":"","content":"View 3 - Compute Savings Plan Eligible Spend This view will be used to create the Compute Savings Plan Eligible Spend dashboard page. Use one of the following queries depending on whether you have Reserved Instances, or Savings Plans.\n    Click here - if you have both Savings Plans and Reserved Instances   Modify the following SQL query for View3 - Compute Savings PlaneEligible spend:\n  Update line 23, replace (database).(tablename) with your CUR database and table name\n CREATE OR REPLACE VIEW \u0026quot;compute_savings_plan_eligible_spend\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN ((((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') AND (NOT (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%'))) AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer')) AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%')) THEN\rCASE\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'ElasticMapReduce') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-Concurrency%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Fargate%') THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0\rEND\rELSE 0 END) \u0026quot;unblended_cost\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY)))\rGROUP BY 1, 2, 3, 4,5,6\r         Click here - if you have Savings Plans, but do not have Reserved Instances   If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View3 - Compute Savings PlaneEligible spend:\n  Update line 23, replace (database).(tablename) with your CUR database and table name\n CREATE OR REPLACE VIEW \u0026quot;compute_savings_plan_eligible_spend\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN ((((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') AND (NOT (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%'))) AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer')) AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%')) THEN\rCASE\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'ElasticMapReduce') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-Concurrency%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Fargate%') THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0\rEND\rELSE 0 END) \u0026quot;unblended_cost\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY)))\rGROUP BY 1, 2, 3, 4,5,6\r         Click here - if you have Reserved Instances, but do not have Savings Plans   If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View3 - Compute Savings PlaneEligible spend:\n  Update line 23, replace (database).(tablename) with your CUR database and table name\n CREATE OR REPLACE VIEW \u0026quot;compute_savings_plan_eligible_spend\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN ((((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') AND (NOT (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%'))) AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer')) AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%')) THEN\rCASE\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'ElasticMapReduce') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-Concurrency%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Fargate%') THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0\rEND\rELSE 0 END) \u0026quot;unblended_cost\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY)))\rGROUP BY 1, 2, 3, 4,5,6\r         Click here - if you do not have Reserved Instances, and do not have Savings Plans   If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View3 - Compute Savings PlaneEligible spend:\n  Update line 23, replace (database).(tablename) with your CUR database and table name\n CREATE OR REPLACE VIEW \u0026quot;compute_savings_plan_eligible_spend\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN ((((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') AND (NOT (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%'))) AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer')) AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%')) THEN\rCASE\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'ElasticMapReduce') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-Concurrency%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Fargate%') THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0\rEND\rELSE 0 END) \u0026quot;unblended_cost\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY)))\rGROUP BY 1, 2, 3, 4,5,6\r       Confirm the view is working, run the following Athena query and you should receive 10 rows of data:\n select * from costmaster.compute_savings_plan_eligible_spend\rlimit 10\r   "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_organization_data_cur_connection/code/4_view4/","title":"","tags":[],"description":"","content":"View 4 - S3 This view will be used to create the S3 dashboard page. Use one of the following queries depending on whether you have Reserved Instances, or Savings Plans.\n    Click here - if you have both Savings Plans and Reserved Instances   Modify the following SQL query for View4 - S3:\n  Update line 22, replace (database).(tablename) with your CUR database and table name\n CREATE OR REPLACE VIEW \u0026quot;s3_view\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;line_item_resource_id\u0026quot; \u0026quot;resource_id\u0026quot;\r, \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot;\r, \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot;\r, \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot;\r, \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot;\r, \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot;\rELSE 0\rEND) \u0026quot;usage_quantity\u0026quot;\r, \u0026quot;sum\u0026quot;(\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;\r, \u0026quot;sum\u0026quot;(\u0026quot;pricing_public_on_demand_cost\u0026quot;) \u0026quot;public_cost\u0026quot;\rFROM (ADD YOUR CUR TABLE NAME)\rWHERE ((((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '3' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY))) AND (\u0026quot;line_item_operation\u0026quot; LIKE '%Storage%')) AND ((\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonGlacier%') OR (\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonS3%')))\rGROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,11,12\r         Click here - if you have Savings Plans, but do not have Reserved Instances   If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View4 - S3:\n  Update line 22, replace (database).(tablename) with your CUR database and table name\n CREATE OR REPLACE VIEW \u0026quot;s3_view\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;line_item_resource_id\u0026quot; \u0026quot;resource_id\u0026quot;\r, \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot;\r, \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot;\r, \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot;\r, \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot;\r, \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot;\rELSE 0\rEND) \u0026quot;usage_quantity\u0026quot;\r, \u0026quot;sum\u0026quot;(\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;\r, \u0026quot;sum\u0026quot;(\u0026quot;pricing_public_on_demand_cost\u0026quot;) \u0026quot;public_cost\u0026quot;\rFROM (ADD YOUR CUR TABLE NAME)\rWHERE ((((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '3' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY))) AND (\u0026quot;line_item_operation\u0026quot; LIKE '%Storage%')) AND ((\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonGlacier%') OR (\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonS3%')))\rGROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,11,12\r         Click here - if you have Reserved Instances, but do not have Savings Plans   If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View4 - S3:\n  Update line 22, replace (database).(tablename) with your CUR database and table name\n CREATE OR REPLACE VIEW \u0026quot;s3_view\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;line_item_resource_id\u0026quot; \u0026quot;resource_id\u0026quot;\r, \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot;\r, \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot;\r, \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot;\r, \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot;\r, \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot;\rELSE 0\rEND) \u0026quot;usage_quantity\u0026quot;\r, \u0026quot;sum\u0026quot;(\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;\r, \u0026quot;sum\u0026quot;(\u0026quot;pricing_public_on_demand_cost\u0026quot;) \u0026quot;public_cost\u0026quot;\rFROM (ADD YOUR CUR TABLE NAME)\rWHERE ((((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '3' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY))) AND (\u0026quot;line_item_operation\u0026quot; LIKE '%Storage%')) AND ((\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonGlacier%') OR (\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonS3%')))\rGROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,11,12\r         Click here - if you do not have Reserved Instances, and do not have Savings Plans   If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View4 - S3:\n  Update line 22, replace (database).(tablename) with your CUR database and table name\n CREATE OR REPLACE VIEW \u0026quot;s3_view\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;line_item_resource_id\u0026quot; \u0026quot;resource_id\u0026quot;\r, \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot;\r, \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot;\r, \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot;\r, \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot;\r, \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot;\rELSE 0\rEND) \u0026quot;usage_quantity\u0026quot;\r, \u0026quot;sum\u0026quot;(\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;\r, \u0026quot;sum\u0026quot;(\u0026quot;pricing_public_on_demand_cost\u0026quot;) \u0026quot;public_cost\u0026quot;\rFROM (ADD YOUR CUR TABLE NAME)\rWHERE ((((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '3' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY))) AND (\u0026quot;line_item_operation\u0026quot; LIKE '%Storage%')) AND ((\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonGlacier%') OR (\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonS3%')))\rGROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,11,12\r       Confirm the view is working, run the following Athena query and you should receive 10 rows of data:\n select * from costmaster.s3_view\rlimit 10   "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_2_cost_and_usage_governance/code/iam_policy/","title":"","tags":[],"description":"","content":"Users will require the following access to complete this lab. Edit the policy below before implementation, replace (Account ID) with the required account ID from the account they will work in. Ensure you remove this policy after the lab is completed.\nThis Policy is only required to complete this lab. It must be removed from the users and delted once the lab is complete.\n { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;iam:ListPolicies\u0026quot;, \u0026quot;iam:GetPolicyVersion\u0026quot;, \u0026quot;iam:CreateGroup\u0026quot;, \u0026quot;iam:GetPolicy\u0026quot;, \u0026quot;iam:DeletePolicy\u0026quot;, \u0026quot;iam:DetachGroupPolicy\u0026quot;, \u0026quot;iam:ListGroupPolicies\u0026quot;, \u0026quot;iam:AttachUserPolicy\u0026quot;, \u0026quot;iam:CreateUser\u0026quot;, \u0026quot;iam:GetGroup\u0026quot;, \u0026quot;iam:CreatePolicy\u0026quot;, \u0026quot;iam:CreateLoginProfile\u0026quot;, \u0026quot;iam:AddUserToGroup\u0026quot;, \u0026quot;iam:ListPolicyVersions\u0026quot;, \u0026quot;iam:AttachGroupPolicy\u0026quot;, \u0026quot;iam:ListUsers\u0026quot;, \u0026quot;iam:ListAttachedGroupPolicies\u0026quot;, \u0026quot;iam:ListGroups\u0026quot;, \u0026quot;iam:GetGroupPolicy\u0026quot;, \u0026quot;iam:CreatePolicyVersion\u0026quot;, \u0026quot;iam:DeletePolicyVersion\u0026quot;, \u0026quot;iam:GetLoginProfile\u0026quot;, \u0026quot;iam:GetAccountPasswordPolicy\u0026quot;, \u0026quot;iam:DeleteLoginProfile\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;iam:*\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:iam::(Account ID):user/TestUser1\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;iam:*\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:iam::(Account ID):group/costtest\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;iam:*\u0026quot;, \u0026quot;Resource\u0026quot;: [ \u0026quot;arn:aws:iam::(Account ID):policy/RegionRestrict\u0026quot;, \u0026quot;arn:aws:iam::(Account ID):policy/EC2EBS_Restrict\u0026quot;, \u0026quot;arn:aws:iam::(Account ID):policy/EC2_FamilyRestrict\u0026quot; ] }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;ec2:DeleteSecurityGroup\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:ec2:*:*:security-group/*\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;ec2:DeleteVolume\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:ec2:*:*:volume/*\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;ec2:ModifyVolume\u0026quot;, \u0026quot;ec2:ModifyVolumeAttribute\u0026quot;, \u0026quot;ec2:DescribeVolumeStatus\u0026quot;, \u0026quot;ec2:DescribeVolumes\u0026quot;, \u0026quot;ec2:DescribeVolumesModifications\u0026quot;, \u0026quot;ec2:DescribeVolumeAttribute\u0026quot;, \u0026quot;ec2:DescribeAvailabilityZones\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] }  "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_enterprise_dashboards/code/5_view5/","title":"","tags":[],"description":"","content":"View 5 - RI SP Mapping This view will be used to create the RI SP Mapping dashboard page. Use one of the following queries depending on whether you have Reserved Instances, or Savings Plans.\n    Click here - if you have both Savings Plans and Reserved Instances   Modify the following SQL query for View5 - RI SP Mapping:\n  Update lines 23 and 44, replace (database).(tablename) with your CUR database and table name\n CREATE OR REPLACE VIEW \u0026quot;ri_sp_mapping\u0026quot; AS SELECT DISTINCT\r\u0026quot;a\u0026quot;.\u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;ri_sp_end_date\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_term\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_offering\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_payment\u0026quot;\rFROM\r((\rSELECT DISTINCT\r\u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;ELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;savings_plan_end_time\u0026quot;) AS timestamp)\rWHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '' AND \u0026quot;reservation_end_time\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;reservation_end_time\u0026quot;) AS timestamp) ELSE NULL END \u0026quot;ri_sp_end_date\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') OR (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee'))\r) a\rLEFT JOIN (\rSELECT DISTINCT\r\u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;ELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_purchase_term\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_lease_contract_length\u0026quot;ELSE '' END \u0026quot;ri_sp_term\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_offering_type\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_offering_class\u0026quot; ELSE '' END \u0026quot;ri_sp_offering\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_payment_option\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_purchase_option\u0026quot;\tELSE '' END \u0026quot;ri_sp_Payment\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') OR (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage'))\r) b ON ((\u0026quot;a\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot;) AND (\u0026quot;a\u0026quot;.\u0026quot;billing_period_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;billing_period_mapping\u0026quot;) AND (\u0026quot;a\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot;)))\r         Click here - if you have Savings Plans, but do not have Reserved Instances   If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View5 - RI SP Mapping:\n  Update lines 25 and 54, replace (database).(tablename) with your CUR database and table name\n CREATE OR REPLACE VIEW \u0026quot;ri_sp_mapping\u0026quot; AS SELECT DISTINCT\r\u0026quot;a\u0026quot;.\u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;ri_sp_end_date\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_term\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_offering\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_payment\u0026quot;\rFROM\r((\rSELECT DISTINCT\r\u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;\rELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot;\r,CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;savings_plan_end_time\u0026quot;) AS timestamp)\r-- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '' AND \u0026quot;reservation_end_time\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;reservation_end_time\u0026quot;) AS timestamp) ELSE NULL END \u0026quot;ri_sp_end_date\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE (\r-- (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') -- OR (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee')\r)\r) a\rLEFT JOIN (\rSELECT DISTINCT\r\u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;\rELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_purchase_term\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_lease_contract_length\u0026quot;\rELSE '' END \u0026quot;ri_sp_term\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_offering_type\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_offering_class\u0026quot; ELSE '' END \u0026quot;ri_sp_offering\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_payment_option\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_purchase_option\u0026quot;\rELSE '' END \u0026quot;ri_sp_Payment\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE (\r-- (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') -- OR (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage')\r)\r) b ON ((\u0026quot;a\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot;) AND (\u0026quot;a\u0026quot;.\u0026quot;billing_period_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;billing_period_mapping\u0026quot;) AND (\u0026quot;a\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot;)))\r         Click here - if you have Reserved Instances, but do not have Savings Plans   If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View5 - RI SP Mapping:\n  Update lines 25 and 54, replace (database).(tablename) with your CUR database and table name\n CREATE OR REPLACE VIEW \u0026quot;ri_sp_mapping\u0026quot; AS SELECT DISTINCT\r\u0026quot;a\u0026quot;.\u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;ri_sp_end_date\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_term\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_offering\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_payment\u0026quot;\rFROM\r((\rSELECT DISTINCT\r\u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;\rELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;savings_plan_end_time\u0026quot;) AS timestamp)\rWHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '' AND \u0026quot;reservation_end_time\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;reservation_end_time\u0026quot;) AS timestamp) ELSE NULL END \u0026quot;ri_sp_end_date\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE (\r(\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') -- OR -- (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee')\r)\r) a\rLEFT JOIN (\rSELECT DISTINCT\r\u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;\rELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_purchase_term\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_lease_contract_length\u0026quot;\rELSE '' END \u0026quot;ri_sp_term\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_offering_type\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_offering_class\u0026quot; ELSE '' END \u0026quot;ri_sp_offering\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_payment_option\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_purchase_option\u0026quot;\rELSE '' END \u0026quot;ri_sp_Payment\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE (\r(\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') -- OR -- (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage')\r)\r) b ON ((\u0026quot;a\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot;) AND (\u0026quot;a\u0026quot;.\u0026quot;billing_period_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;billing_period_mapping\u0026quot;) AND (\u0026quot;a\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot;)))\r         Click here - if you do not have Reserved Instances, and do not have Savings Plans   If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View5 - RI SP Mapping:\n  Update lines 27 and 62, replace (database).(tablename) with your CUR database and table name\n CREATE OR REPLACE VIEW \u0026quot;ri_sp_mapping\u0026quot; AS SELECT DISTINCT\r\u0026quot;a\u0026quot;.\u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;ri_sp_end_date\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_term\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_offering\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_payment\u0026quot;\rFROM\r((\rSELECT DISTINCT\r\u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN ' '\rELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot;\r, CAST(CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;savings_plan_end_time\u0026quot;) AS timestamp)\r-- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '' AND \u0026quot;reservation_end_time\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;reservation_end_time\u0026quot;) AS timestamp) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN ' '\rELSE NULL END AS timestamp) \u0026quot;ri_sp_end_date\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE (\r(\u0026quot;line_item_line_item_type\u0026quot; \u0026lt;\u0026gt; 'Usage') -- OR\r-- (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') -- OR -- (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee')\r)\r) a\rLEFT JOIN (\rSELECT DISTINCT\r\u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN ' '\rELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_purchase_term\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_lease_contract_length\u0026quot;\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN ' '\rELSE '' END \u0026quot;ri_sp_term\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_offering_type\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_offering_class\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN ' '\rELSE '' END \u0026quot;ri_sp_offering\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_payment_option\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_purchase_option\u0026quot;\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN ' '\rELSE '' END \u0026quot;ri_sp_Payment\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE (\r(\u0026quot;line_item_line_item_type\u0026quot; \u0026lt;\u0026gt; 'Usage') -- OR\r-- (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') -- OR -- (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage')\r)\r) b ON ((\u0026quot;a\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot;) AND (\u0026quot;a\u0026quot;.\u0026quot;billing_period_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;billing_period_mapping\u0026quot;) AND (\u0026quot;a\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot;)))\r       Confirm the view is working, run the following Athena query and you should receive 10 rows of data:\n select * from costmaster.ri_sp_mapping\rlimit 10\r   "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_organization_data_cur_connection/code/5_view5/","title":"","tags":[],"description":"","content":"View 5 - RI SP Mapping This view will be used to create the RI SP Mapping dashboard page. Use one of the following queries depending on whether you have Reserved Instances, or Savings Plans.\n    Click here - if you have both Savings Plans and Reserved Instances   Modify the following SQL query for View5 - RI SP Mapping:\n  Update lines 23 and 44, replace (database).(tablename) with your CUR database and table name\n CREATE OR REPLACE VIEW \u0026quot;ri_sp_mapping\u0026quot; AS SELECT DISTINCT\r\u0026quot;a\u0026quot;.\u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;ri_sp_end_date\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_term\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_offering\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_payment\u0026quot;\rFROM\r((\rSELECT DISTINCT\r\u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;ELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;savings_plan_end_time\u0026quot;) AS timestamp)\rWHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;reservation_end_time\u0026quot;) AS timestamp) ELSE NULL END \u0026quot;ri_sp_end_date\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') OR (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee'))\r) a\rLEFT JOIN (\rSELECT DISTINCT\r\u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;ELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_purchase_term\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_lease_contract_length\u0026quot;ELSE '' END \u0026quot;ri_sp_term\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_offering_type\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_offering_class\u0026quot; ELSE '' END \u0026quot;ri_sp_offering\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_payment_option\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_purchase_option\u0026quot;\tELSE '' END \u0026quot;ri_sp_Payment\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') OR (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage'))\r) b ON ((\u0026quot;a\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot;) AND (\u0026quot;a\u0026quot;.\u0026quot;billing_period_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;billing_period_mapping\u0026quot;) AND (\u0026quot;a\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot;)))\r         Click here - if you have Savings Plans, but do not have Reserved Instances   If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View5 - RI SP Mapping:\n  Update lines 25 and 54, replace (database).(tablename) with your CUR database and table name\n CREATE OR REPLACE VIEW \u0026quot;ri_sp_mapping\u0026quot; AS SELECT DISTINCT\r\u0026quot;a\u0026quot;.\u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;ri_sp_end_date\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_term\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_offering\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_payment\u0026quot;\rFROM\r((\rSELECT DISTINCT\r\u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;\rELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;savings_plan_end_time\u0026quot;) AS timestamp)\r-- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;reservation_end_time\u0026quot;) AS timestamp) ELSE NULL END \u0026quot;ri_sp_end_date\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE (\r-- (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') -- OR (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee')\r)\r) a\rLEFT JOIN (\rSELECT DISTINCT\r\u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;\rELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_purchase_term\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_lease_contract_length\u0026quot;\rELSE '' END \u0026quot;ri_sp_term\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_offering_type\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_offering_class\u0026quot; ELSE '' END \u0026quot;ri_sp_offering\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_payment_option\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_purchase_option\u0026quot;\rELSE '' END \u0026quot;ri_sp_Payment\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE (\r-- (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') -- OR (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage')\r)\r) b ON ((\u0026quot;a\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot;) AND (\u0026quot;a\u0026quot;.\u0026quot;billing_period_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;billing_period_mapping\u0026quot;) AND (\u0026quot;a\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot;)))\r         Click here - if you have Reserved Instances, but do not have Savings Plans   If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View5 - RI SP Mapping:\n  Update lines 25 and 54, replace (database).(tablename) with your CUR database and table name\n CREATE OR REPLACE VIEW \u0026quot;ri_sp_mapping\u0026quot; AS SELECT DISTINCT\r\u0026quot;a\u0026quot;.\u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;ri_sp_end_date\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_term\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_offering\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_payment\u0026quot;\rFROM\r((\rSELECT DISTINCT\r\u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;\rELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;savings_plan_end_time\u0026quot;) AS timestamp)\rWHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;reservation_end_time\u0026quot;) AS timestamp) ELSE NULL END \u0026quot;ri_sp_end_date\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE (\r(\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') -- OR -- (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee')\r)\r) a\rLEFT JOIN (\rSELECT DISTINCT\r\u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;\rELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_purchase_term\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_lease_contract_length\u0026quot;\rELSE '' END \u0026quot;ri_sp_term\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_offering_type\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_offering_class\u0026quot; ELSE '' END \u0026quot;ri_sp_offering\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_payment_option\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_purchase_option\u0026quot;\rELSE '' END \u0026quot;ri_sp_Payment\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE (\r(\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') -- OR -- (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage')\r)\r) b ON ((\u0026quot;a\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot;) AND (\u0026quot;a\u0026quot;.\u0026quot;billing_period_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;billing_period_mapping\u0026quot;) AND (\u0026quot;a\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot;)))\r         Click here - if you do not have Reserved Instances, and do not have Savings Plans   If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View5 - RI SP Mapping:\n  Update lines 27 and 60, replace (database).(tablename) with your CUR database and table name\n CREATE OR REPLACE VIEW \u0026quot;ri_sp_mapping\u0026quot; AS SELECT DISTINCT\r\u0026quot;a\u0026quot;.\u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;ri_sp_end_date\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_term\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_offering\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_payment\u0026quot;\rFROM\r((\rSELECT DISTINCT\r\u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN ''\rELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot;\r, CASE --\tWHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;savings_plan_end_time\u0026quot;) AS timestamp)\r--\tWHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;reservation_end_time\u0026quot;) AS timestamp) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN NULL\rELSE NULL END \u0026quot;ri_sp_end_date\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\r-- WHERE (\r-- (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') -- OR -- (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee')\r-- )\r) a\rLEFT JOIN (\rSELECT DISTINCT\r\u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN ''\rELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_purchase_term\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_lease_contract_length\u0026quot;\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN ''\rELSE '' END \u0026quot;ri_sp_term\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_offering_type\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_offering_class\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN ''\rELSE '' END \u0026quot;ri_sp_offering\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_payment_option\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_purchase_option\u0026quot;\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN ''\rELSE '' END \u0026quot;ri_sp_Payment\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\r-- WHERE (\r-- (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') -- OR -- (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage')\r-- )\r) b ON ((\u0026quot;a\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot;) AND (\u0026quot;a\u0026quot;.\u0026quot;billing_period_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;billing_period_mapping\u0026quot;) AND (\u0026quot;a\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot;)))\r       Confirm the view is working, run the following Athena query and you should receive 10 rows of data:\n select * from costmaster.ri_sp_mapping\rlimit 10\r   "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_enterprise_dashboards/code/","title":"","tags":[],"description":"","content":""},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_organization_data_cur_connection/code/","title":"","tags":[],"description":"","content":""},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_2_cost_and_usage_governance/code/","title":"","tags":[],"description":"","content":""},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_splitting_sharing_cur_access/code/","title":"","tags":[],"description":"","content":""},{"uri":"https://wellarchitectedlabs.com/operational-excellence/","title":"Operational Excellence","tags":[],"description":"","content":"Introduction This repository contains documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices.\nFor more information about Operational Excellence on AWS visit the Well-Architected tool in the AWS console, and read the AWS Well-Architected Operational Excellence whitepaper.\nLabs  100 Labs    100 - Inventory and Patch Management   100 - Dependency Monitoring    200 Labs    200 - Automating your operations with Playbooks and Runbooks    "},{"uri":"https://wellarchitectedlabs.com/security/","title":"Security","tags":[],"description":"","content":"Introduction The security labs are documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices. The labs are categorized into levels, where 100 is foundational, 200 is intermediate, 300 is advanced, and 400 is expert. Quests are designed to collate a group of relevant labs and other resources together into a common theme for you to follow and learn.\nFor more information about security on AWS visit AWS Security and read the AWS Well-Architected Security whitepaper in PDF or online . Also check out https://awssecworkshops.com/ for hands-on workshops, and AWS Training and Certification Learning Library for official security training options.\nLabs \u0026amp; Quests  100 Level Foundational Labs    AWS Account Setup and Root User   Creating your first Identity and Access Management User, Group, Role   CloudFront with S3 Bucket Origin   Enable Security Hub   Create a Data Bunker Account    200 Level Intermediate Labs    Automated Deployment of Detective Controls   Automated Deployment of EC2 Web Application   Automated Deployment of IAM Groups and Roles   Level 200: Automated Deployment of VPC   Level 200: Automated Deployment of Web Application Firewall   Level 200: Automated IAM User Cleanup   Level 200: Basic EC2 Web Application Firewall Protection   Level 200: AWS Certificate Manager Request Public Certificate   Level 200: CloudFront for Web Application   Level 200: CloudFront with WAF Protection   Level 200: Remote Configuration, Installation, and Viewing of CloudWatch logs    300 Level Advanced Labs    Level 300: Autonomous Monitoring Of Cryptographic Activity With KMS   Level 300: Autonomous Patching With EC2 Image Builder And Systems Manager   Level 300: IAM Permission Boundaries Delegating Role Creation   Level 300: IAM Tag Based Access Control for EC2   Level 300: Incident Response Playbook with Jupyter - AWS IAM   Level 300: Incident Response with AWS Console and CLI   Level 300: Lambda Cross Account Using Bucket Policy   Level 300: Lambda Cross Account IAM Role Assumption    Quests    Quest: Loft - Introduction to Security   Quest: Quick Steps to Security Success   Quest: AWS Incident Response Day   Quest: re:Invent 2020 - Automate The Well-Architected Way With WeInvest   Quest: AWS Security Best Practices Workshop   Quest: AWS Security Best Practices Day   Quest: Managing Credentials \u0026amp; Authentication   Quest: Control Human Access   Quest: Control Programmatic Access   Quest: Detect \u0026amp; Investigate Events   Quest: Defend Against New Threats   Quest: Protect Networks   Quest: Protect Compute   Quest: Classify Data   Quest: Protect Data at Rest   Quest: Protect Data in Transit   Quest: Incident Response    "},{"uri":"https://wellarchitectedlabs.com/reliability/","title":"Reliability","tags":[],"description":"","content":"These hands-on labs will teach you how to implement reliable workloads using AWS.\n Reliability is the ability of a workload to perform its intended function correctly and consistently when its expected to. This includes the ability to operate and test the workload through its total lifecycle Resiliency is the ability of a workload to recover from infrastructure or service disruptions, dynamically acquire computing resources to meet demand, and mitigate disruptions, such as misconfigurations or transient network issues. Reliability depends on multiple factors, of which resiliency is one the most impactful.  For more information about Reliability, read the AWS Well-Architected Reliability whitepaper .\nReliability Labs Reliability Labs by Level  100 Labs    Level 100: Deploy a Reliable Multi-tier Infrastructure using CloudFormation    200 Labs    Level 200: Implementing Bi-Directional Cross-Region Replication (CRR) for Amazon Simple Storage Service (Amazon S3)   Level 200: Deploy and Update CloudFormation   Level 200: Testing Backup and Restore of Data   Level 200: Testing for Resiliency of EC2 instances    300 Labs    Level 300: Implementing Health Checks and Managing Dependencies to improve Reliability   Level 300: Testing for Resiliency of EC2, RDS, and AZ   Level 300: Fault Isolation with Shuffle Sharding    Reliability Labs by tag (topic)   implement_change    Level 100: Deploy a Reliable Multi-tier Infrastructure using CloudFormation   Level 200: Deploy and Update CloudFormation   data_backup    Level 200: Testing Backup and Restore of Data   Level 200: Implementing Bi-Directional Cross-Region Replication (CRR) for Amazon Simple Storage Service (Amazon S3)   test_resiliency    Level 300: Testing for Resiliency of EC2, RDS, and AZ   Level 200: Testing for Resiliency of EC2 instances   mitigate_failure    Level 300: Implementing Health Checks and Managing Dependencies to improve Reliability   Level 300: Fault Isolation with Shuffle Sharding  "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/","title":"Performance Efficiency","tags":[],"description":"","content":"Introduction This repository contains documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices.\nFor more information about Performance Efficiency on AWS visit the Well-Architected tool in the AWS console, and read the AWS Well-Architected Performance Efficiency whitepaper.\nLabs  100 Labs    Level 100: Monitoring with CloudWatch Dashboards   Level 100: Calculating differences in clock source   Level 100: Monitoring Windows EC2 instance with CloudWatch Dashboards   Level 100: Monitoring an Amazon Linux EC2 instance with CloudWatch Dashboards    "},{"uri":"https://wellarchitectedlabs.com/cost/","title":"Cost Optimization","tags":[],"description":"","content":" Your browser doesn't support video, or if you're on GitHub head to https://wellarchitectedlabs.com to watch the video.  Introduction This repository contains documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices.\nFor more information about Cost Optimization on AWS visit the Well-Architected tool in the AWS console, and read the AWS Well-Architected Cost Optimization whitepaper.\nLabs  Fundamentals     Expenditure Awareness     Cost Effective Resources     100 Labs    Level 100: AWS Account Setup: Lab Guide   Level 100: Cost and Usage Governance   Level 100: Pricing Models   Level 100: Cost and Usage Analysis   Level 100: Cost Visualization   Level 100: EC2 Rightsizing   Level 100: Goals and Targets    200 Labs    Level 200: Cost and Usage Governance   Level 200: Pricing Models   Level 200: Cost and Usage Analysis   Level 200: Cost Visualization   Level 200: EC2 Right Sizing   Level 200: Pricing Model Analysis   Level 200: Enterprise Dashboards   Level 200: Workload Efficiency   Level 200: Licensing   Level 200: Cost Journey    300 Labs    Level 300: Automated Athena CUR Query and E-mail Delivery   Level 300: Automated CUR Updates and Ingestion   Level 300: CUR Queries   Level 300: Splitting the CUR and Sharing Access   Level 300: Organization Data CUR Connection    "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/","title":"Well-Architected Tool","tags":[],"description":"","content":"  Your browser doesn't support video, or if you're on GitHub head to https://wellarchitectedlabs.com to watch the video.   Introduction This repository contains documentation and code in the format of hands-on-labs to help you learn how to learn, measure, and build using architectural best practices. The labs are categorized into levels, where 100 is introductory, 200/300 is intermediate and 400 is advanced.\nFor more information about the Well-Architected tool, read the AWS Well-Architected Tool documentation .\nLabs  100 Labs    Level 100: Walkthrough of the Well-Architected Tool   Contributing to Well-Architected Labs    200 Labs    Level 200: Using AWSCLI to Manage WA Reviews    "},{"uri":"https://wellarchitectedlabs.com/tags/data_backup/","title":"data_backup","tags":[],"description":"","content":""},{"uri":"https://wellarchitectedlabs.com/tags/","title":"Tags","tags":[],"description":"","content":""},{"uri":"https://wellarchitectedlabs.com/tags/amazon-linux/","title":"Amazon Linux","tags":[],"description":"","content":""},{"uri":"https://wellarchitectedlabs.com/tags/cloudwatch/","title":"CloudWatch","tags":[],"description":"","content":""},{"uri":"https://wellarchitectedlabs.com/tags/cloudwatch-dashboard/","title":"CloudWatch Dashboard","tags":[],"description":"","content":""},{"uri":"https://wellarchitectedlabs.com/tags/ec2/","title":"EC2","tags":[],"description":"","content":""},{"uri":"https://wellarchitectedlabs.com/tags/linux/","title":"Linux","tags":[],"description":"","content":""},{"uri":"https://wellarchitectedlabs.com/tags/windows/","title":"Windows","tags":[],"description":"","content":""},{"uri":"https://wellarchitectedlabs.com/tags/windows-server/","title":"Windows Server","tags":[],"description":"","content":""},{"uri":"https://wellarchitectedlabs.com/tags/implement_change/","title":"implement_change","tags":[],"description":"","content":""},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/documentation/cfn_parameters/","title":"CloudFormation Parameters","tags":[],"description":"","content":"All entries are Case-Sensitive\nsingle region stack    Parameter Default Value     CreateTheAutoScalingServiceRole true   CreateTheELBServiceRole true   CreateTheRDSServiceRole true   LambdaFunctionsBucket aws-well-architected-labs-ohio   RDSLambdaKey Reliability/RDSLambda.zip   VPCLambdaKey Reliability/Reliability/VPCLambda.zip   WaitForStackLambdaKey Reliability/WaitForStack.zip   WebAppLambdaKey Reliability/WebAppLambda.zip    multi region stack    Parameter Default Value     CreateTheAutoScalingServiceRole true   CreateTheELBServiceRole true   CreateTheRDSServiceRole true   DMSLambdaKey Reliability/DMSLambda.zip   LambdaFunctionsBucket aws-well-architected-labs-ohio   RDSLambdaKey Reliability/RDSLambda.zip   RDSRRLambdaKey Reliability/RDSReadReplicaLambda.zip   VPCLambdaKey Reliability/Reliability/VPCLambda.zip   WaitForStackLambdaKey Reliability/WaitForStack.zip   WebAppLambdaKey Reliability/WebAppLambda.zip     "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/documentation/self_aws_account/","title":"Creating new AWS credentials for your AWS account","tags":[],"description":"","content":"Use these instructions to get a AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY which you will need for the workshop\nIf you are using your own AWS account\n These instructions are for you. Use this guide if you are running the workshop on your own, or with at an event using your own AWS account you have brought with you  If you are attending an in-person workshop and were provided with an AWS account by the instructor\n STOP \u0026ndash; Follow the If you are attending an in-person workshop and were provided with an AWS account by the instructor instructions here instead    Create new AWS credentials for an IAM User you already control   Sign in to the AWS Management Console as a IAM user who has IAM management permissions and open the IAM console at https://console.aws.amazon.com/iam/\n  In the navigation pane, choose Users.\n  Choose the name of the user whose access keys you want to manage.\n  Select the Permissions tab of this user and confirm that they have either PowerUserAccess or AdministratorAccess policy attached. If not, attach the PowerUserAccess policy using the Add permissions button.\n  Select the Security credentials tab.\n  Choose Create access key. Then choose Download .csv file to save the access key ID and secret access key to a CSV file on your computer. Store the file in a secure location. You will not have access to the secret access key again after this dialog box closes. After you download the CSV file, choose Close.\n   Create a new IAM User for use in the lab Use the instructions only if you cannot Create new AWS credentials for an IAM User you already control. If you have already obtained a AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY using the preceding instructions then STOP and Click here to return to the Lab Guide \n  Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/\n  In the navigation pane, choose Users and then choose Add user.\n  Type the user name for the new user. if you wish you can choose rel300-workshop\n  Select programmatic access. Including access to the AWS Management Console is optional\n  Choose Next: Permissions\n  select Attach existing policies to user directly\n In the search box type PowerUserAccess tick the check box next to PowerUserAccess    Choose Next: Tags\n  Choose Next: Review\n  Choose Create User\n  IMPORTANT: Choose Download.csv file to save the access key ID and secret access key to a CSV file on your computer. Store the file in a secure location. You will not have access to the secret access key again after this dialog box closes. After you download the CSV file, choose Close.\n   "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/documentation/multi_region_stack_deletion/","title":"Delete workshop CloudFormation stacks - Multi region deployment","tags":[],"description":"","content":" Since AWS resources deployed by AWS CloudFormation stacks may have dependencies on the stacks that were created before, then deletion must occur in the opposite order they were created Stacks with the same ordinal can be deleted at the same time. All stacks for a given ordinal must be DELETE_COMPLETE before moving on to the next ordinal The AWS Console does not let you select multiple stacks for deletion. To simultaneously delete stacks, individually select one stack at a time and click the Delete button. Helpful hint: have the AWS CloudFormation console for each region open in separate tabs  CloudFormation console for Ohio  CloudFormation console for Oregon        Order CloudFormation stack Region     1 DMSforResiliencyTesting Oregon   1 MySQLReadReplicaResiliencyTesting Oregon   1 MySQLReadReplicaResiliencyTesting Ohio        2 WebServersforResiliencyTesting Ohio   2 MySQLforResiliencyTesting Ohio   2 WebServersforResiliencyTesting Oregon   2 MySQLforResiliencyTesting Oregon        3 ResiliencyVPC Ohio   3 ResiliencyVPC Oregon   3 DeployResiliencyWorkshop Ohio     "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/documentation/s3_with_aws_cli/","title":"Disable All Public Read Access to an S3 Bucket using AWS CLI","tags":[],"description":"","content":"Disable read access to S3 bucket   This command will disable public read from an entire bucket. If you want to only disable public read from one object, use the AWS Console instructions\n  If your S3 bucket is in a different aWS account, you will need to provide credentials for that account first.\n aws ssm start-automation-execution --document-name AWS-DisableS3BucketPublicReadWrite --parameters \u0026quot;{\\\u0026quot;S3BucketName\\\u0026quot;: [\\\u0026quot;\u0026lt;bucket-name\u0026gt;\\\u0026quot;]}\u0026quot;    Return to the Lab Guide , but keep this page open if you want to re-enable public read access to the bucket after testing.\n Re-enable access (after testing) using the S3 console  This requires using the S3 console. Go to the S3 console: https://console.aws.amazon.com/s3 Select the bucket name where the image is located Select the \u0026ldquo;Permissions\u0026rdquo; tab Click Edit (upper-right) Un-check all the boxes Click Save You are asked to type \u0026ldquo;confirm\u0026rdquo; - this is a security feature to ensure you truly intend this bucket to allow public access.   Click here to return to the Lab Guide \n"},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_remote_configuration_installation_and_viewing_cloudwatch_logs/","title":"Level 200: Remote Configuration, Installation, and Viewing of CloudWatch logs","tags":[],"description":"","content":"Authors  Pavan Baloo, Solutions Architect Intern, Well-Architected  Introduction Most applications require monitoring services that make up the workload to understand the workload state and performance. One way of collecting this data is through log files generated by the application and underlying services. Collecting and analyzing log files improves your security posture by creating a record of activity or audit trail in your workload, enabling you to detect and investigate potential threats.\nManually configuring logging on each instance is tedious and becomes difficult at scale. It increases your risk of human error and unintended access from accessing instances directly with a protocol like SSH. Manually processing the collected data is difficult to scale with large volumes of data.\nUtilizing AWS services such as AWS Systems Manager, Amazon CloudWatch, Amazon Simple Storage Service (S3), Amazon Athena, and Amazon QuickSight, you can collect and store logs without having to directly access the instance, or accessing data directly. You minimize your threat surface area by removing SSH access on your instance and improve your threat detection by collecting valuable log data.\nThis lab illustrates the following Well-Architected Security Best Practices:\n Configure service and application logging: You will configure the CloudWatch agent on an EC2 instance. This enables you to collect logs from the instance used to host your application, such as Apache Web Server logs, SSH logs, boot logs, and more. Configure services and resources centrally: You will centrally configure your CloudWatch log agent by storing the configuration file in Systems Manager Parameter Store. Parameter Store enables you to maintain consistent, reusable configuration data. Analyze logs centrally: You will analyze logs centrally in this lab in two ways. Using the CloudWatch console, you can view all of your raw log data in one location. Through QuickSight, you can create visualizations from your logs that can be shared with others for central viewing of key data. Enable people to perform actions at a distance: You will use Systems Manager Run Command to install and start the CloudWatch agent on your EC2 instance. You will perform these actions at a distance through Run Command, as you will not need to SSH directly into the instance to perform these tasks. Reduce attack surface: Run Command removes the necessity to directly SSH into the EC2 instance. Because of this, you can close the SSH access port on your instance, reducing the attack surface of the workload.  In the lab, you will deploy an EC2 instance with Apache and PHP installed. The web server will host a very simple website. You will configure a CloudWatch Agent on the instance via Amazon Systems Manager (SSM). This agent will collect log files from services running on the EC2 instance, such as Apache access and error logs, yum logs, SSH logs, and CloudWatch agent logs. These logs are exported from the EC2 instance to the CloudWatch logs service for centralized storage. You will export these logs to an S3 bucket for long term storage and archival. These logs will then be queried via Athena, so people are kept away from accessing the log files directly. This data will be visually represented in a QuickSight dashboard.\nPrerequisites  I have access to an AWS Account to use for testing, from which I can deploy EC2 instances, create S3 Buckets, access and export CloudWatch Logs, run Athena queries, and use QuickSight. I am operating in a region in which I can use Amazon EC2, Amazon S3, Amazon CloudWatch, Amazon Athena, AWS Systems Manager, and Amazon QuickSight. To see if these services are available in your region, click to view the service availability page .  Files Used  Lab CloudFormation Template  CloudWatch Agent Configuration    function prevStep(){ window.open(\"\", \"_self\") } function nextStep(){ window.open(\".\\/1_deploy_cfn_stack\\/\", \"_self\") } function togglePopup(){ document.getElementById(\"prevnext-1\").classList.toggle(\"active\"); }   X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool  Start Lab  Steps  Deploy the CloudFormation Stack   Install the CloudWatch Agent   Store the CloudWatch Config File in Parameter Store   Start the CloudWatch Agent   Generate Logs   View your CloudWatch Logs   Export Logs to S3   Query logs from S3 using Athena   Create a QuickSight Visualization   Lab Recap   Lab Teardown   "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/documentation/multi_region_state_machine/","title":"Multi Region State Machine","tags":[],"description":"","content":" "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/documentation/multi_region_event_data/","title":"New Execution Input for **multi region** Deployment","tags":[],"description":"","content":"  On the \u0026ldquo;New execution\u0026rdquo; dialog, for \u0026ldquo;Enter an execution name\u0026rdquo; enter BuildResiliency\n  Then for \u0026ldquo;Input\u0026rdquo; enter JSON that will be used to supply parameter values to the Lambdas in the workflow.\n  multi region uses the following values\n { \u0026quot;region1\u0026quot;: { \u0026quot;log_level\u0026quot;: \u0026quot;DEBUG\u0026quot;, \u0026quot;region_name\u0026quot;: \u0026quot;us-east-2\u0026quot;, \u0026quot;secondary_region_name\u0026quot;: \u0026quot;us-west-2\u0026quot;, \u0026quot;cfn_region\u0026quot;: \u0026quot;us-east-2\u0026quot;, \u0026quot;cfn_bucket\u0026quot;: \u0026quot;aws-well-architected-labs-ohio\u0026quot;, \u0026quot;folder\u0026quot;: \u0026quot;Reliability/\u0026quot;, \u0026quot;workshop\u0026quot;: \u0026quot;300-ResiliencyofEC2RDSandS3\u0026quot;, \u0026quot;boot_bucket\u0026quot;: \u0026quot;aws-well-architected-labs-ohio\u0026quot;, \u0026quot;boot_prefix\u0026quot;: \u0026quot;Reliability/\u0026quot;, \u0026quot;websiteimage\u0026quot; : \u0026quot;https://aws-well-architected-labs-ohio.s3.us-east-2.amazonaws.com/images/Cirque_of_the_Towers.jpg\u0026quot; }, \u0026quot;region2\u0026quot;: { \u0026quot;log_level\u0026quot;: \u0026quot;DEBUG\u0026quot;, \u0026quot;region_name\u0026quot;: \u0026quot;us-west-2\u0026quot;, \u0026quot;secondary_region_name\u0026quot;: \u0026quot;us-east-2\u0026quot;, \u0026quot;cfn_region\u0026quot;: \u0026quot;us-east-2\u0026quot;, \u0026quot;cfn_bucket\u0026quot;: \u0026quot;aws-well-architected-labs-ohio\u0026quot;, \u0026quot;folder\u0026quot;: \u0026quot;Reliability/\u0026quot;, \u0026quot;workshop\u0026quot;: \u0026quot;300-ResiliencyofEC2RDSandS3\u0026quot;, \u0026quot;boot_bucket\u0026quot;: \u0026quot;aws-well-architected-labs-ohio\u0026quot;, \u0026quot;boot_prefix\u0026quot;: \u0026quot;Reliability/\u0026quot;, \u0026quot;websiteimage\u0026quot; : \u0026quot;https://aws-well-architected-labs-ohio.s3.us-east-2.amazonaws.com/images/Cirque_of_the_Towers.jpg\u0026quot; } }    Note: for websiteimage you can supply an alternate link to a public-read-only image in an S3 bucket you control. This will allow you to run S3 resiliency tests as part of the lab\n     "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/documentation/service_linked_roles/","title":"Service-Linked Roles","tags":[],"description":"","content":"Does AWS account already have service-linked roles AWS requires service-linked roles for AWS Auto Scaling, Elastic Load Balancing, and Amazon RDS to create the services and metrics they manage. If your AWS account has been previously been used, then these roles may already exist as they would have been automatically created for you. You will determine if any of the following three IAM service-linked roles already exists in the AWS account you are using for this workshop:\n AWSServiceRoleForElasticLoadBalancing AWSServiceRoleForAutoScaling AWSServiceRoleForRDS  Steps to determine if service-linked roles already exist   Open the IAM console at https://console.aws.amazon.com/iam/   In the navigation pane, click Roles.\n  In the filter box, type Service to find the service linked roles that exist in your account and look for the three roles. In this screenshot, the service linked role for AutoScaling exists (AWSServiceRoleForAutoScaling), but the roles for Elastic Load Balancing and RDS do not. Note which roles already exist as you will use this information when performing the next step.\n  STOP HERE and return to the Lab Guide \n Learn more: After the lab see the AWS documentation on Service-Linked Roles \n Setup CloudFormation for service-linked roles If you are using your own AWS account: Then use these instructions when entering CloudFormation parameters\nIf you are attending an in-person workshop and were provided with an AWS account by the instructor: Skip this step and go to back to the Lab Guide    If you already have this role \u0026hellip;then set this parameter false     AWSServiceRoleForElasticLoadBalancing CreateTheELBServiceRole   AWSServiceRoleForAutoScaling CreateTheAutoScalingServiceRole   AWSServiceRoleForRDS CreateTheRDSServiceRole     If the service-linked role does not already exist, then leave the parameter value as true   Leave all the other parameter values at their default values   "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/documentation/aws_credentials/","title":"Setup AWS credentials and configuration","tags":[],"description":"","content":" You will supply configuration and credentials used by the AWS CLI and AWS SDK to access your AWS account. You identified these credentials back in step 1 of the Lab Guide   Choose an option Select the appropriate option for configuration of your AWS credentials:\n Option 1 - Using AWS instructor supplied accounts with Linux-style environment variables Option 2 - Using AWS CLI Option 3 - Creating configuration files manually Option 4 - Using PowerShell commands for Windows  Option 1 For instructor supplied AWS accounts If BOTH of the following are true then you may use Option 1\n If you are attending an in-person workshop and were provided with an AWS account by the instructor then you should use this option You are running the workshop on a system where environment variables are set using the export command, such as Bash on Amazon Linux  Otherwise you should choose Option 2 or Option 3\n  You should have already copied the credentials for your account. If not then:\n  Click here for instructions to copy the credentials from your assigned AWS account:   1. Go to \u0026lt;https://dashboard.eventengine.run/login\u0026gt;    Enter the 12 character hashcode you were provided and click \u0026ldquo;Proceed\u0026rdquo;   [optional] assign a name to your account (this is referred to as \u0026ldquo;Team name\u0026rdquo;)\n click \u0026ldquo;Set Team Name\u0026rdquo; Enter a name and click \u0026ldquo;Set Team Name\u0026rdquo;    Click \u0026ldquo;AWS Console\u0026rdquo;\n  AWS credentials   IMPORTANT Copy the provided credentials and save them. You wil need these to complete the workshop   Copy the whole code block corresponding to the system you are using.\n  Access the AWS console  Click \u0026ldquo;Open Console\u0026rdquo;. The AWS Console will open.   Now continue the steps to setup your AWS credentials\u0026hellip;.\n    The copied credentials are already in the form of export statements. Run these from your shell command line. Use your values, not the ones below\n export AWS_ACCESS_KEY_ID=ASIIAMFAKENOPZLX6J5L export AWS_SECRET_ACCESS_KEY=w0pE4j5k4FlUrkIIAMFAKEdiLMKLGZlxyct+GpTam export AWS_SESSION_TOKEN=FQoGZXIvYXdzEDwaIIAMFAKEn0LVImWNQHiLuAWKe+KFkLeIvpOHEruWjyCjrEdyjtW8WCbnmJGM1ES20xq1fcaS5TERHDUabZJ60Kk6nc9uHoCDb1QKHi+MerRIcKJTi3OKz0QMVPAGVqVWgvOBBSQ2lylLVjtMMSQF+yLZsP1bvehQ0ke/Bl/X6RJySOHg2TZGyESPL/INqJiZyEHi+MelAnThepVgWUKFPD5mESBVlpy2LVCE3xPpHFqOm0Q79svRSSW2jLj5NkRXL+xhkcvt+g8vNt1ODEwixwMGpFB2sBHryv6EXNeX6c88vxJ8Zyfkmsqi0xmCW1f9jWAPIXNkt/nEYW4J4coyLKP7QU= export AWS_DEFAULT_REGION=us-east-2    Also run this command as written below\n export AWS_DEFAULT_OUTPUT=json    Note that if you end your bash session, or start a new one, you will need to re-execute the export statements\n  If you completed Option 1 then STOP HERE and return to the Lab Guide \nOption 2 AWS CLI This option uses the AWS CLI. Note that running the bash failure testing scripts requires this software. If you are using another programming environment for failure testing, you can use Option 3 if you do not or cannot install the AWS CLI.\n  To see if the AWS CLI is installed:\n $ aws --version aws-cli/1.16.249 Python/3.6.8...   AWS CLI version 1.1 or higher is fine If you instead got command not found then either install the AWS CLI or use Option 3    Run aws configure and provide the following values:\n $ aws configure AWS Access Key ID [*************xxxx]: \u0026lt;Your AWS Access Key ID\u0026gt; AWS Secret Access Key [**************xxxx]: \u0026lt;Your AWS Secret Access Key\u0026gt; Default region name: [us-east-2]: us-east-2 Default output format [None]: json    Option 3 Manually creating credential files If you already did Option 2, then skip this\n  create a .aws directory under your home directory\n mkdir ~/.aws    Change directory to there\n cd ~/.aws    Use a text editor (vim, emacs, notepad) to create a text file (no extension) named credentials. In this file you should have the following text.\n [default] aws_access_key_id = \u0026lt;Your access key\u0026gt; aws_secret_access_key = \u0026lt;Your secret key\u0026gt;    Create a text file (no extension) named config. In this file you should have the following text:\n [default] region = us-east-2 output = json    Configure a session token as part of your credentials If you used Option 2 or Option 3, please follow these steps:\n Determine if you need to configure a session token as part of your credentials     AWS Account Do you need a session token?     You are attending an in-person workshop and were provided with an AWS account by the instructor yes       You are using your own AWS account, and using credentials from an IAM User (most common case) no       You are using your own AWS account, and using credentials from an IAM Role yes     Do this only if \u0026ldquo;yes\u0026rdquo;, you need to configure a session token   Edit the file ~/.aws/credentials\n  The default profile will already be present. Under it add an entry for aws_session_token\n [default] aws_access_key_id = \u0026lt;Your access key\u0026gt; aws_secret_access_key = \u0026lt;Your secret key\u0026gt; aws_session_token = \u0026lt;your session token\u0026gt;      Clear environment variables If you used option 2 or option 3 then you have put your credentials into files that will be used by the AWS CLI or AWS SDK. However these will preferentially use credentials and configuration in environment variables. Therefore ensure that the following env variables are not set:\n AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SESSION_TOKEN AWS_DEFAULT_REGION AWS_DEFAULT_OUTPUT AWS_PROFILE  How to do this varies depending on system. For Linux:\n # Use echo $varname to see if it is set $ echo $AWS_ACCESS_KEY_ID ASIATWOQ3L72RPLOP222 # use unset $ unset AWS_ACCESS_KEY_ID # This now returns no value $ echo $AWS_ACCESS_KEY_ID  For your convenience:\n unset AWS_ACCESS_KEY_ID unset AWS_SECRET_ACCESS_KEY unset AWS_SESSION_TOKEN unset AWS_DEFAULT_REGION unset AWS_DEFAULT_OUTPUT unset AWS_PROFILE  Option 4 (PowerShell)   If you do not have the AWS Tools for Powershell, download and install them following the instructions here. https://aws.amazon.com/powershell/.\n  Start a Windows PowerShell for AWS session. If prompted for AWS Secret Key during initialization, type Control-C to break out of the dialog.\n  Configure your AWS credentials with the following PowerShell commands. Note that if you are using an instructor supplied AWS account, you must include the optional SessionToken flag and value as shown below in brackets (omit the brackets when running the command):\n  Set-AWSCredentials -AccessKey \u0026lt;Your access key\u0026gt; -SecretKey \u0026lt;Your secret key\u0026gt; \\ [ -SessionToken \u0026lt;your session key\u0026gt; ] -StoreAs \u0026lt;SomeProfileName\u0026gt; Initialize-AWSDefaults -ProfileName \u0026lt;SomeProfileName\u0026gt; -Region us-east-2  Return to the Lab Guide to continue the lab\n"},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/documentation/software_install/","title":"Software Install","tags":[],"description":"","content":"This reference will help you install software necessary to setup your workshop environment\n AWS CLI  jq   Install AWS CLI The AWS Command Line Interface (AWS CLI) is a unified tool that provides a consistent interface for interacting with all parts of AWS.\nLinux   This includes:\n  All native Linux installs\n  MacOS\n  Windows Subsystem for Linux (WSL)\n  Run the following command\n$ aws --version aws-cli/1.16.249 Python/3.6.8...      AWS CLI version 1.0 or higher is fine\n  If you instead got command not found then you need to install awscli:\n $ pip3 install awscli --upgrade --user ...(lots of output)... Successfully installed...    If that succeeded, then you are finished. Return to the Lab Guide\n  If that does not work, then do the following:\n See the detailed installation instructions here   Other environments (not Linux)  See the instructions here https://docs.aws.amazon.com/cli/latest/userguide/install-cliv1.html  STOP HERE and return to the Lab Guide \n jq jq is a command-line JSON processor. is like sed for JSON data. It is used in the workshop bash scripts to parse AWS CLI output.\n  Run the following command\n $ jq --version jq-1.5-1-a5b5cbe    Any version is fine.\n  If you instead got command not found then you need to install jq. Follow the instructions at https://stedolan.github.io/jq/download/\n  If that succeeded, then you are finished. Return to the Lab Guide\n  Alternate instructions for Linux If the steps above did not work, and you are running Linux, then try the following\n  Download the jq executable\n $ wget https://github.com/stedolan/jq/releases/download/jq-1.6/jq-linux64 [...lots of output...] jq-linux64 100%[=================================================\u0026gt;] 3.77M 1.12MB/s in 3.5s 2019-10-11 17:41:42 (1.97 MB/s) - jq-linux64 saved [3953824/3953824]    You can find out what your execution path is with the following command.\n $ echo $PATH /usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/aws/bin:/home/ec2-user/.local/bin:/home/ec2-user/bin    If you have sudo rights, then copy the executable to /usr/local/bin/jq and make it executable.\n $ sudo cp jq-linux64 /usr/local/bin/jq $ sudo chmod 755 /usr/local/bin/jq    If you do not have sudo rights, then copy it into your home directory under a /bin directory.\n $ cp jq-linux64 ~/bin/jq $ chmod 755 ~/bin/jq    STOP HERE and return to the Lab Guide \n"},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_bidirectional_replication_for_s3/documentation/tuneinsightsquery/","title":"Troubleshooting Guide to Tuning your Insights Query","tags":[],"description":"","content":"Troubleshooting Guide to Tuning your Insights Query Verify you are in the correct AWS Region You should be in the east region\n If you used the directions in this lab, then this is Ohio (us-east-2)  If your query returned zero results or less than three results: Possibility #1: It was been too recent since you did the bucket operations  It usually takes five minutes after an operation for it to show up in CloudTrail and can take as long as 15 minutes  or\nPossibility #2: It was been too long ago since you did the bucket operations  If you did the previous part of the lab much earlier you should expand the time range for your query, found to the right of Select log group(s)  If you see more than three results   Then you are seeing bucket activity other than the operations you did with this lab\n  You are looking for three events, one for each of the test objects you uploaded. See the key field to see the test object names\n  You can also update the query to only look at the lab buckets. Add the following to your query\n filter requestParameters.bucketName like'crrlab'    Click here to return to the Lab Guide\n"},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_bidirectional_replication_for_s3/documentation/detachiampolicy/","title":"Troubleshooting: CloudFormation stack deletion for Lab S3 replication lab","tags":[],"description":"","content":"If your CloudFormation stack deletion fails with status DELETE_FAILED\nFrom the CloudFormation console  Click on the Events tab and refresh Verify the cause of the failure is the error: Cannot delete entity, must detach all policies first  Delete workshop IAM Roles Go the the IAM console   Click on Roles\n  In the search box enter S3-Replication-Role. There should be either:\n Two roles shown (if you setup both replication buckets) Or just one (if you only setup one replication bucket) If you see more than two roles stop here and investigate which roles are associated with your execution of this lab    Check the box next to each of the IAM Roles\n  Click Delete\n  Confirm the deletion by clicking Yes, delete\n  Delete workshop CloudFormation stacks  Re-initiate deletion of the S3-CRR-lab-east CloudFormation stack in Ohio (us-east-2) Re-initiate deletion of the S3-CRR-lab-west CloudFormation stack in Oregon (us-west-2)   CloudFormation will give you an option for Resources to retain - optional  Do NOT check anything Click Delete stack    Click here to return to the Lab Guide\n"},{"uri":"https://wellarchitectedlabs.com/common/","title":"","tags":[],"description":"","content":""},{"uri":"https://wellarchitectedlabs.com/common/documentation/","title":"Common Documentation","tags":[],"description":"","content":""},{"uri":"https://wellarchitectedlabs.com/common/documentation/createnews3bucketandaddobjects/","title":"Create new S3 bucket and add objects to it","tags":[],"description":"","content":"Create new S3 bucket These steps will guide you to create a bucket\n  Go to the S3 console at https://console.aws.amazon.com/s3\n  Click Create bucket\n  For Bucket name supply a name. This must be unique across all buckets in AWS\n Tip: Name the bucket \u0026lt;first_name\u0026gt;\u0026lt;last_initial\u0026gt;_\u0026lt;date in yyyymmdd format\u0026gt; (do NOT include the angle brackets)    Click Next three times\n  Review screen: click Create bucket\n  Add object(s) to an S3 bucket Use these instructions to add one or more objects to an S3 bucket\nNote: You have the option to make the object(s) publically readable. Do NOT do this for S3 buckets used in production, or containing sensitive data. It is recommended you create a new test S3 bucket if you want to host publically readable objects.\n Click on the name of the bucket you are using (this can be the one you created above) If, and only if, you want to make the uploaded object(s) publically readable then:  Click on the Permissions tab Clear both \u0026hellip;access control lists (ACLs) checkboxes (or verify they are already cleared) Click Save Type confirm Click Confirm Click on the Overview tab   Drag the file(s) you want to upload to the bucket into the object upload area Click Next If, and only if, you want to make this object(s) publically readable then under Manage public permissions select Grant public read access to this object(s) Click Next two more times Click Upload   Return to the Lab Guide to continue the lab\n"},{"uri":"https://wellarchitectedlabs.com/common/examples/usecreatenewcloudformationstack/","title":"example implementations for CreateNewCloudFormationStack","tags":[],"description":"","content":" Case 1 - all parameters left as default   Case 2 - provides directions to update or view one or more parameters   Case 1 - all parameters left as default   Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and click Create Stack \u0026gt; With new resources   Leave Prepare template setting as-is\n For Template source select Upload a template file Click Choose file and supply the CloudFormation template you downloaded: staticwebapp.yaml     Click Next\n  For Stack name use CloudFormationLab\n  Parameters\n  Look over the Parameters and their default values.\n  Click Next\n    For Configure stack options we recommend configuring tags, which are key-value pairs, that can help you identify your stacks and the resources they create. For example, enter Owner in the left column which is the key, and your email address in the right column which is the value. We will not use additional permissions or advanced options so click Next. For more information, see Setting AWS CloudFormation Stack Options .\n  For Review\n Review the contents of the page At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Create stack     This will take you to the CloudFormation stack status page, showing the stack creation in progress.\n Click on the Events tab Scroll through the listing. It shows the activities performed by CloudFormation (newest events at top), such as starting to create a resource and then completing the resource creation. Any errors encountered during the creation of the stack will be listed in this tab.     When it shows status CREATE_COMPLETE, then you are finished with this step.\n  Case 2 - provides directions to update or view one or more parameters   Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and click Create Stack \u0026gt; With new resources   Leave Prepare template setting as-is\n For Template source select Upload a template file Click Choose file and supply the CloudFormation template you downloaded: vpc-alb-app-db.yaml     Click Next\n  For Stack name use WebApp1-VPC\n  Parameters\n  Look over the Parameters and their default values.\n  EC2InstanceSubnetId  The subnet you wish to deploy the 2 EC2 instances into for testing.\n  Set the numberOfAZ parameter to 3\n  Leave other parameters as their default values unless you are experimenting.\n  Click Next\n    For Configure stack options we recommend configuring tags, which are key-value pairs, that can help you identify your stacks and the resources they create. For example, enter Owner in the left column which is the key, and your email address in the right column which is the value. We will not use additional permissions or advanced options so click Next. For more information, see Setting AWS CloudFormation Stack Options .\n  For Review\n Review the contents of the page At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Create stack     This will take you to the CloudFormation stack status page, showing the stack creation in progress.\n Click on the Events tab Scroll through the listing. It shows the activities performed by CloudFormation (newest events at top), such as starting to create a resource and then completing the resource creation. Any errors encountered during the creation of the stack will be listed in this tab.     When it shows status CREATE_COMPLETE, then you are finished with this step.\n  "},{"uri":"https://wellarchitectedlabs.com/tags/mitigate_failure/","title":"mitigate_failure","tags":[],"description":"","content":""},{"uri":"https://wellarchitectedlabs.com/common/documentation/aws_credentials/","title":"Setup AWS credentials and configuration","tags":[],"description":"","content":" You will supply configuration and credentials used by the AWS CLI and AWS SDK to access your AWS account.  Choose an option Select the appropriate option for configuration of your AWS credentials:\n Option 1 - Using AWS instructor supplied accounts with Linux-style environment variables Option 2 - Using AWS CLI Option 3 - Creating configuration files manually Option 4 - Using PowerShell commands for Windows  Option 1 For instructor supplied AWS accounts If BOTH of the following are true then you may use Option 1\n If you are attending an in-person workshop and were provided with an AWS account by the instructor then you should use this option You are running the workshop on a system where environment variables are set using the export command, such as Bash on Amazon Linux  Otherwise you should choose Option 2 or Option 3\n  You should have already copied the credentials for your account. If not then:\n  Click here for instructions to copy the credentials from your assigned AWS account:   1. Go to \u0026lt;https://dashboard.eventengine.run/login\u0026gt;    Enter the 12 character hashcode you were provided and click \u0026ldquo;Proceed\u0026rdquo;   [optional] assign a name to your account (this is referred to as \u0026ldquo;Team name\u0026rdquo;)\n click \u0026ldquo;Set Team Name\u0026rdquo; Enter a name and click \u0026ldquo;Set Team Name\u0026rdquo;    Click \u0026ldquo;AWS Console\u0026rdquo;\n  AWS credentials   IMPORTANT Copy the provided credentials and save them. You wil need these to complete the workshop   Copy the whole code block corresponding to the system you are using.\n  Access the AWS console  Click \u0026ldquo;Open Console\u0026rdquo;. The AWS Console will open.   Now continue the steps to setup your AWS credentials\u0026hellip;.\n    The copied credentials are already in the form of export statements. Run these from your shell command line. Use your values, not the ones below\n export AWS_ACCESS_KEY_ID=ASIIAMFAKENOPZLX6J5L export AWS_SECRET_ACCESS_KEY=w0pE4j5k4FlUrkIIAMFAKEdiLMKLGZlxyct+GpTam export AWS_SESSION_TOKEN=FQoGZXIvYXdzEDwaIIAMFAKEn0LVImWNQHiLuAWKe+KFkLeIvpOHEruWjyCjrEdyjtW8WCbnmJGM1ES20xq1fcaS5TERHDUabZJ60Kk6nc9uHoCDb1QKHi+MerRIcKJTi3OKz0QMVPAGVqVWgvOBBSQ2lylLVjtMMSQF+yLZsP1bvehQ0ke/Bl/X6RJySOHg2TZGyESPL/INqJiZyEHi+MelAnThepVgWUKFPD5mESBVlpy2LVCE3xPpHFqOm0Q79svRSSW2jLj5NkRXL+xhkcvt+g8vNt1ODEwixwMGpFB2sBHryv6EXNeX6c88vxJ8Zyfkmsqi0xmCW1f9jWAPIXNkt/nEYW4J4coyLKP7QU= export AWS_DEFAULT_REGION=us-east-2    Also run this command as written below\n export AWS_DEFAULT_OUTPUT=json    Note that if you end your bash session, or start a new one, you will need to re-execute the export statements\n  If you completed Option 1 then STOP HERE and return to the Lab Guide\nOption 2 AWS CLI This option uses the AWS CLI. You should use Option 3 if you do not or cannot install the AWS CLI.\n  To see if the AWS CLI is installed:\n $ aws --version aws-cli/1.16.249 Python/3.6.8...   AWS CLI version 1.1 or higher is fine If you instead got command not found then either install the AWS CLI or use Option 3    Run aws configure and provide the following values:\n $ aws configure AWS Access Key ID [*************xxxx]: \u0026lt;Your AWS Access Key ID\u0026gt; AWS Secret Access Key [**************xxxx]: \u0026lt;Your AWS Secret Access Key\u0026gt; Default region name: [us-east-2]: us-east-2 Default output format [None]: json    Option 3 Manually creating credential files If you already did Option 2, then skip this\n  create a .aws directory under your home directory\n mkdir ~/.aws    Change directory to there\n cd ~/.aws    Use a text editor (vim, emacs, notepad) to create a text file (no extension) named credentials. In this file you should have the following text.\n [default] aws_access_key_id = \u0026lt;Your access key\u0026gt; aws_secret_access_key = \u0026lt;Your secret key\u0026gt;    Create a text file (no extension) named config. In this file you should have the following text:\n [default] region = us-east-2 output = json    Configure a session token as part of your credentials If you used Option 2 or Option 3, please follow these steps:\n Determine if you need to configure a session token as part of your credentials     AWS Account Do you need a session token?     You are attending an in-person workshop and were provided with an AWS account by the instructor yes       You are using your own AWS account, and using credentials from an IAM User (most common case) no       You are using your own AWS account, and using credentials from an IAM Role yes     Do this only if \u0026ldquo;yes\u0026rdquo;, you need to configure a session token   Edit the file ~/.aws/credentials\n  The default profile will already be present. Under it add an entry for aws_session_token\n [default] aws_access_key_id = \u0026lt;Your access key\u0026gt; aws_secret_access_key = \u0026lt;Your secret key\u0026gt; aws_session_token = \u0026lt;your session token\u0026gt;      Clear environment variables If you used option 2 or option 3 then you have put your credentials into files that will be used by the AWS CLI or AWS SDK. However these will preferentially use credentials and configuration in environment variables. Therefore ensure that the following env variables are not set:\n AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SESSION_TOKEN AWS_DEFAULT_REGION AWS_DEFAULT_OUTPUT AWS_PROFILE  How to do this varies depending on system. For Linux:\n # Use echo $varname to see if it is set $ echo $AWS_ACCESS_KEY_ID ASIATWOQ3L72RPLOP222 # use unset $ unset AWS_ACCESS_KEY_ID # This now returns no value $ echo $AWS_ACCESS_KEY_ID  For your convenience:\n unset AWS_ACCESS_KEY_ID unset AWS_SECRET_ACCESS_KEY unset AWS_SESSION_TOKEN unset AWS_DEFAULT_REGION unset AWS_DEFAULT_OUTPUT unset AWS_PROFILE  Option 4 (PowerShell)   If you do not have the AWS Tools for Powershell, download and install them following the instructions here. https://aws.amazon.com/powershell/.\n  Start a Windows PowerShell for AWS session. If prompted for AWS Secret Key during initialization, type Control-C to break out of the dialog.\n  Configure your AWS credentials with the following PowerShell commands. Note that if you are using an instructor supplied AWS account, you must include the optional SessionToken flag and value as shown below in brackets (omit the brackets when running the command):\n  Set-AWSCredentials -AccessKey \u0026lt;Your access key\u0026gt; -SecretKey \u0026lt;Your secret key\u0026gt; \\ [ -SessionToken \u0026lt;your session key\u0026gt; ] -StoreAs \u0026lt;SomeProfileName\u0026gt; Initialize-AWSDefaults -ProfileName \u0026lt;SomeProfileName\u0026gt; -Region us-east-2  Return to the Lab Guide to continue the lab\n"},{"uri":"https://wellarchitectedlabs.com/common/documentation/software_install/","title":"Software Install","tags":[],"description":"","content":"This reference will help you install software necessary to setup your workshop environment\n AWS CLI  jq   Install AWS CLI The AWS Command Line Interface (AWS CLI) is a unified tool that provides a consistent interface for interacting with all parts of AWS.\nLinux   This includes:\n  All native Linux installs\n  MacOS\n  Windows Subsystem for Linux (WSL)\n  Run the following command\n$ aws --version aws-cli/1.16.249 Python/3.6.8...      AWS CLI version 1.0 or higher is fine\n  If you instead got command not found then you need to install awscli:\n $ pip3 install awscli --upgrade --user ...(lots of output)... Successfully installed...    If that succeeded, then you are finished. Return to the Lab Guide\n  If that does not work, then do the following:\n See the detailed installation instructions here   Other environments (not Linux)  See the instructions here https://docs.aws.amazon.com/cli/latest/userguide/install-cliv1.html  STOP HERE and return to the Lab Guide\n jq jq is a command-line JSON processor. is like sed for JSON data. It is used in the workshop bash scripts to parse AWS CLI output.\n  Run the following command\n $ jq --version jq-1.5-1-a5b5cbe    Any version is fine.\n  If you instead got command not found then you need to install jq. Follow the instructions at https://stedolan.github.io/jq/download/\n  If that succeeded, then you are finished. Return to the Lab Guide\n  Alternate instructions for Linux If the steps above did not work, and you are running Linux, then try the following\n  Download the jq executable\n $ wget https://github.com/stedolan/jq/releases/download/jq-1.6/jq-linux64 [...lots of output...] jq-linux64 100%[=================================================\u0026gt;] 3.77M 1.12MB/s in 3.5s 2019-10-11 17:41:42 (1.97 MB/s) - jq-linux64 saved [3953824/3953824]    You can find out what your execution path is with the following command.\n $ echo $PATH /usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/aws/bin:/home/ec2-user/.local/bin:/home/ec2-user/bin    If you have sudo rights, then copy the executable to /usr/local/bin/jq and make it executable.\n $ sudo cp jq-linux64 /usr/local/bin/jq $ sudo chmod 755 /usr/local/bin/jq    If you do not have sudo rights, then copy it into your home directory under a /bin directory.\n $ cp jq-linux64 ~/bin/jq $ chmod 755 ~/bin/jq    Return to the Lab Guide to continue the lab\n"},{"uri":"https://wellarchitectedlabs.com/tags/test_resiliency/","title":"test_resiliency","tags":[],"description":"","content":""},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_splitting_sharing_cur_access/code/crawler-cfn/","title":"","tags":[],"description":"","content":"Below is a sample crawler config file. It is suggested you modify your existing file, modifications are between \u0026lsquo;***\u0026rsquo; characters.\nVariables that need to be changed in the new code below:\n (region): The region that contains the Lambda function (accountID): The account that contains the Lambda function  AWSTemplateFormatVersion: 2010-09-09 Resources: AWSCURDatabase: Type: 'AWS::Glue::Database' Properties: DatabaseInput: Name: '(Database Name)' CatalogId: !Ref AWS::AccountId AWSCURCrawlerComponentFunction: Type: 'AWS::IAM::Role' Properties: AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: Service: - glue.amazonaws.com Action: - 'sts:AssumeRole' Path: / ManagedPolicyArns: - 'arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole' Policies: - PolicyName: AWSCURCrawlerComponentFunction PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - 'logs:CreateLogGroup' - 'logs:CreateLogStream' - 'logs:PutLogEvents' Resource: 'arn:aws:logs:*:*:*' - Effect: Allow Action: - 'glue:UpdateDatabase' - 'glue:UpdatePartition' - 'glue:CreateTable' - 'glue:UpdateTable' - 'glue:ImportCatalogToGlue' Resource: '*' - Effect: Allow Action: - 's3:GetObject' - 's3:PutObject' Resource: arn:aws:s3:::\u0026lt;bucketname\u0026gt;/\u0026lt;prefix\u0026gt;/\u0026lt;folder\u0026gt;/WorkshopCUR* AWSCURCrawlerLambdaExecutor: Type: 'AWS::IAM::Role' Properties: AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: Service: - lambda.amazonaws.com Action: - 'sts:AssumeRole' Path: / Policies: - PolicyName: AWSCURCrawlerLambdaExecutor PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - 'logs:CreateLogGroup' - 'logs:CreateLogStream' - 'logs:PutLogEvents' ***** - 'lambda:InvokeFunction' ***** Resource: ***** - 'arn:aws:logs:*:*:*' - 'arn:aws:lambda:\u0026lt;region\u0026gt;:\u0026lt;accountID\u0026gt;:function:SubAcctSplit' ***** - Effect: Allow Action: - 'glue:StartCrawler' Resource: '*' AWSCURCrawler: Type: 'AWS::Glue::Crawler' DependsOn: - AWSCURDatabase - AWSCURCrawlerComponentFunction Properties: Name: AWSCURCrawler-WorkshopCUR Description: A recurring crawler that keeps your CUR table in Athena up-to-date. Role: !GetAtt AWSCURCrawlerComponentFunction.Arn DatabaseName: !Ref AWSCURDatabase Targets: S3Targets: - Path: 's3://\u0026lt;bucket\u0026gt;/\u0026lt;prefix\u0026gt;/\u0026lt;folder\u0026gt;/WorkshopCUR' Exclusions: - '**.json' - '**.yml' - '**.sql' - '**.csv' - '**.gz' - '**.zip' SchemaChangePolicy: UpdateBehavior: UPDATE_IN_DATABASE DeleteBehavior: DELETE_FROM_DATABASE AWSCURInitializer: Type: 'AWS::Lambda::Function' DependsOn: AWSCURCrawler Properties: Code: ZipFile: \u0026gt; const AWS = require('aws-sdk'); const response = require('cfn-response'); exports.handler = function(event, context, callback) { if (event.RequestType === 'Delete') { response.send(event, context, response.SUCCESS); } else { const glue = new AWS.Glue(); glue.startCrawler({ Name: 'AWSCURCrawler-WorkshopCUR' }, function(err, data) { if (err) { const responseData = JSON.parse(this.httpResponse.body); if (responseData['__type'] == 'CrawlerRunningException') { callback(null, responseData.Message); } else { const responseString = JSON.stringify(responseData); if (event.ResponseURL) { response.send(event, context, response.FAILED,{ msg: responseString }); } else { callback(responseString); } } } else { if (event.ResponseURL) { response.send(event, context, response.SUCCESS); } else { callback(null, response.SUCCESS); } } }); ***** var lambda = new AWS.Lambda(); var params = { FunctionName: 'SubAcctSplit' }; lambda.invoke(params, function(err, data) { if (err) console.log(err, err.stack); // an error occurred else console.log(data); // successful response }); ***** } }; Handler: 'index.handler' Timeout: 30 Runtime: nodejs8.10 ReservedConcurrentExecutions: 1 Role: !GetAtt AWSCURCrawlerLambdaExecutor.Arn AWSStartCURCrawler: Type: 'Custom::AWSStartCURCrawler' Properties: ServiceToken: !GetAtt AWSCURInitializer.Arn AWSS3CUREventLambdaPermission: Type: AWS::Lambda::Permission Properties: Action: 'lambda:InvokeFunction' FunctionName: !GetAtt AWSCURInitializer.Arn Principal: 's3.amazonaws.com' SourceAccount: !Ref AWS::AccountId SourceArn: 'arn:aws:s3:::\u0026lt;bucket\u0026gt;' AWSS3CURLambdaExecutor: Type: 'AWS::IAM::Role' Properties: AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: Service: - lambda.amazonaws.com Action: - 'sts:AssumeRole' Path: / Policies: - PolicyName: AWSS3CURLambdaExecutor PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - 'logs:CreateLogGroup' - 'logs:CreateLogStream' - 'logs:PutLogEvents' Resource: 'arn:aws:logs:*:*:*' - Effect: Allow Action: - 's3:PutBucketNotification' Resource: 'arn:aws:s3:::\u0026lt;bucket\u0026gt;' AWSS3CURNotification: Type: 'AWS::Lambda::Function' DependsOn: - AWSCURInitializer - AWSS3CUREventLambdaPermission - AWSS3CURLambdaExecutor Properties: Code: ZipFile: \u0026gt; const AWS = require('aws-sdk'); const response = require('cfn-response'); exports.handler = function(event, context, callback) { const s3 = new AWS.S3(); const putConfigRequest = function(notificationConfiguration) { return new Promise(function(resolve, reject) { s3.putBucketNotificationConfiguration({ Bucket: event.ResourceProperties.BucketName, NotificationConfiguration: notificationConfiguration }, function(err, data) { if (err) reject({ msg: this.httpResponse.body.toString(), error: err, data: data }); else resolve(data); }); }); }; const newNotificationConfig = {}; if (event.RequestType !== 'Delete') { newNotificationConfig.LambdaFunctionConfigurations = [{ Events: [ 's3:ObjectCreated:*' ], LambdaFunctionArn: event.ResourceProperties.TargetLambdaArn || 'missing arn', Filter: { Key: { FilterRules: [ { Name: 'prefix', Value: event.ResourceProperties.ReportKey } ] } } }]; } putConfigRequest(newNotificationConfig).then(function(result) { response.send(event, context, response.SUCCESS, result); callback(null, result); }).catch(function(error) { response.send(event, context, response.FAILED, error); console.log(error); callback(error); }); }; Handler: 'index.handler' Timeout: 30 Runtime: nodejs8.10 ReservedConcurrentExecutions: 1 Role: !GetAtt AWSS3CURLambdaExecutor.Arn AWSPutS3CURNotification: Type: 'Custom::AWSPutS3CURNotification' Properties: ServiceToken: !GetAtt AWSS3CURNotification.Arn TargetLambdaArn: !GetAtt AWSCURInitializer.Arn BucketName: '\u0026lt;bucket\u0026gt;' ReportKey: '\u0026lt;prefix\u0026gt;/\u0026lt;folder\u0026gt;/WorkshopCUR' AWSCURReportStatusTable: Type: 'AWS::Glue::Table' DependsOn: AWSCURDatabase Properties: DatabaseName: athenacurcfn_workshop_c_u_r CatalogId: !Ref AWS::AccountId TableInput: Name: 'cost_and_usage_data_status' TableType: 'EXTERNAL_TABLE' StorageDescriptor: Columns: - Name: status Type: 'string' InputFormat: 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat' OutputFormat: 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' SerdeInfo: SerializationLibrary: 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' Location: 's3://\u0026lt;bucket\u0026gt;/\u0026lt;prefix\u0026gt;/\u0026lt;folder\u0026gt;/cost_and_usage_data_status/' "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_splitting_sharing_cur_access/code/iam_athena/","title":"","tags":[],"description":"","content":"IAM policy for access to Athena\nNOTE: This Policy is to be used as a starting point only. Ensure to follow security best practices and only provide the minimum required access. You will also need to modify the and fields before use.\n{\r\u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;,\r\u0026quot;Statement\u0026quot;: [\r{\r\u0026quot;Sid\u0026quot;: \u0026quot;VisualEditor0\u0026quot;,\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Action\u0026quot;: [\r\u0026quot;athena:StartQueryExecution\u0026quot;,\r\u0026quot;glue:GetCrawler\u0026quot;,\r\u0026quot;glue:GetDataCatalogEncryptionSettings\u0026quot;,\r\u0026quot;glue:GetTableVersions\u0026quot;,\r\u0026quot;glue:GetPartitions\u0026quot;,\r\u0026quot;athena:GetQueryResults\u0026quot;,\r\u0026quot;athena:ListWorkGroups\u0026quot;,\r\u0026quot;athena:GetNamedQuery\u0026quot;,\r\u0026quot;glue:GetDevEndpoint\u0026quot;,\r\u0026quot;glue:GetSecurityConfiguration\u0026quot;,\r\u0026quot;glue:GetResourcePolicy\u0026quot;,\r\u0026quot;glue:GetTrigger\u0026quot;,\r\u0026quot;glue:GetUserDefinedFunction\u0026quot;,\r\u0026quot;athena:GetExecutionEngine\u0026quot;,\r\u0026quot;glue:GetJobRun\u0026quot;,\r\u0026quot;athena:GetExecutionEngines\u0026quot;,\r\u0026quot;s3:HeadBucket\u0026quot;,\r\u0026quot;glue:GetUserDefinedFunctions\u0026quot;,\r\u0026quot;glue:GetClassifier\u0026quot;,\r\u0026quot;s3:PutAccountPublicAccessBlock\u0026quot;,\r\u0026quot;athena:GetQueryResultsStream\u0026quot;,\r\u0026quot;glue:GetJobs\u0026quot;,\r\u0026quot;glue:GetTables\u0026quot;,\r\u0026quot;glue:GetTriggers\u0026quot;,\r\u0026quot;athena:GetNamespace\u0026quot;,\r\u0026quot;athena:GetQueryExecutions\u0026quot;,\r\u0026quot;athena:GetCatalogs\u0026quot;,\r\u0026quot;athena:ListNamedQueries\u0026quot;,\r\u0026quot;athena:GetNamespaces\u0026quot;,\r\u0026quot;glue:GetPartition\u0026quot;,\r\u0026quot;glue:GetDevEndpoints\u0026quot;,\r\u0026quot;athena:GetTables\u0026quot;,\r\u0026quot;athena:GetTable\u0026quot;,\r\u0026quot;athena:BatchGetNamedQuery\u0026quot;,\r\u0026quot;athena:BatchGetQueryExecution\u0026quot;,\r\u0026quot;glue:GetJob\u0026quot;,\r\u0026quot;glue:GetConnections\u0026quot;,\r\u0026quot;glue:GetCrawlers\u0026quot;,\r\u0026quot;glue:GetClassifiers\u0026quot;,\r\u0026quot;athena:ListQueryExecutions\u0026quot;,\r\u0026quot;glue:GetCatalogImportStatus\u0026quot;,\r\u0026quot;athena:GetWorkGroup\u0026quot;,\r\u0026quot;glue:GetConnection\u0026quot;,\r\u0026quot;glue:BatchGetPartition\u0026quot;,\r\u0026quot;glue:GetSecurityConfigurations\u0026quot;,\r\u0026quot;glue:GetDatabases\u0026quot;,\r\u0026quot;athena:ListTagsForResource\u0026quot;,\r\u0026quot;glue:GetTable\u0026quot;,\r\u0026quot;glue:GetDatabase\u0026quot;,\r\u0026quot;s3:GetAccountPublicAccessBlock\u0026quot;,\r\u0026quot;glue:GetDataflowGraph\u0026quot;,\r\u0026quot;s3:ListAllMyBuckets\u0026quot;,\r\u0026quot;athena:GetQueryExecution\u0026quot;,\r\u0026quot;glue:GetPlan\u0026quot;,\r\u0026quot;glue:GetCrawlerMetrics\u0026quot;,\r\u0026quot;glue:GetJobRuns\u0026quot;\r],\r\u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;\r},\r{\r\u0026quot;Sid\u0026quot;: \u0026quot;VisualEditor1\u0026quot;,\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Action\u0026quot;: [\r\u0026quot;s3:PutObject\u0026quot;,\r\u0026quot;s3:GetObject\u0026quot;,\r\u0026quot;s3:ListBucketMultipartUploads\u0026quot;,\r\u0026quot;s3:AbortMultipartUpload\u0026quot;,\r\u0026quot;s3:CreateBucket\u0026quot;,\r\u0026quot;s3:ListBucket\u0026quot;,\r\u0026quot;s3:GetBucketLocation\u0026quot;,\r\u0026quot;s3:ListMultipartUploadParts\u0026quot;\r],\r\u0026quot;Resource\u0026quot;: [\r\u0026quot;arn:aws:s3:::aws-athena-query-results-\u0026lt;Account ID\u0026gt;-us-east-1\u0026quot;,\r\u0026quot;arn:aws:s3:::aws-athena-query-results-\u0026lt;Account ID\u0026gt;-us-east-1/*\u0026quot;\r]\r},\r{\r\u0026quot;Sid\u0026quot;: \u0026quot;VisualEditor2\u0026quot;,\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Action\u0026quot;: [\r\u0026quot;s3:ListBucketByTags\u0026quot;,\r\u0026quot;s3:GetLifecycleConfiguration\u0026quot;,\r\u0026quot;s3:GetBucketTagging\u0026quot;,\r\u0026quot;s3:GetInventoryConfiguration\u0026quot;,\r\u0026quot;s3:GetObjectVersionTagging\u0026quot;,\r\u0026quot;s3:ListBucketVersions\u0026quot;,\r\u0026quot;s3:GetBucketLogging\u0026quot;,\r\u0026quot;s3:ListBucket\u0026quot;,\r\u0026quot;s3:GetAccelerateConfiguration\u0026quot;,\r\u0026quot;s3:GetBucketPolicy\u0026quot;,\r\u0026quot;s3:GetObjectVersionTorrent\u0026quot;,\r\u0026quot;s3:GetObjectAcl\u0026quot;,\r\u0026quot;s3:GetEncryptionConfiguration\u0026quot;,\r\u0026quot;s3:GetBucketRequestPayment\u0026quot;,\r\u0026quot;s3:GetObjectVersionAcl\u0026quot;,\r\u0026quot;s3:GetObjectTagging\u0026quot;,\r\u0026quot;s3:GetMetricsConfiguration\u0026quot;,\r\u0026quot;s3:GetBucketPublicAccessBlock\u0026quot;,\r\u0026quot;s3:GetBucketPolicyStatus\u0026quot;,\r\u0026quot;s3:ListBucketMultipartUploads\u0026quot;,\r\u0026quot;s3:GetBucketWebsite\u0026quot;,\r\u0026quot;s3:GetBucketVersioning\u0026quot;,\r\u0026quot;s3:GetBucketAcl\u0026quot;,\r\u0026quot;s3:GetBucketNotification\u0026quot;,\r\u0026quot;s3:GetReplicationConfiguration\u0026quot;,\r\u0026quot;s3:ListMultipartUploadParts\u0026quot;,\r\u0026quot;s3:GetObject\u0026quot;,\r\u0026quot;s3:GetObjectTorrent\u0026quot;,\r\u0026quot;s3:GetBucketCORS\u0026quot;,\r\u0026quot;s3:GetAnalyticsConfiguration\u0026quot;,\r\u0026quot;s3:GetObjectVersionForReplication\u0026quot;,\r\u0026quot;s3:GetBucketLocation\u0026quot;,\r\u0026quot;s3:GetObjectVersion\u0026quot;\r],\r\u0026quot;Resource\u0026quot;: [\r\u0026quot;arn:aws:s3:::\u0026lt;S3 CUR Bucket\u0026gt;/*\u0026quot;,\r\u0026quot;arn:aws:s3:::\u0026lt;S3 CUR Bucket\u0026gt;\u0026quot;\r]\r}\r]\r}\r"},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_splitting_sharing_cur_access/code/s3_bucket_policy/","title":"","tags":[],"description":"","content":"Bucket policy for member/linked account access to CUR files\nNOTE: Replace the Account ID [Sub-Account ID] with your own account ID, and the bucket name [S3 Bucket Name] with your bucket name.\n{ \u0026quot;Version\u0026quot;: \u0026quot;2008-10-17\u0026quot;, \u0026quot;Id\u0026quot;: \u0026quot;Policy1335892530063\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;Stmt1335892150622\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;AWS\u0026quot;: \u0026quot;arn:aws:iam::386209384616:root\u0026quot; }, \u0026quot;Action\u0026quot;: [ \u0026quot;s3:GetBucketAcl\u0026quot;, \u0026quot;s3:GetBucketPolicy\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:s3:::[S3 Bucket Name]\u0026quot; }, { \u0026quot;Sid\u0026quot;: \u0026quot;Stmt1335892526596\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;AWS\u0026quot;: \u0026quot;arn:aws:iam::386209384616:root\u0026quot; }, \u0026quot;Action\u0026quot;: \u0026quot;s3:PutObject\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:s3:::[S3 Bucket Name]/*\u0026quot; }, { \u0026quot;Sid\u0026quot;: \u0026quot;Stmt1546900919345\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;AWS\u0026quot;: \u0026quot;arn:aws:iam::[Sub-Account ID]:root\u0026quot; }, \u0026quot;Action\u0026quot;: \u0026quot;s3:ListBucket\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:s3:::[S3 Bucket Name]\u0026quot; }, { \u0026quot;Sid\u0026quot;: \u0026quot;Stmt1546901049588\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;AWS\u0026quot;: \u0026quot;arn:aws:iam::[Sub-Account ID]:root\u0026quot; }, \u0026quot;Action\u0026quot;: \u0026quot;s3:GetObject\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:s3:::[S3 Bucket Name]/*\u0026quot; } ] } "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_splitting_sharing_cur_access/code/s3linkedputacl/","title":"","tags":[],"description":"","content":"Here is the Lambda function to re-write object ACLs. It is triggered by an S3 Event, reads the folder from the object - and then applies the required object ACL: FULL_CONTROL for the owner, READ for the sub account.\nEdit the following fields in the code below:\n folder1: The name of the folder where new files will be placed Owner Account Name: The owner account name - the account email without the @companyname, they will get FULL_CONTROL permissions Owner Canonical ID: The owner canonical ID, to get the Canonical ID, refer to: https://docs.aws.amazon.com/general/latest/gr/acct-identifiers.html Sub Account Name: The sub account name - the account email without the @companyname, they will get READ permissions Sub Acct Canonical ID: The sub account canonical ID  const AWS = require('aws-sdk'); const util = require('util'); // Permissions for the new objects // Key MUST match the top level folder // Format: \u0026lt;owner account name\u0026gt; - \u0026lt;Canonical ID\u0026gt; - \u0026lt;sub account name\u0026gt; - \u0026lt;canonical ID\u0026gt; // This will give owner full permission \u0026amp; sub account read only permission var permissions = new Array(); var permissions = { '\u0026lt;folder1\u0026gt;': ['\u0026lt;owner acct name\u0026gt;','\u0026lt;Owner Canonical ID\u0026gt;','\u0026lt;sub account name\u0026gt;','\u0026lt;Sub Acct Canonical ID\u0026gt;'], '\u0026lt;folder2\u0026gt;': ['\u0026lt;owner acct name\u0026gt;','\u0026lt;Owner Canonical ID\u0026gt;','\u0026lt;sub account name\u0026gt;','\u0026lt;Sub Acct Canonical ID\u0026gt;'] }; // Main Loop exports.handler = function(event, context, callback) { // If its an object delete, do nothing if (event.RequestType === 'Delete') { } else // Its an object put { // Get the source bucket from the S3 event var srcBucket = event.Records[0].s3.bucket.name; // Object key may have spaces or unicode non-ASCII characters, decode it var srcKey = decodeURIComponent(event.Records[0].s3.object.key.replace(/\\+/g, \u0026quot; \u0026quot;)); // Gets the top level folder, which is the key for the permissions array var folderID = srcKey.split(\u0026quot;/\u0026quot;)[0]; // Define the object permissions, using the permissions array var params = { Bucket: srcBucket, Key: srcKey, AccessControlPolicy: { 'Owner': { 'DisplayName': permissions[folderID][0], 'ID': permissions[folderID][1] }, 'Grants': [ { 'Grantee': { 'Type': 'CanonicalUser', 'DisplayName': permissions[folderID][0], 'ID': permissions[folderID][1] }, 'Permission': 'FULL_CONTROL' }, { 'Grantee': { 'Type': 'CanonicalUser', 'DisplayName': permissions[folderID][2], 'ID': permissions[folderID][3] }, 'Permission': 'READ' }, ] } }; // get reference to S3 client var s3 = new AWS.S3(); // Put the ACL on the object s3.putObjectAcl(params, function(err, data) { if (err) console.log(err, err.stack); // an error occurred else console.log(data); // successful response }); } }; "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_splitting_sharing_cur_access/code/sub_account_split/","title":"","tags":[],"description":"","content":"Below is the code for the lambda function.\nYou will need to modify the following variable:\n athena_output: This is where Athena puts output data, this is typically the management/payer Account ID, which is the default folder for Athena output queries bucketname: This is the output bucket for the Athena queries  You will need to modify the following arrays, the order is important - the first folder in the subfolder array, will be given the permissions of the first element of the S3ObjectPolicies array.\n subfolders: This contains the list of folders that the queries write to S3ObjectPolicies: This contains the S3 Object permissions ACL that will be written to objects in the corresponding folder. You will need to add the owners details (management/payer account) and the grantee (sub account) details.  import boto3 import json import datetime import time from dateutil.relativedelta import relativedelta bucketname = '(output bucket)' #List of Subfolders \u0026amp; ACLs to apply to objects in them #There MUST be a 1:1 between subfolders \u0026amp; policies subfolders = ['\u0026lt;folder1\u0026gt;'] # Arrays to hold the Athena delete \u0026amp; create queries that we need to run delete_query_strings = [] create_query_strings = [] # Athena output folder athena_output = 's3://aws-athena-query-results-us-east-1-\u0026lt;account ID\u0026gt;/' # Main loop def lambda_handler(event, context): #clear the strings for every execution context - useful for troubleshooting after failures create_query_strings.clear() delete_query_strings.clear() # Get the current date, so you know which months folder you're working on now = datetime.datetime.now() lastmonth = now - relativedelta(months=1) # Variables to construct the s3 folder name # YES! you can do multiple subfolders if you have multiple queries to run, 1 subfolder per query # We need current and previous month because otherwise we can miss data in the last day of the month currentmonth = '/year_1=' + str(now.year) + '/month_1=' + str(now.month) + '/' previousmonth = '/year_1=' + str(lastmonth.year) + '/month_1=' + str(lastmonth.month) + '/' # Clear the current and previous months S3 folder s3_clear_folders(currentmonth) s3_clear_folders(previousmonth) # Get the athena queries to run get_athena_queries(currentmonth,'0') get_athena_queries(previousmonth,'1') # Make sure to delete any existing temp tables, so no wobbly's are thrown run_delete_athena_queries() # Create the athena tables, which will actually output data to S3 folders run_create_athena_queries() # Delete the array in case of another Lambda invocation return { 'statusCode': 200, 'body': json.dumps('Finished!') } # Clear the S3 folders for the current month def s3_clear_folders(month): # Get S3 client/object client = boto3.client('s3') # For each subfolder - in case you have multilpe subfolders, i.e. multilpe accounts/business units to split data out to for subfolder in subfolders: # List all objects in the current months bucket response = client.list_objects_v2( Bucket=bucketname, Prefix=subfolder + month ) # Get how many objects there are to delete, if any keys = response['KeyCount'] # Only try to delete if there's objects if (keys \u0026gt; 0): # Get the ojbects from the response s3objects = response['Contents'] # For each object, we're going to delete it # cycle through the list of objects for s3object in s3objects: # Get the object key objectkey = s3object['Key'] # Delete the object response = client.delete_object( Bucket=bucketname, Key=objectkey ) # Get the Athena saved queries to run # They need to be labelled 'create_linked' or 'delete_linked' def get_athena_queries(month,interval): # Get Athena client/object client = boto3.client('athena') # Get all the saved queries in Athena response = client.list_named_queries() # Get the named query IDs from the response named_query_IDs = response['NamedQueryIds'] # Go through all the query ID, to find the delete \u0026amp; create queries we need to run for query_ID in named_query_IDs: # Get all the details of a named query using its ID named_query = client.get_named_query( NamedQueryId=query_ID ) # Get the query string \u0026amp; query name of the query querystring = named_query['NamedQuery']['QueryString'] queryname = named_query['NamedQuery']['Name'] # If its a create query, add it to the list of create queries # We also replace the '/subfolder' string in the query with the folder structure for the current month if 'create_linked_' in queryname: # Get a unique ID for the temp table tableID = queryname.split('_')[2] + interval # String replacements to make the tablename unique, and work with the current months data new_query1 = querystring.replace('/subfolder', month).replace('__interval__',interval) new_query2 = new_query1.replace('temp_table', 'temp_' + tableID) # Add the create query string to the array create_query_strings.append(new_query2) # If its a delete query, add it to the list of delete queries to execute later if 'delete_linked_' in queryname: # Get a unique ID for the temp table tableID = queryname.split('_')[2] + interval # String replacements to make the tablename unique, and work with the current months data new_query1 = querystring.replace('temp_table', 'temp_'+tableID) # Add the delete query string to the array delete_query_strings.append(new_query1) # Run the delete Athena queries to remove any temp tables def run_delete_athena_queries(): # Get Athena client/object client = boto3.client('athena') # Go through each of the delete query strings in the list for delete_query_string in delete_query_strings: # Execute the query string executionID = client.start_query_execution( QueryString=delete_query_string, ResultConfiguration={ 'OutputLocation': athena_output, 'EncryptionConfiguration': { 'EncryptionOption': 'SSE_S3', } } ) # Get the state of the delete execution response = client.get_query_execution( QueryExecutionId=executionID['QueryExecutionId'] )['QueryExecution']['Status']['State'] # A busy wait to make sure its finished before moving on # Tables must not exist before creation # If the function runs for a long time ($) you should implement step functions or a cost effective wait # This is a low \u0026quot;cost of complexity\u0026quot; solution while response in ['QUEUED','RUNNING']: # Busy wait to make sure it finishes time.sleep(1) # Get the current state of the query response = client.get_query_execution( QueryExecutionId=executionID['QueryExecutionId'] )['QueryExecution']['Status']['State'] # Run the Athena queries to create the table \u0026amp; populate the S3 data def run_create_athena_queries(): # Get Athena client/object client = boto3.client('athena') # Go through each of the create query strings in the list for create_query_string in create_query_strings: # Execute the query string executionID = client.start_query_execution( QueryString=create_query_string, ResultConfiguration={ 'OutputLocation': athena_output, 'EncryptionConfiguration': { 'EncryptionOption': 'SSE_S3', } } ) "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_splitting_sharing_cur_access/code/subacctsplit_role/","title":"","tags":[],"description":"","content":"Review the policy below, and use it as a starting point to create your policy for the Lambda fuction.\nThe following fields will need to be changed:\n Output bucket: The S3 bucket that will contain the output from the Athena queries Account ID: the management/payer account ID Source bucket: the location of the original CUR files in the management/payer  { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;VisualEditor0\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;athena:StartQueryExecution\u0026quot;, \u0026quot;s3:DeleteObjectVersion\u0026quot;, \u0026quot;athena:GetQueryResults\u0026quot;, \u0026quot;s3:ListBucket\u0026quot;, \u0026quot;athena:GetNamedQuery\u0026quot;, \u0026quot;logs:PutLogEvents\u0026quot;, \u0026quot;athena:ListQueryExecutions\u0026quot;, \u0026quot;athena:ListNamedQueries\u0026quot;, \u0026quot;s3:PutObject\u0026quot;, \u0026quot;s3:GetObject\u0026quot;, \u0026quot;logs:CreateLogStream\u0026quot;, \u0026quot;athena:GetQueryExecution\u0026quot;, \u0026quot;s3:DeleteObject\u0026quot; ], \u0026quot;Resource\u0026quot;: [ \u0026quot;arn:aws:s3:::(output bucket)/*\u0026quot;, \u0026quot;arn:aws:logs:us-east-1:(account ID):log-group:/aws/lambda/SubAcctSplit:*\u0026quot;, \u0026quot;arn:aws:athena:*:*:workgroup/*\u0026quot; ] }, { \u0026quot;Sid\u0026quot;: \u0026quot;VisualEditor1\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;s3:ListBucket\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:s3:::*\u0026quot; }, { \u0026quot;Sid\u0026quot;: \u0026quot;VisualEditor2\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;glue:GetDatabase\u0026quot;, \u0026quot;glue:CreateTable\u0026quot;, \u0026quot;glue:GetPartitions\u0026quot;, \u0026quot;glue:GetPartition\u0026quot;, \u0026quot;glue:DeleteTable\u0026quot;, \u0026quot;glue:GetTable\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; }, { \u0026quot;Sid\u0026quot;: \u0026quot;VisualEditor3\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;s3:GetBucketLocation\u0026quot;, \u0026quot;s3:GetObject\u0026quot;, \u0026quot;s3:ListBucket\u0026quot;, \u0026quot;s3:ListBucketMultipartUploads\u0026quot;, \u0026quot;s3:ListMultipartUploadParts\u0026quot;, \u0026quot;s3:AbortMultipartUpload\u0026quot;, \u0026quot;s3:CreateBucket\u0026quot;, \u0026quot;s3:PutObject\u0026quot; ], \u0026quot;Resource\u0026quot;: [ \u0026quot;arn:aws:s3:::aws-athena-query-results-us-east-1-(account ID)/*\u0026quot;, \u0026quot;arn:aws:s3:::aws-athena-query-results-us-east-1-(account ID)\u0026quot; ] }, { \u0026quot;Sid\u0026quot;: \u0026quot;VisualEditor4\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;s3:GetObject\u0026quot;, \u0026quot;s3:ListBucket\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:s3:::(source bucket)/*\u0026quot; }, { \u0026quot;Sid\u0026quot;: \u0026quot;VisualEditor5\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;logs:CreateLogGroup\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:logs:us-east-1:(account ID):*\u0026quot; } ] } "},{"uri":"https://wellarchitectedlabs.com/categories/","title":"Categories","tags":[],"description":"","content":""}]